{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LUMI-BE training and event materials","text":"<p>Welcome to the future site of training materials of LUMI-BE,  the Belgian participation to LUMI.</p> <ul> <li>Latest 1-day intro training materials</li> </ul>"},{"location":"#training-archive","title":"Training archive","text":"<p>Past trainings, in reverse chronological order.</p> <ul> <li>LUMI 1-day introductory training October 2023</li> </ul>"},{"location":"intro-202310xx/","title":"LUMI 1-day training October 2023","text":""},{"location":"intro-202310xx/#organisation","title":"Organisation","text":"<ul> <li>Schedule</li> </ul>"},{"location":"intro-202310xx/#setting-up-for-the-exercises","title":"Setting up for the exercises","text":"<p>THIS TEXT ALREADY ASSUMES A TRAINING PROJECT WHICH DOES NOT YET EXIST.</p> <ul> <li> <p>Create a directory in the scratch of the training project, or if you want to     keep the exercises around for a while after the session and have already     another project on LUMI, in a subdirectory or your project directory      or in your home directory (though we don't recommend the latter).     Then go into that directory.</p> <p>E.g., in the scratch directory of the project:</p> <pre><code>mkdir -p /scratch/project_465000523/$USER/exercises\ncd /scratch/project_465000523/$USER/exercises\n</code></pre> </li> <li> <p>Now download the exercises and un-tar:</p> <pre><code>wget https://465000095.lumidata.eu/1day-20230509/files/exercises-202310xx.tar.gz\ntar -xf exercises-20230509.tar.gz\n</code></pre> <p>Link to the tar-file with the exercises</p> </li> <li> <p>You're all set to go!</p> </li> </ul>"},{"location":"intro-202310xx/#downloads","title":"Downloads","text":"Presentation Slides Notes recording Introduction / / / LUMI Architecture slides notes / HPE Cray Programming Environment slides notes / LUMI Organisation slides notes / Modules on LUMI slides notes / LUMI Software Stacks slides notes / Exercises 1 / notes / Running Jobs on LUMI slides / / Exercises 2 / notes / Introduction to Lustre and Best Practices slides / / LUMI User Support slides / /"},{"location":"intro-202310xx/#web-links","title":"Web links","text":"<ul> <li> <p>LUMI documentation</p> <ul> <li> <p>Main documentation</p> </li> <li> <p>Shortcut to the LUMI Software Library</p> </li> </ul> </li> <li> <p>Clusters in Belgium</p> <ul> <li> <p>VSC documentation</p> </li> <li> <p>C\u00c9CI documentation</p> </li> <li> <p>Lucia@Cenaero documentation</p> </li> </ul> </li> </ul>"},{"location":"intro-202310xx/01_Architecture/","title":"The LUMI Architecture","text":"<p>In this presentation, we will build up LUMI part by part, stressing those aspects that are important to know to run on LUMI efficiently and define jobs that can scale.</p>"},{"location":"intro-202310xx/01_Architecture/#why-do-i-kneed-to-know-this","title":"Why do I kneed to know this?","text":"<p>You may wonder why you need to know about system architecture if all you want to do is to run  some programs.</p> <p>A supercomputer is not simply a scaled-ups smartphone or PC that will offer good performance automatically. But it is a very expensive infrastructure, with an investment of 160M EURO for LUMI and an estimated total operation cost of 150M EURO. So it is important to use the computer efficiently.</p> <p>And that efficiency comes not for free. Instead in most cases it is important to properly map an  application on the available resources to run efficiently.  The way an application is developed is important for this, but it is not the only factor. Every application needs some user help  to run in the most efficient way, and that requires an understanding of</p> <ol> <li> <p>The hardware architecture of the supercomputer, which is something that we discuss in this     section.</p> </li> <li> <p>The middleware: the layers of software that sit between the application on one hand and the     hardware and operating system on the other hand. This is a topic of discussion in several sessions     of this course.</p> </li> <li> <p>The application. This is very domain-specific and application-specific and hence cannot be the     topic of a general course like this one. In fact, there are so many different applications and     often considerable domain knowledge is required so that a small support team like the central LUMI     User Support Team or the VSC Tier-0 Support Team cannot provide that information.      It is up to scientific communities to organise such trainings,     and then up to users to combine the knowledge of an application obtained from such a course with the     knowledge about the computer you want to use and its middleware obtained from courses such as this one     or our 4-day more advanced course provided by the LUMI User Support Team.</p> </li> </ol>"},{"location":"intro-202310xx/01_Architecture/#lumi-is","title":"LUMI is ...","text":"<p>LUMI is a pre-exascale supercomputer, and not a superfast PC nor a compute cloud architecture.</p> <p>Each of these architectures have their own strengths and weaknesses and offer different  compromises and it is key to chose the right infrastructure for the job and use the right  tools for each infrastructure.</p> <p>Just some examples of using the wrong tools or infrastructure:</p> <ul> <li> <p>The single thread performance of the CPU is lower than on a high-end PC.      We've had users who were disappointed about the speed of a single core and were expecting     that this would be much faster than their PCs. Supercomputers however are optimised for      performance per Watt and get their performance from using lots of cores through well-designed     software. If you want the fastest core possible, you'll need a gaming PC.</p> <p>E.g., the AMD 5800X is a popular CPU for high end gaming PCs using the same core architecture  as the CPUs in the LUMI. It runs at a base clock of 3.8 GHz and a boost clock of 4.7 GHz if only one core is used and the system has proper cooling. The 7763 used in the compute nodes of LUMI-C runs at a base clock of 2.45 GHz and a boost clock of 3.5 GHz. If you have only one single core job to run on your PC, you'll be able to reach that boost clock while on LUMI you'd probably need to have a large part of the node for yourself, and even then the performance for jobs that are not memory bandwidth limited will be lower than that of the gaming PC.</p> </li> <li> <p>For some data formats the GPU performance may be lower also than on a high end gaming PC.     This is even more so because     an MI250x should be treated as two GPUs for most practical purposes. The better double precision     floating point operations and matrix operations, also at full precision, require transistors also      that on some other GPUs are used for rendering hardware or for single precision compute units.</p> <p>E.g., a single GPU die of the MI250X (half a GPU) has a peak FP32 performance at the boost clock of almost 24 TFlops or 48 TFlops in the packed format which is actually hard for a compiler to exploit, while the high-end AMD graphics GPU RX 7900 XTX claims 61 TFlops at the boost clock. But the FP64 performance of one MI250X  die is also close to 24 TFlops in vector math, while the RX 7900 XTX does less than 2 TFlops in that data format which is important for a lot of scientific computing applications.</p> </li> <li> <p>Compute GPUs and rendering GPUs are different beasts these days.     We had a user who wanted to use the ray tracing units to do rendering. The MI250X does not     have texture units or ray tracing units though. It is not a real graphics processor anymore.</p> </li> <li> <p>The environment is different also. It is not that because it runs some Linux it handles are your     Linux software.     A user complained that they did not succeed in getting their nice remote development environment to     work on LUMI. The original author of these notes took a test license and downloaded a trial version.     It was a very nice environment but really made for local development and remote development in a      cloud environment with virtual machines individually protected by personal firewalls and was      not only hard to get working on a supercomputer but also insecure.</p> </li> <li> <p>And supercomputer need proper software that exploits the strengths and works around the weaknesses     of their architecture.     CERN came telling on a EuroHPC Summit Week before the COVID pandemic that they would start using more     HPC and less cloud and that they expected a 40% cost reduction that way. A few years later they     published a paper with their experiences and it was mostly disappointment. The HPC infrastructure     didn't fit their model for software distribution and performance was poor. Basically their solution     was designed around the strengths of a typical cloud infrastructure and relied precisely on those things     that did make their cloud infrastructure more expensive than the HPC infrastructure they tested. It relied     on fast local disks that require a proper management layer in the software, (ab)using the file system as     a database for unstructured data, a software distribution mechanism that requires an additional daemon     running permanently on the compute nodes (and local storage on those nodes), ...</p> </li> </ul> <p>True supercomputers, and LUMI in particular, are built for scalable parallel applications and features that are found on smaller clusters or on workstations that pose a threat to scalability are removed from the system. It is also a shred infrastructure but with a much more lightweight management layer than a cloud infrastructure and far less isolation between users, meaning that abuse by one user can have more of a negative impact on  other users than in a cloud infrastructure. Supercomputers since the mid to late '80s are also build according to the principle of trying to reduce the hardware cost by using cleverly designed software both at the system and application level. They perform best when streaming data through the machine at all levels of the  memory hierarchy and are not built at all for random access to small bits of data (where the definition of \"small\" depends on the level in the memory hierarchy).</p> <p>At several points in this course you will see how this impacts what you can do with a supercomputer and how you work with a supercomputer.</p>"},{"location":"intro-202310xx/01_Architecture/#lumi-spec-sheet-a-modular-system","title":"LUMI spec sheet: A modular system","text":"<p>So we've already seen that LUMI is in the first place a EuroHPC pre-exascale machine. LUMI is built to prepare for the exascale era and to fit in the EuroHPC ecosystem.  But it does not even mean that it has to cater to all pre-exascale compute needs. The EuroHPC JU tries to build systems that have some flexibility, but also does not try to cover  all needs with a single machine. They are building 3 pre-exascale systems with different architecture to explore multiple architectures and to cater to a more diverse audience.</p> <p>LUMI is also a very modular machine designed according to the principles explored in a series of European projects, and in particular DEEP and its successors) that explored the cluster-booster concept. E.g., in a complicated multiphysics simulation  you could be using regular CPU nodes for the physics that cannot be GPU-accelerated communicating with compute GPU nodes for the physics that can be GPU-accelerated, then add a number of CPU nodes to do the I/O and a specialised render GPU node for in-situ visualisation.</p> <p>LUMI is in the first place a huge GPGPU supercomputer. The GPU partition of LUMI, called LUMI-G, contains 2560 nodes with a single 64-core AMD EPYC 7A53 CPU and 4 AMD MI250x GPUs. Each node has 512 GB of RAM attached to the CPU (the maximum the CPU can handle without compromising bandwidth) and 128 GB of HBM2e memory per GPU. Each GPU node has a theoretical peak performance of nearly 200 TFlops in single (FP32) or double (FP64) precision vector arithmetic (and twice that with the packed FP32 format, but that  is not well supported so this number is not often quoted). The matrix units are capable of about 400 TFlops in FP32 or FP64. However, compared to the NVIDIA GPUs, the performance for lower precision formats used in some AI applications is not that stellar.</p> <p>LUMI also has a large CPU-only partition, called LUMI-C, for jobs that do not run well on GPUs, but also integrated enough with the GPU partition that it is possible to have applications that combine both node types. LUMI-C consists of 1536 nodes with 2 64-core AMD EPYC 7763 CPUs. 32 of those nodes have 1TB of RAM (with some of these nodes actually reserved for special purposes such as connecting to a Quantum computer), 128 have 512 GB and 1376 have 256 GB of RAM.</p> <p>LUMI also has two smaller groups of nodes for interactive data analytics.  8 of those nodes have two  64-core Zen2/Rome CPUs with 4 TB of RAM per node, while 8 others have dual 64-core Zen2/Rome CPUs and 8 NVIDIA A40 GPUs for visualisation. Currently we are working on an Open OnDemand based service to make some fo those facilities available. Note though that these nodes are meant for a very specific use, so it is not that we will also be offering, e.g., GPU compute facilities on NVIDIA hardware, and that these are shared resources that should not be monopolised by a single user (so no hope to run an MPI job on 8 4TB nodes).</p> <p>LUMI also has a 7 PB flash based file system running the Lustre parallel file system. This system is often denoted as LUMI-F. The bandwidth of that system is 1740 GB/s.  Note however that this is still a remote file system with a parallel file system on it, so do not expect that it will behave as the local SSD in your laptop.  But that is  also the topic of another session in this course.</p> <p>The main work storage is provided by 4 20 PB hard disk based Lustre file systems with a bandwidth of 240 GB/s each. That section of the machine is often denoted  as LUMI-P. </p> <p>Big parallel file systems need to be used in the proper way to be able to offer the performance that one would expect from their specifications. This is important enough that  we have a separate session about that in this course.</p> <p>An object based file system similar to the Allas service of CSC that some of the Finnish users may be familiar with is also being worked on. At the  moment the interface to that system is still rather primitive.</p> <p>Currently LUMI has 4 login nodes, called user access nodes in the HPE Cray world. They each have 2 64-core AMD EPYC 7742 processors and 1 TB of RAM. Note that  whereas the GPU and CPU compute nodes have the Zen3 architecture code-named \"Milan\", the processors on the login nodes are Zen2 processors, code-named \"Rome\". Zen3 adds some new instructions so if a compiler generates them, that code would not run on the login nodes. These instructions are basically used in cryptography though. However, many instructions have very different latency, so a compiler that optimises specifically for Zen3 may chose another ordering of instructions then when optimising for Zen2 so it may still make sense to compile specifically for the compute nodes on LuMI.</p> <p>All compute nodes, login nodes and storage are linked together through a  high-performance interconnect. LUMI uses the Slingshot 11 interconnect which is developed by HPE Cray, so not the Mellanox/NVIDIA InfiniBand that you may be familiar with from many smaller clusters, and as we shall discuss later this also influences how you work on LUMI.</p> <p>Early on a small partition for containerised micro-services managed with Kubernetes was also planned, but that may never materialize due to lack of  people to set it up and manage it.</p> <p>In this section of the course we will now build up LUMI step by step.</p>"},{"location":"intro-202310xx/01_Architecture/#building-lumi-the-cpu-amd-7xx3-milanzen3-cpu","title":"Building LUMI: The CPU AMD 7xx3 (Milan/Zen3) CPU","text":"<p>The LUMI-C and LUMI-G compute nodes use third generation AMD EPYC CPUs. Whereas Intel CPUs launched in the same period were built out of a single large monolithic piece of silicon (that only changed recently with some variants of the Sapphire Rapids CPU launched in early 2023), AMD CPUs are build out of multiple so-called chiplets. </p> <p>The basic building block of Zen3 CPUs is the Core Complex Die (CCD). Each CCD contains 8 cores, and each core has 32 kB of L1 instruction  and 32 kB of L1 data cache, and 512 kB of L2 cache. The L3 cache is shared across all cores on a chiplet and has a total size of 32 MB on LUMI (there are some variants of the processor where this is 96MB). At the user level, the instruction set is basically equivalent to that of the Intel Broadwell generation. AVX2 vector instructions and the FMA instruction are fully supported, but there is no support for any of the AVX-512 versions that can be found on Intel Skylake server processors and later generations. Hence the number of floating point operations that a core can in theory do each clock cycle is 16 (in  double precision) rather than the 32 some Intel processors are capable of. </p> <p></p> <p>The full processor package for the AMD EPYC processors used in LUMI have 8 such Core Complex Dies for a total of 64 cores. The caches are not shared between different CCDs, so it also implies that the processor has 8 so-called L3 cache regions. (Some cheaper variants have only 4 CCDs, and some have CCDs with only 6 or fewer cores enabled but the same 32 MB of L3 cache per CCD).</p> <p>Each CCD connects to the memory/IO die through an Infinity Fabric link.  The memory/IO die contains the memory controllers, connections to connect two CPU packages together, PCIe lanes to connect to external hardware, and some additional hardware, e.g., for managing the processor.  The memory/IO die supports 4 dual channel DDR4 memory controllers providing  a total of 8 64-bit wide memory channels. From a logical point of view the memory/IO-die is split in 4 quadrants, with each quadrant having a dual channel memory controller and 2 CCDs. They basically act as 4 NUMA domains. For a core it is slightly faster to access memory in its own quadrant than memory attached to another quadrant, though for the 4 quadrants within the same socket the difference is small. (In fact, the BIOS can be set to show only two or one NUMA domain which is advantageous in some cases, like the typical load pattern of login nodes where it is impossible to nicely spread processes and their memory across the 4 NUMA domains).</p> <p>The theoretical memory bandwidth of a complete package is around 200 GB/s. However, that bandwidth is not available to a single core but can only be used if enough  cores spread over all CCDs are used.</p> <p>Clusters in Belgium</p> <p>The CPUs used in the LUMI-C compute nodes are identical to those used in  the Cenaero/C\u00c9CI cluster lucia or the Milan partition of the VSC cluster  hortense. The UGent VSC cluster gallade uses a very similar processor also but with more cache per CCD. Some other cluster, e.g., accelgor and doduo+  at UGent or the Milan partition of vaughan in at UAntwerpen, also use CPUs of this generation but with only 4 CCDs per processor and/or 6 active cores  per CCD. But many topics that we cover in this course also applies to those clusters.</p> <p>Some other clusters, e.g., the older Rome partition of the VSC cluster hortense, the C\u00c9CI cluster NIC5 at ULi\u00e8ge or the older main partition of the VSC cluster vaughan at UAntwerpen, use the older Zen2/Rome CPUs which are also used on the login nodes of LUMI. These have two groups of 4 cores each with their own separated L3 cache per CCD and 4 or 8 CCDs per  socket. </p>"},{"location":"intro-202310xx/01_Architecture/#building-lumi-a-lumi-c-node","title":"Building LUMI: a LUMI-C node","text":"<p>A compute node is then built out of two such processor packages, connected  through 4 16-bit wide Infinity Fabric connections with a total theoretical bandwidth of 144 GB/s in each direction. So note that the bandwidth in each direction is less than the memory bandwidth of a socket. Again, it is not really possible to use the full memory bandwidth of a node using just cores on a single socket. Only one of the two sockets has a direct connection to the high performance Slingshot interconnect though.</p>"},{"location":"intro-202310xx/01_Architecture/#a-strong-hierarchy-in-the-node","title":"A strong hierarchy in the node","text":"<p>As can be seen from the node architecture in the previous slide, the CPU compute nodes have a very hierarchical architecture. When mapping an application onto  one or more compute nodes, it is key for performance to take that hierarchy into account. This is also the reason why we will pay so much attention to thread and process pinning in this tutorial course.</p> <p>At the coarsest level, each core supports two hardware threads (what Intel calls hyperthreads). Those hardware threads share all the resources of a core, including the  L1 data and instruction caches and the L2 cache, execution units and space for register renaming.  At the next level, a Core Complex Die contains (up to) 8 cores. These cores share the L3 cache and the link to the memory/IO die.  Next, as configured on the LUMI compute nodes, there are 2 Core Complex Dies in a NUMA node. These two CCDs share the DRAM channels of that NUMA node. At the fourth level in our hierarchy 4 NUMA nodes are grouped in a socket. Those 4  nodes share an inter-socket link. At the fifth and last level in our shared memory hierarchy there are two sockets in a node. On LUMI, they share a single Slingshot inter-node link.</p> <p>The finer the level (the lower the number), the shorter the distance and hence the data delay is between threads that need to communicate with each other through the memory hierarchy, and the higher the bandwidth.</p> <p>This table tells us a lot about how one should map jobs, processes and threads onto a node. E.g., if a process has fewer then 8 processing threads running concurrently, these should be mapped to cores on a single CCD so that they can share  the L3 cache, unless they are sufficiently independent of one another, but even in the latter case the additional cores on those CCDs should not be used by other processes as they may push your data out of the cache or saturate the link to the memory/IO die and hence slow down some threads of your process. Similarly, on a 256 GB compute node each NUMA node has 32 GB of RAM (or actually a bit less as the OS also needs memory, etc.), so if you have a job that uses 50 GB of memory but only, say, 12 threads, you should really have two NuMA nodes reserved for that job as otherwise other threads or processes running on cores in those NUMA nodes could saturate some resources needed by your job. It might also be preferential to spread those 12 threads over the 4  CCDs in those 2 NUMA domains unless communication through the L3 threads would be the bottleneck in your application.</p>"},{"location":"intro-202310xx/01_Architecture/#hierarchy-delays-in-numbers","title":"Hierarchy: delays in numbers","text":"<p>This slide shows the ACPI System Locality distance Information Table (SLIT) as returned by, e.g., <code>numactl -H</code> which gives relative distances to memory from a core. E.g., a value of 32 means that access takes 3.2x times the  time it would take to access memory attached to the same NUMA node.  We can see from this table that the penalty for accessing memory in  another NUMA domain in the same socket is still relatively minor (20%  extra time), but accessing memory attached to the other socket is a lot  more expensive. If a process running on one socket would only access memory attached to the other socket, it would run a lot slower which is why Linux has mechanisms to try to avoid that, but this cannot be done in all scenarios which is why on some clusters you will be allocated cores in proportion to the amount of memory you require, even if that is more cores than you really need (and you will be billed for them).</p>"},{"location":"intro-202310xx/01_Architecture/#building-lumi-concept-lumi-g-node","title":"Building LUMI: Concept LUMI-G node","text":"<p>This slide shows a conceptual view of a LUMI-G compute node. This node is unlike any Intel-architecture-CPU-with-NVIDIA-GPU compute node you may have  seen before, and rather mimics the architecture of the USA pre-exascale machines Summit and Sierra which have IBM POWER9 CPUs paired with  NVIDIA V100 GPUs.</p> <p>Each GPU node consists of one 64-core AMD EPYC CPU and 4 AMD MI250x GPUs.  So far nothing special. However, two elements make this compute node very special. First, the GPUs are not connected to the CPU though a PCIe bus. Instead they are connected through the same links that AMD uses to link the GPUs together, or to link the two sockets in the LUMI-C compute nodes, known as xGMI or Infinity Fabric. This enables unified memory across CPU and GPUS and  provides partial cache coherency across the system. The CPUs coherently cache the DPU DDR and GPU HBM memory, but each GPU only coherently caches  its own local memory. The second remarkable element is that the Slingshot interface cards connect directly to the GPUs (through a PCIe interface on the GPU) rather than two the CPU. The CPUs have a shorter path to the communication  network than the CPU in this design. </p> <p>This makes the LUMI-G compute node really a \"GPU first\" system. The architecture looks more like a GPU system with a CPU as the accelerator for tasks that a GPU is not good at such as some scalar processing or running an OS, rather than a CPU node with GPU accelerator.</p> <p>It is also a good fit with the cluster-booster design explored in the DEEP project series. In that design, parts of your application that cannot be properly accelerated would run on CPU nodes, while booster GPU nodes would be used for those parts  that can (at least if those two could execute concurrently with each other). Different node types are mixed and matched as needed for each specific application,  rather than building clusters with massive and expensive nodes that few applications can fully exploit. As the cost per transistor does not decrease anymore, one has to look for ways to use each transistor as efficiently as possible...</p> <p>It is also important to realise that even though we call the partition \"LUMI-G\", the MI250x is not a GPU in the true sense of the word. It is not a rendering GPU, which for AMD is  currently the RDNA architecture with version 3 just out, but a compute accelerator with an architecture that evolved from a GPU architecture, in this case the VEGA architecture from AMD. The architecture of the MI200 series is also known as CDNA2, with the MI100 series being just CDNA, the first version. Much of the hardware that does not serve compute purposes has been removed from the design to have more transistors available for compute.  Rendering is possible, but it will be software-based  rendering with some GPU acceleration for certain parts of the pipeline, but not full hardware rendering. </p> <p>This is not an evolution at AMD only. The same is happening with NVIDIA GPUs and there is a reason why the latest generation is called \"Hopper\" for compute and \"Ada Lovelace\" for rendering GPUs.  Several of the functional blocks in the Ada Lovelace architecture are missing in the Hopper  architecture to make room for more compute power and double precision compute units. E.g., Hopper does not contain the ray tracing units of Ada Lovelace. The Intel Data Center GPU Max code named \"Ponte Vecchio\" is the only current GPU for  HPC that still offers full hardware rendering support (and even ray tracing).</p> <p>Graphics on one hand and HPC and AI on the other hand are becoming separate workloads for which manufacturers make different, specialised cards, and if you have applications that need both, you'll have to rework them to work in two phases, or to use two types of nodes and communicate between them over the interconnect, and look for supercomputers that support both workloads.</p> <p>But so far for the sales presentation, let's get back to reality...</p>"},{"location":"intro-202310xx/01_Architecture/#building-lumi-what-a-lumi-g-node-really-looks-like","title":"Building LUMI: What a LUMI-G node really looks like","text":"<p>Or the full picture with the bandwidths added to it:</p> <p>The LUMI-G node uses the 64-core AMD 7A53 EPYC processor, known under the code name \"Trento\". This is basically a Zen3 processor but with a customised memory/IO die, designed specifically  for HPE Cray (and in fact Cray itself, before the merger) for the USA Coral-project to build the Frontier supercomputer, the fastest system in the world at the end of 2022 according to at least the Top500 list. Just as the CPUs in the LUMI-C nodes, it is a design with 8 CCDs and a memory/IO die.</p> <p>The MI250x GPU is also not a single massive die, but contains two compute dies besides the 8 stacks of HBM2e memory, 4 stacks or 64 GB per compute die. The two compute dies in a package are linked together  through 4 16-bit Infinity Fabric links. These links run at a higher speed than the links between two CPU sockets in a LUMI-C node, but per link the bandwidth is still only 50 GB/s per direction, creating a total bandwidth of 200 GB/s per direction between the two compute dies in an MI250x GPU. That amount of bandwidth is very low compared to even the memory bandwidth, which is roughly 1.6 TB/s peak per die, let alone compared to whatever bandwidth caches on the compute dies would have or the bandwidth of the internal structures that  connect all compute engines on the compute die. Hence the two dies in a single package cannot function efficiently as as single GPU which is one reason why each MI250x GPU on LUMI is actually seen as two GPUs. </p> <p>Each compute die uses a further 2 or 3 of those Infinity Fabric (or xGNI) links to connect to some compute dies in other MI250x packages. In total, each MI250x package is connected through 5 such links to other MI250x packages. These links run at the same 25 GT/s speed as the  links between two compute dies in a package, but even then the bandwidth is only a meager  250 GB/s per direction, less than an NVIDIA A100 GPU which offers 300 GB/s per direction or the NVIDIA H100 GPU which offers 450 GB/s per direction. Each Infinity Fabric link may be twice as fast as each NVLINK 3 or 4 link (NVIDIA Ampere and Hopper respectively), offering 50 GB/s per direction rather than 25 GB/s per direction for NVLINK,  but each Ampere GPU has 12 such links and each Hopper GPU 18 (and in fact a further 18 similar ones to link to a Grace CPU), while each MI250x package has only 5 such links available to link to other GPUs (and the three that we still need to discuss).</p> <p>Note also that even though the connection between MI250x packages is all-to-all, the connection between GPU dies is all but all-to-all. as each GPU die connects to only 3 other GPU dies. There are basically two bidirectional rings that don't need to share links in the topology, and then some extra connections. The rings are:</p> <ul> <li>Green ring: 1 - 0 - 6 - 7 - 5 - 4 - 2 - 3 - 1</li> <li>Red ring: 1 - 0 - 2 - 3 - 7 - 6 - 4 - 5 - 1</li> </ul> <p>These rings play a role in the inter-GPU communication in AI applications using RCCL.</p> <p>Each compute die is also connected to one CPU Core Complex Die (or as documentation of the node sometimes says, L3 cache region). This connection only runs at the same speed as the links between CPUs on the LUMI-C CPU nodes, i.e., 36 GB/s per direction (which is still enough for  all 8 GPU compute dies together to saturate the memory bandwidth of the CPU).  This implies that each of the 8 GPU dies has a preferred CPU die to work with, and this should definitely be taken into account when mapping processes and threads on a LUMI-G node. </p> <p>The figure also shows another problem with the LUMI-G node: The mapping between CPU cores/dies and GPU dies is all but logical:</p> GPU die CCD hardware threads NUMA node 0 6 48-55, 112-119 3 1 7 56-63, 120-127 3 2 2 16-23, 80-87 1 3 3 24-31, 88-95 1 4 0 0-7, 64-71 0 5 1 8-15, 72-79 0 6 4 32-39, 96-103 2 7 5 40-47, 104, 11 2 <p>and as we shall see later in the course, exploiting this is a bit tricky at the moment.</p>"},{"location":"intro-202310xx/01_Architecture/#what-the-future-looks-like","title":"What the future looks like...","text":"<p>Some users may be annoyed by the \"small\" amount of memory on each node. Others may be annoyed by the limited CPU capacity on a node compared to some systems  with NVIDIA GPUs. It is however very much in line with the cluster-booster philosophy already mentioned a few times, and it does seem to be the future according to AMD (with Intel also working into that direction).  In fact, it looks like with respect to memory  capacity things may even get worse.</p> <p>We saw the first little steps of bringing GPU and CPU closer together and  integrating both memory spaces in the USA pre-exascale systems Summit and Sierra. The LUMI-G node which was really designed for one of the first USA exascale systems continues on this philosophy, albeit with a CPU and GPU from a different manufacturer. Given that manufacturing large dies becomes prohibitively expensive in newer semiconductor processes and that the transistor density on a die is also not increasing at the same rate anymore with process shrinks, manufacturers are starting to look at other ways of increasing the number of transistors per \"chip\" or should we say package. So multi-die designs are here to stay, and as is already the case in the AMD CPUs, different dies may be manufactured with different processes for economical reasons.</p> <p>Moreover, a closer integration of CPU and GPU would not only make programming easier as memory management becomes easier, it would also enable some codes to run on GPU  accelerators that are currently bottlenecked by memory transfers between GPU and CPU.</p> <p>AMD at its 2022 Investor day and at CES 2023 in early January, and Intel at an Investor day in 2022 gave a glimpse of how they see the future. The future is one where one or more CPU dies, GPU dies and memory controllers are combined in a single package and - contrary to the Grace Hopper design of NVIDIA - where CPU and GPU share  memory controllers. At CES 2023, AMD already showed a MI300A package that will be used in El Capitan, one of the next USA exascale systems (the third one if Aurora gets built in time). It employs 13 chiplets in two layers, linked to (still only) 8  memory stacks (albeit of a slightly faster type than on the MI250x).  The 4 chiplets on the bottom layer are the memory controllers and inter-GPU links (an they can be at the bottom as they produce less heat). Furthermore each package features 6 GPU dies and 3 Zen4 \"Genoa\" CPU dies. The MI300A still uses only 8 HBM stacks and is also limited to 16 GB stacks, providing a total of 128 GB of RAM.</p> <p>Intel at some point has shown only very conceptual drawings of its Falcon Shores chip  which it calls an XPU, but those drawings suggest that that chip will also support some low-bandwidth but higher capacity external memory, similar to the approach taken in some Sapphire  Rapids Xeon processors that combine HBM memory on-package with DDR5 memory outside  the package. Falcon Shores will be the next generation of Intel GPUs for HPC, after  Ponte Vecchio which will be used in the Aurora supercomputer. It is currently very likely though that Intel will revert to a traditional design for Falcon Shores and push out the integrated CPU+GPU model to a later generation.</p> <p>However, a CPU closely integrated with accelerators is nothing new as Apple Silicon is  rumoured to do exactly that in its latest generations, including the M-family chips.</p>"},{"location":"intro-202310xx/01_Architecture/#building-lumi-the-slingshot-interconnect","title":"Building LUMI: The Slingshot interconnect","text":"<p>All nodes of LUMI, including the login, management and storage nodes, are linked together using the Slingshot interconnect (and almost all use Slingshot 11, the full implementation with 200 Gb/s bandwidth per direction).</p> <p>Slingshot is an interconnect developed by HPE Cray and based on Ethernet, but with proprietary extensions for better HPC performance. It adapts to the regular Ethernet protocols when talking to a node that only supports Ethernet, so one of the attractive features is that regular servers with Ethernet can be directly connected to the  Slingshot network switches. HPE Cray has a tradition of developing their own interconnect for very large systems. As in previous generations, a lot of attention went to adaptive routing and congestion control. There are basically two versions of it. The early version was named Slingshot 10, ran at 100 Gb/s per direction and did not yet have all features. It was used on the initial deployment of LUMI-C compute nodes but has since been upgraded to the full version. The full version with all features is called Slingshot 11. It supports a bandwidth of 200 Gb/s per direction, comparable to HDR InfiniBand with 4x links. </p> <p>Slingshot is a different interconnect from your typical Mellanox/NVIDIA InfiniBand implementation and hence also has a different software stack. This implies that there are no UCX libraries on the system as the Slingshot 11 adapters do not support that. Instead, the software stack is  based on libfabric (as is the stack for many other Ethernet-derived solutions and even Omni-Path has switched to libfabric under its new owner).</p> <p>LUMI uses the dragonfly topology. This topology is designed to scale to a very large number of  connections while still minimizing the amount of long cables that have to be used. However, with its complicated set of connections it does rely heavily on adaptive routing and congestion control for optimal performance more than the fat tree topology used in many smaller clusters. It also needs so-called high-radix switches. The Slingshot switch, code-named Rosetta, has 64 ports. 16 of those ports connect directly to compute nodes (and the next slide will show you how). Switches are then combined in groups. Within a group there is an all-to-all connection between  switches: Each switch is connected to each other switch. So traffic between two nodes of a  group passes only via two switches if it takes the shortest route. However, as there is typically only one 200 Gb/s direct connection between two switches in a group, if all 16 nodes on two  switches in a group would be communicating heavily with each other, it is clear that some traffic will have to take a different route. In fact, it may be statistically better if the 32 involved nodes would be spread  more evenly over the group, so topology based scheduling of jobs and getting the processes of a job on as few switches as possible may not be that important on a dragonfly Slingshot network.  The groups in a slingshot network are then also connected in an all-to-all fashion, but the number of direct links between two groups is again limited so traffic again may not always want to take  the shortest path. The shortest path between two nodes in a dragonfly topology never involves  more than 3 hops between switches (so 4 switches): One from the switch the node is connected to  the switch in its group that connects to the other group, a second hop to the other group, and then a third hop in the destination group to the switch the destination node is attached to.</p>"},{"location":"intro-202310xx/01_Architecture/#assembling-lumi","title":"Assembling LUMI","text":"<p>Let's now have a look at how everything connects together to the supercomputer LUMI. It does show that LUMI is not your standard cluster build out of standard servers.</p> <p>LUMI is built very compactly to minimise physical distance between nodes and to reduce the cabling mess typical for many clusters. LUMI does use a custom rack design for the compute nodes that is also fully water cooled. It is build out of units that can contain up to 4 custom cabinets, and a cooling distribution unit (CDU). The size of the complex as depicted in the slide is approximately 12 m2. Each cabinet contains 8 compute chassis in 2 columns of 4 rows. In between the two columns is all the power circuitry. Each compute chassis can contain 8 compute blades that are mounted vertically. Each compute blade can contain multiple nodes, depending on the type of compute blades. HPE Cray have multiple types of compute nodes, also with  different types of GPUs. In fact, the Aurora supercomputer which uses Intel CPUs and GPUs and El Capitan, which uses the MI300A APUs (integrated CPU and GPU) will use the same design with a different compute blade. Each LUMI-C compute blade contains 4 compute nodes and two network interface cards, with each network interface card implementing two Slingshot interfaces and connecting to two nodes. A LUMI-G compute blade contains two nodes and 4 network interface cards, where each interface card now connects to two GPUs in the same  node. All connections for power, management network and high performance interconnect of the compute node are at the back of the compute blade. At the front of the compute blades one can find the connections to the cooling manifolds that distribute cooling water to the blades. One compute blade of LUMI-G can consume up to 5kW, so the power density of this setup is incredible, with 40 kW for a single compute chassis.</p> <p>The back of each cabinet is equally genius. At the back each cabinet has 8 switch chassis, each matching the position of a compute chassis. The switch chassis contains the connection to the power delivery system and a switch for the management network and has 8 positions for  switch blades. These are mounted horizontally and connect directly to the compute blades. Each slingshot switch has 8x2 ports on the inner side for that purpose, two for each compute blade. Hence for LUMI-C two switch blades are needed in each switch chassis as each blade has 4 network interfaces, and for LUMI-G 4 switch blades are needed for each compute chassis as those nodes have 8 network interfaces. Note that this also implies that the nodes on the same  compute blade of LUMI-C will be on two different switches even though in the node numbering they are numbered consecutively. For LUMI-G both nodes on a blade will be on a different pair of switches  and each node is connected to two switches. So when you get a few sequentially numbered nodes, they will not be on a single switch (LUMI-C) or switch pair (LUMI-G). The switch blades are also water cooled (each one can  consume up to 250W). No currently possible configuration of the Cray EX system needs that  all switch positions in the switch chassis.</p> <p>This does not mean that the extra positions cannot be useful in the future. If not for an interconnect, one could, e.g., export PCIe ports to the back and attach, e.g., PCIe-based storage via blades as the  switch blade environment is certainly less hostile to such storage than the very dense and very hot compute blades.</p>"},{"location":"intro-202310xx/01_Architecture/#lumi-assembled","title":"LUMI assembled","text":"<p>This slide shows LUMI fully assembled (as least as it was at the end of 2022).</p> <p>At the front there are 5 rows of cabinets similar to the ones in the exploded Cray EX picture  on the previous slide. Each row has 2 CDUs and 6 cabinets with compute nodes.  The first row, the one with the wolf, contains all nodes of LUMI-C, while the other four  rows, with the letters of LUMI, contain the GPU accelerator nodes. At the back of the room there are more  regular server racks that house the storage, management nodes, some special compute nodes , etc. The total size is roughly the size of a tennis court. </p> <p>Remark</p> <p>The water temperature that a system like the Cray EX can handle is so high that in fact the water can be cooled again with so-called \"free cooling\", by just radiating the heat to the environment rather  than using systems with compressors similar to air conditioning systems, especially in regions with a colder climate. The LUMI supercomputer is housed in Kajaani in Finland, with moderate temperature almost  year round, and the heat produced by the supercomputer is fed into the central heating system of the city, making it one of the greenest supercomputers in the world as it is also fed with renewable energy.</p>"},{"location":"intro-202310xx/02_CPE/","title":"The HPE Cray Programming Environment","text":"<p>In this session we discuss some of the basics of the operating system and programming environment on LUMI. Whether you like it or not, every user of a supercomputer like LUMI gets confronted with these elements at some point.</p>"},{"location":"intro-202310xx/02_CPE/#why-do-i-need-to-know-this","title":"Why do I need to know this?","text":"<p>The typical reaction of someone who only wants to run software on an HPC system when confronted with a talk about development tools is \"I only want to run some programs, why do I need to know about programming environments?\"</p> <p>The answer is that development environments are an intrinsic part of an HPC system.  No HPC system is as polished as a personal computer and the software users want to use is typically very unpolished. </p> <p>Programs on an HPC cluster are preferably installed from sources to generate binaries optimised for the system. CPUs have gotten new instructions over time that can sometimes speed-up execution of a program a lot, and compiler optimisations that take specific strengths and weaknesses of particular CPUs into account can also gain some performance. Even just a 10% performance gain on an investment of 160 million EURO such as LUMI means a lot of money. When running, the build environment on most systems needs to be at least partially recreated. This is somewhat less relevant on Cray systems as we will see at the end of this part of the course, but if you want reproducibility it becomes important again.</p> <p>Even when installing software from prebuild binaries some modules might still be needed, e.g., as you may want to inject an optimised MPI library as we shall see in the container section of this course.</p> <p>Clusters in Belgium</p> <p>There are differences in the setup of the operating system on LUMI compared to the VSC, C\u00c9CI and Cenearo clusters in Belgium.</p> <p>Lucia, the Cenaero cluster and Walloon tier-1 system, has a programming environment which is similar to LUMI. But all other Belgian systems have  a programming environment that is more inspired on the GNU and other  Open Source software approach. (I don't want to say \"a more traditional\"  programming environment as in fact the HPE Cray PE goes back in its approach to the environment on UNIX workstations and supercomputers in the '90s, so is in fact the more traditional one. It's just that we have forgotten those  traditions...)</p>"},{"location":"intro-202310xx/02_CPE/#the-operating-system-on-lumi","title":"The operating system on LUMI","text":"<p>The login nodes of LUMI run a regular SUSE Linux Enterprise Server 15 SP4 distribution. The compute nodes however run Cray OS, a restricted version of the SUSE Linux that runs on the login nodes. Some daemons are inactive or configured differently and Cray also  does not support all regular file systems. The goal of this is to minimize OS jitter, interrupts that the OS handles and slow down random cores at random moments, that can  limit scalability of programs. Yet on the GPU nodes there was still the need to reserve one core for the OS and driver processes.</p> <p>This also implies that some software that works perfectly fine on the login nodes may not work on the compute nodes. E.g., there is no <code>/run/user/$UID</code> directory and we have experienced that D-Bus (which stands for Desktop-Bus) also does not work as one should expect.</p> <p>Large HPC clusters also have a small system image, so don't expect all the bells-and-whistles  from a Linux workstation to be present on a large supercomputer. Since LUMI compute nodes are diskless, the system image actually occupies RAM which is another reason to keep it small.</p> Some missing pieces <p>Compute nodes don't run a per-user dbus daemon, so some if not all DBUS functionality is missing. And D-Bus may sometimes show up in places where you don't expect it... It may come from freedesktop.org but is is not only used for desktop software.</p> <p>Compute nodes on a Cray system have Lustre as the main file system. They do not import any networked file system like NFS, GPFS or CernVM-FS (the latter used by, e.g., Cern for  distributing software for the Large Haedron Collider and the EESSI project). Instead these file systems are mounted on external servers in the admin section of the cluster and the Cray Data Virtualisation Service (DVS) is then used to access those file systems from the compute nodes over the high-speed interconnect.</p>"},{"location":"intro-202310xx/02_CPE/#programming-models","title":"Programming models","text":"<p>On LUMI we have several C/C++ and Fortran compilers. These will be discussed more in this session.</p> <p>There is also support for MPI and SHMEM for distributed applications. And we also support RCCL, the ROCm-equivalent of the  CUDA NCCL library that is popular in machine learning packages.</p> <p>All compilers have some level of OpenMP support,  and two compilers support OpenMP offload to  the AMD GPUs, but again more about that later.</p> <p>OpenACC, the other directive-based model for GPU offloading,  is only supported in the Cray Fortran compiler. There is no commitment of neither HPE Cray or AMD to extend that support to C/C++ or other compilers, even though there is work going on in the LLVM community and several compilers on the system are based on LLVM.</p> <p>The other important programming model for AMD GPUs is HIP,  which is their alternative for the proprietary CUDA model. It does not support all CUDA features though (basically it is more CUDA 7 or 8 level)  and there is also no equivalent to CUDA Fortran.</p> <p>The commitment to OpenCL is very unclear, and this actually holds for other GPU vendors also.</p> <p>We also try to provide SYCL as it is a programming language/model that works on all three GPU families currently used in HPC. </p> <p>Python is of course pre-installed on the system but we do ask to use big Python installations in a special way as Python puts a tremendous load on the file system. More about that later in this course.</p> <p>Some users also report some success in running Julia. We don't have full support though and have to depend on binaries as provided by julialang.org. The AMD GPUs are not yet fully supported by Julia.</p> <p>It is important to realise that there is no CUDA on AMD GPUs and there will never be as this is a  proprietary technology that other vendors cannot implement. LUMI will in the future have some nodes with NVIDIA GPUs but these nodes are meant for visualisation and not for compute.</p>"},{"location":"intro-202310xx/02_CPE/#the-development-environment-on-lumi","title":"The development environment on LUMI","text":"<p>Long ago, Cray designed its own processors and hence had to develop their own compilers. They kept doing so, also when they moved to using more standard components, and had a lot of expertise in that field, especially when it comes to the needs of  scientific codes, programming models that are almost only used in scientific computing or stem from such projects. As they develop their own interconnects, it does make sense to also develop an MPI implementation that can use the interconnect in an optimal way. They also have a long tradition in developing performance measurement and analysis tools  and debugging tools that work in the context of HPC.</p> <p>The first important component of the HPE Cray Programming Environment is the compilers. Cray still builds its own compilers for C/C++ and Fortran, called the Cray Compiling Environment (CCE). Furthermore, the GNU compilers are also supported on every Cray system, though at the moment AMD GPU support is not enabled. Depending on the hardware of the  system other compilers will also be provided and integrated in the environment. On LUMI two other compilers are available: the AMD AOCC compiler for CPU-only code and the  AMD ROCm compilers for GPU programming. Both contain a C/C++ compiler based on Clang and LLVM and a Fortran compiler which is currently based on the former PGI frontend with LLVM backend. The ROCm compilers also contain the support for HIP, AMD's CUDA clone.</p> <p>The second component is the Cray Scientific and Math libraries, containing the usual suspects as BLAS, LAPACK and ScaLAPACK, and FFTW, but also some data libraries and Cray-only libraries.</p> <p>The third component is the Cray Message Passing Toolkit. It provides an MPI implementation optimized for Cray systems, but also the Cray SHMEM libraries, an implementation of OpenSHMEM 1.5.</p> <p>The fourth component is some Cray-unique sauce to integrate all these components, and  support for hugepages to make memory access more efficient for some programs that  allocate huge chunks of memory at once.</p> <p>Other components include the Cray Performance Measurement and Analysis Tools and the  Cray Debugging Support Tools that will not be discussed in this one-day course, and Python and R modules that both also provide some packages compiled with support for the Cray Scientific Libraries.</p> <p>Besides the tools provided by HPE Cray, several of the development tools from the ROCm stack are also available on the system while some others can be user-installed (and one of those, Omniperf, is not available due to security concerns). Furthermore there are some third party tools available on LUMI, including Linaro Forge (previously ARM Forge) and Vampir and some open source profiling tools.</p> <p>Specifically not on LUMI are the Intel and NVIDIA programming environments, nor is the regular Intel oneAPI HPC Toolkit. The classic Intel compilers pose problems on AMD CPUs as <code>-xHost</code> cannot be relied on, but it appears that the new compilers that are based on Clang and an LLVM backend behave better. Various MKL versions are also troublesome, with different workarounds for different versions, though here also it seems that Intel now has  code that works well on AMD for many MKL routines. We have experienced problems with Intel  MPI when testing it on LUMI though in principle it should be possible to use Cray MPICH as they are derived from the same version of MPICH. The NVIDIA programming environment doesn't make sense on an AMD GPU system, but it could have been usefull for some visualisation software on the  visualisation nodes.</p> <p>We will now discuss some of these components in a little bit more detail, but refer to the 4-day trainings that we organise three times a year with HPE for more material.</p>"},{"location":"intro-202310xx/02_CPE/#the-cray-compiling-environment","title":"The Cray Compiling Environment","text":"<p>The Cray Compiling Environment are the default compilers on many Cray systems and on LUMI. These compilers are designed specifically for scientific software in an HPC environment. The current versions use are LLVM-based with extensions by HPE Cray for automatic vectorization and shared memory parallelization, technology that they have experience with since the late '70s or '80s.</p> <p>The compiler offers extensive standards support. The C and C++ compiler is essentially their own build of Clang with LLVM with some of their optimisation plugins and OpenMP run-time. The version numbering of the CCE currently follows the major versions of the Clang compiler used. The support for C and C++ language standards corresponds to that of Clang. The Fortran compiler uses a frontend developed by HPE Cray, but an LLVM-based backend.  The compiler supports most of Fortran 2018 (ISO/IEC 1539:2018). The CCE Fortran compiler is known to be very strict with language standards. Programs that use GNU or Intel extensions will usually fail to compile, and unfortunately since many developers only test with these compilers, much Fortran code is not fully standards compliant and will fail.</p> <p>All CCE compilers support OpenMP, with offload for AMD and NVIDIA GPUs. They claim full OpenMP 4.5 support with partial (and growing) support for OpenMP 5.0 and 5.1. More  information about the OpenMP support is found by checking a manual page: <pre><code>man intro_openmp\n</code></pre> which does require that the <code>cce</code> module is loaded. The Fortran compiler also supports OpenACC for AMD and NVIDIA GPUs. That implementation claims to be fully OpenACC 2.0 compliant, and offers partial support for OpenACC 2.x/3.x.  Information is available via <pre><code>man intro_openacc\n</code></pre> AMD and HPE Cray still recommend moving to OpenMP which is a much broader supported standard. There are no plans to also support OpenACC in the Cray C/C++ compiler, nor are there any  plans for support by AMD in the ROCm stack.</p> <p>The CCE compilers also offer support for some PGAS (Partitioned Global Address Space) languages. UPC 1.2 is supported, as is Fortran 2008 coarray support. These implementations do not require a preprocessor that first translates the code to regular C or Fortran. There is also support for debugging with Linaro Forge.</p> <p>Lastly, there are also bindings for MPI.</p>"},{"location":"intro-202310xx/02_CPE/#scientific-and-math-libraries","title":"Scientific and math libraries","text":"<p>Some mathematical libraries have become so popular that they basically define an API for which several implementations exist, and CPU manufacturers and some open source groups spend a significant amount of resources to make optimal implementations for each CPU architecture.</p> <p>The most notorious library of that type is BLAS, a set of basic linear algebra subroutines for vector-vector, matrix-vector and matrix-matrix implementations. It is the basis for many other libraries that need those linear algebra operations, including Lapack, a library with solvers for linear systems and eigenvalue problems.</p> <p>The HPE Cray LibSci library contains BLAS and its C-interface CBLAS, and LAPACK and its C interface LAPACKE. It also adds ScaLAPACK, a distributed memory version of LAPACK, and BLACS, the  Basic Linear Algebra Communication Subprograms, which is the communication layer used by ScaLAPACK. The BLAS library combines implementations from different sources, to try to offer the most optimal one for several architectures and a range of matrix and vector sizes.</p> <p>LibSci also contains one component which is HPE Cray-only: IRT, the Iterative Refinement Toolkit,  which allows to do mixed precision computations for LAPACK operations that can speed up the generation of a double precision result with nearly a factor of two for those problems that are suited for iterative refinement. If you are familiar with numerical analysis, you probably know that the matrix should not be too ill-conditioned for that.</p> <p>There is also a GPU-optimized version of LibSci, called LibSci_ACC, which contains a subset of the routines of LibSci. We or the LUMI USer Support Team don't have much experience with this library though. It can be compared with what Intel is doing with oneAPI MKL which also offers GPU versions of some of the traditional MKL routines.</p> <p>Another separate component of the scientific and mathematical libraries is FFTW3,  Fastest Fourier Transforms in the West, which comes with optimized versions for all CPU architectures supported by recent HPE Cray machines.</p> <p>Finally, the scientific and math libraries also contain HDF5 and netCDF libraries in sequential and parallel versions.</p> <p>Cray used to offer more pre-installed third party libraries for which the only added value was that they compiled the binaries. Instead they now offer build scripts in a GitHub repository.</p>"},{"location":"intro-202310xx/02_CPE/#cray-mpi","title":"Cray MPI","text":"<p>HPE Cray build their own MPI library with optimisations for their own interconnects. The Cray MPI library is derived from the ANL MPICH 3.4 code base and fully supports the  ABI (Application Binary Interface) of that application which implies that in principle it should be possible to swap the MPI library of applications build with that ABI with the Cray MPICH library. Or in other words, if you can only get a binary distribution of an application and that application was build against an MPI library compatible with  the MPICH 3.4 ABI (which includes Intel MPI) it should be possible to exchange that library for the Cray one to have optimised communication on the Cray Slingshot interconnect.</p> <p>Cray MPI contains many tweaks specifically for Cray systems. HPE Cray claim improved algorithms for many collectives, an asynchronous progress engine to improve overlap of communications and computations,  customizable collective buffering when using MPI-IO, and optimized remote memory access (MPI one-sided communication) which also supports passive remote memory access.</p> <p>When used in the correct way (some attention is needed when linking applications) it is allo fully GPU aware with currently support for AMD and NVIDIA GPUs.</p> <p>The MPI library also supports bindings for Fortran 2008.</p> <p>MPI 3.1 is almost completely supported, with two exceptions. Dynamic process management is not supported (and a problem anyway on systems with batch schedulers), and when using CCE MPI_LONG_DOUBLE and MPI_C_LONG_DOUBLE_COMPLEX are also not supported.</p> <p>The Cray MPI library does not support the <code>mpirun</code> or <code>mpiexec</code> commands, which is in fact allowed by the standard which only requires a process starter and suggest <code>mpirun</code> or <code>mpiexec</code>  depending on the version of the standard. Instead the Slurm <code>srun</code> command is used as the  process starter. This actually makes a lot of sense as the MPI application should be mapped correctly on the allocated resources, and the resource manager is better suited to do so.</p> <p>Cray MPI on LUMI is layered on top of libfabric, which in turn uses the so-called Cassini provider to interface with the hardware. UCX is not supported on LUMI (but Cray MPI can support it when used on InfiniBand clusters). It also uses a GPU Transfer Library (GTL) for GPU-aware MPI.</p>"},{"location":"intro-202310xx/02_CPE/#lmod","title":"Lmod","text":"<p>Virtually all clusters use modules to enable the users to configure the environment and select the versions of software they want. There are three different module systems around. One is an old implementation that is hardly evolving anymore but that can still be found on\\ a number of clusters. HPE Cray still offers it as an option. Modulefiles are written in TCL, but the tool itself is in C. The more popular tool at the moment is probably Lmod. It is largely compatible with modulefiles for the old tool, but prefers modulefiles written in LUA. It is also supported by the HPE Cray PE and is our choice on LUMI. The final implementation is a full TCL implementation developed  in France and also in use on some large systems in Europe.</p> <p>Fortunately the basic commands are largely similar in those implementations, but what differs is the way to search for modules. We will now only discuss the basic commands, the more advanced ones will be discussed in the next session of this tutorial course.</p> <p>Modules also play an important role in configuring the HPE Cray PE, but before touching that  topic we present the basic commands:</p> <ul> <li><code>module avail</code>: Lists all modules that can currently be loaded. </li> <li><code>module list</code>: Lists all modules that are currently loaded</li> <li><code>module load</code>: Command used to load a module. Add the name and version of the module.</li> <li><code>module unload</code>: Unload a module. Using the name is enough as there can only one version be      loaded of a module.</li> <li><code>module swap</code>:  Unload the first module given and then load the second one. In Lmod this is      really equivalent to a <code>module unload</code> followed by a <code>module load</code>.</li> </ul> <p>Lmod supports a hierarchical module system. Such a module setup distinguishes between installed modules and available modules. The installed modules are all modules that can be loaded in one way or another by the module systems, but loading some of those may require loading other modules first. The available modules are the modules that can be loaded directly without loading any other module. The list of available modules changes all the time based on modules that are already loaded, and if you unload a module that makes other loaded modules unavailable, those will also be deactivated by Lmod. The advantage of a hierarchical module system is that one can support multiple configurations of a module while all configurations can have the same name and version. This is not fully exploited on LUMI, but it  is used a lot in the HPE Cray PE. E.g., the MPI libraries for the various compilers on the system all have the same name and version yet make different binaries available depending on the compiler that is being used.</p> <p>Different configurations on some Belgian clusters</p> <p>Depending on the configuration Lmod can behave rather differently on different systems. Some clusters in Belgium have Lmod configured to mimic the original Tcl module system better rather than exposing the full power of Lmod.</p> <p>E.g., on LUMI, <code>module swap</code> isn't really needed as the auto-swap feature of Lmod is  enabled. Automatically unloading a module if another module with the same name is  loaded, is also enabled. Both features make with a hierarchical scheme much more powerful and using the HPE Cray PE with these features disabled would be very difficult.</p>"},{"location":"intro-202310xx/02_CPE/#compiler-wrappers","title":"Compiler wrappers","text":"<p>The HPE Cray PE compilers are usually used through compiler wrappers. The wrapper for C is <code>cc</code>, the one for C++ is <code>CC</code> and the one for Fortran is <code>ftn</code>. The wrapper then calls the selected compiler. Which compiler will be called is determined by which compiler module is loaded. As shown on the slide  \"Development environment on LUMI\", on LUMI the Cray Compiling Environment (module <code>cce</code>), GNU Compiler Collection (module <code>gcc</code>),  the AMD Optimizing Compiler for CPUs (module <code>aocc</code>) and the ROCm LLVM-based compilers (module <code>amd</code>) are available. On other HPE Cray systems, you may also find the Intel compilers or on systems with NVIDIA GPUS, the NVIDIA HPC compilers.</p> <p>The target architectures for CPU and GPU are also selected through modules, so it is better to not use compiler options such as <code>-march=native</code>. This makes cross compiling also easier.</p> <p>The wrappers will also automatically link in certain libraries, and make the include files available, depending on which other modules are loaded. In some cases it tries to do so cleverly, like selecting an MPI, OpenMP, hybrid or sequential option depending on whether the MPI module is loaded and/or OpenMP compiler flag is used. This is the case for:</p> <ul> <li>The MPI libraries. There is no <code>mpicc</code>, <code>mpiCC</code>, <code>mpif90</code>, etc. on LUMI. The regular compiler     wrappers do the job as soon as the <code>cray-mpich</code> module is loaded.</li> <li>LibSci and FFTW are linked automatically if the corresponding modules are loaded. So no need     to look, e.g., for the BLAS or LAPACK libraries: They will be offered to the linker if the     <code>cray-libsci</code> module is loaded (and it is an example of where the wrappers try to take the     right version based not only on compiler, but also on whether MPI is loaded or not and the     OpenMP compiler flag).</li> <li>netCDF and HDF5</li> </ul> <p>It is possible to see which compiler and linker flags the wrappers add through the <code>--craype-verbose</code> flag.</p> <p>The wrappers do have some flags of their own, but also accept all flags of the selected compiler and  simply pass those to those compilers.</p>"},{"location":"intro-202310xx/02_CPE/#selecting-the-version-of-the-cpe","title":"Selecting the version of the CPE","text":"<p>The version numbers of the HPE Cray PE are of the form <code>yy.dd</code>, e.g., <code>22.08</code> for the version released in August 2022. There are usually 10 releases per year (basically every month except July and January), though not all versions are ever offered on LUMI. </p> <p>There is always a default version assigned by the sysadmins when installing the programming environment. It is possible to change the default version for loading further modules by loading one of the versions of the <code>cpe</code> module. E.g., assuming the 22.08 version would be present on the system, it can be loaded through <pre><code>module load cpe/22.08\n</code></pre> Loading this module will also try to switch the already loaded PE modules to the versions from that release. This does not always work correctly, due to some bugs in most versions of this module and a limitation of Lmod. Executing the <code>module load</code> twice will fix this: <pre><code>module load cpe/22.08\nmodule load cpe/22.08\n</code></pre> The module will also produce a warning when it is unloaded (which is also the case when you do a <code>module load</code> of <code>cpe</code> when one is already loaded, as it then first unloads the already loaded <code>cpe</code> module). The warning can be ignored, but keep in mind that what it says is true, it cannot restore the environment you found on LUMI at login.</p> <p>The <code>cpe</code> module is also not needed when using the LUMI software stacks, but more about that later.</p>"},{"location":"intro-202310xx/02_CPE/#the-target-modules","title":"The target modules","text":"<p>The target modules are used to select the CPU and GPU optimization targets and to  select the network communication layer. </p> <p>On LUMI there are three CPU target modules that are relevant:</p> <ul> <li><code>craype-x86-rome</code> selects the Zen2 CPU family code named Rome. These CPUs are     used on the login nodes and the nodes of the data analytics and visualisation      partition of LUMI. However, as Zen3 is a superset of Zen2, software compiled     to this target should run everywhere, but may not exploit the full potential     of the LUMI-C and LUMI-G nodes (though the performance loss is likely minor).</li> <li><code>craype-x86-milan</code> is the target module for the Zen3 CPUs code named Milan that     are used on the CPU-only compute nodes of LUMI (the LUMI-C partition).</li> <li><code>craype-x86-trento</code> is the target module for the Zen3 CPUs code named Trento that     are used on the GPU compute nodes of LUMI (the LUMI-G partition).</li> </ul> <p>Two GPU target modules are relevant for LUMI:</p> <ul> <li><code>craype-accel-host</code>: Will tell some compilers to compile offload code for the host     instead.</li> <li><code>craype-accel-gfx90a</code>: Compile offload code for the MI200 series GPUs that are used on LUMI-G.</li> </ul> <p>Two network target modules are relevant for LUMI:</p> <ul> <li><code>craype-network-ofi</code> selects the libfabric communication layer which is needed for     Slingshot 11.</li> <li><code>craype-network-none</code> omits all network specific libraries.</li> </ul> <p>The compiler wrappers also have corresponding compiler flags that can be used to overwrite these settings: <code>-target-cpu</code>, <code>-target-accel</code> and <code>-target-network</code>.</p>"},{"location":"intro-202310xx/02_CPE/#prgenv-and-compiler-modules","title":"PrgEnv and compiler modules","text":"<p>In the HPE Cray PE, the <code>PrgEnv-*</code> modules are usually used to load a specific variant of the programming environment. These modules will load the compiler wrapper (<code>craype</code>), compiler, MPI and LibSci module and may load some other modules also.</p> <p>The following table gives an overview of the available <code>PrgEnv-*</code> modules and the compilers they activate:</p> PrgEnv Description Compiler module Compilers PrgEnv-cray Cray Compiling Environment <code>cce</code> <code>craycc</code>, <code>crayCC</code>, <code>crayftn</code> PrgEnv-gnu GNU Compiler Collection <code>gcc</code> <code>gcc</code>, <code>g++</code>, <code>gfortran</code> PrgEnv-aocc AMD Optimizing Compilers(CPU only) <code>aocc</code> <code>clang</code>, <code>clang++</code>, <code>flang</code> PrgEnv-amd AMD ROCm LLVM compilers (GPU support) <code>amd</code> <code>amdclang</code>, <code>amdclang++</code>, <code>amdflang</code> <p>There is also a second module that offers the AMD ROCm environment, <code>rocm</code>. That module has to be used with <code>PrgEnv-cray</code> and <code>PrgEnv-gnu</code> to enable MPI-aware GPU, hipcc with the GNU compilers or GPU support with the Cray compilers.</p>"},{"location":"intro-202310xx/02_CPE/#getting-help","title":"Getting help","text":"<p>Help on the HPE Cray Programming Environment is offered mostly through manual pages and compiler flags. Online help is limited and difficult to locate.</p> <p>For the compilers and compiler wrappers, the following man pages are relevant:</p> PrgEnv C C++ Fortran PrgEnv-cray <code>man craycc</code> <code>man crayCC</code> <code>man crayftn</code> PrgEnv-gnu <code>man gcc</code> <code>man g++</code> <code>man gfortran</code> PrgEnv-aocc/PrgEnv-amd - - - Compiler wrappers <code>man cc</code> <code>man CC</code> <code>man ftn</code> <p>Recently, HPE Cray have also created  a web version of some of the CPE documentation.</p> <p>Some compilers also support the <code>--help</code> flag, e.g., <code>amdclang --help</code>. For the wrappers, the switch <code>-help</code> should be used instead as the double dash version is passed to the  compiler.</p> <p>The wrappers also support the <code>-dumpversion</code> flag to show the version of the underlying compiler. Many other commands, including the actual compilers, use <code>--version</code> to show the version.</p> <p>For Cray Fortran compiler error messages, the <code>explain</code> command is also helpful. E.g.,</p> <pre><code>$ ftn\nftn-2107 ftn: ERROR in command line\n  No valid filenames are specified on the command line.\n$ explain ftn-2107\n\nError : No valid filenames are specified on the command line.\n\nAt least one file name must appear on the command line, with any command-line\noptions.  Verify that a file name was specified, and also check for an\nerroneous command-line option which may cause the file name to appear to be\nan argument to that option.\n</code></pre> <p>On older Cray systems this used to be a very useful command with more compilers but as  HPE Cray is using more and more open source components instead there are fewer commands that give additional documentation via the <code>explain</code> command.</p> <p>Lastly, there is also a lot of information in the \"Developing\" section of the LUMI documentation.</p>"},{"location":"intro-202310xx/02_CPE/#google-chatgpt-and-lumi","title":"Google, ChatGPT and LUMI","text":"<p>When looking for information on the HPE Cray Programming Environment using search engines such as Google, you'll be disappointed how few results show up. HPE doesn't put much information on the  internet, and the environment so far was mostly used on Cray systems of which there are not that many. </p> <p>The same holds for ChatGPT. In fact, much of the training of the current version of ChatGPT was done with data of two or so years ago and there is not that much suitable training data available on the internet either.</p> <p>The HPE Cray environment has a command line alternative to search engines though: the <code>man -K</code> command that searches for a term in the manual pages. It is often useful to better understand some error messages. E.g., sometimes Cray MPICH will suggest you to set some environment variable to work around some problem. You may remember that <code>man intro_mpi</code> gives a lot of information about Cray MPICH, but if you don't and, e.g., the error message suggests you to set <code>FI_CXI_RX_MATCH_MODE</code> to either <code>software</code> or <code>hybrid</code>, one way to find out where you can get more information about this environment variable is</p> <pre><code>man -K FI_CXI_RX_MATCH_MODE\n</code></pre>"},{"location":"intro-202310xx/02_CPE/#other-modules","title":"Other modules","text":"<p>Other modules that are relevant even to users who do not do development:</p> <ul> <li>MPI: <code>cray-mpich</code>. </li> <li>LibSci: <code>cray-libsci</code></li> <li>Cray FFTW3 library: <code>cray-fftw</code></li> <li>HDF5:<ul> <li><code>cray-hdf5</code>: Serial HDF5 I/O library</li> <li><code>cray-hdf5-parallel</code>: Parallel HDF5 I/O library</li> </ul> </li> <li>NetCDF:<ul> <li><code>cray-netcdf</code></li> <li><code>cray-netcdf-hdf5parallel</code></li> <li><code>cray-parallel-netcdf</code></li> </ul> </li> <li>Python: <code>cray-python</code>, already contains a selection of packages that interface with     other libraries of the HPE Cray PE, including mpi4py, NumPy, SciPy and pandas.</li> <li>R: <code>cray-R</code></li> </ul> <p>The HPE Cray PE also offers other modules for debugging, profiling, performance analysis, etc. that are not covered in this short version of the LUMI course. Many more are covered in the 4-day courses for developers that we organise several times per year with the help of HPE and AMD.</p>"},{"location":"intro-202310xx/02_CPE/#warning-1-you-do-not-always-get-what-you-expect","title":"Warning 1: You do not always get what you expect...","text":"<p>The HPE Cray PE packs a surprise in terms of the libraries it uses, certainly for users who come from an environment where the software is managed through EasyBuild, but also for most other users.</p> <p>The PE does not use the versions of many libraries determined by the loaded modules at runtime but instead uses default versions of libraries (which are actually in <code>/opt/cray/pe/lib64</code> on the system) which correspond to the version of the programming environment that is set as the default when installed. This is very much the behaviour of Linux applications also that pick standard libraries in a few standard directories and it enables many programs build with the HPE Cray PE to run without reconstructing the environment and in some cases to mix programs compiled with different compilers with ease (with the emphasis on some as there may still be library conflicts between other libraries when not using the  so-called rpath linking). This does have an annoying side effect though: If the default PE on the system  changes, all applications will use different libraries and hence the behaviour of your application may  change. </p> <p>Luckily there are some solutions to this problem.</p> <p>By default the Cray PE uses dynamic linking, and does not use rpath linking, which is a form of dynamic linking where the search path for the libraries is stored in each executable separately. On Linux, the search path for libraries is set through the environment variable <code>LD_LIBRARY_PATH</code>. Those Cray PE modules that have their libraries also in the default location, add the directories that contain the actual version of the libraries corresponding to the version of the module to the PATH-style environment variable <code>CRAY_LD_LIBRARY_PATH</code>. Hence all one needs to do is to ensure that those directories are put in <code>LD_LIBRARY_PATH</code> which is searched before the default location: <pre><code>export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH\n</code></pre></p> Small demo of adapting <code>LD_LIBRARY_PATH</code>: <p>An example that can only be fully understood after the section on the LUMI software stacks: <pre><code>$ module load LUMI/22.08\n$ module load lumi-CPEtools/1.0-cpeGNU-22.08\n$ ldd $EBROOTLUMIMINCPETOOLS/bin/mpi_check\n      linux-vdso.so.1 (0x00007f420cd55000)\n      libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f420c929000)\n      libmpi_gnu_91.so.12 =&gt; /opt/cray/pe/lib64/libmpi_gnu_91.so.12 (0x00007f4209da4000)\n      ...\n$ export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH\n$ ldd $EBROOTLUMIMINCPETOOLS/bin/mpi_check\n        linux-vdso.so.1 (0x00007fb38c1e0000)\n      libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007fb38bdb4000)\n      libmpi_gnu_91.so.12 =&gt; /opt/cray/pe/mpich/8.1.18/ofi/gnu/9.1/lib/libmpi_gnu_91.so.12 (0x00007fb389198000)\n      ...\n</code></pre> The <code>ldd</code> command shows which libraries are used by an executable. Only a part of the very long output is shown in the above example. But we can already see that in the first case, the library <code>libmpi_gnu_91.so.12</code> is taken from <code>opt/cray/pe/lib64</code> which is the directory with the default versions, while in the second case it is taken from <code>/opt/cray/pe/mpich/8.1.18/ofi/gnu/9.1/lib/</code> which clearly is for a specific version of <code>cray-mpich</code>.</p> <p>We do provide an experimental module <code>lumi-CrayPath</code>  that tries to fix <code>LD_LIBRARY_PATH</code> in a way that unloading the module fixes <code>LD_LIBRARY_PATH</code> again to the state before adding <code>CRAY_LD_LIBRARY_PATH</code> and that reloading the module adapts <code>LD_LIBRARY_PATH</code> to the current value of <code>CRAY_LD_LIBRARY_PATH</code>. Loading that module after loading all other modules should fix this issue for most if not all software.</p> <p>The second solution would be to use rpath-linking for the Cray PE libraries, which can be done by setting the <code>CRAY_ADD_RPATH</code>environment variable: <pre><code>export CRAY_ADD_RPATH=yes\n</code></pre></p> <p>However, there is also a good side to the standard Cray PE behaviour. Updates of the underlying operating system or network software stack may break older versions of the MPI library. By letting the applications use the default libraries and updating the defaults to a newer version, most applications will still run while they would fail if any of the two tricks to force the use of the intended library version are used. This has actually happened after a big LUMI update in March 2023, when all software that used rpath-linking had to be rebuild as the MPICH library that was present before the update did not longer work.</p>"},{"location":"intro-202310xx/02_CPE/#warning-2-order-matters","title":"Warning 2: Order matters","text":"<p>Lmod is a hierarchical module scheme and this is exploited by the HPE Cray PE. Not all modules are available right away and some only become available after loading other modules. E.g.,</p> <ul> <li><code>cray-fftw</code> only becomes available when a processor target module is loaded</li> <li><code>cray-mpich</code> requires both the network target module <code>craype-network-ofi</code> and a compiler module to be loaded</li> <li><code>cray-hdf5</code> requires a compiler module to be loaded and <code>cray-netcdf</code> in turn requires <code>cray-hdf5</code></li> </ul> <p>but there are many more examples in the programming environment.</p> <p>In the next section of the course we will see how unavailable modules can still be found with <code>module spider</code>. That command can also tell which other modules should be loaded  before a module can be loaded, but unfortunately due to the sometimes non-standard way  the HPE Cray PE uses Lmod that information is not always complete for the PE, which is also why we didn't demonstrate it here.</p>"},{"location":"intro-202310xx/03_LUMI_organisation/","title":"LUMI organisation","text":""},{"location":"intro-202310xx/03_LUMI_organisation/#who-pays-the-bills","title":"Who pays the bills?","text":""},{"location":"intro-202310xx/03_LUMI_organisation/#users-and-projects","title":"Users and projects","text":""},{"location":"intro-202310xx/03_LUMI_organisation/#project-management","title":"Project management","text":"<p>Tell something about Puhuri.</p>"},{"location":"intro-202310xx/03_LUMI_organisation/#file-spaces","title":"File spaces","text":""},{"location":"intro-202310xx/03_LUMI_organisation/#access","title":"Access","text":""},{"location":"intro-202310xx/03_LUMI_organisation/#data-transfer","title":"Data transfer","text":""},{"location":"intro-202310xx/04_Modules/","title":"Modules on LUMI","text":"<p>Intended audience</p> <p>As this course is designed for people already familiar with HPC systems and as virtually any cluster nowadays uses some form of module environment, this section assumes that the reader is already familiar with a module environment but not necessarily the one used on LUMI.</p>"},{"location":"intro-202310xx/04_Modules/#module-environments","title":"Module environments","text":"<p>Modules are commonly used on HPC systems to enable users to create  custom environments and select between multiple versions of applications. Note that this also implies that applications on HPC systems are often not installed in the regular directories one would expect from the documentation of some packages, as that location may not even always support proper multi-version installations and as one prefers to have a software stack which is as isolated as possible from the system installation to keep the image that has to be loaded on the compute nodes small.</p> <p>Another use of modules not mentioned on the slide is to configure the programs that is being activated. E.g., some packages expect certain additional environment variables to be set and modules can often take care of that also.</p> <p>There are 3 systems in use for module management. The oldest is a C implementation of the commands using module files written in Tcl. The development of that system stopped around 2012, with version 3.2.10.  This system is supported by the HPE Cray Programming Environment. A second system builds upon the C implementation but now uses Tcl also for the module command and not only for the module files. It is developed in France at the C\u00c9A compute centre. The version numbering was continued from the C implementation, starting with version 4.0.0.  The third system and currently probably the most popular one is Lmod, a version written in Lua with module files also written in Lua. Lmod also supports most Tcl module files. It is also supported by HPE Cray, though they tend to be a bit slow in following versions.</p> <p>On LUMI we have chosen to use Lmod. As it is very popular, many users may already be familiar with it, though it does make sense to revisit some of the commands that are specific for Lmod and differ from those in the two other implementations.</p> <p>It is important to realise that each module that you see in the overview corresponds to a module file that contains the actual instructions that should be executed when loading  or unloading a module, but also other information such as some properties of the module, information for search and help information.</p> Links <ul> <li>Old-style environment modules on SourceForge</li> <li>TCL Environment Modules home page on SourceForge and the     development on GitHub</li> <li>Lmod documentation and      Lmod development on GitHub</li> </ul> <p>I know Lmod, should I continue?</p> <p>Lmod is a very flexible tool. Not all sides using Lmod use all features, and Lmod can be configured in different ways to the extent that it may even look like a very different module system for people coming from another cluster. So yes, it makes sense to continue reading as Lmod on LUMI may have some tricks that are not available on your home cluster.</p>"},{"location":"intro-202310xx/04_Modules/#exploring-modules-with-lmod","title":"Exploring modules with Lmod","text":"<p>Contrary to some other module systems, or even some other Lmod installations, not all modules are immediately available for loading. So don't be disappointed by the few modules you will see with <code>module available</code> right after login. Lmod has a so-called hierarchical setup that tries to protect you from being confronted with all modules at the same time, even those that may conflict with  each other, and we use that to some extent on LUMI. Lmod distinguishes between installed modules and available modules. Installed modules are all modules on the system that can be loaded one way or another, sometimes through loading other modules first. Available modules are all those modules that can be loaded at a given point in time without first loading other modules.</p> <p>The HPE Cray Programming Environment also uses a hierarchy though it is not fully implemented in the way the Lmod developer intended so that some features do not function as they should.</p> <ul> <li>For example, the <code>cray-mpich</code> module can only be loaded if both a network target module and a     compiler module are loaded (and that is already the example that is implemented differently from     what the Lmod developer had in mind). </li> <li>Another example is the performance monitoring tools. Many of those     tools only become available after loading the <code>perftools-base</code> module. </li> <li>Another example is the     <code>cray-fftw</code> module which requires a processor target module to be loaded first.</li> </ul> <p>Lmod has several tools to search for modules. </p> <ul> <li>The <code>module avail</code> command is one that is also     present in the various Environment Modules implementations and is the command to search in the     available modules. </li> <li>But Lmod also has other commands, <code>module spider</code> and <code>module keyword</code>, to      search in the list of installed modules.</li> </ul>"},{"location":"intro-202310xx/04_Modules/#benefits-of-a-hierarchy","title":"Benefits of a hierarchy","text":"<p>When the hierarchy is well designed, you get some protection from loading modules that do not work together well. E.g., in the HPE Cray PE it is not possible to load the MPI library built for another compiler than your current main compiler. This is currently not exploited as much as we could on LUMI, mainly because we realised at the start that too many users are not familiar enough with hierarchies and would get confused more than the hierarchy helps them.</p> <p>Another benefit is that when \"swapping\" a module that makes other modules available with a different one, Lmod will try to look for equivalent modules in the list of modules made available by the newly loaded module.</p> <p>An easy example (though a tricky one as there are other mechanisms at play also) it to load a different programming environment in the default login environment right after login:</p> <pre><code>$ module load PrgEnv-aocc\n</code></pre> <p>which results in</p> <p></p> <p>The first two lines of output are due to to other mechanisms that are at work here,  and the order of the lines may seem strange but that has to do with the way Lmod works internally. Each of the PrgEnv modules hard loads a compiler module which is why Lmod tells you that it is loading <code>aocc/3.2.0</code>. However, there is also another mechanism at work that causes <code>cce/15.0.0</code> and <code>PrgEnv-cray/8.3.3</code> to be unloaded, but more about that in the next subsection (next slide).</p> <p>The important line for the hierarchy in the output are the lines starting with  \"Due to MODULEPATH changes...\". Remember that we said that each module has a corresponding module file. Just as binaries on a system, these are organised in a directory structure, and there is a path, in this case MODULEPATH, that determines where Lmod will look for module files. The hierarchy is implemented with a directory structure and the environment variable MODULEPATH, and when the <code>cce/15.0.0</code> module was unloaded and <code>aocc/3.2.0</code> module was loaded, that  MODULEPATH was changed. As a result, the version of the cray-mpich module for the  <code>cce/15.0.0</code> compiler became unavailable, but one with the same module name for the <code>aocc/3.2.0</code> compiler became available and hence Lmod unloaded the version for the <code>cce/15.0.0</code> compiler as it is no longer available but loaded the matching one for the <code>aocc/3.2.0</code> compiler. </p>"},{"location":"intro-202310xx/04_Modules/#about-module-names-and-families","title":"About module names and families","text":"<p>In Lmod you cannot have two modules with the same name loaded at the same time. On LUMI, when you load a module with the same name as an already loaded module, that other module will be unloaded automatically before loading the new one. There is  even no need to use the <code>module swap</code> command for that (which in Lmod corresponds to a <code>module unload</code> of the first module and a <code>module load</code> of the second). This gives you an automatic protection against some conflicts if the names of the modules are properly chosen. </p> <p>Note</p> <p>Some clusters do not allow the automatic unloading of a module with the same name as the one you're trying to load, but on LUMI we felt that this is a  necessary feature to fully exploit a hierarchy.</p> <p>Lmod goes further also. It also has a family concept: A module can belong to a family (and at most 1) and no two modules of the same family can be loaded together.  The family property is something that is defined in the module file. It is commonly  used on systems with multiple compilers and multiple MPI implementations to ensure  that each compiler and each MPI implementation can have a logical name without  encoding that name in the version string (like needing to have <code>compiler/gcc-11.2.0</code> rather than <code>gcc/11.2.0</code>), while still having an easy way to avoid having two  compilers or MPI implementations loaded at the same time.  On LUMI, the conflicting module of the same family will be unloaded automatically when loading another module of that particular family.</p> <p>This is shown in the example in the previous subsection (the <code>module load PrgEnv-gnu</code> in  a fresh long shell) in two places. It is the mechanism that unloaded <code>PrgEnv-cray</code> when loading <code>PrgEnv-gnu</code> and that then unloaded <code>cce/14.0.1</code> when the  <code>PrgEnv-gnu</code> module loaded the <code>gcc/11.2.0</code> module.</p> <p>Note</p> <p>Some clusters do not allow the automatic unloading of a module of the same family as the one you're trying to load and produce an error message instead. On LUMI, we felt that this is a necessary feature to fully exploit the  hierarchy and the HPE Cray Programming Environment also relies very much on this feature being enabled to make live easier for users.</p>"},{"location":"intro-202310xx/04_Modules/#extensions","title":"Extensions","text":"<p>It would not make sense to have a separate module for each of the hundreds of R packages or tens of Python packages that a software stack may contain. In fact, as the software for each module is installed in a separate directory it would also create a performance problem due to excess directory accesses simply to find out where a command is located, and very long search path environment variables such as PATH or the various variables packages such as Python, R or Julia use to find extension packages. On LUMI related packages are often bundled in a single module. </p> <p>Now you may wonder: If a module cannot be simply named after the package it contains as it contains several ones, how can I then find the appropriate module to load? Lmod has a solution for that through the so-called extension mechanism. An Lmod module can define extensions, and some of the search commands for modules will also search in the extensions of a module. Unfortunately, the HP{E Cray PE cray-python and cray-R modules do not provide that  information at the moment as they too contain several packages that may benefit from linking to optimised math libraries.</p>"},{"location":"intro-202310xx/04_Modules/#searching-for-modules-the-module-spider-command","title":"Searching for modules: the module spider command","text":"<p>There are three ways to use <code>module spider</code>, discovering software in more and more detail.</p> <ol> <li> <p><code>module spider</code> by itself will show a list of all installed software with a short description.     Software is bundled by name of the module, and it shows the description taken from the default     version. <code>module spider</code> will also look for \"extensions\" defined in a module and show those also     and mark them with an \"E\". Extensions are a useful Lmod feature to make clear that a module offers     features that one would not expect from its name. E.g., in a Python module the extensions could be     a list of major Python packages installed in the module which would allow you to find <code>NumPy</code> if     it were hidden in a module with a different name. This is also a very useful feature to make     tools that are bundled in one module to reduce the module clutter findable.</p> </li> <li> <p><code>module spider</code> with the name of a package will show all versions of that package installed on     the system. This is also case-insensitive.      The spider command will not only search in module names for the package, but also in extensions     of the modules and so will be able to tell you that a package is delivered by another module. See      Example 4 below where we will search for the CMake tools.</p> </li> <li> <p>The third use of <code>module spider</code> is with the full name of a module.      This shows two kinds of information. First it shows which combinations of other modules one     might have to load to get access to the package. That works for both modules and extensions     of modules. In the latter case it will show both the module, and other modules that you might     have to load first to make the module available.     Second it will also show help information for the module if the module file provides      such information. </p> </li> </ol>"},{"location":"intro-202310xx/04_Modules/#example-1-running-module-spider-on-lumi","title":"Example 1: Running <code>module spider</code> on LUMI","text":"<p>Let's first run the <code>module spider</code> command. The output varies over time, but at the time of writing, and leaving out a lot of the output, one would have gotten:</p> <p></p> <p></p> <p></p> <p>On the second screen we see, e.g., the ARMForge module which was available in just a single version at that time, and then Autoconf where the version is in blue and followed by <code>(E)</code>. This denotes that the Autoconf package is actually provided as an extension of another module, and one of the next examples will tell us how to figure out which one.</p> <p>The third screen shows the last few lines of the output, which actually also shows some help information for the command.</p>"},{"location":"intro-202310xx/04_Modules/#example-2-searching-for-the-fftw-module-which-happens-to-be-provided-by-the-pe","title":"Example 2: Searching for the FFTW module which happens to be provided by the PE","text":"<p>Next let us search for the popular FFTW library on LUMI:</p> <pre><code>$ module spider FFTW\n</code></pre> <p>produces</p> <p></p> <p>This shows that the FFTW library is actually provided by the <code>cray-fftw</code> module and was at the time that this was tested available in 3 versions.  Note that (a) it is not case sensitive as FFTW is not in capitals in the module name and (b) it also finds modules where the argument of module spider is only part of the name.</p> <p>The output also suggests us to dig a bit deeper and  check for a specific version, so let's run</p> <pre><code>$ module spider cray-fftw/3.3.10.3\n</code></pre> <p>This produces:</p> <p></p> <p></p> <p>We now get a long list of possible combinations of modules that would enable us to load this module. What these modules are will be explained in the next session of this course. However, it does show a weakness when module spider is used with the HPE Cray PE. In some cases, not all possible combinations are shown (and this is the case here as the module is actually available directly after login and also via some other combinations of modules that are not shown). This is because the HPE Cray Programming Environment is system-installed and sits next to the application software stacks that are managed differently, but in some cases also because the HPE Cray PE sometimes fails to give the complete combination of modules that is needed. The command does work well with the software managed by the LUMI User Support Team as the next two examples will show.</p>"},{"location":"intro-202310xx/04_Modules/#example-3-searching-for-gnuplot","title":"Example 3: Searching for GNUplot","text":"<p>To see if GNUplot is available, we'd first search for the name of the package:</p> <pre><code>$ module spider GNUplot\n</code></pre> <p>This produces:</p> <p></p> <p></p> <p>The output again shows that the search is not case sensitive which is fortunate as uppercase and lowercase letters are not always used in the same way on different clusters. Some management tools for scientific software stacks will only use lowercase letters, while the package we use on LUMI often uses both.</p> <p>We see that there are a lot of versions installed on the system and that the version actually contains more  information (e.g., <code>-cpeGNU-22.12</code>) that we will explain in the next part of this course. But you might of course guess that it has to do with the compilers that were used. It may look strange to you to have the same software built with different compilers. However, mixing compilers is sometimes risky as a library compiled with one compiler may not work in an executable compiled with another one, so to enable workflows that use multiple tools we try  to offer many tools compiled with multiple compilers (as for most software we don't use rpath linking which could help to solve that problem). So you want to chose the appropriate line in terms of the other software that you will be using.</p> <p>The output again suggests to dig a bit further for more information, so let's try</p> <pre><code>$ module spider gnuplot/5.4.6-cpeGNU-22.12\n</code></pre> <p>This produces:</p> <p></p> <p></p> <p>In this case, this module is provided by 3 different combinations of modules that also will be explained in the next part of this course. Furthermore, the output of the command now also shows some help information about the module, with some links to further documentation available on the system or on the web. The format of the output is generated automatically by the software installation tool that we use and we sometimes have to do some effort to fit all information in there.</p> <p>For some packages we also have additional information in our LUMI Software Library web site so it is often worth looking there also.</p>"},{"location":"intro-202310xx/04_Modules/#example-4-searching-for-an-extension-of-a-module-cmake","title":"Example 4: Searching for an extension of a module: CMake.","text":"<p>The <code>cmake</code> command on LUMI is available in the operating system image, but as is often the case with such tools distributed with the OS, it is a rather old version and you may want to use a newer one.</p> <p>If you would just look through the list of available modules, even after loading some other modules to activate a larger software stack, you will not find any module called <code>CMake</code> though. But let's use the powers of <code>module spider</code> and try</p> <pre><code>$ module spider cmake\n</code></pre> <p>which produces</p> <p></p> <p>The output above shows us that there are actually four other versions of CMake on the system, but their version is followed by <code>(E)</code> which says that they are extensions of other modules. There is no module called <code>CMake</code> on the system.  But Lmod already tells us how to find out which module actually provides the CMake tools. So let's try</p> <pre><code>$ module spider CMake/3.25.2\n</code></pre> <p>which produces</p> <p></p> <p></p> <p>This shows us that the version is provided by a number of <code>buildtools</code> modules, and for each of those modules also shows us which other modules should be loaded to get access to the commands. E.g., the first line tells us that there is a module <code>buildtools/22.08</code> that provides that version of CMake, but that we first need to load some other modules, with <code>LUMI/22.08</code> and <code>partition/L</code> (in that order)  one such combination.</p> <p>So in this case, after</p> <pre><code>$ module load LUMI/22.12 partition/L buildtools/22.12\n</code></pre> <p>the <code>cmake</code> command would be available.</p> <p>And you could of course also use</p> <pre><code>$ module spider buildtools/22.12\n</code></pre> <p>to get even more information about the buildtools module, including any help included in the module.</p>"},{"location":"intro-202310xx/04_Modules/#alternative-search-the-module-keyword-command","title":"Alternative search: the module keyword command","text":"<p>Lmod has a second way of searching for modules: <code>module keyword</code>, but unfortunately it does not yet work very well on LUMI as the version of Lmod is rather old and still has some bugs in the processing of the command. </p> <p>The <code>module keyword</code> command searches in some of the information included in module files for the given keyword, and shows in which modules the keyword was found.</p> <p>We do an effort to put enough information in the modules to make this a suitable additional way to discover software that is installed on the system.</p> <p>Let us look for packages that allow us to download software via the <code>https</code> protocol. One could try</p> <pre><code>$ module keyword https\n</code></pre> <p>which produces a lot of output:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>The bug in the Lmod 8.3 version on LUMI is that all extensions are shown in the output while they are irrelevant. On the second screen though we see <code>cURL</code> and on the fourth screen <code>wget</code> which are two tools that can be used to fetch files from the internet.</p> <p>LUMI Software Library</p> <p>The LUMI Software Library also has a search box in the upper right. We will see in the next section of this course that much of the software of LUMI is managed through a tool called EasyBuild, and each module file corresponds to an EasyBuild recipe which is a file with the <code>.eb</code> extension. Hence the keywords can also be found in the EasyBuild recipes which are included in this web site, and from a page with an EasyBuild recipe (which may not mean much for you) it is easy to go back to the software package page itself for more information. Hence you can use the search box to search for packages that may not be installed on the system.</p> <p>The example given above though, searching for `https, would not work via that box as most EasyBuild recipes include https web links to refer to, e.g., documentation and would be  shown in the result.</p> <p>The LUMI Software Library site includes both software installed in our central software stack and software for which we make customisable build recipes available for user installation, but more about that in the tutorial section on LUMI software stacks.</p>"},{"location":"intro-202310xx/04_Modules/#sticky-modules-and-the-module-purge-command","title":"Sticky modules and the module purge command","text":"<p>On some systems you will be taught to avoid <code>module purge</code> as many HPC systems do their default user configuration also through modules. This advice is often given on Cray systems as it is a common practice to preload a suitable set of target modules and a programming environment. On LUMI both are used. A default programming environment and set of target modules suitable for the login nodes is preloaded when you log in to the system, and next the <code>init-lumi</code> module is loaded which in turn makes the LUMI software stacks available that we will discuss in the next session.</p> <p>Lmod however has a trick that help to avoid removing necessary modules and it is called sticky modules. When issuing the <code>module purge</code> command these modules are automatically reloaded. It is very important to realise that those modules will not just be kept \"as is\" but are in fact unloaded and loaded again as we shall see later that this may have consequences. It is still possible to force unload all these modules using <code>module --force purge</code> or selectively unload those using <code>module --force unload</code>.</p> <p>The sticky property is something that is defined in the module file and not used by the module files ot the HPE Cray Programming Environment, but we shall see that there is a partial workaround for this in some of the LUMI software stacks. The <code>init-lumi</code> module mentioned above though is a sticky module, as are the modules that activate a software stack so that you don't have to start from scratch if you have already chosen a software stack but want to clean up your environment.</p> <p>Let us look at the output of the <code>module avail</code> command, taken just after login on the system at the time of writing of these notes (the exact list of modules shown is a bit fluid):</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Next to the names of modules you sometimes see one or more letters. The <code>(D)</code> means that that is currently the default version of the module, the one that will be loaded if you do not specify a version. Note that the default version may depend on other modules that are already loaded as we have seen in the discussion of the programming environment.</p> <p>The <code>(L)</code> means that a module is currently loaded.</p> <p>The <code>(S)</code> means that the module is a sticky module.</p> <p>Next to the <code>rocm</code> module you see <code>(D:5.0.2:5.2.0)</code>.  The <code>D</code> means that this version of the module, <code>5.2.3</code>, is currently the default on the system. The two version numbers next to this module show that the module can also  be loaded as <code>rocm/5.0.2</code> and <code>rocm/5.2.0</code>. These are two modules that were removed from the system during the last update of the system, but version 5.2.3 can be loaded as a replacement of these modules so that software that used the removed modules may still work without recompiling.</p> <p>At the end of the overview the extensions are also shown. If this would be fully implemented on LUMI, the list might become very long. There is a way in Lmod to hide that output but unfortunately it does not work on LUMI yet due to another bug in the already old version of Lmod.</p>"},{"location":"intro-202310xx/04_Modules/#changing-how-the-module-list-is-displayed","title":"Changing how the module list is displayed","text":"<p>You may have noticed in the above example that we don't show directories of module files in the overview (as is the case on most clusters) but descriptive texts about the module group. This is just one view on the module tree though, and it can be changed easily by loading a  version of the <code>ModuleLabel</code> module.</p> <ul> <li><code>ModuleLabel/label</code> produces the default view of the previous example</li> <li><code>ModuleLabel/PEhierarchy</code> still uses descriptive texts but will show the whole      module hierarchy of the HPE Cray Programming Environment.</li> <li><code>ModuleLabel/system</code> does not use the descriptive texts but shows module directories instead.</li> </ul> <p>When using any kind of descriptive labels, Lmod can actually bundle module files from different  directories in a single category and this is used heavily when <code>ModuleLabel/label</code> is loaded  and to some extent also when <code>ModuleLabel/PEhierarchy</code> is loaded.</p> <p>It is rather hard to provide multiple colour schemes in Lmod, and as we do not know how your  terminal is configured it is also impossible to find a colour scheme that works for all users. Hence we made it possible to turn on and off the use of colours by Lmod through the <code>ModuleColour/on</code> and <code>ModuleColour/off</code> modules.</p> <p>In the future, as soon as we have a version of Lmod where module extensions function properly, we will also provide a module to turn on and off the display of extension in the output of <code>module avail</code> .</p> <p>We also hide some modules from regular users because we think they are not useful at all for regular users or not useful in the context you're in at the moment.  You can still load them if you know they exist and specify the full version but  you cannot see them with <code>module available</code>. It is possible though to still show most if not all of  them by loading <code>ModulePowerUser/LUMI</code>. Use this at your own risk however, we will not help you to make things work if you use modules that are hidden in the context you're in or if you try to use any module that was designed for us to maintain the system and is therefore hidden  from regular users.</p> <p>Example</p> <p>An example that will only become clear in the next session: When working with the software stack called <code>LUMI/22.08</code>, which is built upon the HPE Cray Programming Environment version 22.08, all (well, most) of the modules corresponding to other version of the Cray PE are hidden.</p>"},{"location":"intro-202310xx/04_Modules/#getting-help-with-the-module-help-command","title":"Getting help with the module help command","text":"<p>Lmod has the <code>module help</code> command to get help on modules</p> <pre><code>$ module help\n</code></pre> <p>without further arguments will show some help on the <code>module</code> command. </p> <p>With the name of a module specified, it will show the help information for the default version of that module, and with a full name and version specified it will show this information specifically for that version of the module. But note that <code>module help</code> can only show help for currently available modules.</p> <p>Try, e.g., the following commands:</p> <pre><code>$ module help cray-mpich\n$ module help cray-python/3.9.13.1\n$ module help buildtools/22.12\n</code></pre> <p>Lmod also has another command that produces more limited information (and is currently not fully exploited on LUMI): <code>module whatis</code>. It is more a way to tag a module with different kinds of information, some of  which has a special meaning for Lmod and is used at some places, e.g., in the output of <code>module spider</code> without arguments.</p> <p>Try, e.g.,:</p> <pre><code>$ module whatis Subversion\n$ module whatis Subversion/1.14.2\n</code></pre>"},{"location":"intro-202310xx/04_Modules/#a-note-on-caching","title":"A note on caching","text":"<p>Modules are stored as (small) files in the file system. Having a large module system with much software preinstalled for everybody means a lot of small files which will make our Lustre file system very unhappy. Fortunately Lmod does use caches by default. On LUMI we currently have no  system cache and only a user cache. That cache can be found in <code>$HOME/.lmod.d</code>. </p> <p>That cache is also refreshed automatically every 24 hours. You'll notice when this happens as, e.g., the <code>module spider</code> and <code>module available</code> commands will be slow during the rebuild. you may need to clean the cache after installing new software as on LUMI Lmod does not always detect changes to the installed software,</p> <p>Sometimes you may have to clear the cache also if you get very strange answers from  <code>module spider</code>. It looks like the non-standard way in which the HPE Cray Programming Environment does certain things in Lmod can cause inconsistencies in the cache. This is also one of the reasons whey we do not yet have a central cache for that  software that is installed in the central stacks as we are not sure when that cache is in good shape.</p>"},{"location":"intro-202310xx/04_Modules/#a-note-on-other-commands","title":"A note on other commands","text":"<p>As this tutorial assumes some experience with using modules on other clusters, we haven't paid much attention to some of the basic commands that are mostly the same across all three module environments implementations.  The <code>module load</code>, <code>module unload</code> and <code>module list</code> commands work largely as you would expect, though the output style of <code>module list</code> may be a little different from what you expect. The latter may show some inactive modules. These are modules that were loaded at some point, got unloaded when a module closer to the root of the hierarchy of the module system got unloaded, and they will be reloaded automatically when that module or an equivalent (family or name) module is loaded that makes this one or an equivalent module available again.</p> <p>Example</p> <p>To demonstrate this, try in a fresh login shell (with the lines starting with a <code>$</code> the commands that you should enter at the command prompt):</p> <pre><code>$ module unload craype-network-ofi\n\nInactive Modules:\n  1) cray-mpich\n\n$ module load craype-network-ofi\n\nActivating Modules:\n  1) cray-mpich/8.1.23\n</code></pre> <p>The <code>cray-mpich</code> module needs both a valid network architecture target module to be loaded (not <code>craype-network-none</code>) and a compiler module. Here we remove the network target module which inactivates the <code>cray-mpich</code> module, but the module gets reactivated again as soon as the network target module is reloaded.</p> <p>The <code>module swap</code> command is basically equivalent to a <code>module unload</code> followed by a <code>module load</code>.  With one argument it will look for a module with the same name that is loaded and unload that one  before loading the given module. With two modules, it will unload the first one and then load the second one. The <code>module swap</code> command is not really needed on LUMI as loading a conflicting module (name or family) will automatically unload the previously loaded one. However, in case of replacing  a module of the same family with a different name, <code>module swap</code> can be a little faster than just a <code>module load</code> as that command will need additional operations as in the first step it will  discover the family conflict and then try to resolve that in the following steps (but explaining that in detail would take us too far in the internals of Lmod).</p>"},{"location":"intro-202310xx/04_Modules/#links","title":"Links","text":"<p>These links were OK at the time of the course. This tutorial will age over time though and is not maintained but may be replaced with evolved versions when the course is organised again, so links may break over time.</p> <ul> <li>Lmod documentation and more specifically     the User Guide for Lmod which is the part specifically for regular users who do not     want to design their own modules.</li> <li>Information on the module environment in the LUMI documentation</li> </ul>"},{"location":"intro-202310xx/05_Software_stacks/","title":"LUMI Software Stacks","text":"<p>In this section we discuss</p> <ul> <li>Several of the ways in which we offer software on LUMI</li> <li>Managing software in our primary software stack which is based on EasyBuild</li> </ul>"},{"location":"intro-202310xx/05_Software_stacks/#the-software-stacks-on-lumi","title":"The software stacks on LUMI","text":""},{"location":"intro-202310xx/05_Software_stacks/#design-considerations","title":"Design considerations","text":"<ul> <li> <p>LUMI is a very leading edge and also an inhomogeneous machine. Leading edge often implies     teething problems and inhomogeneous doesn't make life easier either.</p> <ol> <li>It uses a novel interconnect which is an extension of Ethernet rather than being based on InfiniBand,      and that interconnect has a different software stack of your typical Mellanox InfiniBand cluster. </li> <li>It also uses a relatively new GPU architecture, AMD CDNA2, with an immature software ecosystem.      The GPU nodes are really GPU-first, with the interconnect cards connected directly to the GPU packages      and only one CPU socket, and another feature which is relatively new: the option to use a coherent unified memory     space between the CPU and GPUs, though of course very NUMA. This is a feature that has previously     only been seen in some clusters with NVIDIA P100 and V100 GPUs and IBM Power 8 and 9 CPUs used     for some USA pre-exascale systems, and of course in Apple Silicon M-series but then without the NUMA character     (except maybe for the Ultra version that consists of two dies).</li> <li>LUMI is also inhomogeneous because some nodes have zen2 processors while the two main compute partitions     have zen3-based CPUs, and the compute GPU nodes have AMD GPUs while the visualisation nodes have     NVIDIA GPUs. </li> </ol> <p>Given the novel interconnect and GPU we do expect that both system and application software will be immature at first and evolve quickly, hence we needed a setup that enables us to remain very agile, which leads to different compromises compared to a software stack for a more conventional and mature system as an x86 cluster with NVIDIA GPUs and Mellanox InfiniBand.</p> </li> <li> <p>Users also come to LUMI from 11 different channels, not counting subchannels as some countries have     multiple organisations managing allocations, and those channels all have different expectations about     what LUMI should be and what kind of users should be served. For our major stakeholder, the EuroHPC JU,     LUMI is a pre-exascale system meant to prepare users and applications to make use of future even large     systems, while some of the LUMI consortium countries see LUMI more as an extension of their tier-1 or     even tier-2 machines.</p> </li> <li> <p>The central support team of LUMI is also relatively small compared to the nature of LUMI with its     many different partitions and storage services and the expected number of projects and users.      Support from users coming in via the national channels will rely a lot on efforts from local organisations     also. So we must set up a system so that they can support their users without breaking things on     LUMI, and to work with restricted rights. And in fact, LUMI User Support team members also have very limited additional     rights on the machine compared to regular users or support people from the local organisations.     LUST is currently 9 FTE. Compare this to 41 people in the J\u00fclich Supercomputer Centre for software     installation and support only... (I give this number because it was mentioned in a a talk in the     EasyBuild user meeting in 2022.)</p> </li> <li> <p>The Cray Programming Environment is also a key part of LUMI and the environment for which we get     support from HPE Cray. It is however different from more traditional environments such as a typical     Intel oneAPI installation of a typical installation build around the GNU Compiler Collection and Open MPI     or MPICH. The programming environment is installed with the operating system rather than through the     user application software stack hence not managed through the tools used for the application software     stack, and it also works differently with its universal compiler wrappers that are typically configured     through modules. </p> </li> <li> <p>We also see an increasing need for customised setups. Everybody wants a central stack as long as their     software is in there but not much more as otherwise it is hard to find, and as long as software is      configured in the way they are used to. And everybody would like LUMI to look as much as possible      as their home system. But this is of course impossible. Moreover, there are more and more conflicts     between software packages and modules are only a partial solution to this problem. The success of     containers, conda and Python virtual environments is certainly to some extent explained by the      need for more customised setups and the need for multiple setups as it has become nearly impossible     to combine everything in a single setup due to conflicts between packages and the dependencies they need.</p> </li> </ul>"},{"location":"intro-202310xx/05_Software_stacks/#the-lumi-solution","title":"The LUMI solution","text":"<p>We tried to take all these considerations into account and came up with a solution that may look a little unconventional to many users.</p> <p>In principle there should be a high degree of compatibility between releases of the HPE Cray Programming Environment but we decided not to take the risk and build our software for a specific release of the  programming environment, which is also a better fit with the typical tools used to manage a scientific  software stack such as EasyBuild and Spack as they also prefer precise versions for all dependencies and compilers etc. We also made the stack very easy to extend. So we have many base libraries and some packages already pre-installed but also provide an easy and very transparent way to install additional packages in your project space in exactly the same way as we do for the central stack, with the same performance but the benefit that the installation can be customised more easily to the needs of your project. Not everybody needs the same configuration of GROMACS or LAMMPS or other big packages, and in fact a one-configuration-that-works-for-everybody may even be completely impossible due to conflicting options that cannot be used together.</p> <p>For the module system we could chose between two systems supported by HPE Cray. They support  Environment Modules with module files based on the TCL scripting language, but only the old version that is no longer really developed and not the newer versions 4 and 5 developed in France, and Lmod, a module system based on the LUA scripting language that also support many TCL module files through a translation layer. We chose to go with Lmod as LUA is an easier and more modern language to work with and as Lmod is much more powerful than Environment Modules 3, certainly for searching modules.</p> <p>To manage the software installations we could chose between EasyBuild, which is mostly developed in Europe and hence a good match with a EuroHPC project as EuroHPC wants to develop a European HPC technology stack from hardware to application software, and Spack, a package developed in the USA national labs. We chose to go with EasyBuild as our primary tool for which we also do some development.  However, as we shall see, our EasyBuild installation is not your typical EasyBuild installation that you may be accustomed with from clusters at your home institution. It uses toolchains specifically for the HPE Cray programming environment so recipes need to be adapted. We do offer an growing library of Cray-specific installation recipes though. The whole setup of EasyBuild is done such that you can build on top of the central software stack and such that your modules appear in your module view without having to add directories by hand to environment variables etc. You only need to point to the place where you want to install software for your project as we cannot automatically determine a suitable place. We do offer some help so set up Spack also but it is mostly offered \"as is\" an we will not do bug-fixing or development in Spack package files.</p>"},{"location":"intro-202310xx/05_Software_stacks/#software-policies","title":"Software policies","text":"<p>As any site, we also have a number of policies about software installation, and we're still further developing them as we gain experience in what we can do with the amount of people we have and what we cannot do.</p> <p>LUMI uses a bring-your-on-license model except for a selection of tools that are useful to a larger community. </p> <ul> <li>This is partly caused by the distributed user management as we do not even have the necessary     information to determine if a particular user can use a particular license, so we must shift that      responsibility to people who have that information, which is often the PI of your project.</li> <li>You also have to take into account that up to 20% of LUMI is reserved for industry use which makes      negotiations with software vendors rather difficult as they will want to push us onto the industrial     rather than academic pricing as they have no guarantee that we will obey to the academic license     restrictions. </li> <li>And lastly, we don't have an infinite budget. There was a questionnaire send out to      some groups even before the support team was assembled and that contained a number of packages that     by themselves would likely consume our whole software budget for a single package if I look at the      size of the company that produces the package and the potential size of their industrial market.      So we'd have to make choices and with any choice for a very specialised package you favour a few      groups. And there is also a political problem as without doubt the EuroHPC JU would prefer that we     invest in packages that are developed by European companies or at least have large development     teams in Europe.</li> </ul> <p>The LUMI User Support Team tries to help with installations of recent software but porting or bug correction in software is not our task. As a user, you have to realise that not all Linux or even supercomputer software will work on LUMI. This holds even more for software that comes only as a binary. The biggest problems are the GPU and anything that uses distributed memory and requires high performance from the interconnect. For example,</p> <ul> <li>software that use NVIDIA proprietary programming models and     libraries needs to be ported. </li> <li>Binaries that do only contain NVIDIA code paths, even if the programming     model is supported on AMD GPUs, will not run on LUMI. </li> <li>The LUMI interconnect requires libfabric     using a specific provider for the NIC used on LUMI, the so-called Cassini provider,      so any software compiled with an MPI library that     requires UCX, or any other distributed memory model build on top of UCX, will not work on LUMI, or at     least not work efficiently as there might be a fallback path to TCP communications. </li> <li>Even intra-node interprocess communication can already cause problems as there are three different kernel extensions     that provide more efficient interprocess messaging than the standard Linux mechanism. Many clusters     use knem for that but on LUMI xpmem is used. So software that is not build to support xpmem will     also fall back to the default mechanism or fail. </li> <li>Also, the MPI implementation needs to collaborate     with certain modules in our Slurm installation to start correctly and experience has shown that this     can also be a source of trouble as the fallback mechanisms that are often used do not work on LUMI. </li> <li>Containers solve none of these problems. There can be more subtle compatibility problems also.      As has been discussed earlier in the course, LUMI runs SUSE Linux and not Ubuntu which is popular on      workstations or a Red Hat-derived Linux popular on many clusters. Subtle differences between Linux      versions can cause compatibility problems that in some cases can be solved with containers. But containers     won't help you if they are build for different kernel extensions and hardware interfaces.</li> <li>The compute nodes also lack some Linux daemons that may be present on smaller clusters. HPE Cray use an     optimised Linux version called COS or Cray Operating System on the compute nodes. It is optimised to     reduce OS jitter and hence to enhance scalability of applications as that is after all the primary     goal of a pre-exascale machine. But that implies that certain Linux daemons that your software may      expect to find are not present on the compute nodes. D-bus comes to mind.</li> </ul> <p>Also, the LUMI user support team is too small to do all software installations which is why we currently state in our policy that a LUMI user should be capable of installing their software themselves or have another support channel. We cannot install every single piece of often badly documented research-quality code that was never meant to be used by people who don't understand the code.</p> <p>Another soft compatibility problem that I did not yet mention is that software that accesses tens of thousands of small files and abuses the file system as a database rather than using structured data formats designed to organise data on supercomputers is not welcome on LUMI. For that reason we also require to containerize conda and Python installations. We do offer a container-based wrapper that offers a way to install conda packages or to install Python packages with pip on top of  the Python provided by the <code>cray-python</code> module. On LUMI the tool is called lumi-container-wrapper but it may by some from CSC also be known as Tykky.</p>"},{"location":"intro-202310xx/05_Software_stacks/#organisation-of-the-software-in-software-stacks","title":"Organisation of the software in software stacks","text":"<p>On LUMI we have several software stacks.</p> <p>CrayEnv is the software stack for users who only need the Cray Programming Environment but want a more recent set of build tools etc than the OS provides. We also take care of a few issues that we will discuss on the next slide that are present right after login on LUMI.</p> <p>Next we have the stacks called \"LUMI\". Each one corresponds to a particular release of the HPE Cray Programming Environment. It is the stack in which we install software using that programming environment and mostly EasyBuild. The Cray Programming Environment modules are still used, but they are accessed through a replacement for the PrgEnv modules that is managed by EasyBuild. We have tuned versions for the 3 types of hardware in the regular LUMI system: zen2 CPUs in the login nodes and large memory nodes, zen3 for the  LUMI-C compute nodes and zen3 + MI250X for the LUMI-G partition. We were also planning to have a fourth version for the visualisation nodes with  zen2 CPUs combined with NVIDIA GPUs, but that may never materialise and we may manage those differently.</p> <p>In the far future we will also look at a stack based on the common EasyBuild toolchains as-is, but we do expect problems with MPI that will make this difficult to implement, and the common toolchains also do not yet support the AMD GPU ecosystem, so we make no promises whatsoever about a time frame for this development.</p> <p>We also have an extensible software stack based on Spack which has been pre-configured to use the compilers from the Cray PE. This stack is offered as-is for users who know how to use Spack, but we don't offer much support nor do we do any bugfixing in Spack.</p>"},{"location":"intro-202310xx/05_Software_stacks/#3-ways-to-access-the-cray-programming-environment-on-lumi","title":"3 ways to access the Cray Programming environment on LUMI.","text":""},{"location":"intro-202310xx/05_Software_stacks/#bare-environment-and-crayenv","title":"Bare environment and CrayEnv","text":"<p>Right after login you have a very bare environment available with the Cray Programming Environment with the PrgEnv-cray module loaded. It gives you basically what you can expect on a typical Cray system. There aren't many tools available, basically mostly only the tools in the base OS image and some tools that we are sure will not impact software installed in one of the software stacks. The set of target modules loaded is the one for the login nodes and not tuned to any particular node type. As a user you're fully responsible for managing the target modules, reloading them when needed or loading the appropriate set for the hardware you're using or want to cross-compile for.</p> <p>The second way to access the Cray Programming Environment is through the CrayEnv software stack. This stack offers an \"enriched\" version of the Cray environment. It takes care of the target modules: Loading or reloading CrayEnv will reload an optimal set of target modules for the node you're on. It also provides some additional  tools like newer build tools than provided with the OS. They are offered here and not in the bare environment to be sure that those tools don't create conflicts with software in other stacks. But otherwise the Cray Programming  Environment works exactly as you'd expect from this course.</p>"},{"location":"intro-202310xx/05_Software_stacks/#lumi-stack","title":"LUMI stack","text":"<p>The third way to access the Cray Programming Environment is through the LUMI software stacks, where each stack is based on a particular release of the HPE Cray Programming Environment. We advise against mixing with modules that came with other versions of the Cray PE, but they remain accessible although they are hidden from the default view for regular users. It ia also better to not use the PrgEnv modules, but the equivalent LUMI EasyBuild  toolchains instead as indicated by the following table:</p> HPE Cray PE LUMI toolchain What? <code>PrgEnv-cray</code> <code>cpeCray</code> Cray Compiler Environment <code>PrgEnv-gnu</code> <code>cpeGNU</code> GNU C/C++ and Fortran <code>PrgEnv-aocc</code> <code>cpeAOCC</code> AMD CPU compilers (login nodes and LUMI-C only) <code>PrgEnv-amd</code> <code>cpeAMD</code> AMD ROCm GPU compilers (LUMI-G only) <p>The cpeCray etc modules also load the MPI libraries and Cray LibSci just as the PrgEnv modules do. And we sometimes use this to work around problems in Cray-provided modules that we cannot change. </p> <p>This is also the environment in which we install most software, and from the name of the modules you can see which compilers we used.</p>"},{"location":"intro-202310xx/05_Software_stacks/#lumi-stack-module-organisation","title":"LUMI stack module organisation","text":"<p>To manage the heterogeneity in the hardware, the LUMI software stack uses two levels of modules</p> <p>First there are the LUMI/22.08, LUMI/22.12 and LUMI/23.03 modules. Each of the LUMI modules loads a particular version of the LUMI stack.</p> <p>The second level consists of partition modules. There is partition/L for the login and large memory nodes, partition/C for the regular compute nodes and partition/G for the AMD GPU nodes. We may have a separate partition for the visualisation nodes in the future but that is not clear yet.</p> <p>There is also a hidden partition/common module in which we install software that is available everywhere,  but we advise you to be careful to install software in there in your own installs as it is risky to rely on software in one of the regular partitions, and impossible in our EasyBuild setup.</p> <p>The LUMI module will automatically load the best partition module for the current hardware whenever it is loaded or reloaded. So if you want to cross-compile, you can do so by loading a different partition  module after loading the LUMI module, but you'll have to reload every time you reload the LUMI module.</p> <p>Hence you should also be very careful in your job scripts. On LUMI the environment from the login nodes is used when your job starts, so unless you switched to the suitable partition for the compute nodes, your job will start with the software stack for the login nodes. If in your job script you reload the  LUMI module it will instead switch to the software stack that corresponds to the type of compute node you're using and more optimised binaries can be available. If for some reason you'd like to use the same software on LUMI-C and on the login or large memory nodes and don't want two copies of locally installed software, you'll have to make sure that after reloading the LUMI module in your job script you explicitly load the partition/L module.</p>"},{"location":"intro-202310xx/05_Software_stacks/#easybuild-to-extend-the-lumi-software-stack","title":"EasyBuild to extend the LUMI software stack","text":""},{"location":"intro-202310xx/05_Software_stacks/#installing-software-on-hpc-systems","title":"Installing software on HPC systems","text":"<p>Software on HPC systems is rarely installed from RPMs for various reasons. Generic RPMs are rarely optimised for the specific CPU of the system as they have to work on a range of systems and including optimised code paths in a single executable for multiple architectures is hard to even impossible.  Secondly generic RPMs might not even work with the specific LUMI environment. They may not fully support the SlingShot interconnect and hence run at reduced speed, or they may need particular kernel modules or daemons that are not present on the system or they may not work well with the resource manager on the system. We expect this to happen especially with packages that  require specific MPI versions. Moreover, LUMI is a multi-user system so there is usually no \"one version fits all\". And we need a small system image as nodes are diskless which means that RPMs need to be relocatable so that they can be installed elsewhere.</p> <p>Spack and EasyBuild are the two most popular HPC-specific software build and installation frameworks.  These two systems usually install packages from sources so that the software can be adapted to the underlying hardware and operating system. They do offer a mean to communicate and execute installation instructions easily so that in practice once a package is well supported by these tools a regular user can install them also. Both packages make software available via modules so that you can customise your environment and select appropriate versions for your work.  And they do take care of dependency handling in a way that is compatible with modules.</p>"},{"location":"intro-202310xx/05_Software_stacks/#extending-the-lumi-stack-with-easybuild","title":"Extending the LUMI stack with EasyBuild","text":"<p>On LUMI EasyBuild is our primary software installation tool. We selected this as there is already a lot of experience with EasyBuild in several LUMI consortium countries and as it is also a tool developed in Europe which makes it a nice fit with EuroHPC's goal of creating a fully European HPC ecosystem.</p> <p>EasyBuild is fully integrated in the LUMI software stack. Loading the LUMI module will not only make centrally installed packages available, but also packages installed in your personal or project stack. Installing packages in that space is done by loading the EasyBuild-user module that will load a suitable version of EasyBuild and configure it for installation in a way that is compatible with the LUMI stack. EasyBuild will then use existing modules for dependencies if those are already on the system or in your personal or project stack.</p> <p>Note however that the build-in easyconfig files that come with EasyBuild do not work on LUMI at the moment.</p> <ul> <li>For the GNU toolchain we would have problems with MPI. EasyBuild uses Open MPI and that     needs to be configured differently to work well on LUMI, and there are also still issues with     getting it to collaborate with the resource manager as it is installed on LUMI.</li> <li>The Intel-based toolchains have their problems also. At the moment, the Intel compilers with the     AMD CPUs are a problematic cocktail. There have recently been performance and correctness problems      with the MKL math library and also failures with some versions of Intel MPI,      and you need to be careful selecting compiler options and not use <code>-xHost</code>     or the Intel compiler will simply optimize for a two decades old CPU.</li> </ul> <p>Instead we make our own EasyBuild build recipes that we also make available in the  LUMI-EasyBuild-contrib GitHub repository. The EasyBuild configuration done by the EasyBuild-user module will find a copy of that repository on the system or in your own install directory. The latter is useful if you always want the very latest, before we deploy it on the system. </p> <p>We also have the LUMI Software Library which documents all software for which we have EasyBuild recipes available.  This includes both the pre-installed software and the software for which we provide recipes in the LUMI-EasyBuild-contrib GitHub repository, and even instructions for some software that is not suitable for installation through EasyBuild or Spack, e.g., because it likes to write in its own directories while running.</p>"},{"location":"intro-202310xx/05_Software_stacks/#easybuild-recipes-easyconfigs","title":"EasyBuild recipes - easyconfigs","text":"<p>EasyBuild uses a build recipe for each individual package, or better said, each individual module as it is possible to install more than one software package in the same module. That installation description relies on either a generic or a specific installation process provided by an easyblock. The build recipes are called easyconfig files or simply easyconfigs and are Python files with  the extension <code>.eb</code>. </p> <p>The typical steps in an installation process are:</p> <ol> <li>Downloading sources and patches. For licensed software you may have to provide the sources as     often they cannot be downloaded automatically.</li> <li>A typical configure - build - test - install process, where the test process is optional and     depends on the package providing useable pre-installation tests.</li> <li>An extension mechanism can be used to install perl/python/R extension packages</li> <li>Then EasyBuild will do some simple checks (some default ones or checks defined in the recipe)</li> <li>And finally it will generate the module file using lots of information specified in the      EasyBuild recipe.</li> </ol> <p>Most or all of these steps can be influenced by parameters in the easyconfig.</p>"},{"location":"intro-202310xx/05_Software_stacks/#the-toolchain-concept","title":"The toolchain concept","text":"<p>EasyBuild uses the toolchain concept. A toolchain consists of compilers, an MPI implementation and some basic mathematics libraries. The latter two are optional in a toolchain. All these  components have a level of exchangeability as there are language standards, as MPI is standardised, and the math libraries that are typically included are those that provide a standard API for which several implementations exist. All these components also have in common that it is risky to combine  pieces of code compiled with different sets of such libraries and compilers because there can be conflicts in names in the libraries.</p> <p>On LUMI we don't use the standard EasyBuild toolchains but our own toolchains specifically for Cray and these are precisely the <code>cpeCray</code>, <code>cpeGNU</code>, <code>cpeAOCC</code> and <code>cpeAMD</code> modules already mentioned  before.</p> HPE Cray PE LUMI toolchain What? <code>PrgEnv-cray</code> <code>cpeCray</code> Cray Compiler Environment <code>PrgEnv-gnu</code> <code>cpeGNU</code> GNU C/C++ and Fortran <code>PrgEnv-aocc</code> <code>cpeAOCC</code> AMD CPU compilers (login nodes and LUMI-C only) <code>PrgEnv-amd</code> <code>cpeAMD</code> AMD ROCm GPU compilers (LUMI-G only) <p></p> <p>There is also a special toolchain called the SYSTEM toolchain that uses the compiler provided by the operating system. This toolchain does not fully function in the same way as the other toolchains when it comes to handling dependencies of a package and is therefore a bit harder to use. The EasyBuild designers had in mind that this compiler would only be used to bootstrap an EasyBuild-managed software stack, but we do use it for a bit more on LUMI as it offers us a relatively easy way to compile some packages also for the CrayEnv stack and do this in a way that they interact as little as possible with other software.</p> <p>It is not possible to load packages from different cpe toolchains at the same time. This is an EasyBuild restriction, because mixing libraries compiled with different compilers does not always work. This could happen, e.g., if a package compiled with the Cray Compiling Environment and one compiled with the GNU compiler collection would both use a particular  library, as these would have the same name and hence the last loaded one would be used by both executables (we don't use rpath or runpath linking in EasyBuild for those familiar with that technique).</p> <p>However, as we did not implement a hierarchy in the Lmod implementation of our software stack at the toolchain level, the module system will not protect you from these mistakes.  When we set up the software stack, most people in the support team considered it too misleading and difficult to ask users to first select the toolchain they want to use and then see the  software for that toolchain.</p> <p>It is however possible to combine packages compiled with one CPE-based toolchain with packages compiled with teh system toolchain, but we do avoid mixing those when linking as that may cause problems. The reason is that we try to use as much as possible static linking in the SYSTEM toolchain so that these packages are as independent as possible.</p> <p>And with some tricks it might also be possible to combine packages from the LUMI software stack with packages compiled with Spack, but one should make sure that no Spack packages are available when building as mixing libraries could cause problems. Spack uses rpath linking which is why this may work.</p>"},{"location":"intro-202310xx/05_Software_stacks/#easyconfig-names-and-module-names","title":"EasyConfig names and module names","text":"<p>There is a convention for the naming of an EasyConfig as shown on the slide. This is not mandatory, but EasyBuild will fail to automatically locate easyconfigs for dependencies  of a package that are not yet installed if the easyconfigs don't follow the naming convention. Each part of the name also corresponds to a parameter in the easyconfig  file.</p> <p>Consider, e.g., the easyconfig file <code>GROMACS-2021.4-cpeCray-22.08-PLUMED-2.8.0-CPU.eb</code>.</p> <ol> <li>The first part of the name, <code>GROMACS</code>, is the name of the package, specified by the     <code>name</code> parameter in the easyconfig, and is after installation also the name of the     module.</li> <li>The second part, <code>2021.4</code>, is the version of GROMACS and specified by the     <code>version</code> parameter in the easyconfig.</li> <li> <p>The next part, <code>cpeCray-22.08</code> is the name and version of the toolchain,     specified by the <code>toolchain</code> parameter in the easyconfig. The version of the     toolchain must always correspond to the version of the LUMI stack. So this is     an easyconfig for installation in <code>LUMI/22.08</code>.</p> <p>This part is not present for the SYSTEM toolchain</p> </li> <li> <p>The final part, <code>-PLUMED-2.8.0-CPU</code>, is the version suffix and used to provide     additional information and distinguish different builds with different options     of the same package. It is specified in the <code>versionsuffix</code> parameter of the     easyconfig.</p> <p>This part is optional.</p> </li> </ol> <p>The version, toolchain + toolchain version and versionsuffix together also combine to the version of the module that will be generated during the installation process. Hence this easyconfig file will generate the module  <code>GROMACS/2021.4-cpeCray-22.08-PLUMED-2.8.0-CPE</code>.</p>"},{"location":"intro-202310xx/05_Software_stacks/#installing","title":"Installing","text":""},{"location":"intro-202310xx/05_Software_stacks/#step-1-where-to-install","title":"Step 1: Where to install","text":"<p>Let's now discuss how you can extend the central LUMI software stack with packages that you need for your project.</p> <p>The default location for the EasyBuild user modules and software is in <code>$HOME/EasyBuild</code>. This is not the ideal place though as then the software is not available for other users in your project, and as the size of your home directory is also limited and cannot be expanded. The home file system on LUMI  is simply not meant to install software. However, as LUMI users can have multiple projects there is no easy way to figure out automatically where else to install software.</p> <p>The best place to install software is in your project directory so that it also becomes available for the whole project. After all, a project is meant to be a collaboration between all participants on a scientific problem. You'll need to point LUMI to the right location though and that has to be done by setting the environment variable <code>EBU_USER_PREFIX</code> to point to the location where you want to have your custom installation. Also don't forget to export that variable as otherwise the module system and EasyBuild will not find it when they need it. So a good choice would be  something like  <code>export EBU_USER_PREFIX=/project/project_465000000/EasyBuild</code>.  You have to do this before loading the <code>LUMI</code> module as it is then already used to ensure that user modules are included in the module search path. You can do this in your <code>.profile</code> or <code>.bashrc</code>.  This variable is not only used by EasyBuild-user to know where to install software, but also  by the <code>LUMI</code> - or actually the <code>partition</code> - module to find software so all users in your project who want to use the software should set that variable.</p>"},{"location":"intro-202310xx/05_Software_stacks/#step-2-configure-the-environment","title":"Step 2: Configure the environment","text":"<p>The next step is to configure your environment. First load the proper version of the LUMI stack for which you want to install software, and you may want to change to the proper partition also if you are cross-compiling.</p> <p>Once you have selected the software stack and partition, all you need to do to activate EasyBuild to install additional software is to load the <code>LUMI</code> module, load a partition module if you want a different one from the default, and  then load the <code>EasyBuild-user</code> module. In fact, if you switch to a different <code>partition</code>  or <code>LUMI</code> module after loading <code>EasyBuild-user</code> EasyBuild will still be correctly reconfigured  for the new stack and new partition. </p> <p>Cross-compilation which is installing software for a different partition than the one you're working on does not always work since there is so much software around with installation scripts that don't follow good practices, but when it works it is easy to do on LUMI by simply loading a different partition module than the one that is auto-loaded by the <code>LUMI</code> module.</p> <p>Note that the <code>EasyBuild-user</code> module is only needed for the installation process. For using the software that is installed that way it is sufficient to ensure that <code>EBU_USER_PREFIX</code> has the proper value before loading the <code>LUMI</code> module.</p>"},{"location":"intro-202310xx/05_Software_stacks/#step-3-install-the-software","title":"Step 3: Install the software.","text":"<p>Let's look at GROMACS as an example. I will not try to do this completely live though as the  installation takes 15 or 20 minutes. First we need to figure out for which versions of GROMACS we already have support. At the moment we have to use <code>eb -S</code> or <code>eb --search</code> for that. So in our example this is <pre><code>eb --search GROMACS\n</code></pre> We now also have the LUMI Software Library which lists all software that we manage via EasyBuild and make available either pre-installed on the system or as an EasyBuild recipe for user installation.</p> <p>Output of the search commands:</p> <p><code>eb --search GROMACS</code> produces:</p> <p> </p> <p>while <code>eb -S GROMACS</code> produces:</p> <p> </p> <p>The information provided by both variants of the search command is the same, but <code>-S</code> presents the information in a more compact form.</p> <p>Now let's take the variant <code>GROMACS-2021.4-cpeCray-22.08-PLUMED-2.8.0-CPU.eb</code>.  This is GROMACS 2021.4 with the PLUMED 2.8.0 plugin, build with the Cray compilers from <code>LUMI/22.08</code>, and a build meant for CPU-only systems. The <code>-CPU</code> extension is not always added for CPU-only system, but in case of GROMACS there already is a GPU version for AMD GPUs in active development so even before LUMI-G was active we chose to ensure that we could distinguish between GPU and CPU-only versions. To install it, we first run  <pre><code>eb \u2013r GROMACS-2021.4-cpeCray-22.08-PLUMED-2.8.0-CPU.eb \u2013D\n</code></pre> The <code>-D</code> flag tells EasyBuild to just perform a check for the dependencies that are needed when installing this package, while the <code>-r</code> argument is needed to tell EasyBuild to also  look for dependencies in a preset search path. The installation of dependencies is not automatic since there are scenarios where this is not desired and it cannot be turned off as easily as it can be turned on.</p> <p>The output of this command looks like:</p> <p></p> <p></p> <p>Looking at the output we see that EasyBuild will also need to install <code>PLUMED</code> for us. But it will do so automatically when we run <pre><code>eb \u2013r GROMACS-2021.4-cpeCray-22.08-PLUMED-2.8.0-CPU.eb\n</code></pre></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>This takes too long to wait for, but once it finished the software should be available and you should be able to see the module in the output of <pre><code>module avail\n</code></pre></p>"},{"location":"intro-202310xx/05_Software_stacks/#step-3-install-the-software-note","title":"Step 3: Install the software - Note","text":"<p>There is a little problem though that you may run into. Sometimes the module does not show up immediately. This is because Lmod keeps a cache when it feels that Lmod searches become too slow and often fails to detect that the cache is outdated. The easy solution is then to simply remove the cache which is in <code>$HOME/.lmod.d/.cache</code>,  which you can do with  <pre><code>rm -rf $HOME/.lmod.d/.cache\n</code></pre> And we have seen some very rare cases where even that did not help likely because some internal data structures in Lmod where corrupt. The easiest way to solve this is to simply log out and log in again and rebuild your environment.</p> <p>Installing software this way is 100% equivalent to an installation in the central software tree. The application is compiled in exactly the same way as we would do and served from the same file systems. But it helps keep the output of <code>module avail</code> reasonably short and focused on your projects, and it puts you in control of installing updates. For instance, we may find out that something in a module does not work for some users and that it needs to be re-installed.  Do this in the central stack and either you have to chose a different name or risk breaking running jobs as the software would become unavailable during the re-installation and also jobs may get confused if they all of a sudden find different binaries. However, have this in your own stack extension and you can update whenever it suits your project best or even not update at all if  you figure out that the problem we discovered has no influence on your work.</p>"},{"location":"intro-202310xx/05_Software_stacks/#more-advanced-work","title":"More advanced work","text":"<p>You can also install some EasyBuild recipes that you got from support. For this it is best to create a subdirectory where you put those files, then go into that directory and run  something like <pre><code>eb my_recipe.eb -r . </code></pre> The dot after the <code>-r</code> is very important here as it does tell EasyBuild to also look for  dependencies in the current directory, the directory where you have put the recipes you got from support, but also in its subdirectories so for speed reasons you should not do this just in your home directory but in a subdirectory that only contains those files.</p> <p>In some cases you will have to download sources by hand as packages don't allow to download  software unless you sign in to their web site first. This is the case for a lot of licensed software, for instance, for VASP. We'd likely be in violation of the license if we would put the download somewhere where EasyBuild can find it, and it is also a way for us to ensure that you have a license for VASP. For instance,  <pre><code>eb --search VASP\n</code></pre> will tell you for which versions of VASP we already have build instructions, but you will still have to download the file that the EasyBuild recipe expects. Put it somewhere in a directory, and then from that directory run EasyBuild, for instance for VASP 6.3.0 with the GNU compilers: <pre><code>eb VASP-6.3.0-cpeGNU-22.08.eb \u2013r . </code></pre></p>"},{"location":"intro-202310xx/05_Software_stacks/#more-advanced-work-2-repositories","title":"More advanced work (2): Repositories","text":"<p>It is also possible to have your own clone of the <code>LUMI-EasyBuild-contrib</code> GitHub repository in your <code>$EBU_USER_PREFIX</code> subdirectory if you want the latest and greatest before it is in the centrally maintained clone of the repository. All you need to do is <pre><code>cd $EBU_USER_PREFIX\ngit clone https://github.com/Lumi-supercomputer/LUMI-EasyBuild-contrib.git\n</code></pre> and then of course keep the repository up to date.</p> <p>And it is even possible to maintain your own GitHub repository. The only restrictions are that it should also be in <code>$EBU_USER_PREFIX</code> and that the subdirectory should be called <code>UserRepo</code>, but that doesn't stop you from using a different name for the repository on GitHub. After cloning your GitHub version you can always change the name of the directory. The structure should also be compatible with the structure that EasyBuild uses, so easyconfig files go in <code>$EBU_USER_PREFIX/easybuild/easyconfigs</code>.</p>"},{"location":"intro-202310xx/05_Software_stacks/#more-advanced-work-3-reproducibility","title":"More advanced work (3): Reproducibility","text":"<p>EasyBuild also takes care of a high level of reproducibility of installations.</p> <p>It will keep a copy of all the downloaded sources in the <code>$EBU_USER_PREFIX/sources</code> subdirectory, and use that source file again rather than downloading it again. Of course in some cases those \"sources\" could be downloaded tar files with binaries instead as EasyBuild can install downloaded binaries or relocatable RPMs. And if you know the structure of those directories, this is also a place where you could manually put the downloaded installation files for licensed software.</p> <p>Moreover, EasyBuild also keeps copies of all installed easyconfig files in two locations.</p> <ol> <li>There is a copy in <code>$EBU_USER_PREFIX/ebrepo_files</code>. And in fact, EasyBuild will use this version     first if you try to re-install and did not delete this version first. This is a policy     we set on LUMI which has both its advantages and disadvantages. The advantage is that it ensures     that the information that EasyBuild has about the installed application is compatible with what is     in the module files. But the disadvantage of course is that if you install an EasyConfig file     without being in the subdirectory that contains that file, it is easily overlooked that it     is installing based on the EasyConfig in the <code>ebrepo_files</code> subdirectory and not based on the     version of the recipe that you likely changed and is in your user repository or one of the      other repositories that EasyBuild uses.</li> <li>The second copy is with the installed software in <code>$EBU_USER_PREFIX/SW</code> in a subdirectory     called <code>easybuild</code>. This subdirectory is meant to have all information about how EasyBuild     installed the application, also some other files that play a role in the installation process, and hence     to help in reproducing an installation or checking what's in an existing installation. It is     also the directory where you will find the extensive log file with all commands executed during     the installation and their output.</li> </ol>"},{"location":"intro-202310xx/05_Software_stacks/#easybuild-tips-and-tricks","title":"EasyBuild tips and tricks","text":"<p>Updating the version of a package often requires only trivial changes in the easyconfig file. However, we do tend to use checksums for the sources so that we can detect if the available sources have changed. This may point to files being tampered with, or other changes that might need us to be a bit more careful when installing software and check a bit more again.  Should the checksum sit in the way, you can always disable it by using  <code>--ignore-checksums</code> with the <code>eb</code> command.</p> <p>Updating an existing recipe to a new toolchain might be a bit more involving as you also have to make build recipes for all dependencies. When we update a toolchain on the system, we often bump the versions of all installed libraries to one of the latest versions to have most bug fixes and security patches in the software stack, so you need to check for those versions also to avoid installing yet another unneeded version of a library.</p> <p>We provide documentation on the available software that is either pre-installed or can be user-installed with EasyBuild in the  LUMI Software Library. For most packages this documentation does also contain information about the license. The user documentation for some packages gives more information about how to use the package on LUMI, or sometimes also about things that do not work. The documentation also shows all EasyBuild recipes, and for many packages there is  also some technical documentation that is more geared towards users who want to build or modify recipes. It sometimes also tells why we did things in a particular way.</p>"},{"location":"intro-202310xx/05_Software_stacks/#easybuild-training-for-advanced-users-and-developers","title":"EasyBuild training for advanced users and developers","text":"<p>Pointers to all information about EasyBuild can be found on the EasyBuild web site  easybuild.io. This page also includes links to training materials, both written and as recordings on YouTube, and the EasyBuild documentation.</p> <p>Generic EasyBuild training materials are available on  easybuilders.github.io/easybuild-tutorial. The site also contains a LUST-specific tutorial oriented towards Cray systems.</p> <p>There is also a later course developed by LUST for developers of EasyConfigs for LUMI that can be found on  lumi-supercomputer.github.io/easybuild-tutorial.</p>"},{"location":"intro-202310xx/06_Exercises_1/","title":"Exercises 1: Modules, the HPE Cray PE and EasyBuild","text":"<p>See the instructions to set up for the exercises.</p>"},{"location":"intro-202310xx/06_Exercises_1/#exercises-on-the-use-of-modules","title":"Exercises on the use of modules","text":"<ol> <li> <p>The <code>Bison</code> program installed in the OS image is pretty old (version 3.0.4) and     we want to use a newer one. Is there one available on LUMI?</p> Click to see the solution. <pre><code>module spider Bison\n</code></pre> <p>tells us that there are indeed newer versions available on the system. </p> <p>The versions that have a compiler name (usually <code>gcc</code>) in their name followed by some seemingly random characters are installed with Spack and not in the CrayEnv or LUMI environments.</p> <p>To get more information about <code>Bison/3.8.2</code>: </p> <pre><code>module spider Bison/3.8.2\n</code></pre> <p>tells us that Bison 3.8.2 is provided by a couple of <code>buildtools</code> modules and available in all partitions in several versions of the <code>LUMI</code> software stack and in <code>CrayEnv</code>.</p> <p>Alternatively, in this case</p> <pre><code>module keyword Bison\n</code></pre> <p>would also have shown that Bison is part of several versions of the <code>buildtools</code> module.</p> <p>The <code>module spider</code> command is often the better command if you use names that with a high  likelihood could be the name of a package, while <code>module keyword</code> is often the better choice for words that are more a keyword. But if one does not return the solution it is a good idea  to try the other one also.</p> </li> <li> <p>The <code>htop</code> command is a nice alternative for the <code>top</code> command with a more powerful user interface.     However, typing <code>htop</code> on the command line produces an error message. Can you find and run <code>htop</code>?</p> Click to see the solution. <p>We can use either <code>module spider htop</code> or <code>module keyword htop</code> to find out that <code>htop</code> is indeed available on the system. With <code>module keyword htop</code> we'll find out immediately that it is in the  <code>systools</code> modules and some of those seem to be numbered after editions of the LUMI stack suggesting that they may be linked to a stack, with <code>module spider</code> you'll first see that it is an extension of a module and see the versions. You may again see some versions installed with Spack.</p> <p>Let's check further for <code>htop/3.2.1</code> that should exist according to <code>module spider htop</code>:</p> <pre><code>module spider htop/3.2.1\n</code></pre> <p>tells us that this version of <code>htop</code> is available in all partitions of <code>LUMI/22.08</code> and <code>LUMI/22.06</code>, and in <code>CrayEnv</code>. Let us just run it in the <code>CrayEnv</code> environment:</p> <pre><code>module load CrayEnv\nmodule load systools/22.08\nhtop\n</code></pre> <p>(You can quit <code>htop</code> by pressing <code>q</code> on the keyboard.)</p> </li> <li> <p>In the future LUMI will offer Open OnDemand as a browser-based interface to LUMI that will also enable     running some graphical programs. At the moment the way to do this is through a so-called VNC server.     Do we have such a tool on LUMI, and if so, how can we use it?</p> Click to see the solution. <p><code>module spider VNC</code> and <code>module keyword VNC</code> can again both be used to check if there is software available to use VNC. Both will show that there is a module <code>lumi-vnc</code> in several versions. If you  try loading the older ones of these (the version number points at the date of some scripts) you will notice that some produce a warning as they are deprecated. However, when installing a new version we  cannot remove older ones in one sweep, and users may have hardcoded full module names in scripts they use to set their environment, so we chose to not immediate delete these older versions.</p> <p>One thing you can always try to get more information about how to run a program, is to ask for the help information of the module. For this to work the module must first be available, or you have to use  <code>module spider</code> with the full name of the module. We see that version <code>20230110</code> is the newest version of the module, so let's try that one:</p> <pre><code>module spider lumi-vnc/20230110\n</code></pre> <p>The output may look a little strange as it mentions <code>init-lumi</code> as one of the modules that you can load. That is because this tool is available even outside <code>CrayEnv</code> or the LUMI stacks. But this command also shows a long help test telling you how to use this module (though it does assume some familiarity with how X11 graphics work on Linux).</p> <p>Note that if there is only a single version on the system, as is the case for the course in May 2023, the <code>module spider VNC</code> command without specific version or correct module name will already display the help information.</p> </li> <li> <p>Search for the <code>bzip2</code> tool (and not just the <code>bunzip2</code> command as we also need the <code>bzip2</code> command) and make      sure that you can use software compiled with the Cray compilers in the LUMI stacks in the same session.</p> Click to see the solution. <pre><code>module spider bzip2\n</code></pre> <p>shows that there are versions of <code>bzip2</code> for several of the <code>cpe*</code> toolchains and in several versions of the LUMI software stack.</p> <p>Of course we prefer to use a recent software stack, the <code>22.08</code> or <code>22.12</code> (but as of early May 2023,  there is a lot more software ready-to-install for <code>22.08</code>).  And since we want to use other software compiled with the Cray compilers also, we really want a <code>cpeCray</code> version to avoid conflicts between  different toolchains. So the module we want to load is <code>bzip2/1.0.8-cpeCray-22.08</code>.</p> <p>To figure out how to load it, use</p> <pre><code>module spider bzip2/1.0.8-cpeCray-22.08\n</code></pre> <p>and see that (as expected from the name) we need to load <code>LUMI/22.08</code> and can then use it in any of the partitions.</p> </li> </ol>"},{"location":"intro-202310xx/06_Exercises_1/#exercises-on-compiling-software-by-hand","title":"Exercises on compiling software by hand","text":"<p>These exercises are optional during the session, but useful if you expect  to be compiling software yourself. The source files mentioned can be found in the subdirectory CPE of the download.</p>"},{"location":"intro-202310xx/06_Exercises_1/#compilation-of-a-program-1-a-simple-hello-world-program","title":"Compilation of a program 1: A simple \"Hello, world\" program","text":"<p>Four different implementations of a simple \"Hello, World!\" program are provided in the <code>CPE</code> subdirectory:</p> <ul> <li><code>hello_world.c</code> is an implementation in C,</li> <li><code>hello_world.cc</code> is an implementation in C++,</li> <li><code>hello_world.f</code> is an implementation in Fortran using the fixed format source form,</li> <li><code>hello_world.f90</code> is an implementation in Fortran using the more modern free format source form.</li> </ul> <p>Try to compile these programs using the programming environment of your choice.</p> Click to see the solution. <p>We'll use the default version of the programming environment (22.12 at the moment of the course in May 2023), but in case you want to use a particular version, e.g., the 22.08 version, and want to be very sure that all modules are loaded correctly from the start you could consider using</p> <pre><code>module load cpe/22.08\nmodule load cpe/22.08\n</code></pre> <p>So note that we do twice the same command as the first iteration does not always succeed to reload all modules in the correct version. Do not combine both lines into a single <code>module load</code> statement as that would again trigger the bug that prevents all modules to be reloaded in the first iteration.</p> <p>The sample programs that we asked you to compile do not use the GPU. So there are three programming environments that we can use: <code>PrgEnv-gnu</code>, <code>PrgEnv-cray</code> and <code>PrgEnv-aocc</code>. All three will work, and they work almost the same.</p> <p>Let's start with an easy case, compiling the C version of the program with the GNU C compiler. For this all we need to do is</p> <pre><code>module load PrgEnv-gnu\ncc hello_world.c\n</code></pre> <p>which will generate an executable named <code>a.out</code>.  If you are not comfortable using the default version of <code>gcc</code> (which produces the warning message when loading the <code>PrgEnv-gnu</code> module) you can always load the <code>gcc/11.2.0</code> module instead after loading <code>PrgEnv-gnu</code>.</p> <p>Of course it is better to give the executable a proper name which can be done with the <code>-o</code> compiler option:</p> <pre><code>module load PrgEnv-gnu\ncc hello_world.c -o hello_world.x\n</code></pre> <p>Try running this program:</p> <pre><code>./hello_world.x\n</code></pre> <p>to see that it indeed works. We did forget another important compiler option, but we'll discover that in the next exercise.</p> <p>The other programs are equally easy to compile using the compiler wrappers:</p> <pre><code>CC hello_world.cc -o hello_world.x\nftn hello_world.f -o hello_world.x\nftn hello_world.f90 -o hello_world.x\n</code></pre>"},{"location":"intro-202310xx/06_Exercises_1/#compilation-of-a-program-2-a-program-with-blas","title":"Compilation of a program 2: A program with BLAS","text":"<p>In the <code>CPE</code> subdirectory you'll find the C program <code>matrix_mult_C.c</code> and the Fortran program <code>matrix_mult_F.f90</code>. Both do the same thing: a matrix-matrix multiplication using the 6 different orders of the three nested loops involved in doing a matrix-matrix multiplication, and a call to the BLAS routine DGEMM that does the same for comparison.</p> <p>Compile either of these programs using the Cray LibSci library for the BLAS routine. Do not use OpenMP shared memory parallelisation. The code does not use MPI.</p> <p>The resulting executable takes one command line argument, the size of the square matrix. Run the script using <code>1000</code> for the matrix size and see what happens.</p> <p>Note that the time results may be very unreliable as we are currently doing this on the login nodes. In the session of Slurm you'll learn how to request compute nodes and it might be interesting to redo this on a compute node with a larger matrix size as the with a matrix size of 1000 all data may stay in the third level cache and you will not notice the differences that you should note. Also, because these nodes are shared with a lot of people any benchmarking is completely unreliable.</p> <p>If this program takes more than half a minute or so before the first result line in the table, starting with <code>ijk-variant</code>, is printed, you've very likely done something wrong (unless the load on the system is extreme). In fact, if you've done things well the time reported for the <code>ijk</code>-variant should be well under 3 seconds for both the C and Fortran versions...</p> Click to see the solution. <p>Just as in the previous exercise, this is a pure CPU program so we can chose between the same three programming environments.</p> <p>The one additional \"difficulty\" is that we need to link with the BLAS library. This is very easy however in  the HPE Cray PE if you use the compiler wrappers rather than calling the compilers yourself: you only need to make sure that the <code>cray-libsci</code> module is loaded and the wrappers will take care of the rest. And on most systems (including LUMI) this module will be loaded automatically when you load the <code>PrgEnv-*</code> module.</p> <p>To compile with the GNU C compiler, all you need to do is</p> <pre><code>module load PrgEnv-gnu\ncc -O3 matrix_mult_C.c -o matrix_mult_C_gnu.x\n</code></pre> <p>will generate the executable <code>matrix_mult_C_gnu.x</code>.</p> <p>Note that we add the <code>-O3</code> option and it is very important to add either <code>-O2</code> or <code>-O3</code> as by default the GNU compiler will generate code without any optimization for debugging purposes, and that code is in this case easily five times or more slower. So if you got much longer run times than indicated this is likely the mistake that you made.</p> <p>To use the Cray C compiler instead only one small change is needed: Loading a different programming  environment module:</p> <pre><code>module load PrgEnv-cray\ncc -O3 matrix_mult_C.c -o matrix_mult_C_cray.x\n</code></pre> <p>will generate the executable <code>matrix_mult_C_cray.x</code>.</p> <p>Likewise for the AMD AOCC compiler we can try with loading yet another <code>PrgEnv-*</code> module:</p> <pre><code>module load PrgEnv-aocc\ncc -O3 matrix_mult_C.c -o matrix_mult_C_aocc.x\n</code></pre> <p>but it turns out that this fails with linker error messages about not being able to find the <code>sin</code> and <code>cos</code> functions. When using the AOCC compiler the <code>libm</code> library with basic math functions is not linked automatically, but this is easily done by adding the <code>-lm</code> flag:</p> <pre><code>module load PrgEnv-aocc\ncc -O3 matrix_mult_C.c -lm -o matrix_mult_C_aocc.x\n</code></pre> <p>For the Fortran version of the program we have to use the <code>ftn</code> compiler wrapper instead, and the issue with the math libraries in the AOCC compiler does not occur. So we get</p> <pre><code>module load PrgEnv-gnu\nftn -O3 matrix_mult_F.f90 -o matrix_mult_F_gnu.x\n</code></pre> <p>for the GNU Fortran compiler,</p> <pre><code>module load PrgEnv-cray\nftn -O3 matrix_mult_F.f90 -o matrix_mult_F_cray.x\n</code></pre> <p>for the Cray Fortran compiler and</p> <pre><code>module load PrgEnv-aocc\nftn -O3 matrix_mult_F.f90 -o matrix_mult_F_aocc.x\n</code></pre> <p>for the AMD Fortran compiler.</p> <p>When running the program you will see that even though the 6 different loop orderings  produce the same result, the time needed to compile the matrix-matrix product is very different and those differences would be even more pronounced with bigger matrices (which you can do after the session on using Slurm).</p> <p>The exercise also shows that not all codes are equal even if they produce a result of the same quality. The six different loop orderings run at very different speed, and none of our simple implementations can beat a good library, in this case the BLAS library included in LibSci.</p> <p>The results with the Cray Fortran compiler are particularly interesting. The result for the BLAS library is slower which we do not yet understand, but it also turns out that  for four of the six loop orderings we get the same result as with the BLAS library DGEMM routine. It looks like the compiler simply recognized that this was code for a matrix-matrix multiplication and replaced it with a call to the BLAS library. The Fortran 90 matrix multiplication is also replaced by a call of the DGEMM routine. To confirm all this, unload the <code>cray-libsci</code> module and try to compile again and you will see five error messages about not being able to find DGEMM.</p>"},{"location":"intro-202310xx/06_Exercises_1/#compilation-of-a-program-3-a-hybrid-mpiopenmp-program","title":"Compilation of a program 3: A hybrid MPI/OpenMP program","text":"<p>The file <code>mpi_omp_hello.c</code> is a hybrid MPI and OpenMP C program that sends a message from each thread in each MPI rank. It is basically a simplified version of the programs found in the <code>lumi-CPEtools</code> modules that can be used to quickly check  the core assignment in a hybrid MPI and OpenMP job (see later in this tutorial). It is again just a CPU-based program.</p> <p>Compile the program with your favourite C compiler on LUMI.</p> <p>We have not yet seen how to start an MPI program. However, you can run the executable on the login nodes and it will then contain just a single MPI rank. </p> Click to see the solution. <p>In the HPE Cray PE environment, you don't use <code>mpicc</code> to compile a C MPI program, but you just use the <code>cc</code> wrapper as for any other C program. To enable MPI you  have to make sure that the <code>cray-mpich</code> module is loaded. This module will usually be loaded by loading one of the <code>PrgEnv-*</code> modules, but only if the right network target module, which is <code>craype-network-ofi</code>, is also already loaded. </p> <p>Compiling the program is very simple:</p> <pre><code>module load PrgEnv-gnu\ncc -O3 -fopenmp mpi_omp_hello.c -o mpi_omp_hello_gnu.x\n</code></pre> <p>to compile with the GNU C compiler, </p> <pre><code>module load PrgEnv-cray\ncc -O3 -fopenmp mpi_omp_hello.c -o mpi_omp_hello_cray.x\n</code></pre> <p>to compile with the Cray C compiler, and</p> <pre><code>module load PrgEnv-aocc\ncc -O3 -fopenmp mpi_omp_hello.c -o mpi_omp_hello_aocc.x\n</code></pre> <p>to compile with the AMD AOCC compiler.</p> <p>To run the executables it is not even needed to have the respective <code>PrgEnv-*</code> module loaded since the binaries will use a copy of the libraries stored in a default directory, though there have been bugs in the past preventing this to work with <code>PrgEnv-aocc</code>.</p>"},{"location":"intro-202310xx/06_Exercises_1/#information-in-the-lumi-software-library","title":"Information in the LUMI Software Library","text":"<p>Explore the LUMI Software Library.</p> <ul> <li>Search for information for the package ParaView and quickly read through the page</li> </ul> Click to see the solution. <p>Link to the ParaView documentation</p> <p>It is an example of a package for which we have both user-level and some technical information. The page will first show some license information, then the actual user information which in case of this package is very detailed and long. But it is also a somewhat complicated package to use. It will become easier when LUMI evolves a bit further, but there will always be some pain. Next comes the more technical part: Links to the EasyBuild recipe and some information about how we build the package.</p> <p>We currently only provide ParaView in the cpeGNU toolchain. This is because it has a lot of dependencies that are not trivial to compile and to port to the other compilers on the system, and EasyBuild is  strict about mixing compilers basically because it can cause a lot of problems, e.g., due to conflicts between OpenMP runtimes.</p>"},{"location":"intro-202310xx/06_Exercises_1/#installing-software-with-easybuild","title":"Installing software with EasyBuild","text":"<p>These exercises are based on material from the EasyBuild tutorials (and we have a special version for LUMI also).</p> <p>Note: If you want to be able to uninstall all software installed through the exercises easily, we suggest you make a separate EasyBuild installation for the course, e.g., in <code>/scratch/project_465000523/$USER/eb-course</code> if you make the exercises during the course:</p> <ul> <li>Start from a clean login shell with only the standard modules loaded.</li> <li> <p>Set <code>EBU_USER_PREFIX</code>: </p> <pre><code>export EBU_USER_PREFIX=/scratch/project_465000523/$USER/eb-course\n</code></pre> <p>You'll need to do that in every shell session where you want to install or use that software.</p> </li> <li> <p>From now on you can again safely load the necessary <code>LUMI</code> and <code>partition</code> modules for the exercise.</p> </li> <li> <p>At the end, when you don't need the software installation anymore, you can simply remove the directory     that you just created.</p> <pre><code>rm -rf /scratch/project_465000523/$USER/eb-course\n</code></pre> </li> </ul>"},{"location":"intro-202310xx/06_Exercises_1/#installing-a-simple-program-without-dependencies-with-easybuild","title":"Installing a simple program without dependencies with EasyBuild","text":"<p>The LUMI Software Library contains the package <code>eb-tutorial</code>. Install the version of the package for the <code>cpeCray</code> toolchain in the 22.08 version of the software stack.</p> <p>At the time of this course, in early May 2023, we're still working on EasyBuild build recipes for the 22.12 version of the software stack.</p> Click to see the solution. <ul> <li> <p>We can check the      eb-tutorial page     in the      LUMI Software Library     if we want to see more information about the package.</p> <p>You'll notice that there are versions of the EasyConfigs for <code>cpeGNU</code> and <code>cpeCray</code>. As we want to install software with the <code>cpeCray</code> toolchain for <code>LUMI/22.08</code>, we'll need the <code>cpeCray-22.08</code> version which is the EasyConfig <code>eb-tutorial-1.0.1-cpeCray-22.08.eb</code>.</p> </li> <li> <p>Obviously we need to load the <code>LUMI/22.08</code> module. If we would like to install software     for the CPU compute nodes, you need to also load <code>partition/C</code>.     To be able to use EasyBuild, we also need the <code>EasyBuild-user</code> module.</p> <pre><code>module load LUMI/22.08 partition/C\nmodule load EasyBuild-user\n</code></pre> </li> <li> <p>Now all we need to do is run the <code>eb</code> command from EasyBuild to install the software.</p> <p>Let's however take the slow approach and first check if what dependencies the package needs:</p> <pre><code>eb eb-tutorial-1.0.1-cpeCray-22.08.eb -D\n</code></pre> <p>We can do this from any directory as the EasyConfig file is already in the LUMI Software Library and will be located automatically by EasyBuild. You'll see that all dependencies are already on  the system so we can proceed with the installation:</p> <pre><code>eb eb-tutorial-1.0.1-cpeCray-22.08.eb \n</code></pre> </li> <li> <p>After this you should have a module <code>eb-tutorial/1.0.1-cpeCray-22.08</code> but it may not show up      yet due to the caching of Lmod. Try</p> <pre><code>module av eb-tutorial/1.0.1-cpeCray-22.08\n</code></pre> <p>If this produces an error message complaining that the module cannot be found, it is time to clear the Lmod cache:</p> <pre><code>rm -rf $HOME/.lmod.d/.cache\n</code></pre> </li> <li> <p>Now that we have the module, we can check what it actually does:</p> <pre><code>module help eb-tutorial/1.0.1-cpeCray-22.08\n</code></pre> <p>and we see that it provides the <code>eb-tutorial</code> command.</p> </li> <li> <p>So let's now try to run this command:</p> <pre><code>module load eb-tutorial/1.0.1-cpeCray-22.08\neb-tutorial\n</code></pre> <p>Note that if you now want to install one of the other versions of this module, EasyBuild will complain that some modules are loaded that it doesn't like to see, including the <code>eb-tutorial</code> module and the <code>cpeCray</code> modules so it is better to unload those first:</p> <pre><code>module unload cpeCray eb-tutorial\n</code></pre> </li> </ul>"},{"location":"intro-202310xx/06_Exercises_1/#installing-an-easyconfig-given-to-you-by-lumi-user-support","title":"Installing an EasyConfig given to you by LUMI User Support","text":"<p>Sometimes we have no solution ready in the LUMI Software Library, but we prepare one or more custom EasyBuild recipes for you. Let's mimic this case. In practice we would likely send  those as attachments to a mail from the ticketing system and you would be asked to put them in a separate directory (basically since putting them at the top of your home directory would in some cases let EasyBuild search your whole home directory for dependencies which would be a very slow process).</p> <p>You've been given two EasyConfig files to install a tool called <code>py-eb-tutorial</code> which is in fact a Python package that uses the <code>eb-tutorial</code> package installed in the previous exercise. These EasyConfig files are in the <code>EasyBuild</code> subdirectory of the exercises for this course. In the first exercise you are asked to install the version of <code>py-eb-tutorial</code> for the <code>cpeCray/22.08</code> toolchain.</p> Click to see the solution. <ul> <li> <p>Go to the <code>EasyBuild</code> subdirectory of the exercises and check that it indeed contains the     <code>py-eb-tutorial-1.0.0-cpeCray-22.08-cray-python-3.9.12.1.eb</code> and     <code>py-eb-tutorial-1.0.0-cpeGNU-22.08-cray-python-3.9.12.1.eb</code> files.     It is the first one that we need for this exercise.</p> <p>You can see that we have used a very long name as we are also using a version suffix to make clear which version of Python we'll be using.</p> </li> <li> <p>Let's first check for the dependencies (out of curiosity):</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeCray-22.08-cray-python-3.9.12.1.eb -D\n</code></pre> <p>and you'll see that all dependencies are found (at least if you made the previous exercise  successfully). You may find it strange that it shows no Python module but that is because we are using the <code>cray-python</code> module which is not installed through EasyBuild and only known to EasyBuild as an external module.</p> </li> <li> <p>And now we can install the package:</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeCray-22.08-cray-python-3.9.12.1.eb\n</code></pre> </li> <li> <p>To use the package all we need to do is to load the module and to run the command that it     defines:</p> <pre><code>module load py-eb-tutorial/1.0.0-cpeCray-22.08-cray-python-3.9.12.1\npy-eb-tutorial\n</code></pre> <p>with the same remark as in the previous exercise if Lmod fails to find the module.</p> <p>You may want to do this step in a separate terminal session set up the same way, or you will get an error message in the next exercise with EasyBuild complaining that there are some modules loaded that should not be loaded.</p> </li> </ul>"},{"location":"intro-202310xx/06_Exercises_1/#installing-software-with-uninstalled-dependencies","title":"Installing software with uninstalled dependencies","text":"<p>Now you're asked to also install the version of <code>py-eb-tutorial</code> for the <code>cpeGNU</code> toolchain in <code>LUMI/22.08</code> (and the solution given below assumes you haven'ty accidentally installed the wrong EasyBuild recipe in one of the previous two exercises).</p> Click to see the solution. <ul> <li> <p>We again work in the same environment as in the previous two exercises. Nothing has changed here.     Hence if not done yet we need</p> <pre><code>module load LUMI/22.08 partition/C\nmodule load EasyBuild-user\n</code></pre> </li> <li> <p>Now go to the <code>EasyBuild</code> subdirectory of the exercises (if not there yet from the previous     exercise) and check what the <code>py-eb-tutorial-1.0.0-cpeGNU-22.08-cray-python-3.9.12.1.eb</code> needs:</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeGNU-22.08-cray-python-3.9.12.1.eb -D\n</code></pre> <p>We'll now see that there are two missing modules. Not only is the  <code>py-eb-tutorial/1.0.0-cpeGNU-22.08-cray-python-3.9.12.1</code> that we try to install missing, but also the <code>eb-tutorial/1.0.1-cpeGNU-22.08</code>. EasyBuild does however manage to find a recipe from which this module can be built in the pre-installed build recipes.</p> </li> <li> <p>We can install both packages separately, but it is perfectly possible to install both packages in a single     <code>eb</code> command by using the <code>-r</code> option to tell EasyBuild to also install all dependencies.</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeGNU-22.08-cray-python-3.9.12.1.eb -r\n</code></pre> </li> <li> <p>At the end you'll now notice (with <code>module avail</code>) that both the module      <code>eb-tutorial/1.0.1-cpeGNU-22.08</code> and <code>py-eb-tutorial/1.0.0-cpeGNU-22.08-cray-python-3.9.12.1</code>     are now present.</p> <p>To run you can use</p> <pre><code>module load py-eb-tutorial/1.0.0-cpeGNU-22.08-cray-python-3.9.12.1\npy-eb-tutorial\n</code></pre> </li> </ul>"},{"location":"intro-202310xx/06_Running_jobs/","title":"Running jobs","text":"<p>No notes for now.</p> <p>See the slides (PDF).</p>"},{"location":"intro-202310xx/07_Exercises_2/","title":"Exercises 2: Running jobs with Slurm","text":""},{"location":"intro-202310xx/07_Exercises_2/#exercises-on-the-slurm-allocation-modes","title":"Exercises on the Slurm allocation modes","text":"<ol> <li> <p>Run single task on the CPU partition with <code>srun</code> using multiple cpu cores. Inspect default task allocation with <code>taskset</code> command (<code>taskset -cp $$</code> will show you cpu numbers allocated to a current process). </p> Click to see the solution. <pre><code>srun --partition=small --nodes=1 --tasks=1 --cpus-per-task=16 --time=5 --partition=small --account=&lt;project_id&gt; bash -c 'taskset -cp $$' \n</code></pre> <p>Note you need to replace <code>&lt;project_id&gt;</code> with actual project account ID in a form of <code>project_</code> plus 9 digits number.</p> <p>The command runs single process (<code>bash</code> shell with a native Linux <code>taskset</code> tool showing process's CPU affinity) on a compute node. You can use <code>man taskset</code> command to see how the tool works.</p> </li> <li> <p>Try Slurm allocations with <code>hybrid_check</code> tool program from the LUMI Software Stack. The program is preinstalled on the system. </p> <p>Use the simple job script to run parallel program with multiple tasks (MPI ranks) and threads (OpenMP). Test task/threads affinity with <code>sbatch</code> submission on the CPU partition.</p> <pre><code>#!/bin/bash -l\n#SBATCH --partition=small           # Partition (queue) name\n#SBATCH --nodes=1                   # Total number of nodes\n#SBATCH --ntasks-per-node=8         # 8 MPI ranks per node\n#SBATCH --cpus-per-task=16          # 16 threads per task\n#SBATCH --time=5                    # Run time (minutes)\n#SBATCH --account=&lt;project_id&gt;      # Project for billing\n\nmodule load LUMI/22.12\nmodule load lumi-CPEtools\n\nsrun hybrid_check -n -r\n</code></pre> <p>Be careful with copy/paste of script body while it may brake some specific characters.</p> Click to see the solution. <p>Save script contents into <code>job.sh</code> file (you can use <code>nano</code> console text editor for instance), remember to use valid project account name.</p> <p>Submit job script using <code>sbatch</code> command. </p> <pre><code>sbatch job.sh\n</code></pre> <p>The job output is saved in the <code>slurm-&lt;job_id&gt;.out</code> file. You can view it's contents with either <code>less</code> or <code>more</code> shell commands.</p> <p>Actual task/threads affinity may depend on the specific OpenMP runtime but you should see \"block\" thread affinity as a default behaviour.</p> </li> <li> <p>Improve threads affinity with OpenMP runtime variables. Alter your script and add MPI runtime variable to see another cpu mask summary. </p> Click to see the solution. <p>Export <code>SRUN_CPUS_PER_TASK</code> environment variable to follow convention from recent Slurm's versions in your script. Add this line before the <code>hybrid_check</code> call:</p> <pre><code>export SRUN_CPUS_PER_TASK=16 \n</code></pre> <p>Add OpenMP environment variables definition to your script:</p> <pre><code>export OMP_NUM_THREADS=${SRUN_CPUS_PER_TASK}\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=cores\n</code></pre> <p>You can also add MPI runtime variable to see another cpu mask summary:</p> <pre><code>export MPICH_CPUMASK_DISPLAY=1\n</code></pre> <p>Note <code>hybrid_check</code> and MPICH cpu mask may not be consistent. It is found to be confusing.</p> </li> <li> <p>Build <code>hello_jobstep</code> program tool using interactive shell on a GPU node. You can pull the source code for the program from git repository <code>https://code.ornl.gov/olcf/hello_jobstep.git</code>. It uses <code>Makefile</code> for building. Try to run the program interactively. </p> Click to see the solution. <p>Clone the code using <code>git</code> command:</p> <pre><code>git clone https://code.ornl.gov/olcf/hello_jobstep.git\n</code></pre> <p>It will create <code>hello_jobstep</code> directory consisting source code and <code>Makefile</code>.</p> <p>Allocate resources for a single task with a single GPU with <code>salloc</code>:</p> <pre><code>salloc --partition=small-g --nodes=1 --tasks=1 --cpus-per-task=1 --gpus-per-node=1 --time=10 --account=&lt;project_id&gt;\n</code></pre> <p>Note that, after allocation being granted, you receive new shell but still on the compute node. You need to use <code>srun</code> to execute on the allocated node. </p> <p>Start interactive session on a GPU node:</p> <pre><code>srun --pty bash -i\n</code></pre> <p>Note now you are on the compute node. <code>--pty</code> option for srun is required to interact with the remote shell.</p> <p>Enter the <code>hello_jobstep</code> directory and issue <code>make</code> command. It will fail without additional options and modules.</p> <pre><code>module load rocm\n</code></pre> <p>Note compiler (and entire programming environment) is the one you have set (or not) in the origin shell on the login node.  </p> <p>Nevertheless <code>rocm</code> module is required to build code for GPU.</p> <pre><code>make LMOD_SYSTEM_NAME=\"frontier\"\n</code></pre> <p>You need to add <code>LMOD_SYSTEM_NAME=\"frontier\"</code> variable for make while the code originates from the Frontier system.</p> <p>You can exercise to fix <code>Makefile</code> and enable it for LUMI :)</p> <p>Eventually you can just execute <code>./hello_jobstep</code> binary program to see how it behaves:</p> <pre><code>./hello_jobstep\n</code></pre> <p>Note executing the program with <code>srun</code> in the srun interactive session will result in a hang. You need to work with <code>--overlap</code> option for srun to mitigate it.</p> <p>Still remember to terminate your interactive session with <code>exit</code> command.</p> <pre><code>exit\n</code></pre> </li> </ol>"},{"location":"intro-202310xx/07_Exercises_2/#slurm-custom-binding-on-gpu-nodes","title":"Slurm custom binding on GPU nodes","text":"<ol> <li> <p>Allocate one GPU node with one task per GPU and bind tasks to each CCD (8-core group sharing L3 cache) leaving first (#0) and last (#7) cores unused. Run a program with 6 threads per task and inspect actual task/threads affinity.</p> Click to see the solution. <p>Begin with the example from the slides with 7 cores per task:</p> <pre><code>#!/bin/bash -l\n#SBATCH --partition=standard-g  # Partition (queue) name\n#SBATCH --nodes=1               # Total number of nodes\n#SBATCH --ntasks-per-node=8     # 8 MPI ranks per node\n#SBATCH --gpus-per-node=8       # Allocate one gpu per MPI rank\n#SBATCH --time=5                # Run time (minutes)\n#SBATCH --account=&lt;project_id&gt;  # Project for billing\n#SBATCH --hint=nomultithread\n\ncat &lt;&lt; EOF &gt; select_gpu\n#!/bin/bash\n\nexport ROCR_VISIBLE_DEVICES=\\$SLURM_LOCALID\nexec \\$*\nEOF\n\nchmod +x ./select_gpu\n\nCPU_BIND=\"mask_cpu:0xfe000000000000,0xfe00000000000000,\"\nCPU_BIND=\"${CPU_BIND}0xfe0000,0xfe000000,\"\nCPU_BIND=\"${CPU_BIND}0xfe,0xfe00,\"\nCPU_BIND=\"${CPU_BIND}0xfe00000000,0xfe0000000000\"\n\nexport OMP_NUM_THREADS=7\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=cores\n\nexport MPICH_CPUMASK_DISPLAY=1\n\nsrun --cpu-bind=${CPU_BIND} ./select_gpu ./hello_jobstep/hello_jobstep\n</code></pre> <p>If you save the script in the <code>job_step.sh</code> then simply submit it with sbatch. Inspect the job output.</p> <p>Now you would need to alter masks to disable 7th core of each of the group (CCD). Base mask is then <code>01111110</code> which is <code>0x7e</code> in hexadecimal notation.</p> <p>Try to apply new bitmask, change the corresponding variable to spawn 6 threads per task and check how new binding works.</p> </li> </ol>"},{"location":"intro-202310xx/08_Lustre_intro/","title":"I/O and file systems","text":"<p>No notes for now.</p> <p>See the slides (PDF).</p>"},{"location":"intro-202310xx/09_LUMI_support/","title":"How to get support and documentation","text":"<p>No notes for now.</p> <p>See the slides (PDF).</p>"},{"location":"intro-202310xx/schedule/","title":"Schedule (tentative)","text":"09:00 CEST\u00a0\u00a0                       Welcome and introduction             Presenter: Kurt Lust              09:10 CEST                       LUMI Architecture             Presenter: Kurt Lust              09:40 CEST                       HPE Cray Programming Environment             Presenter: Kurt Lust              10:10 CEST                       Modules on LUMI             Presenter: Kurt Lust              10:45 CEST          Break              11:00 CEST                       LUMI Software Stacks             Presenter: Kurt Lust              11:45 CEST                       Hands-on                       12:15 CEST          Lunch break              13:15 CEST                       Running jobs on LUMI             Presenter: TBA              15:15 CEST             16:15 EEST              Hands-on                       15:30 CEST          Break              15:40 CEST                       Introduction to Lustre and Best Practices             Presenter: TBA              15:50 CEST                       LUMI User Support             Presenter: TBA              16:15 CEST          General Q&amp;A              16:30 CEST          Course end"}]}