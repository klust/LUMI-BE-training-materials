{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LUMI-BE training and event materials","text":"<p>Welcome to the future site of training materials of LUMI-BE,  the Belgian participation to LUMI.</p> <ul> <li>Continuously evolving version of the 1-day intro training materials</li> </ul>"},{"location":"intro-evolving/","title":"LUMI introductory training evolving version","text":"<p>This document includes the base materials for the introductory training that we provide to Belgian LUMI users. We try to evolve it as soon as possible when it becomes outdated due to changes on LUMI, and it is certainly more up-to-date than any other version you may find on this site in the future.</p> <p>Short URL: https://klust.github.io/intro-evolving.</p>"},{"location":"intro-evolving/#setting-up-for-the-exercises","title":"Setting up for the exercises","text":"<p>If you have an active project on LUMI, you should be able to make the exercises in that project. You will only need an very minimum of CPU and GPU billing units for this.</p> <ul> <li> <p>Create a directory in the scratch of your project, or if you want to     keep the exercises around for a while, in a subdirectory of your project directory      or in your home directory (though we don't recommend the latter).     Then go into that directory.</p> <p>E.g., in the scratch directory of your project:</p> <pre><code>mkdir -p /scratch/project_46YXXXXXX/course-$USER/exercises\ncd /scratch/project_46YXXXXXX/course-$USER/exercises\n</code></pre> <p>where you have to replace <code>project_46YXXXXXX</code> using the number of your own project.</p> </li> <li> <p>Now download the exercises and un-tar:</p> <pre><code>wget https://465000095.lumidata.eu/training-materials-web/intro-evolving/files/exercises-evolving.tar.bz2\ntar -xf exercises-evolving.tar.bz2\n</code></pre> <p>Link to the tar-file with the exercises or bzip2-compressed tar file.</p> </li> <li> <p>You're all set to go!</p> </li> </ul>"},{"location":"intro-evolving/#downloads","title":"Downloads","text":"Presentation Slides Notes Exercises Notes Introduction / notes / Theme: Exploring LUMI from the login nodes LUMI Architecture slides notes / HPE Cray Programming Environment slides notes exercises Getting access to LUMI slides notes exercises Modules on LUMI slides notes exercises LUMI Software Stacks slides notes exercises LUMI User Support slides notes / Theme: Running jobs efficiently Slurm on LUMI slides notes exercises Binding resources slides notes exercises Theme: Data on LUMI Using Lustre slides notes / LUMI-O object storage slides notes exercises Theme: Containers on LUMI Containers on LUMI slides notes / Container demo 1 / notes and video / Container demo 2 / notes and video / Appendices A1 Slurm issues / notes / A2 Additional documentation / notes /"},{"location":"intro-evolving/#web-links","title":"Web links","text":"<ul> <li> <p>Links to additional HPE Cray documentation</p> </li> <li> <p>LUMI documentation</p> <ul> <li> <p>Main documentation</p> </li> <li> <p>Shortcut to the LUMI Software Library</p> </li> </ul> </li> <li> <p>Clusters in Belgium</p> <ul> <li> <p>VSC documentation</p> </li> <li> <p>C\u00c9CI documentation</p> </li> <li> <p>Lucia@Cenaero documentation</p> </li> </ul> </li> <li> <p>LUMI training resources</p> <ul> <li> <p>LUMI training announcements</p> </li> <li> <p>LUMI training materials archive</p> </li> </ul> </li> <li> <p>Local (BE) trainings</p> <ul> <li> <p>VSC training page</p> <ul> <li> <p>Local materials UAntwerpen</p> </li> <li> <p>Local materials VUB</p> </li> <li> <p>Local materials UGent</p> </li> </ul> </li> <li> <p>C\u00c9CI HPC training page</p> <p>Training materials of previous sessions are available via the  CISM indico event management system.  For each training, go to the \"Contribution List\" and then to the presentation itself, and training materials are available at the bottom of the page.</p> <p>Recordings of several presentations are available on the  C\u00c9CI and CISM HPC YouTube Channel</p> </li> <li> <p>EuroCC Belgium training page</p> </li> </ul> </li> <li> <p>Other course materials</p> <ul> <li>Archive of PRACE training materials (no https unfortunately)</li> </ul> </li> </ul>"},{"location":"intro-evolving/00-Introduction/","title":"Introduction to the notes","text":"<p>These notes and training materials were prepared in the framework of the  VSC Tier-0 support activities.</p> <p>The notes, and all other materials used for the course, borrow a lot from the course material made available by the LUMI User Support Team. Their course materials are avialable on the LUMI training materias web site. However, this version of the notes is not official LUMI material but material from the  VSC Tier-0 support team, and contains enhancements that may not occur in any version of the official LUMI training materials.</p> <p>Various training materials and the documentation from the Walloon HPC project C\u00c9CI were also a great source of inspiration.</p> <p>This course is offered to the LUMI-BE organisation.</p>"},{"location":"intro-evolving/00-Introduction/#about-the-structure-of-the-notes","title":"About the structure of the notes","text":"<p>Colour coding and boxes in the material:</p> <p>Remark</p> <p>This is a remark: Some additional information that may be nice to read. or some additional information that you may want to have a look at.</p> <p>Note</p> <p>Just a quick note on the side, but do have a look at it.</p> <p>Audience</p> <p>A box telling you who this part of the notes is written for, or why it would be good to read it even if you think you don't need it.</p> <p>Lumi-be</p> <p>This is used to point out differences with local infrastructures in Belgium and is less useful if you are not familiar with any of the HPC clusters in Belgium.</p> <p>Example</p> <p>An example to make the material clearer or to try something out.</p> <p>Exercise</p> <p>An exercise</p> <p>Solution</p> <p>The solution to the exercise. You will have to click on the box to see the solution.</p> <p>Seealso</p> <p>Connect to other chapters in the notes or pages for course sessions.</p> <p>Bug</p> <p>This is a discussion about a bug.</p> <p>Nice-to-know</p> <p>This is a little fact which is nice-to-know but not necessary to understand the material.</p> <p>Intermediate</p> <p>Information that may not be useful to every LUMI user, but if you are the kind of person who likes to explore manuals and try out things that we did not discuss in the course, you may want to read this...</p> <p>Advanced</p> <p>Similar to the intermediate material, but it requires even more technical skills to understand this stuff.</p> <p>Technical</p> <p>Material specific to readers with very technical interests.</p>"},{"location":"intro-evolving/01-Architecture/","title":"Architecture","text":"<p>Last update of this page: October 1, 2025</p>"},{"location":"intro-evolving/01-Architecture/#the-lumi-architecture","title":"The LUMI Architecture","text":"<p>In this presentation, we will build up LUMI part by part, stressing those aspects that are important to know to run on LUMI efficiently and define jobs that can scale.</p>"},{"location":"intro-evolving/01-Architecture/#why-do-i-kneed-to-know-this","title":"Why do I kneed to know this?","text":"<p>You may wonder why you need to know about system architecture if all you want to do is to run  some programs.</p> <p>A supercomputer is not simply a scaled-up smartphone or PC that will offer good performance automatically.  It is a shared infrastructure and you don't get the whole machine to yourself. Instead you have to request a suitable fraction of the computer for the work you want to do. But it is also a very expensive infrastructure, with an investment of 160M EURO for LUMI and an estimated total cost (including operations) of 250M EURO. So it is important to use the computer efficiently. </p> <p>LUMI, as other large supercomputers, is built for running large parallel applications efficiently. But that efficiency does not for free. Scaling from a small problem size on a small computer  does not come for free, not in hardware and neither in software.</p> <p>In fact, in most cases it is important to properly map an  application on the available resources to run efficiently.  The way an application is developed is important for this, but it is not the only factor. Every application needs some user help  to run in the most efficient way, and that requires an understanding of</p> <ol> <li> <p>The hardware architecture of the supercomputer, which is something that we discuss in this     section.</p> </li> <li> <p>The middleware: the layers of software that sit between the application on one hand and the     hardware and operating system on the other hand. LUMI runs a sligthly modified version of Linux.     But Linux is not a supercomputer operating system. Missing functionality in Linux is offered     by other software layers instead that on supercomputers often come as part of the programming     environment.     This is a topic of discussion in several sessions of this course.</p> </li> <li> <p>The application. This is very domain-specific and application-specific and hence cannot be the     topic of a general course like this one. In fact, there are so many different applications and     often considerable domain knowledge is required so that a small support team like the one of      LUMI cannot provide that information. </p> </li> <li> <p>Moreover, the way an application should be used may even depend on the particular problem that you     are trying to solve. Bigger problems, bigger computers, and different settings may be needed in     the application.</p> <p>It is up to scientific communities to organise trainings that teach you individual applications and how to use them for different problem types, and then up to users to combine the knowledge of an application obtained from such a course with the knowledge about the computer you want to use and its middleware obtained from courses such as this one or our 4-day more advanced course.</p> </li> </ol> <p>Some users expect that a support team can give answers to all those questions, even to the third and fourth bullet of the above list. If a support team could do that, it would basically imply that they could simply do all the research that users do and much faster as they are assumed to have the answer ready in hours...</p>"},{"location":"intro-evolving/01-Architecture/#lumi-is","title":"LUMI is ...","text":"<p>LUMI is a pre-exascale supercomputer, and not a superfast PC nor a compute cloud architecture.</p> <p>Each of these architectures have their own strengths and weaknesses and offer different  compromises and it is key to chose the right infrastructure for the job and use the right  tools for each infrastructure.</p> <p>Just some examples of using the wrong tools or infrastructure:</p> <ul> <li> <p>The single thread performance of the CPU is lower than on a high-end PC.      We've had users who were disappointed about the speed of a single core and were expecting     that this would be much faster than their PCs. Supercomputers however are optimised for      performance per Watt and get their performance from using lots of cores through well-designed     software. If you want the fastest core possible, you'll need a gaming PC.</p> <p>E.g., the AMD 5800X was a popular CPU for high end gaming PCs using the same core architecture  as the CPUs in LUMI. It runs at a base clock of 3.8 GHz and a boost clock of 4.7 GHz if only one core is used and the system has proper cooling. The 7763 used in the compute nodes of LUMI-C runs at a base clock of 2.45 GHz and a boost clock of 3.5 GHz. If you have only one single core job to run on your PC, you'll be able to reach that boost clock while on LUMI you'd probably need to have a large part of the node for yourself, and even then the performance for jobs that are not memory bandwidth limited will be lower than that of the gaming PC.</p> </li> <li> <p>For some data formats the GPU performance may be slower also than on a high end gaming PC.     This is even more so because     an MI250X should be treated as two GPUs for most practical purposes. The better double precision     floating point operations and matrix operations, also at full precision, require transistors      that on some other GPUs are used for rendering hardware or for single precision compute units.</p> <p>E.g., a single GPU die of the MI250X (half a GPU) has a peak FP32 performance at the boost clock of almost 24 TFlops or 48 TFlops in the packed format which is actually hard for a compiler to exploit, while the high-end AMD graphics GPU RX 7900 XTX claims 61 TFlops at the boost clock. But the FP64 performance of one MI250X  die is also close to 24 TFlops in vector math, while the RX 7900 XTX does less than 2 TFlops in that data format which is important for a lot of scientific computing applications.</p> </li> <li> <p>Compute GPUs and rendering GPUs are different beasts these days.     We had a user who wanted to use the ray tracing units to do rendering. The MI250X does not     have texture units or ray tracing units though. It is not a real graphics processor anymore.</p> </li> <li> <p>The environment is different also. It is not that because it runs some Linux it handles are your     Linux software.     A user complained that they did not succeed in getting their nice remote development environment to     work on LUMI. The original author of these notes took a test license and downloaded a trial version.     It was a very nice environment but really made for local development and remote development in a      cloud environment with virtual machines individually protected by personal firewalls and was      not only hard to get working on a supercomputer but also insecure.</p> </li> </ul> <ul> <li> <p>And supercomputer need proper software that exploits the strengths and works around the weaknesses     of their architecture.     Supercomputers are optimised to run very scalable applications cost-efficiently, but that     requires well parallelised software and data storage in a proper way so that data can be      streamed in and out of the machine efficiently from big shared filesystems that are also      optimised more for bandwidth than small individual operations. </p> <p>A nice illustration of this is the  case study: Bringing CERN LHC computations to an HPC infrastructure in the course notes of the UAntwerpen \"Supercomputers for Starters\" course which is part of the VSC introductory courses offered in Antwerp.</p> </li> </ul> <p>True supercomputers, and LUMI in particular, are built for scalable parallel applications and features that are found on smaller clusters or on workstations that pose a threat to scalability are removed from the system. It is also a shared infrastructure but with a much more lightweight management layer than a cloud infrastructure and far less isolation between users, meaning that abuse by one user can have more of a negative impact on  other users than in a cloud infrastructure. Supercomputers since the mid to late '80s are also built according to the principle of trying to reduce the hardware cost by using cleverly designed software both at the system and application level. They perform best when streaming data through the machine at all levels of the  memory hierarchy and are not built at all for random access to small bits of data (where the definition of \"small\" depends on the level in the memory hierarchy).</p> <p>At several points in this course you will see how this impacts what you can do with a supercomputer and how you work with a supercomputer.</p> <p>And LUMI is not just a supercomputer, it is a pre-exascale supercomputer. This implies that it is using new and leading edge technology and pushing the limits of current technology. But this also means that it will have some features that many observe as problems that smaller clusters using more conventional technology will not have. Stability is definitely less, bigger networks definitely come with more  problems (and are an important cause of those stability problems), not everything scales as you would hope (think of the scheduler and file system IOPS discussed later in this course), ...</p>"},{"location":"intro-evolving/01-Architecture/#lumi-spec-sheet-a-modular-system","title":"LUMI spec sheet: A modular system","text":"<p>So we've already seen that LUMI is in the first place a EuroHPC pre-exascale machine. LUMI is built to prepare for the exascale era and to fit in the EuroHPC ecosystem.  But it does not even mean that it has to cater to all pre-exascale compute needs. The EuroHPC JU tries to build systems that have some flexibility, but also does not try to cover  all needs with a single machine. They are building 3 pre-exascale systems with different architecture to explore multiple architectures and to cater to a more diverse audience. LUMI is an AMD GPU-based supercomputer,  Leonardo uses NVIDIA A100 GPUS, and MareNostrum5 has a very large CPU section besides an NVIDIA Hopper GPU section.</p> <p>LUMI is also a very modular machine designed according to the principles explored in a series of European projects, and in particular DEEP and its successors) that explored the cluster-booster concept. E.g., in a complicated multiphysics simulation  you could be using regular CPU nodes for the physics that cannot be GPU-accelerated communicating with compute GPU nodes for the physics that can be GPU-accelerated, then add a number of CPU nodes to do the I/O and a specialised render GPU node for in-situ visualisation.</p> <p>LUMI is in the first place a huge GPGPU supercomputer. The GPU partition of LUMI, called LUMI-G, contains 2978 nodes with a single 64-core AMD EPYC 7A53 CPU and 4 AMD MI250X GPUs. Each node has 512 GB of RAM attached to the CPU (the maximum the CPU can handle without compromising bandwidth) and 128 GB of HBM2e memory per GPU. Each GPU node has a theoretical peak performance of nearly 200 TFlops in single (FP32) or double (FP64) precision vector arithmetic (and twice that with the packed FP32 format, but that  is not well supported so this number is not often quoted). The matrix units are capable of about 400 TFlops in FP32 or FP64. However, compared to the NVIDIA GPUs, the performance for lower precision formats used in some AI applications is not that stellar.</p> <p>LUMI also has a large CPU-only partition, called LUMI-C, for jobs that do not run well on GPUs, but also integrated enough with the GPU partition that it is possible to have applications that combine both node types. LUMI-C consists of 2048 nodes with 2 64-core AMD EPYC 7763 CPUs. 32 of those nodes have 1TB of RAM (with some of these nodes actually reserved for special purposes such as connecting to a Quantum computer), 128 have 512 GB and 1888 have 256 GB of RAM.</p> <p>LUMI also has two smaller groups of nodes for interactive data analytics.  8 of those nodes have two  64-core Zen2/Rome CPUs with 4 TB of RAM per node, while 8 others have dual 64-core Zen2/Rome CPUs and 8 NVIDIA A40 GPUs for visualisation. Together these are known as LUMI-D, but as we shall see in the Slurm part of the training, this name is misleading as these are two node types corresponding to two partitions in the scheduler. There is also an Open OnDemand based service (web interface) to make some fo those facilities available. Note though that these nodes are meant for a very specific use, so it is not that we will also be offering, e.g., GPU compute facilities on NVIDIA hardware, and that these are shared resources that should not be monopolised by a single user (so no hope to run an MPI job on 8 4TB nodes).</p> <p>LUMI also has a 8 PB flash based file system running the Lustre parallel file system. This system is often denoted as LUMI-F. The bandwidth of that system is over 2 TB/s.  Note however that this is still a remote file system with a parallel file system on it, so do not expect that it will behave as the local SSD in your laptop.  But that is  also the topic of another session in this course.</p> <p>The main work storage is provided by 4 20 PB hard disk based Lustre file systems with a bandwidth of 240 GB/s each. That section of the machine is often denoted  as LUMI-P. </p> <p>Big parallel file systems need to be used in the proper way to be able to offer the performance that one would expect from their specifications. This is important enough that  we have a separate session about that in this course.</p> <p>There is also a 30 PB object based file system  similar to the Allas service of CSC that some of the Finnish users may be familiar with is also being worked on. At the  moment the interface to that system is still rather primitive. This part of LUMI is also known as LUMI-O.</p> <p>Currently LUMI has 4 login nodes for ssh access, called user access nodes in the HPE Cray world. They each have 2 64-core AMD EPYC 7742 processors and 1 TB of RAM. Note that  whereas the GPU and CPU compute nodes have the Zen3 architecture code-named \"Milan\", the processors on the login nodes are Zen2 processors, code-named \"Rome\". Zen3 adds some new instructions so if a compiler generates them, that code would not run on the login nodes. These instructions are basically used in cryptography though. However, many instructions have very different latency, so a compiler that optimises specifically for Zen3 may chose another ordering of instructions then when optimising for Zen2 so it may still make sense to compile specifically for the compute nodes on LUMI. There are also an additional login nodes for access via the web-based Open OnDemand interface. Together these are sometimes called LUMI-L.</p> <p>All compute nodes, login nodes and storage are linked together through a  high-performance interconnect. LUMI uses the Slingshot 11 interconnect which is developed by HPE Cray, so not the Mellanox/NVIDIA InfiniBand that you may be familiar with from many smaller clusters, and as we shall discuss later this also influences how you work on LUMI.</p> <p>Early on a small partition for containerised micro-services managed with Kubernetes was also planned, but that may never materialize due to lack of  people to set it up and manage it.</p> <p>In this section of the course we will now build up LUMI step by step.</p>"},{"location":"intro-evolving/01-Architecture/#building-lumi-the-cpu-amd-7xx3-milanzen3-cpu","title":"Building LUMI: The CPU AMD 7xx3 (Milan/Zen3) CPU","text":"<p>The LUMI-C and LUMI-G compute nodes use third generation AMD EPYC CPUs. Whereas Intel CPUs launched in the same period were built out of a single large monolithic piece of silicon (that only changed recently with some variants of the Sapphire Rapids CPU launched in early 2023), AMD CPUs are made up of multiple so-called chiplets. </p> <p>The basic building block of Zen3 CPUs is the Core Complex Die (CCD). Each CCD contains 8 cores, and each core has 32 kB of L1 instruction  and 32 kB of L1 data cache, and 512 kB of L2 cache. The L3 cache is shared across all cores on a chiplet and has a total size of 32 MB on LUMI (there are some variants of the processor where this is 96MB). At the user level, the instruction set is basically equivalent to that of the Intel Broadwell generation. AVX2 vector instructions and the FMA instruction are fully supported, but there is no support for any of the AVX-512 versions that can be found on Intel Skylake server processors and later generations. Hence the number of floating point operations that a core can in theory do each clock cycle is 16 (in  double precision) rather than the 32 some Intel processors are capable of. </p> <p></p> <p>The full processor package for the AMD EPYC processors used in LUMI have 8 such Core Complex Dies for a total of 64 cores. The caches are not shared between different CCDs, so it also implies that the processor has 8 so-called L3 cache regions or domains. (Some cheaper variants have only 4 CCDs, and some have CCDs with only 6 or fewer cores enabled but the same 32 MB of L3 cache per CCD).</p> <p>Each CCD connects to the memory/IO die through an Infinity Fabric link (also called GMI link which stands for Global Memory Interface). The connection is asymmetric on Milan with 51.2 GB/s bandwidth to and 25.6 GB/s bandwidth from the CCD (32 bytes and 16 byte wide connections running at the memory clock with is 1.6 GHz for DDR4 3200). The memory/IO die contains the memory controllers, connections to connect two CPU packages together, PCIe lanes to connect to external hardware, and some additional hardware, e.g., for managing the processor.  The memory/IO die supports 4 dual channel DDR4 memory controllers providing  a total of 8 64-bit wide memory channels. Each memory channel has a theoretical peak bandwidth of 25.6 GB/s. From a logical point of view the memory/IO-die is split in 4 quadrants, with each quadrant having a dual channel memory controller and 2 CCDs. They basically act as 4 NUMA domains. For a core it is slightly faster to access memory in its own quadrant than memory attached to another quadrant, though for the 4 quadrants within the same socket the difference is small. (In fact, the BIOS can be set to show only two or one NUMA domain which is advantageous in some cases, like the typical load pattern of login nodes where it is impossible to nicely spread processes and their memory across the 4 NUMA domains).</p> <p>The theoretical memory bandwidth of a complete package is around 200 GB/s. However, that bandwidth is not available to a single core but can only be used if enough  cores spread over all CCDs are used.</p> <p>Clusters in Belgium</p> <p>The CPUs used in the LUMI-C compute nodes are identical to those used in  the Cenaero/C\u00c9CI cluster lucia or the Milan partition of the VSC cluster  hortense. The UGent VSC cluster gallade uses a very similar processor also but with more cache per CCD. Some other cluster, e.g., accelgor and doduo+  at UGent or the Milan partition of vaughan in at UAntwerpen, also use CPUs of this generation but with only 4 CCDs per processor and/or 6 active cores  per CCD. But many topics that we cover in this course also applies to those clusters.</p> <p>Some other clusters, e.g., the older Rome partition of the VSC cluster hortense, the C\u00c9CI cluster NIC5 at ULi\u00e8ge or the older main partition of the VSC cluster vaughan at UAntwerpen, use the older Zen2/Rome CPUs which are also used on the login nodes of LUMI. These have two groups of 4 cores each with their own separated L3 cache per CCD and 4 or 8 CCDs per  socket. </p>"},{"location":"intro-evolving/01-Architecture/#building-lumi-a-lumi-c-node","title":"Building LUMI: a LUMI-C node","text":"<p>A compute node is then built out of two such processor packages, connected  through 4 16-bit wide Infinity Fabric connections with a total theoretical bandwidth of 144 GB/s in each direction. So note that the bandwidth in each direction is less than the memory bandwidth of a socket. Again, it is not really possible to use the full memory bandwidth of a node using just cores on a single socket. Only one of the two sockets has a direct connection to the high performance Slingshot interconnect though.</p>"},{"location":"intro-evolving/01-Architecture/#a-strong-hierarchy-in-the-node","title":"A strong hierarchy in the node","text":"<p>As can be seen from the node architecture in the previous slide, the CPU compute nodes have a very hierarchical architecture. When mapping an application onto  one or more compute nodes, it is key for performance to take that hierarchy into account. This is also the reason why we will pay so much attention to thread and process pinning in this tutorial course.</p> <p>At the coarsest level, each core supports two hardware threads (what Intel calls hyperthreads). Those hardware threads share all the resources of a core, including the  L1 data and instruction caches and the L2 cache, execution units and space for register renaming.  At the next level, a Core Complex Die contains (up to) 8 cores. These cores share the L3 cache and the link to the memory/IO die.  Next, as configured on the LUMI compute nodes, there are 2 Core Complex Dies in a NUMA node. These two CCDs share the DRAM channels of that NUMA node. At the fourth level in our hierarchy 4 NUMA nodes are grouped in a socket. Those 4  nodes share an inter-socket link. At the fifth and last level in our shared memory hierarchy there are two sockets in a node. On LUMI, they share a single Slingshot inter-node link.</p> <p>The finer the level (the lower the number), the shorter the distance and hence the data delay is between threads that need to communicate with each other through the memory hierarchy, and the higher the bandwidth.</p> <p>This table tells us a lot about how one should map jobs, processes and threads onto a node. E.g., if a process has fewer then 8 processing threads running concurrently, these should be mapped to cores on a single CCD so that they can share  the L3 cache, unless they are sufficiently independent of one another, but even in the latter case the additional cores on those CCDs should not be used by other processes as they may push your data out of the cache or saturate the link to the memory/IO die and hence slow down some threads of your process. Similarly, on a 256 GB compute node each NUMA node has 32 GB of RAM (or actually a bit less as the OS also needs memory, etc.), so if you have a job that uses 50 GB of memory but only, say, 12 threads, you should really have two NUMA nodes reserved for that job as otherwise other threads or processes running on cores in those NUMA nodes could saturate some resources needed by your job. It might also be preferential to spread those 12 threads over the 4  CCDs in those 2 NUMA domains unless communication through the L3 threads would be the bottleneck in your application.</p>"},{"location":"intro-evolving/01-Architecture/#hierarchy-delays-in-numbers","title":"Hierarchy: delays in numbers","text":"<p>This slide shows the Advanced Configuration and Power Interface System Locality distance Information Table (ACPI SLIT) as returned by, e.g., <code>numactl -H</code> which gives relative distances to memory from a core. E.g., a value of 32 means that access takes 3.2x times the  time it would take to access memory attached to the same NUMA node.  We can see from this table that the penalty for accessing memory in  another NUMA domain in the same socket is still relatively minor (20%  extra time), but accessing memory attached to the other socket is a lot  more expensive. If a process running on one socket would only access memory attached to the other socket, it would run a lot slower which is why Linux has mechanisms to try to avoid that, but this cannot be done in all scenarios which is why on some clusters you will be allocated cores in proportion to the amount of memory you require, even if that is more cores than you really need (and you will be billed for them).</p>"},{"location":"intro-evolving/01-Architecture/#building-lumi-concept-lumi-g-node","title":"Building LUMI: Concept LUMI-G node","text":"<p>This slide shows a conceptual view of a LUMI-G compute node. This node is unlike any Intel-architecture-CPU-with-NVIDIA-GPU compute node you may have  seen before, and rather mimics the architecture of the USA pre-exascale machines Summit and Sierra which have IBM POWER9 CPUs paired with  NVIDIA V100 GPUs.</p> <p>Each GPU node consists of one 64-core AMD EPYC CPU and 4 AMD MI250X GPUs.  So far nothing special. However, two elements make this compute node very special. First, the GPUs are not connected to the CPU though a PCIe bus. Instead they are connected through the same links that AMD uses to link the GPUs together, or to link the two sockets in the LUMI-C compute nodes, known as xGMI or Infinity Fabric. This enables unified memory across CPU and GPUs and  provides partial cache coherency across the system. The CPUs coherently cache the CPU DDR and GPU HBM memory, but each GPU only coherently caches  its own local memory. The second remarkable element is that the Slingshot interface cards connect directly to the GPUs (through a PCIe interface on the GPU) rather than to the CPU. The GPUs have a shorter path to the communication  network than the CPU in this design. </p> <p>This makes the LUMI-G compute node really a \"GPU first\" system. The architecture looks more like a GPU system with a CPU as the accelerator for tasks that a GPU is not good at such as some scalar processing or running an OS, rather than a CPU node with GPU accelerator.</p> <p>It is also a good fit with the cluster-booster design explored in the DEEP project series. In that design, parts of your application that cannot be properly accelerated would run on CPU nodes, while booster GPU nodes would be used for those parts  that can (at least if those two could execute concurrently with each other). Different node types are mixed and matched as needed for each specific application,  rather than building clusters with massive and expensive nodes that few applications can fully exploit. As the cost per transistor does not decrease anymore, one has to look for ways to use each transistor as efficiently as possible...</p> <p>It is also important to realise that even though we call the partition \"LUMI-G\", the MI250X is not a GPU in the true sense of the word. It is not a rendering GPU, which for AMD is  currently the RDNA architecture which is currently at version 4, but a compute accelerator with an architecture that evolved from a GPU architecture, in this case the VEGA architecture from AMD. The architecture of the MI200 series is also known as CDNA2, with the MI100 series being just CDNA, the first version. Much of the hardware that does not serve compute purposes has been removed from the design to have more transistors available for compute.  Rendering is possible, but it will be software-based  rendering with some GPU acceleration for certain parts of the pipeline, but not full hardware rendering. </p> <p>This is not an evolution at AMD only. The same is happening with NVIDIA GPUs and there is a reason why the latest generation is called \"Hopper\" for compute and \"Ada Lovelace\" for rendering GPUs.  Several of the functional blocks in the Ada Lovelace architecture are missing in the Hopper  architecture to make room for more compute power and double precision compute units. E.g., Hopper does not contain the ray tracing units of Ada Lovelace. The Intel Data Center GPU Max code named \"Ponte Vecchio\" is the only current GPU for  HPC that still offers full hardware rendering support (and even ray tracing), but that line looks increasingly like a dead end.</p> <p>Graphics on one hand and HPC and AI on the other hand are becoming separate workloads for which manufacturers make different, specialised cards, and if you have applications that need both, you'll have to rework them to work in two phases, or to use two types of nodes and communicate between them over the interconnect, and look for supercomputers that support both workloads. And nowadays we're even starting to see a split between chips that really target AI and chips that target a more traditional HPC workload, with the latter threatened as there is currently much more money to make in the AI market. And within AI we're starting to  see specialised accelerators for inference (e.g., the NVIDIA Rubin CPX).</p> <p>But so far for the sales presentation, let's get back to reality...</p>"},{"location":"intro-evolving/01-Architecture/#building-lumi-what-a-lumi-g-node-really-looks-like","title":"Building LUMI: What a LUMI-G node really looks like","text":"<p>Or the cleaner picture:</p> <p>The LUMI-G node uses the 64-core AMD 7A53 EPYC processor, known under the code name \"Trento\". This is basically a Zen3 processor but with a customised memory/IO die, designed specifically  for HPE Cray (and in fact Cray itself, before the merger) for the USA Coral-project to build the Frontier supercomputer, the fastest system in the world at the end of 2022 according to at least the Top500 list. Just as the CPUs in the LUMI-C nodes, it is a design with 8 CCDs and a memory/IO die.</p> <p>The MI250X GPU is also not a single massive die, but contains two compute dies besides the 8 stacks of HBM2e memory, 4 stacks or 64 GB per compute die. The two compute dies in a package are linked together  through 4 16-bit Infinity Fabric links. These links run at a higher speed than the links between two CPU sockets in a LUMI-C node, but per link the bandwidth is still only 50 GB/s per direction, creating a total bandwidth of 200 GB/s per direction between the two compute dies in an MI250X GPU. That amount of bandwidth is very low compared to even the memory bandwidth, which is roughly 1.6 TB/s peak per die, let alone compared to whatever bandwidth caches on the compute dies would have or the bandwidth of the internal structures that  connect all compute engines on the compute die. Hence the two dies in a single package cannot function efficiently as as single GPU which is one reason why each MI250X GPU on LUMI is actually seen as two GPUs. </p> <p>Each compute die uses a further 2 or 3 of those Infinity Fabric (or xGNI) links to connect to some compute dies in other MI250X packages. In total, each MI250X package is connected through 5 such links to other MI250X packages. These links run at the same 25 GT/s speed as the  links between two compute dies in a package, but even then the bandwidth is only a meager  250 GB/s per direction, less than an NVIDIA A100 GPU which offers 300 GB/s per direction or the NVIDIA H100 GPU which offers 450 GB/s per direction. Each Infinity Fabric link may be twice as fast as each NVLINK 3 or 4 link (NVIDIA Ampere and Hopper respectively), offering 50 GB/s per direction rather than 25 GB/s per direction for NVLINK,  but each Ampere GPU has 12 such links and each Hopper GPU 18 (and in fact a further 18 similar ones to link to a Grace CPU), while each MI250X package has only 5 such links available to link to other GPUs (and the three that we still need to discuss).</p> <p>Note also that even though the connection between MI250X packages is all-to-all, the connection between GPU dies is all but all-to-all. as each GPU die connects to only 3 other GPU dies. There are basically two bidirectional rings that don't need to share links in the topology, and then some extra connections. The rings are:</p> <ul> <li>Green ring: 1 - 0 - 6 - 7 - 5 - 4 - 2 - 3 - 1</li> <li>Red ring: 1 - 0 - 2 - 3 - 7 - 6 - 4 - 5 - 1</li> </ul> <p>These rings play a role in the inter-GPU communication in AI applications using RCCL.</p> <p>Each compute die is also connected to one CPU Core Complex Die (or as documentation of the node sometimes says, L3 cache region). This connection only runs at the same speed as the links between CPUs on the LUMI-C CPU nodes, i.e., 36 GB/s per direction (which is still enough for  all 8 GPU compute dies together to saturate the memory bandwidth of the CPU).  This implies that each of the 8 GPU dies has a preferred CPU die to work with, and this should definitely be taken into account when mapping processes and threads on a LUMI-G node. </p> <p>The figure also shows another problem with the LUMI-G node: The mapping between CPU cores/dies and GPU dies is all but logical:</p> GPU die CCD hardware threads NUMA node 0 6 48-55, 112-119 3 1 7 56-63, 120-127 3 2 2 16-23, 80-87 1 3 3 24-31, 88-95 1 4 0 0-7, 64-71 0 5 1 8-15, 72-79 0 6 4 32-39, 96-103 2 7 5 40-47, 104, 11 2 <p>and as we shall see later in the course, exploiting this is a bit tricky at the moment.</p>"},{"location":"intro-evolving/01-Architecture/#what-the-future-looks-like","title":"What the future looks like...","text":"<p>Some users may be annoyed by the \"small\" amount of memory on each node. Others may be annoyed by the limited CPU capacity on a node compared to some systems  with NVIDIA GPUs. It is however very much in line with the cluster-booster philosophy already mentioned a few times, and it does seem to be the future according to AMD (with Intel also working into that direction).  In fact, it looks like with respect to memory  capacity things may even get worse.</p> <p>We saw the first little steps of bringing GPU and CPU closer together and  integrating both memory spaces in the USA pre-exascale systems Summit and Sierra. The LUMI-G node which was really designed for one of the first USA exascale systems continues on this philosophy, albeit with a CPU and GPU from a different manufacturer. Given that manufacturing large dies becomes prohibitively expensive in newer semiconductor processes and that the transistor density on a die is also not increasing at the same rate anymore with process shrinks, manufacturers are starting to look at other ways of increasing the number of transistors per \"chip\" or should we say package. So multi-die designs are here to stay, and as is already the case in the AMD CPUs, different dies may be manufactured with different processes for economical reasons.</p> <p>Moreover, a closer integration of CPU and GPU would not only make programming easier as memory management becomes easier, it would also enable some codes to run on GPU  accelerators that are currently bottlenecked by memory transfers between GPU and CPU.</p> <p>Such a chip is exactly what AMD launched in December 2023 with the MI300A version of  the MI300 series.  It employs 13 chiplets in two layers, linked to (still only) 8  memory stacks (albeit of a much faster type than on the MI250X).  The 4 chiplets on the bottom layer are the memory controllers and inter-GPU links (an they can be at the bottom as they produce less heat). Furthermore each package features 6 GPU dies (now called XCD or Accelerated Compute Die as they really can't do graphics) and 3 Zen4 \"Genoa\" CPU dies. In the MI300A the memory is still limited to 8 16 GB stacks,  providing a total of 128 GB of RAM. The MI300X,  which is the regular version  without built-in CPU, already uses 24 GB stacks for a total of 192 GB of memory, but presumably those were not yet available when the design of MI300A was tested for the launch customer, the El Capitan supercomputer which became the number 1 in the TOP500 list of November 2024. HLRS is building the Hunter cluster based on AMD MI300A  as a transitional system to their first exascale-class system Herder that will become operational by 2027. The fact that the chip has recently been selected for the Hunter development system also indicates that even if no successor using the same techniques to combine GPU and CPU compute dies and memory would be made, there should at least be a successor that towards software behaves very similarly.</p> <p>Intel at some point has shown only very conceptual drawings of its Falcon Shores chip  which it calls an XPU, but those drawings suggest that that chip will also support some low-bandwidth but higher capacity external memory, similar to the approach taken in some Sapphire  Rapids Xeon processors that combine HBM memory on-package with DDR5 memory outside  the package. Falcon Shores was meant to be the next generation of Intel GPUs for HPC, after  Ponte Vecchio which is used in the Aurora supercomputer. However, it then first reverted to a more traditional design and ultimately got canceled except for some internal use, with  Jaguar Shores now being the successor for the Ponte Vecchio GPU in Aurora and no architecture known yet. The NVIDIA Grace-Hopper and Grace-Blackwell chips also do not completely follow the design philosophy of MI300A as that chip also still has separate memory for the CPU die and GPU dies, but the connection between both is so fast that at least the CPU should be able to  work with data in GPU-attached memory without the need for copying.</p>"},{"location":"intro-evolving/01-Architecture/#building-lumi-the-slingshot-interconnect","title":"Building LUMI: The Slingshot interconnect","text":"<p>All nodes of LUMI, including the login, management and storage nodes, are linked together using the Slingshot interconnect (and almost all use Slingshot 11, the full implementation with 200 Gb/s bandwidth per direction).</p> <p>Slingshot is an interconnect developed by HPE Cray and based on Ethernet, but with proprietary extensions for better HPC performance. It adapts to the regular Ethernet protocols when talking to a node that only supports Ethernet, so one of the attractive features is that regular servers with Ethernet can be directly connected to the  Slingshot network switches. HPE Cray has a tradition of developing their own interconnect for very large systems. As in previous generations, a lot of attention went to adaptive routing and congestion control. There are basically two versions of it. The early version was named Slingshot 10, ran at 100 Gb/s per direction and did not yet have all features. It was used on the initial deployment of LUMI-C compute nodes but has since been upgraded to the full version. The full version with all features is called Slingshot 11. It supports a bandwidth of 200 Gb/s per direction, comparable to HDR InfiniBand with 4x links.  The network also has some features for MPI acceleration.</p> <p>Slingshot is a different interconnect from your typical Mellanox/NVIDIA InfiniBand implementation and hence also has a different software stack. This implies that there are no UCX libraries on the system as the Slingshot 11 adapters do not support that. Instead, the software stack is  based on libfabric (as is the stack for many other Ethernet-derived solutions and even Omni-Path has switched to libfabric under its new owner).</p> <p>LUMI uses the dragonfly topology. This network topology has been used by Cray for a long time already. To function well, it does require a number of features from the switches though.</p>"},{"location":"intro-evolving/01-Architecture/#dragonfly-topology","title":"Dragonfly topology","text":"<p>The dragonfly topology is designed to scale to a very large number of  connections while still minimizing the amount of long cables that have to be used. However, with its complicated set of connections it does rely heavily on adaptive routing and congestion control for optimal performance more than the fat tree topology used in many smaller clusters. It also needs so-called high-radix switches. The Slingshot switch, code-named Rosetta, has 64 ports. 16 of those ports connect directly to compute nodes (and the in a few slides, we will show you how), while the other ports are used to connect the switches.</p> <p></p> <p>The figure in the slide above gives an idea how a dragonfly network is organised. It is shown with far fewer nodes and ports per switch as is used on LUMI, but that makes it easier to grasp the concept.</p> <p>Contrary to a fat tree network used in a lot of clusters, where some switches connect to nodes and other switches only connect to switches, in a dragonfly topology, each switch connects to a number of nodes and to other switches. The figure shows two colours for the connections between switches, and on modern clusters, they will correspond to a different type of cabling being used.</p> <p>Switches are organised in groups. All switches in a group have all-to-all connections to one another. A group usually corresponds with a single rack. Within the rack, distances are short enough that at current network speeds, copper cables can still be used to make the connections.</p> <p>Groups are then also connected in an all-to-all way. These connections go over longer distances (LUMI is the size of a tennis court if you also count storage etc. that is also in the high performance interconnect) and are made with optical cables. Within a group, each switch is used to make connections to some other groups, but no switch is connected to all groups. So if two nodes in different groups communicate to each other, the path is typically:</p> <ol> <li>Node talks to the switch it is connected to.</li> <li>Switch talks to the switch in the group that connects to the receiving group.</li> <li>That switch then talks to its counterpart in the receiving group.</li> <li>From there the message is sent to the switch connecting to the node</li> <li>and finally to the node.</li> </ol> <p>So the shortest path between two nodes in a dragonfly topology never involves  more than 3 hops between switches (so 4 switches): One from the switch the node is connected to  the switch in its group that connects to the other group, a second hop to the other group, and then a third hop in the destination group to the switch the destination node is attached to.</p> <p>We can see the problem here though: Assume that our code would be running on 32 nodes, 16 each connected to the same switch, but both switches in different groups. The nodes connected to the same switch would likely each be able to communicate with one another at the full network speed. However, if all that traffic would go over the single connection which we just outlined to reach the other 16 nodes, all that traffic would go over a single network connection, so  essentially at the speed that a single node can talk to the network. The Slingshot network will then intervene and send some of that traffic over a longer path to avoid saturation. Whereas you may have been told on your local cluster that you should try to get your compute nodes on as few switches as possible, on LUMI that does not make sense unless they would all be on  a single switch (which by the way is non-trivial to request on LUMI as we shall see later), and you're often best off if the nodes are distributed across the cluster. Which fortunately is also a strategy that is less demanding for the scheduler, enabling better throughput of jobs for everybody.</p> <p>(We're a bit pessimistic here though. On smaller slingshot configurations, there could be multiple  links between two switches in a group or multiple links between two groups.)</p> <p></p> <p>On LUMI,</p> <ul> <li> <p>Each switch has 16 node-facing ports. Depending on the node type, these will connect to 8 or 16     different nodes.</p> <p>There are another 48 ports to connect to other switches.</p> </li> <li> <p>Groups can contain 16 or 32 switches, all in a single rack, and those switches     are then connected in an all-to-all way via copper cables.</p> </li> <li> <p>The groups are then connected with optical cables.</p> </li> </ul>"},{"location":"intro-evolving/01-Architecture/#assembling-lumi","title":"Assembling LUMI","text":""},{"location":"intro-evolving/01-Architecture/#compute-blades","title":"Compute blades","text":"<p>LUMI has two types of compute blades: CPU node blades that each contain 4 CPU nodes, and GPU node blades that each contain 2 nodes.</p> <p>The left picture is from a typical Cray EX CPU node, but it is not the LUMI node. The LUMI blades have only two network cards per blade, as each network card is capable of connecting to PCIe slots on two nodes.</p> <p>The right picture is a GPU node blade, again very similar to those on LUMI with one difference  (an additional board covering the CPU). The blade as a whole now has 4 network boards, again with each offering two network connections. </p> <p>Both types of blades are fully water cooled.</p>"},{"location":"intro-evolving/01-Architecture/#switch-blades","title":"Switch blades","text":"<p>The upper right of the above slide shows the switch side facing the compute blades. The picture in the upper right shows how the side where the inter-switch connections are made. You may think that the switch has only 32 connections  (8 on one side and 24 on the other), but all these connections are actually double.</p> <p>The bottom left picture shows how a compute blade would connect to the switches, including a  separate network for management. The picture does contain an error though as NMC0 should connect to the switch 3 slot. Note also that the node layout is 2x2, with each network card connecting to two connectors on the motherboard that come from different nodes.</p> <p>From this picture we can already derive that consecutively numbered nodes on LUMI are not on the same switch. Node 0 and 1 are on \"Switch 3\", node 2 and 3 on \"Switch 7\", then on the next node blade the first two nodes would again be on \"Switch 3\" and so on. So asking for consecutive nodes on LUMI does not give you nodes on a single switch and as we have discussed, this shouldn't really bother you at all. In fact, users should never make such requests due to the way the Slurm scheduler works, as such request can block launching other jobs if the highest priority job is waiting to gather a set of consecutively numbered nodes...</p> <p>The picture on the bottom right is for a CPU node with two network connections per node. In this case, each node would actually be on two different switches as both NMC0 and NMC2 serve node 0 and 1. However, if we think of node 0 and 1 as one node, and node 2 and 3 the other node, this picture would also work for a GPU node (but again with the remark that NMC0 which is shown to be connected to HSS is actually connected to \"Switch 3\"). </p> <p>Each GPU node has 4 network connections, one per GPU, but these are made through two boards with two network interfaces each. So GPU node 0 on the board would be connected to \"Switch 1\" and \"Switch 3\" through two network cards, and similarly GPU node 1 is connected to \"Switch 5\" and \"Switch 7\". So note that two consecutively numbered GPU nodes are actually connected to a different pair of switches!</p> <p>These figures actually also already explain why we have only 8 connectors on the compute node-facing side of the  switches: Each connection goes to a single network card, but that card takes care of two independent connections through a single set of wires.</p>"},{"location":"intro-evolving/01-Architecture/#putting-it-all-in-racks","title":"Putting it all in racks","text":"<p>Let's now have a look at how everything connects together to the supercomputer LUMI. It does show that LUMI is not your standard cluster build out of standard servers.</p> <p>LUMI is built very compactly to minimise physical distance between nodes and to reduce the cabling mess typical for many clusters and the costs of cabling. High-speed copper cables  are expensive, but optical cables and the transceivers that are needed are even more expensive and actually also consume a significant amount of power compared to the switch power. The design of LUMI is compact enough that within a rack, switches can be connected with copper cables in the current network technology and optical cabling is only needed between racks.</p> <p>LUMI does use a custom rack design for the compute nodes that is also fully water cooled. It is build out of units that can contain up to 4 custom cabinets, and a cooling distribution unit (CDU). The size of the complex as depicted in the slide is approximately 12 m<sup>2</sup>. Each cabinet contains 8 compute chassis in 2 columns of 4 rows. In between the two columns is all the power circuitry. Each compute chassis can contain 8 compute blades that are mounted vertically. Each compute blade can contain multiple nodes, depending on the type of compute blades. HPE Cray have multiple types of compute nodes, also with  different types of GPUs. In fact, the Aurora supercomputer which uses Intel CPUs and GPUs and El Capitan, which uses the MI300A APUs (integrated CPU and GPU) use the same design with a different compute blade. Aurora uses compute blades that each contain only a single node, with two Intel Xeon CPUs and 6 Intel Data Centre GPU Max's (code named  Ponte Vecchio). The El Capitan compute blades contain two nodes, each with 4 NMI300A APUs. Each LUMI-C compute blade contains 4 compute nodes and two network interface cards, with each network interface card implementing two Slingshot interfaces and connecting to two nodes. A LUMI-G compute blade contains two nodes and 4 network interface cards, where each interface card now connects to two GPUs in the same  node. All connections for power, management network and high performance interconnect of the compute node are at the back of the compute blade. At the front of the compute blades one can find the connections to the cooling manifolds that distribute cooling water to the blades. One compute blade of LUMI-G can consume up to 5kW, so the power density of this setup is incredible, with 40 kW for a single compute chassis.</p> <p>The back of each cabinet is equally genius. At the back each cabinet has 8 switch chassis, each matching the position of a compute chassis. The switch chassis contains the connection to the power delivery system and a switch for the management network and has 8 positions for  switch blades. These are mounted horizontally and connect directly to the compute blades. Each slingshot switch has 8x2 ports on the inner side for that purpose, two for each compute blade. Hence for LUMI-C two switch blades are needed in each switch chassis as each blade has 4 network interfaces, and for LUMI-G 4 switch blades are needed for each compute chassis as those nodes have 8 network interfaces. Note that this also implies that the nodes on the same  compute blade of LUMI-C will be on two different switches even though in the node numbering they are numbered consecutively. For LUMI-G both nodes on a blade will be on a different pair of switches  and each node is connected to two switches. So when you get a few sequentially numbered nodes, they will not be on a single switch (LUMI-C) or switch pair (LUMI-G). The switch blades are also water cooled (each one can  consume up to 250W). No currently possible configuration of the Cray EX system needs  all switch positions in the switch chassis.</p> <p>This does not mean that the extra positions cannot be useful in the future. If not for an interconnect, one could, e.g., export PCIe ports to the back and attach, e.g., PCIe-based storage via blades as the  switch blade environment is certainly less hostile to such storage than the very dense and very hot compute blades.</p> <p>This architecture is very popular for very large supercomputers. In fact, in the  June 2025 Top-500 list, 6 of the top-10 systems and 10 of the top 20 systems use this system architecture, but with different types of compute blades.</p>"},{"location":"intro-evolving/01-Architecture/#lumi-assembled","title":"LUMI assembled","text":"<p>This slide shows LUMI fully assembled (as least as it was at the end of 2022).</p> <p>At the front there are 5 rows of cabinets similar to the ones in the exploded Cray EX picture  on the previous slide. Each row has 2 CDUs and 6 cabinets with compute nodes.  The first row, the one with the wolf, contains all nodes of LUMI-C, while the other four  rows, with the letters of LUMI, contain the GPU accelerator nodes. At the back of the room there are more  regular server racks that house the storage, management nodes, some special compute nodes , etc. The total size is roughly the size of a tennis court. </p> <p>Remark</p> <p>The water temperature that a system like the Cray EX can handle is so high that in fact the water can be cooled again with so-called \"free cooling\", by just radiating the heat to the environment rather  than using systems with compressors similar to air conditioning systems, especially in regions with a colder climate. The LUMI supercomputer is housed in Kajaani in Finland, with moderate temperature almost  year round, and the heat produced by the supercomputer is fed into the central heating system of the city, making it one of the greenest supercomputers in the world as it is also fed with renewable energy.</p>"},{"location":"intro-evolving/01-Architecture/#local-trainings","title":"Local trainings","text":"<p>The following trainings provide useful preliminary material for this section, and in one case even more details.</p> <ul> <li> <p>VSC:</p> <ul> <li> <p>VSC@UAntwerpen: Supercomputers for Starters course,     organised twice each year.</p> <p>Course notes are available</p> <p>This course given in 2 4 hour sessions goes into more detail about CPU architecture, memory, storage, accelerators, and the software that binds all hardware together to build a cluster.</p> </li> <li> <p>The other introductory courses have a high-level overview of a cluster</p> <ul> <li> <p>VSC@VUB HPC Introduction</p> </li> <li> <p>VSC@UGent Introduction to HPC</p> </li> <li> <p>VSC@KULeuven HPC-intro</p> </li> </ul> </li> </ul> </li> <li> <p>C\u00c9CI: The annual introductory course \"Learning how to use HPC infrastructure\" covers the basics      of cluster architecture.</p> <ul> <li>2022 edition: presentation \"Introduction to high-performance computing</li> </ul> </li> </ul>"},{"location":"intro-evolving/02-CPE/","title":"Programming Environment","text":"<p>Last update of this page: October 2, 2025</p>"},{"location":"intro-evolving/02-CPE/#hpe-cray-programming-environment","title":"HPE Cray Programming Environment","text":"<p>In this session we discuss some of the basics of the operating system and programming environment on LUMI. Whether you like it or not, every user of a supercomputer like LUMI gets confronted with these elements at some point.</p>"},{"location":"intro-evolving/02-CPE/#why-do-i-need-to-know-this","title":"Why do I need to know this?","text":"<p>The typical reaction of someone who only wants to run software on an HPC system when confronted with a talk about development tools is \"I only want to run some programs, why do I need to know about programming environments?\"</p> <p>The answer is that Linux itself is not a proper supercomputer operating system. As we shall see, it is both too much and too little, and development environments are therefore an intrinsic part of an HPC system.  The answer is that development environments are an intrinsic part of an HPC system.  No HPC system is as polished as a personal computer and the software users want to use is typically very unpolished. And some of the essential middleware that turns the hardware with some variant of Linux into a parallel supercomputers is part of the programming  environment. The binary interfaces to those libraries are also not as standardised  as for the more common Linux system libraries.</p> <p>Programs on an HPC cluster are preferably installed from sources to generate binaries optimised for the system. CPUs have gotten new instructions over time that can sometimes speed-up execution of a program a lot, and compiler optimisations that take specific strengths and weaknesses of particular CPUs into account can also gain some performance. Even just a 10% performance gain on an investment of 160 million EURO such as LUMI means a lot of money. When running, the build environment on most systems needs to be at least partially recreated. This is somewhat less relevant on Cray systems as we will see at the end of this part of the course, but if you want reproducibility it becomes important again.</p> <p>Compiling on the system is also the easiest way to guarantee compatibility of the binaries with the system. </p> <p>Even when installing software from prebuilt binaries some modules might still be needed. Prebuilt binaries will typically include the essential runtime libraries for the parallel technologies they use, but these may not be compatible with LUMI.  In some cases this can be solved by injecting a library from LUMI, e.g., you may want to inject an optimised MPI library as we shall see in the container section of this course. But sometimes a binary is simply incompatible with LUMI and there is no other solution than to build the software from sources.</p> <p>Clusters in Belgium</p> <p>There are differences in the setup of the operating system on LUMI compared to the VSC, C\u00c9CI and Cenaero clusters in Belgium.</p> <p>Lucia, the Cenaero cluster and Walloon tier-1 system, has a programming environment which is similar to LUMI. But all other Belgian systems have  a programming environment that is more inspired on the GNU and other  Open Source software approach. (I don't want to say \"a more traditional\"  programming environment as in fact the HPE Cray PE goes back in its approach to the environment on UNIX workstations and supercomputers in the '90s, so is in fact the more traditional one. It's just that we have forgotten those  traditions...)</p>"},{"location":"intro-evolving/02-CPE/#the-operating-system-on-lumi","title":"The operating system on LUMI","text":"<p>The login nodes of LUMI run a regular SUSE Linux Enterprise Server 15 SP5 distribution. That alone already causes subtle differences with the Red Hat clones and Debian Linux derivatives (like Ubuntu) that are popular on many clusters. The compute nodes however run Cray OS, a restricted version of the SUSE Linux that runs on the login nodes. Some daemons are inactive or configured differently and Cray also  does not support all regular file systems. The goal of this is to minimize OS jitter, interrupts that the OS handles and slow down random cores at random moments, that can  limit scalability of programs. Yet on the GPU nodes there was still the need to reserve one core for the OS and driver processes. This in turn led to an asymmetry in the setup so now 8 cores are reserved, one per CCD, so that all CCDs are equal again.</p> <p>This also implies that some software that works perfectly fine on the login nodes may not work on the compute nodes. E.g., you will see that there is no <code>/run/user/$UID</code> directory.</p> <p>Large HPC clusters also have a small system image, so don't expect all the bells-and-whistles  from a Linux workstation to be present on a large supercomputer (and certainly not in the same way as they would be on a workstation). Since LUMI compute nodes are diskless, the system image actually occupies RAM which is another reason to keep it small.</p> Some missing pieces <p>Compute nodes don't run a per-user dbus daemon, so some if not all DBUS functionality is missing. And D-Bus may sometimes show up in places where you don't expect it... It may come from freedesktop.org but is is not only used for desktop software.</p> <p>Compute nodes on a Cray system have Lustre as the main remote file system.  They do not directly mount any other networked file system like NFS, GPFS  or CernVM-FS (the latter used by, e.g., Cern for  distributing software for the Large Haedron Collider and the EESSI project). Instead these file systems are mounted on external servers in the admin section of the cluster and the Cray Data Virtualisation Service (DVS) is then used to access those file systems from the compute nodes over the high-speed interconnect. This does require additional server capacity in the admin nodes of the cluster, but the benefit of this approach is  that all background jitter from the daemons of those remote file systems, cannot negatively affect the scalability of programs running on the compute nodes.</p>"},{"location":"intro-evolving/02-CPE/#battling-os-jitter-low-noise-mode","title":"Battling OS jitter: Low-noise mode","text":"<p>Low-noise mode has meant different things throughout the history of Cray systems.  Sometimes the mode described above, using only a selection of the regular Linux daemons on the compute nodes, was already called low-noise mode while some Cray systems provided another mode in which those daemons were activated. Depending on the cluster this was then called \"emulation mode\" or \"Cluster Compatibility Mode\". The latter is not implemented on LUMI, and even if it would, compatibility would still be limited by the special requirements to use the Slingshot interconnect and to have GPU-aware communication over Slingshot. The mode we use on the LUMI CPU-nodes used to be called \"Extreme Scalability Mode\" (ESM) in the Cray documentation.</p> <p>However, it turned out that even the noise reduction described above was not yet sufficient to pass some large-scale scalability tests, and therefore another form of \"low-noise\" mode is implemented on the GPU nodes of LUMI where OS processes are restricted to a reserved core, actually core 0. Cray calls this \"core specialization\" and this idea already goes back to around 2010 when we got the first compute nodes with more than 20 cores (with the AMD \"Magny Cours\" Opteron). This leaves us with an asymmetric structure of the node, where the first CCD has 7 available cores while the other ones have 8, but as that created a headache for users to get a proper distribution of tasks and threads over the CPU  (see the \"Process and thread distribution and binding\" chapter), the choice was made to also disable the first core on each of the other CCDs so that users now effectively see a 56-core node with 8 CCDs with 7 cores each.</p> <p>This shows perfectly that sometimes compromises need to be made to improve application scalability and that a supercomputer does not always function as a network of workstations. Not all workstation software may work properly on a LUMI compute node.</p>"},{"location":"intro-evolving/02-CPE/#programming-models","title":"Programming models","text":"<p>On LUMI we have several C/C++ and Fortran compilers. These will be discussed more in this session.</p> <p>There is also support for MPI and SHMEM for distributed applications. And we also support RCCL, the ROCm-equivalent of the  CUDA NCCL library that is popular in machine learning packages.</p> <p>All compilers have some level of OpenMP support,  and two compilers support OpenMP offload to  the AMD GPUs, but again more about that later.</p> <p>OpenACC, the other directive-based model for GPU offloading,  is only supported in the Cray Fortran compiler. There is no commitment of neither HPE Cray or AMD to extend that support to C/C++ or other compilers, even though there is work going on in the LLVM community and several compilers on the system are based on LLVM.</p> <p>The other important programming model for AMD GPUs is HIP (Heterogeneous-Compute Interface for Portability),  which is their alternative for the proprietary CUDA model. It does not support all CUDA features though (basically it is more CUDA 7 or 8 level)  and there is also no equivalent to CUDA Fortran.</p> <p>The commitment to OpenCL is very unclear, and this actually holds for other GPU vendors also.</p> <p>We also try to provide SYCL as it is a programming language/model that works on all three GPU families currently used in HPC. </p> <p>Python is of course pre-installed on the system but we do ask to use big Python installations in a special way as Python puts a tremendous load on the file system. More about that later in this course.</p> <p>Some users also report some success in running Julia. We don't have full support though and have to depend on binaries as provided by julialang.org.</p> <p>It is important to realise that there is no CUDA on AMD GPUs and there will never be as this is a  proprietary technology that other vendors cannot implement. The visualisation nodes in LUMI have NVIDIA rendering GPUs but these nodes are meant for visualisation and not for compute.</p>"},{"location":"intro-evolving/02-CPE/#the-development-environment-on-lumi","title":"The development environment on LUMI","text":"<p>Long ago, Cray designed its own processors and hence had to develop their own compilers. They kept doing so, also when they moved to using more standard components, and had a lot of expertise in that field, especially when it comes to the needs of  scientific codes, programming models that are almost only used in scientific computing or stem from such projects. As they develop their own interconnects, it does make sense to also develop an MPI implementation that can use the interconnect in an optimal way. They also have a long tradition in developing performance measurement and analysis tools  and debugging tools that work in the context of HPC.</p> <p>The first important component of the HPE Cray Programming Environment is the compilers.</p> <ul> <li> <p>Cray still builds its own compilers for C/C++ and Fortran, called the Cray Compiling     Environment (CCE). </p> </li> <li> <p>The GNU compilers are also supported on every Cray system,     though at the moment AMD GPU support is not enabled.      In recent versions of the HPE Cray PE, HPE uses the GNU compilers as packaged in      the SUSE Enterprise distribution. As a result of this, the GNU compiler executables     now also have their version in the name as modules are not a standard element of every     SUSE system.</p> </li> </ul> <p>Depending on the hardware of the system, other compilers will also be provided and  integrated in the environment. On LUMI two other compilers are available: </p> <ul> <li> <p>The AMD AOCC compiler for CPU-only code with a C/C++ compiler and Fortran compiler.</p> </li> <li> <p>AMD ROCm\u2122 compilers for GPU programming with a C/C++ compiler and Fortran compiler.</p> <p>The ROCm compilers also contain the support for HIP, AMD's CUDA clone.</p> </li> </ul> <p>Both AMD compilers are discussed in the next section.</p> <p>The second component is the Cray Scientific and Math libraries, containing the usual suspects as BLAS, LAPACK and ScaLAPACK, and FFTW, but also some data libraries  such as some configurations of netCDF and HDF5, and Cray-only libraries.</p> <p>The third component is the Cray Message Passing Toolkit. It provides an MPI implementation optimized for Cray systems, but also the Cray SHMEM libraries, an implementation of OpenSHMEM 1.5.</p> <p>The fourth component is some Cray-unique sauce to integrate all these components, and  support for hugepages to make memory access more efficient for some programs that  allocate huge chunks of memory at once.</p> <p>Other components include the Cray Performance Measurement and Analysis Tools and the  Cray Debugging Support Tools that will not be discussed in this introductory course, and Python and R modules that both also provide some packages compiled with support for the Cray Scientific Libraries.</p> <p>Besides the tools provided by HPE Cray, several of the development tools from the ROCm stack are also available on the system while some others can be user-installed (and one of those, the Grafana-based GUI for Omniperf, is not available due to security concerns). Furthermore there are some third party tools available on LUMI, including Linaro Forge (previously ARM Forge) and Vampir and some open source profiling tools.</p> <p>Specifically not on LUMI is the Intel programming environment, nor is the regular Intel oneAPI HPC Toolkit. The classic Intel compilers pose problems on AMD CPUs as <code>-xHost</code> cannot be relied on, but it appears that the new compilers that are based on Clang and an LLVM backend behave better. However, various MKL versions are also troublesome, with different workarounds for different versions, though here also it seems that Intel now has  code that works well on AMD for many MKL routines. We have experienced problems with Intel  MPI when testing it on LUMI though in principle it should be possible to use Cray MPICH as they are derived from the same version of MPICH.  The NVIDIA programming environment doesn't make sense on an AMD GPU system,  but it could be useful for some visualisation software on the  visualisation nodes so it is currently installed on those nodes.</p> <p>We will now discuss some of these components in a little bit more detail, but refer to the advanced 4 or 5-day trainings that the LUMI User Support Team organises several times a year with HPE for more material.</p>"},{"location":"intro-evolving/02-CPE/#amd-tools-and-technologies-on-lumi","title":"AMD tools and technologies on LUMI","text":""},{"location":"intro-evolving/02-CPE/#gpu-tools","title":"GPU tools","text":"<p>The AMD software stack for GPU compute is ROCm\u2122. It is fully open-source, so  no proprietary components as in the NVIDIA software stack. It does borrow from  the CUDA stack though wherever legally possibly, but keep in mind that the AMD and NVIDIA hardware is very different, so code that runs well on one may not run as well on the other after a very simple port.</p> <p>The key element of the ROCm\u2122 stack is HIP,  which stands for Heterogeneous-computing Interface for Portability. It is a C++ runtime API and kernel language  that lets you create portable applications for AMD and NVIDIA GPUs from a single source code. It basically supports a subset of CUDA, though all functions names are different (but in a straightforward way) as NVIDIA does not allow copying the API.  However, with some AMD tools installed, HIP code can be compiled for NVIDIA  hardware with virtually no performance loss compared to native CUDA.  This comes with a catch though: NVIDIA and AMD hardware are very different, so the tricks needed to optimise code may also be different and HIP code optimised for AMD hardware may not be optimal for NVIDIA hardware and vice-versa. In fact, on AMD hardware, there are also significant differences between the Instinct compute GPUs that are based on the CDNA architecture, and recent consumer rendering GPUs based on the RDNA architecture.</p> <p>Many libraries are also very close to their CUDA counterpart, and those libraries often come in two variants, one with the name starting with roc and the other with  the name starting with hip. The rocX variant is then the actual library for the ROCm\u2122 stack and AMD hardware, while the hipX variant is a thin interface library that will link to rocX when building for AMD GPUs and to the corresponding CUDA library when building for NVIDIA hardware. So if you want the most performance, you'd directly call the rocX library, but if you're more concerned about also recompiling the same code for NVIDIA hardware, you'd use the hipX variant.</p> <p>The ROCm\u2122 stack also contains two tools to assist with converting CUDA code into  HIP code. The <code>hipify-perl</code>  tool is a Perl script that is very easy to use and makes heavy use of regular expressions. It can however not really interpret the CUDA code so some more complex conversions are not possible. The <code>hipify-clang</code> tool on the other hand is based on clang and can deal with more complex conversions, but it requires 100% correct CUDA code to start with and a partial installation of CUDA.</p> <p>Another important component are the AMD ROCm\u2122 compilers for GPU programming. There is a C/C++ compiler with advanced OpenMP offload support, based on clang and LLVM. The unified memory architecture of MI250X (and MI300A) is fully supported. The Fortran compiler is currently still based on the former PGI Fortran frontend (also known as classic flang) and misses OpenMP offload support. With ROCm\u2122 7 AMD is switching to the new Fortran frontend, and in fact,  with ROCm 6.3 users can have early access, but unfortunately neither of these versions of ROCm can be supported on LUMI with the current driver version and will only be supportable after the update planned for the summer of 2025. The other issue is that switching to a different Fortran frontend may break the Fortran MPI bindings that come with Cray MPICH, as they include precompiled module files. So even if you can get an early beta that works with a ROCm version that  we can support, we cannot guarantee that it will also work with MPI, or other libraries from HPE Cray.</p> <p>The ROCm\u2122 stack also has its own command line interface debugging tool,  rocgdb. It builds on the GNU debugger gdb and can be used directly by users, but is also used internally by other debugging tools from third parties.</p> <p>The ROCm\u2122 stack also includes three tools for performance measurement and analysis.</p> <ul> <li> <p>ROC-profiler      with the <code>rocprof</code> executable is the basic tool to extract information     from the performance counters and to collect that information while running programs.</p> <p>It is a command-line only tool so interpreting the output is hard. It does however serve as the information collecting tool for other profiling tools.</p> </li> <li> <p>OmniTrace is a more comprehensive      profiling and tracing tool, supporting C, C++, Fortran and Python on CPU     and CP+GPU. It also supports interactive visualisation of trace data in a      web-based GUI (that can be installed as an app on your laptop).</p> <p>Before ROCm\u2122 6.2, this was not an official product but a product from AMD Research. With ROCm\u2122 6.2, it became an official part of ROCm\u2122 and in ROCm\u21226.3, it was renamed to ROCm Systems Profiler.</p> </li> <li> <p>Omniperf is      a performance profiler for HPC and machine learning applications. It uses      ROC-profiler under the hood to collect data, but offers several high level     performance analysis tools (like roofline analyses, ...) to better understand     application behaviour at a high level.</p> <p>Before ROCm\u2122 6.2, this was not an official product but a product from AMD Research. With ROCm\u2122 6.2, it became an official part of ROCm\u2122 and in ROCm\u21226.3, it was renamed to ROCm Compute Profiler or <code>rocprof-compute</code>.</p> <p>The tool now also has a standalone GUI which is much safer than the Grafana-based GUI used before.</p> </li> </ul> <p>The AMD profiling tools are currently going through a very quick evolution.  ROCm\u2122 6.2 saw a great improvement of these tools. The 6.2 versions can be used on  LUMI already, even though we cannot fully support them as only ROCm 6.0 is fully supported in the current programming environment.</p> <p>The debugger and profiling tools, and the structure of HIP programs, are discussed in several sessions of the advanced LUMI courses (the 4 or 5-day courses) given by  LUST in collaboration with HPE and AMD.</p>"},{"location":"intro-evolving/02-CPE/#cpu-tools","title":"CPU tools","text":"<p>On the CPU side, the AMD AOCC compiler  is fully integrated in the HPE Cray PE. These are C/C++ and Fortran compilers based on clang, classic flang and LLVM technology but with some AMD extensions to the optimisations. </p> <p>The version installed on the system is determined by the version officially supported by the HPE Cray PE, as MPI and the math libraries come from the HPE Cray PE.  As these compilers are freely available, users are free to experiment with newer versions, but we cannot guarantee that they will work with the MPI and mathematics libraries on the system. </p> <p>The AMD Optimizing CPU libraries (AOCL) are not part of the HPE Cray PE though, as the environment uses its own math libraries that would conflict with some of the AOCL libraries. For some of those libraries, LUST does provide installation scripts so that users can experiment, but be aware that they do conflict with Cray-provided libraries and hence many of the pre-installed libraries will conflict. LUST does not build software using AOCL instead of the Cray scientific libraries and does not have enough resources to support this, but if you feel confident, you can use them.</p>"},{"location":"intro-evolving/02-CPE/#the-cray-compiling-environment","title":"The Cray Compiling Environment","text":"<p>The Cray Compiling Environment are the default compilers on many Cray systems and on LUMI. These compilers are designed specifically for scientific software in an HPC environment. The current versions are LLVM-based with extensions by HPE Cray for automatic vectorization and shared memory parallelization, technology that they have experience with since the late '70s or '80s.</p> <p>The compiler offers extensive standards support. The C and C++ compiler is essentially their own build of Clang with LLVM with some of their optimisation plugins and OpenMP run-time. The version numbering of the CCE currently follows the major versions of the Clang compiler used. The support for C and C++ language standards corresponds to that of Clang. The Fortran compiler uses a frontend and optimiser developed by HPE Cray, but an LLVM-based code generator.  The compiler supports most of Fortran 2018 (ISO/IEC 1539:2018). The CCE Fortran compiler is known to be very strict with language standards. Programs that use GNU or Intel extensions will usually fail to compile, and unfortunately since many developers only test with these compilers, much Fortran code is not fully standards compliant and will fail.</p> <p>All CCE compilers support OpenMP, with offload for AMD and NVIDIA GPUs.  In their most recent versions, they claim full OpenMP 5.0 support with partial (and growing) support for OpenMP 5.1 and 5.2. More  information about the OpenMP support is found by checking a manual page: <pre><code>man intro_openmp\n</code></pre> which does require that the <code>cce</code> module is loaded, or the web version of that page which may be for a more recent version of the programming environment than available on LUMI. The Fortran compiler also supports OpenACC for AMD and NVIDIA GPUs. That implementation claims to be fully OpenACC 2.0 compliant, and offers partial support for OpenACC 2.x/3.x.  Information is available via <pre><code>man intro_openacc\n</code></pre> or the corresponding web version of that page which again may be for a more recent version of the programming environment than available on LUMI. AMD and HPE Cray still recommend moving to OpenMP which is a much broader supported standard. There are no plans to also support OpenACC in the Cray C/C++ compiler, nor are there any  plans for support by AMD in the ROCm stack.</p> <p>The CCE compilers also offer support for some PGAS (Partitioned Global Address Space) languages. UPC 1.2 is supported, as is Fortran 2008 coarray support. These implementations do not require a preprocessor that first translates the code to regular C or Fortran. There is also support for debugging with Linaro Forge.</p> <p>Lastly, there are also bindings for MPI.</p>"},{"location":"intro-evolving/02-CPE/#scientific-and-math-libraries","title":"Scientific and math libraries","text":"<p>Cray Scientific and Math Libraries overview web page</p> <p>Some mathematical libraries have become so popular that they basically define an API for which several implementations exist, and CPU manufacturers and some open source groups spend a significant amount of resources to make optimal implementations for each CPU architecture.</p> <p>The most notorious library of that type is BLAS, a set of basic linear algebra subroutines for vector-vector, matrix-vector and matrix-matrix implementations. It is the basis for many other libraries that need those linear algebra operations, including Lapack, a library with solvers for linear systems and eigenvalue problems.</p> <p>The HPE Cray LibSci library  contains BLAS and its C-interface CBLAS, and LAPACK and its C interface LAPACKE. It also adds ScaLAPACK, a distributed memory version of LAPACK, and BLACS, the  Basic Linear Algebra Communication Subprograms, which is the communication layer used by ScaLAPACK. The BLAS library combines implementations from different sources, to try to offer the most optimal one for several architectures and a range of matrix and vector sizes.</p> <p>LibSci also contains one component which is HPE Cray-only: IRT, the Iterative Refinement Toolkit,  which allows to do mixed precision computations for LAPACK operations that can speed up the generation of a double precision result with nearly a factor of two for those problems that are suited for iterative refinement. If you are familiar with numerical analysis, you probably know that the matrix should not be too ill-conditioned for that.</p> <p>There is also a GPU-optimized version of LibSci, called  LibSci_ACC, which contains a subset of the routines of LibSci. We or the LUMI User Support Team don't have much experience with this library though. It can be compared with what Intel is doing with oneAPI MKL which also offers GPU versions of some of the traditional MKL routines.</p> <p>Another separate component of the scientific and mathematical libraries is  FFTW3, Fastest Fourier Transforms in the West,  which comes with optimized versions for all CPU architectures supported by recent HPE Cray machines.</p> <p>Finally, the scientific and math libraries also contain HDF5 and netCDF libraries in sequential and parallel versions.  These are included because it is essential that  they interface properly with MPI parallel I/O and the Lustre file system to offer the best bandwidth to and from storage. </p> <p>Cray used to offer more pre-installed third party libraries for which the only added value was that they compiled the binaries. Instead they now offer build scripts in a GitHub repository.</p>"},{"location":"intro-evolving/02-CPE/#cray-mpi","title":"Cray MPI","text":"<p>HPE Cray build their own MPI library with optimisations for their own interconnects. The Cray MPI library is derived from the ANL MPICH 3.4 code base and fully supports the  ABI (Application Binary Interface) of that implementation which implies that in principle it should be possible to swap the MPI library of applications build with that ABI with the Cray MPICH library. Or in other words, if you can only get a binary distribution of an application and that application was build against an MPI library compatible with  the MPICH 3.4 ABI (which includes Intel MPI), it should be possible to exchange that library for the Cray one to have optimised communication on the Cray Slingshot interconnect.</p> <p>Cray MPI contains many tweaks specifically for Cray systems. HPE Cray claim improved algorithms for many collectives, an asynchronous progress engine to improve overlap of communications and computations,  customizable collective buffering when using MPI-IO, and optimized remote memory access (MPI one-sided communication) which also supports passive remote memory access.</p> <p>When used in the correct way (some attention is needed when linking applications) it is also fully GPU aware with currently support for AMD and NVIDIA GPUs.</p> <p>The MPI library also supports bindings for Fortran 2008.</p> <p>MPI 3.1 is almost completely supported, with two exceptions. Dynamic process management is not supported (and a problem anyway on systems with batch schedulers), and when using CCE <code>MPI_LONG_DOUBLE</code> and <code>MPI_C_LONG_DOUBLE_COMPLEX</code> are also not supported. The MPI standard is currently at version 4.1 however, but the 4.X extensions are not yet supported by Cray MPICH. A new version with support for MPI 4.0 based on MPICH 4.1 is in the works and the first preview versions are available to users.</p> <p>The Cray MPI library does not support the <code>mpirun</code> or <code>mpiexec</code> commands, which is in fact allowed by the standard which only requires a process starter and suggest <code>mpirun</code> or <code>mpiexec</code>  depending on the version of the standard. Instead the Slurm <code>srun</code> command is used as the  process starter. This actually makes a lot of sense as the MPI application should be mapped correctly on the allocated resources, and the resource manager is better suited to do so.</p> <p>Cray MPI on LUMI is layered on top of libfabric, which in turn uses the so-called Cassini provider to interface with the hardware. UCX is not supported on LUMI (but Cray MPI can support it when used on InfiniBand clusters). It also uses a GPU Transfer Library (GTL) for GPU-aware MPI.</p>"},{"location":"intro-evolving/02-CPE/#gpu-aware-mpi","title":"GPU-aware MPI","text":"<p>Cray MPICH does support GPU-aware MPI, so it is possible to directly use GPU-attached communication buffers using device pointers. The implementation supports (a) GPU-NIC RDMA for efficient inter-node MPI transfers and (b) GPU Peer2Peer IPC for efficient intra-node transfers. The latter mechanism comes with some restrictions though that we will discuss in the chapter \"Process and thread distribution and binding\".</p> <p>GPU-aware MPI needs to be enabled explicitly, which you can do by setting an environment variable:</p> <pre><code>export MPICH_GPU_SUPPORT_ENABLED=1\n</code></pre> <p>In addition to this, if the GPU code does use MPI operations that access GPU-attached memory regions it is best to also set</p> <pre><code>export MPICH_OFI_NIC_POLICY=GPU\n</code></pre> <p>to tell MPICH to always use the NIC closest to the GPU.</p> <p>If only CPU communication buffers are used, then it may be better to set</p> <pre><code>export MPICH_OFI_NIC_POLICY=NUMA\n</code></pre> <p>which tells MPICH to use the NIC closest to the CPU NUMA domain.</p> <p>Neither of those values is the default (the default is <code>BLOCK</code> and is more suited  for multi-NIC CPU nodes in Cray systems) so setting <code>MPICH_OFI_NIC_POLICY</code> properly for GPU nodes can make a difference.</p> <p>Depending on how Slurm is used, Peer2Peer IPC may not work and in those cases you may want to turn it off using <pre><code>export MPICH_GPU_IPC_ENABLED=0\n</code></pre> or alternatively <pre><code>export MPICH_SMP_SINGLE_COPY_MODE=NONE\n</code></pre> Both options entail a serious loss of performance. The underlying problem is that the way in which Slurm does GPU binding using control groups makes other GPUS from other tasks in the node invisible to a task.</p> <p>More information about Cray MPICH and the many environment variables to fine-tune performance can be found in the manual page</p> <pre><code>man intro_mpi\n</code></pre> <p>or its web-based version which may be for a newer version than available on LUMI.</p>"},{"location":"intro-evolving/02-CPE/#lmod","title":"Lmod","text":"<p>Virtually all clusters use modules to enable the users to configure the environment and select the versions of software they want. There are three different module systems around. One is an old implementation that is hardly evolving anymore but that can still be found on a number of clusters. HPE Cray still offers it as an option. Modulefiles are written in TCL, but the tool itself is in C. The more popular tool at the moment is probably Lmod. It is largely compatible with modulefiles for the old tool, but prefers modulefiles written in LUA. It is also supported by the HPE Cray PE and is our choice on LUMI. The final implementation is a full TCL implementation developed  in France and also in use on some large systems in Europe.</p> <p>Fortunately the basic commands are largely similar in those implementations, but what differs is the way to search for modules. We will now only discuss the basic commands, the more advanced ones will be discussed in the next session of this tutorial course.</p> <p>Modules also play an important role in configuring the HPE Cray PE, but before touching that  topic we present the basic commands:</p> <ul> <li><code>module avail</code>: Lists all modules that can currently be loaded. </li> <li><code>module list</code>: Lists all modules that are currently loaded</li> <li><code>module load</code>: Command used to load a module. Add the name and version of the module.</li> <li><code>module unload</code>: Unload a module. Using the name is enough as there can only one version be      loaded of a module.</li> <li><code>module swap</code>:  Unload the first module given and then load the second one. In Lmod this is      really equivalent to a <code>module unload</code> followed by a <code>module load</code>.</li> </ul> <p>Lmod supports a hierarchical module system. Such a module setup distinguishes between installed modules and available modules. The installed modules are all modules that can be loaded in one way or another by the module systems, but loading some of those may require loading other modules first. The available modules are the modules that can be loaded directly without loading any other module. The list of available modules changes all the time based on modules that are already loaded, and if you unload a module that makes other loaded modules unavailable, those will also be deactivated by Lmod. The advantage of a hierarchical module system is that one can support multiple configurations of a module while all configurations can have the same name and version. This is not fully exploited on LUMI, but it  is used a lot in the HPE Cray PE. E.g., the MPI libraries for the various compilers on the system all have the same name and version yet make different binaries available depending on the compiler that is being used.</p> <p>Understanding how Lmod is used on LUMI, is important. There is a separate chapter, \"Modules on LUMI\" devoted to them in these notes.</p> <p>Different configurations on some Belgian clusters</p> <p>Depending on the configuration Lmod can behave rather differently on different systems. Some clusters in Belgium have Lmod configured to mimic the original Tcl module system better rather than exposing the full power of Lmod.</p> <p>E.g., on LUMI, <code>module swap</code> isn't really needed as the auto-swap feature of Lmod is  enabled. Automatically unloading a module if another module with the same name is  loaded, is also enabled. Both features make with a hierarchical scheme much more powerful and using the HPE Cray PE with these features disabled would be very difficult.</p>"},{"location":"intro-evolving/02-CPE/#compiler-wrappers","title":"Compiler wrappers","text":"<p>The HPE Cray PE compilers are usually used through compiler wrappers. The wrapper for C is <code>cc</code>, the one for C++ is <code>CC</code> and the one for Fortran is <code>ftn</code>. The wrapper then calls the selected compiler. Which compiler will be called is determined by which compiler module is loaded. As shown on the slide  \"Development environment on LUMI\", on LUMI the Cray Compiling Environment (module <code>cce</code>), GNU Compiler Collection (module <code>gcc-native</code>  or <code>gcc</code> depending on the version of the Cray Programming Environment),  the AMD Optimizing Compiler for CPUs (module <code>aocc</code>) and the ROCm LLVM-based compilers (module <code>amd</code>) are available. On the visualisation nodes, the NVIDIA HPC compiler is currently also installed (module <code>nvhpc</code>). On other HPE Cray systems, you may also find the Intel compilers. The target architectures for CPU and GPU are also selected through modules, so it is better to not use compiler options such as <code>-march=native</code>. This makes cross compiling also easier.</p> <p>The wrappers will also automatically link in certain libraries, and make the include files available, depending on which other modules are loaded. In some cases it tries to do so cleverly, like selecting an MPI, OpenMP, hybrid or sequential option depending on whether the MPI module is loaded and/or OpenMP compiler flag is used. This is the case for:</p> <ul> <li>The MPI libraries. There is no <code>mpicc</code>, <code>mpiCC</code>, <code>mpif90</code>, etc. on LUMI (well, there is nowadays, but their     use is discouraged). The regular compiler     wrappers do the job as soon as the <code>cray-mpich</code> module is loaded.</li> <li>LibSci and FFTW are linked automatically if the corresponding modules are loaded. So no need     to look, e.g., for the BLAS or LAPACK libraries: They will be offered to the linker if the     <code>cray-libsci</code> module is loaded (and it is an example of where the wrappers try to take the     right version based not only on compiler, but also on whether MPI is loaded or not and the     OpenMP compiler flag).</li> <li>netCDF and HDF5</li> </ul> <p>It is possible to see which compiler and linker flags the wrappers add through the <code>-craype-verbose</code> flag.</p> <p>The wrappers do have some flags of their own, but also accept all flags of the selected compiler and  simply pass those to those compilers.</p> <p>The compiler wrappers are provided by the <code>craype</code> module (but you don't have to load that module by hand).</p>"},{"location":"intro-evolving/02-CPE/#selecting-the-version-of-the-cpe","title":"Selecting the version of the CPE","text":"<p>The version numbers of the HPE Cray PE are of the form <code>yy.dd</code>, e.g., <code>24.03</code> for the version released in March 2024. There are several releases each year, but not all of them are offered on LUMI as installing them may require downtime and as changes in the system every few months are not appreciated by all users. </p> <p>There is always a default version assigned by the sysadmins when installing the programming environment. It is possible to change the default version for loading further modules by loading one of the versions of the <code>cpe</code> module. E.g., assuming the 24.03 version would be present on the system, it can be loaded through <pre><code>module load cpe/24.03\n</code></pre> Loading this module will also try to switch the already loaded PE modules to the versions from that release. This does not always work correctly, due to some bugs in most versions of this module and a limitation of Lmod. Executing the <code>module load</code> twice will fix this: <pre><code>module load cpe/24.03\nmodule load cpe/24.03\n</code></pre> The module will also produce a warning when it is unloaded (which is also the case when you do a <code>module load</code> of <code>cpe</code> when one is already loaded, as it then first unloads the already loaded <code>cpe</code> module). The warning can be ignored, but keep in mind that what it says is true, it cannot restore the environment you found on LUMI at login.</p> <p>The <code>cpe</code> module is also not needed when using the LUMI software stacks, but more about that later.</p>"},{"location":"intro-evolving/02-CPE/#the-target-modules","title":"The target modules","text":"<p>The target modules are used to select the CPU and GPU optimization targets and to  select the network communication layer. </p> <p>On LUMI there are three CPU target modules that are relevant:</p> <ul> <li><code>craype-x86-rome</code> selects the Zen2 CPU family code named Rome. These CPUs are     used on the login nodes and the nodes of the data analytics and visualisation      partition of LUMI. However, as Zen3 is a superset of Zen2, software compiled     to this target should run everywhere, but may not exploit the full potential     of the LUMI-C and LUMI-G nodes (though the performance loss is likely minor).</li> <li><code>craype-x86-milan</code> is the target module for the Zen3 CPUs code named Milan that     are used on the CPU-only compute nodes of LUMI (the LUMI-C partition).</li> <li><code>craype-x86-trento</code> is the target module for the Zen3 CPUs code named Trento that     are used on the GPU compute nodes of LUMI (the LUMI-G partition).</li> </ul> <p>Two GPU target modules are relevant for LUMI:</p> <ul> <li><code>craype-accel-gfx90a</code>: Compile offload code for the MI200 series GPUs that are used on LUMI-G.</li> <li><code>craype-accel-host</code>: Will tell some compilers to compile offload code for the host     instead.</li> </ul> <p>Two network target modules are relevant for LUMI:</p> <ul> <li><code>craype-network-ofi</code> selects the libfabric communication layer which is needed for     Slingshot 11.</li> <li><code>craype-network-none</code> omits all network specific libraries.</li> </ul> <p>The compiler wrappers also have corresponding compiler flags that can be used to overwrite these settings: <code>-target-cpu</code>, <code>-target-accel</code> and <code>-target-network</code>.</p>"},{"location":"intro-evolving/02-CPE/#prgenv-and-compiler-modules","title":"PrgEnv and compiler modules","text":"<p>In the HPE Cray PE, the <code>PrgEnv-*</code> modules are usually used to load a specific variant of the programming environment. These modules will load the compiler wrapper (<code>craype</code>), compiler, MPI and LibSci module and may load some other modules also.</p> <p>The following table gives an overview of the available <code>PrgEnv-*</code> modules and the compilers they activate:</p> PrgEnv Description Compiler module Compilers PrgEnv-cray Cray Compiling Environment <code>cce</code> <code>craycc</code>, <code>crayCC</code>, <code>crayftn</code> PrgEnv-gnu GNU Compiler Collection <code>gcc-native</code><code>gcc</code><sup>(*)</sup> <code>gcc-13</code>, <code>g++-13</code>, <code>gfortran-13</code><code>gcc</code>, <code>g++</code>, <code>gfortran</code> PrgEnv-aocc AMD Optimizing Compilers(CPU only) <code>aocc</code> <code>clang</code>, <code>clang++</code>, <code>flang</code> PrgEnv-amd AMD ROCm LLVM compilers (GPU support) <code>amd</code> <code>amdclang</code>, <code>amdclang++</code>, <code>amdflang</code> <p><sup>(*)</sup> In the 23.12 programming environment, HPE Cray switched from own-built gcc binaries to those from SUSE, and the module name was changed to <code>gcc-native</code> to reflect this change.</p> <p>There is also a second module that offers the AMD ROCm environment, <code>rocm</code>. That module has to be used with <code>PrgEnv-cray</code>, <code>PrgEnv-gnu</code> and recently also with <code>PrgEnv-amd</code> to  enable MPI-aware GPU, hipcc with the GNU compilers or GPU support with the Cray compilers.</p> Changes to the GNU compilers since 23.12 <p>The HPE Cray PE has changed the way it offers the GNU compilers in releases starting from 23.12. Rather than packaging the GNU compilers, HPE Cray now uses the development compiler packages of SUSE Linux (not to be confused with the system default which is still 7.5, the compiler that was offered with the initial release of SUSE Enterprise Linux 15).</p> <p>In releases up to the 23.09, the GNU compilers are offered through the <code>gcc</code> compiler module. When loaded, it adds newer versions of the <code>gcc</code>, <code>g++</code> and <code>gfortran</code> compilers to the path, calling the version indicated by the version of the <code>gcc</code> module.</p> <p>In releases from 23.12 on, that compiler module is now called <code>gcc-native</code>, and the compilers now have the major version attached to the name of binary, e.g., <code>gcc-13</code>, <code>g++-13</code> and <code>gfortran-13</code>, while <code>gcc</code>, <code>g++</code> and <code>gfortran</code> will compile with version 7.5, the default version for SUSE 15.</p>"},{"location":"intro-evolving/02-CPE/#getting-help","title":"Getting help","text":"<p>Help on the HPE Cray Programming Environment is offered mostly through manual pages and compiler flags. Online help is limited and difficult to locate.</p> <p>For the compilers, the following man pages are relevant:</p> PrgEnv C C++ Fortran PrgEnv-cray <code>man craycc</code> <code>man crayCC</code> <code>man crayftn</code> PrgEnv-gnu <code>man gcc-13</code> <code>man g++-13</code> <code>man gfortran-13</code> PrgEnv-aocc/PrgEnv-amd - - - Wrappers <code>man 1 cc</code> <code>man 1 CC</code> <code>man 1 ftn</code> <p>Recently, HPE Cray have also created  a web version of some of the CPE documentation.</p> <p>Some compilers also support the <code>--help</code> flag, e.g., <code>amdclang --help</code>. For the wrappers, the switch <code>-help</code> should be used instead as the double dash version is passed to the  compiler.</p> <p>The wrappers have a number of options specific to them. Information about them can be obtained by using the <code>--craype-help</code> flag with the wrappers. The wrappers also support the <code>-dumpversion</code> flag to show the version of the underlying compiler. Many other commands, including the actual compilers, use <code>--version</code> to show the version.</p> <p>For Cray Fortran compiler error messages, the <code>explain</code> command is also helpful. E.g.,</p> <pre><code>$ ftn\nftn-2107 ftn: ERROR in command line\n  No valid filenames are specified on the command line.\n$ explain ftn-2107\n\nError : No valid filenames are specified on the command line.\n\nAt least one file name must appear on the command line, with any command-line\noptions.  Verify that a file name was specified, and also check for an\nerroneous command-line option which may cause the file name to appear to be\nan argument to that option.\n</code></pre> <p>On older Cray systems this used to be a very useful command with more compilers but as  HPE Cray is using more and more open source components instead there are fewer commands that give additional documentation via the <code>explain</code> command.</p> <p>Lastly, there is also a lot of information in the \"Developing\" section of the LUMI documentation.</p>"},{"location":"intro-evolving/02-CPE/#google-chatgpt-and-lumi","title":"Google, ChatGPT and LUMI","text":"<p>Finding information on the HPE Cray Programming Environment using search engines such as Google, was often disappointing in the past. However, in May 2023 HPE started putting more  information on the web, and as the Cray EX has been rather successful over the past years, there is also a growing amount of information from various sites on the web.</p> <p>The same holds for ChatGPT, copilot or other similar AI assistants. Nowadays they can produce rather useful results, certainly if you can use the more complex models that you usually need to pay for. The results are certainly not always right, but they often put you in the right direction to solve your issues. But you may have to go into a dialog with the system and also ask for the sources of information so that you can check yourself how relevant the information can be.</p> <p>There is now proper online documentation and it makes sense trying the search box on that page. It is often useful to better understand some error messages. E.g., sometimes Cray MPICH will suggest you to set some environment variable to work around some problem. You may remember that <code>man intro_mpi</code> gives a lot of information about Cray MPICH, but if you don't and, e.g., the error message suggests you to set <code>FI_CXI_RX_MATCH_MODE</code> to either <code>software</code> or <code>hybrid</code>, you could just search for <code>FI_CXI_RX_MATCH_MODE</code> and it would take you to the page where this environment variable is discussed (though you'd still have to search in the page also, but all browsers have good features for that).</p> <p>The HPE Cray environment also has a command line alternative to search engines though: the <code>man -K</code> command that searches for a term in the manual pages. So another way to find out where you can get more information about the environment variable <code>FI_CXI_RX_MATCH_MODE</code>, is</p> <pre><code>man -K FI_CXI_RX_MATCH_MODE\n</code></pre>"},{"location":"intro-evolving/02-CPE/#other-modules","title":"Other modules","text":"<p>Other modules that are relevant even to users who do not do development:</p> <ul> <li>MPI: <code>cray-mpich</code>. </li> <li>LibSci: <code>cray-libsci</code></li> <li>Cray FFTW3 library: <code>cray-fftw</code></li> <li>HDF5:<ul> <li><code>cray-hdf5</code>: Serial HDF5 I/O library</li> <li><code>cray-hdf5-parallel</code>: Parallel HDF5 I/O library</li> </ul> </li> <li>NetCDF:<ul> <li><code>cray-netcdf</code></li> <li><code>cray-netcdf-hdf5parallel</code></li> <li><code>cray-parallel-netcdf</code></li> </ul> </li> <li>Python: <code>cray-python</code>, already contains a selection of packages that interface with     other libraries of the HPE Cray PE, including mpi4py, NumPy, SciPy and pandas.</li> <li>R: <code>cray-R</code></li> </ul> <p>The HPE Cray PE also offers other modules for debugging, profiling, performance analysis, etc. that are not covered in this short version of the LUMI course. Many more are covered in the advanced courses for developers and performance analysis and optimisation workshops  that we organise several times per year with the help of HPE and AMD.</p>"},{"location":"intro-evolving/02-CPE/#warning-1-you-do-not-always-get-what-you-expect","title":"Warning 1: You do not always get what you expect...","text":"<p>The HPE Cray PE packs a surprise in terms of the libraries it uses, certainly for users who come from an environment where the software is managed through EasyBuild, but also for most other users.</p> <p>The PE does not use the versions of many libraries determined by the loaded modules at runtime but instead uses default versions of libraries (which are actually in <code>/opt/cray/pe/lib64</code> on the system) which correspond to the version of the programming environment that is set as the system default when installed. This is very much the behaviour of Linux applications also that pick standard libraries in a few standard directories and it enables many programs built with the HPE Cray PE to run without reconstructing the environment and in some cases to mix programs compiled with different compilers with ease (with the emphasis on some as there may still be library conflicts between other libraries when not using the  so-called rpath linking). This does have an annoying side effect though: If the default PE on the system  changes, all applications will use different libraries and hence the behaviour of your application may  change. </p> <p>Luckily there are some solutions to this problem.</p> <p>By default the Cray PE uses dynamic linking, and does not use rpath linking, which is a form of dynamic linking where the search path for the libraries is stored in each executable separately. On Linux, the search path for libraries is set through the environment variable <code>LD_LIBRARY_PATH</code>. Those Cray PE modules that have their libraries also in the default location, add the directories that contain the actual version of the libraries corresponding to the version of the module to the PATH-style environment variable <code>CRAY_LD_LIBRARY_PATH</code>. Hence all one needs to do is to ensure that those directories are put in <code>LD_LIBRARY_PATH</code> which is searched before the default location: <pre><code>export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH\n</code></pre></p> Small demo of adapting <code>LD_LIBRARY_PATH</code>: <p>An example that can only be fully understood after the section on the LUMI software stacks: <pre><code>$ module load LUMI/24.03\n$ module load lumi-CPEtools/1.2-cpeGNU-24.03\n$ ldd $EBROOTLUMIMINCPETOOLS/bin/mpi_check\n      linux-vdso.so.1 (0x00007ffeb39be000)\n      libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f64e3d84000)\n      libmpi_gnu_123.so.12 =&gt; /opt/cray/pe/lib64/libmpi_gnu_123.so.12 (0x00007f64e1858000)\n      ...\n$ export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH\n$ ldd $EBROOTLUMIMINCPETOOLS/bin/mpi_check\n      linux-vdso.so.1 (0x00007ffc23c3d000)\n      libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f5e023ad000)\n      libmpi_gnu_123.so.12 =&gt; /opt/cray/pe/mpich/8.1.29/ofi/gnu/12.3/lib/libmpi_gnu_123.so.12 (0x00007f5dffe81000)\n      ...\n</code></pre> The <code>ldd</code> command shows which libraries are used by an executable. Only a part of the very long output is shown in the above example. But we can already see that in the first case, the library <code>libmpi_gnu_123.so.12</code> is taken from <code>opt/cray/pe/lib64</code> which is the directory with the default versions, while in the second case it is taken from <code>/opt/cray/pe/mpich/8.1.29/ofi/gnu/12.3/lib/</code> which clearly is for a specific version of <code>cray-mpich</code>.</p> <p>We do provide the module <code>lumi-CrayPath</code>  that tries to fix <code>LD_LIBRARY_PATH</code> in a way that unloading the module fixes <code>LD_LIBRARY_PATH</code> again to the state before adding <code>CRAY_LD_LIBRARY_PATH</code> and that reloading the module adapts <code>LD_LIBRARY_PATH</code> to the current value of <code>CRAY_LD_LIBRARY_PATH</code>. Loading that module after loading all other modules should fix this issue for most if not all software.</p> <p>The second solution would be to use rpath-linking for the Cray PE libraries, which can be done by setting the <code>CRAY_ADD_RPATH</code> environment variable: <pre><code>export CRAY_ADD_RPATH=yes\n</code></pre></p> <p>However, there is also a good side to the standard Cray PE behaviour. Updates of the underlying operating system or network software stack may break older versions of the MPI library. By letting the applications use the default libraries and updating the defaults to a newer version, most applications will still run while they would fail if any of the two tricks to force the use of the intended library version are used. This has actually happened after a big LUMI update in March 2023, when all software that used rpath-linking had to be rebuilt as the MPICH library that was present before the update did not longer work.</p>"},{"location":"intro-evolving/02-CPE/#warning-2-order-matters","title":"Warning 2: Order matters","text":"<p>Lmod is a hierarchical module scheme and this is exploited by the HPE Cray PE. Not all modules are available right away and some only become available after loading other modules. E.g.,</p> <ul> <li><code>cray-fftw</code> only becomes available when a processor target module is loaded</li> <li><code>cray-mpich</code> requires both the network target module <code>craype-network-ofi</code> and a compiler module to be loaded</li> <li><code>cray-hdf5</code> requires a compiler module to be loaded and <code>cray-netcdf</code> in turn requires <code>cray-hdf5</code></li> </ul> <p>but there are many more examples in the programming environment.</p> <p>In the next section of the course we will see how unavailable modules can still be found with <code>module spider</code>. That command can also tell which other modules should be loaded  before a module can be loaded, but unfortunately due to the sometimes non-standard way  the HPE Cray PE uses Lmod that information is not always complete for the PE, which is also why we didn't demonstrate it here.</p>"},{"location":"intro-evolving/02-CPE/#note-compiling-without-the-hpe-cray-pe-wrappers","title":"Note: Compiling without the HPE Cray PE wrappers","text":"<p>It is now possible to work without the HPE Cray PE compiler wrappers  and to use the compilers in a way you may be more familiar with from other  HPC systems. </p> <p>In that case, you would likely want to load a compiler module without loading the  <code>PrgEnv-*</code> module and <code>craype</code> module (which would be loaded automatically by the <code>PrgEnv-*</code> module). The compiler module and compiler driver names are then given by the following table:</p> Description Compiler module Compilers Cray Compiling Environment <code>cce</code> <code>craycc</code>, <code>crayCC</code>, <code>crayftn</code> PrgEnv-gnu <code>gcc-native</code><code>gcc</code><sup>(*)</sup> <code>gcc-13</code>, <code>g++-13</code>, <code>gfortran-13</code><code>gcc</code>, <code>g++</code>, <code>gfortran</code> AMD Optimizing Compilers(CPU only) <code>aocc</code> <code>clang</code>, <code>clang++</code>, <code>flang</code> AMD ROCm LLVM compilers (GPU support) <code>amd</code> <code>amdclang</code>, <code>amdclang++</code>, <code>amdflang</code> <p><sup>(*)</sup>: The <code>gcc</code> modules are from older versions of the HPE Cray PE and will over time disappear from LUMI. They contain a set of GCC compilers packaged by HPE Cray, while the <code>gcc-native</code> modules provide GCC compilers packages by SUSE.</p> <p>Recent versions of the <code>cray-mpich</code> module now also provide the traditional MPI compiler wrappers such as <code>mpicc</code>, <code>mpicxx</code> or <code>mpifort</code>. Note that you will still need to ensure that the network target module <code>craype-network-ofi</code> is loaded to be able to load the <code>cray-mpich</code> module, which is needed to get access to the regular MPI compiler wrappers.</p> <p>There is an issue when using these wrappers with the compilers from the <code>gcc-native</code> modules.  The wrappers are the regular MPICH wrappers, and these were not made to integrate with the HPE Cray PE, hence are not able to detect the right names for the C, C++ and Fortran compiler. You'll have to set <code>MPICH_CC</code>, <code>MPICH_CXX</code> and <code>MPICH_FC</code> to point to the right compilers as otherwise the <code>mpi*</code> MPI wrappers will use the system GCC installation:</p> <pre><code>export MPICH_CC=\"gcc-13\"\nexport MPICH_CXX=\"g++-13\"\nexport MPICH_FC=\"gfortran-13\"\n</code></pre> <p>The <code>cray-mpich</code> module also defines the environment variable <code>MPICH_DIR</code> that points to the MPI installation for the selected compiler. This environment variable is useful if you don't want to use the <code>mpi*</code> wrappers.</p> <p>To manually use the BLAS and LAPACK libraries, you'll still have to load the <code>cray-libsci</code> module. This module defines the <code>CRAY_PE_LIBSCI_PREFIX_DIR</code> environment variable that points to the directory with the library and include file subdirectories for the selected compiler. (Or <code>CRAY_LIBSCI_PREFIX_DIR</code> before release 23.12 of the programming environment.)  See the <code>intro_libsci</code> manual page for information about the different libraries.</p> <p>To be able to use the <code>cray-fftw</code> FFTW libraries, you still need to load the right CPU target module, even though you need to specify the target architecture yourself now when calling the compilers.  This is because the HPE Cray PE does not come with a multi-cpu version of the FFTW libraries, but  specific versions for each CPU (or sometimes group of similar CPUs). Here again some environment variables may be useful to point the compiler and linker to the installation: <code>FFTW_ROOT</code> for the  root of the installation for the specific CPU (the library is otherwise compiler-independent), <code>FFTW_INC</code> for the subdirectory with the include files and <code>FFTW_DIR</code> for the directory with the libraries.</p> <p>Other modules that you may want to use also typically define some useful environment variables.</p>"},{"location":"intro-evolving/03-Access/","title":"Getting access","text":"<p>Last update of this page: October 2, 2025</p>"},{"location":"intro-evolving/03-Access/#getting-access-to-lumi","title":"Getting Access to LUMI","text":""},{"location":"intro-evolving/03-Access/#lumi-is-part-of-the-eurohpc-ecosystem","title":"LUMI is part of the EuroHPC ecosystem","text":"<p>EuroHPC currently funds  supercomputers in four different classes:</p> <ol> <li> <p>There are a number of so-called petascale supercomputers. The first ones of those are     Meluxina (in Luxembourg),      VEGA (in Slovenia),      Karolina (in the Czech Republic),      Discoverer (in Bulgaria) and      Deucalion (in Portugal).</p> </li> <li> <p>A number of pre-exascale supercomputers, LUMI being one of them. The other two are      Leonardo (in Italy)     and MareNostrum 5 (in Spain)</p> </li> <li> <p>Already two exascale supercomputers:      Jupiter (in Germany) is already in      operation and     Alice Recoque,      hosted by the Jules Verne consortium (in France) is under procurement.</p> </li> <li> <p>Another 2 mid-range systems are currently under procurement or under construction:     Daedalus (in Greece) and     Arrhenius (in Sweden).</p> </li> <li> <p>Various upgrades are planned to the current systems and there is also a programme for     \"AI-optimised\" supercomputers, whatever that may mean, as these will be very large systems      serving lots of users so there will still be lots of technological compromises.</p> </li> </ol> <p>Depending on the machine, EuroHPC pays one third up to half of the bill, while the remainder of the budget comes from the hosting country, usually with the help of a consortium of countries. For LUMI, EuroHPC paid half of the bill and is the actual owner of the machine.</p>"},{"location":"intro-evolving/03-Access/#who-pays-the-bills-for-lumi","title":"Who pays the bills for LUMI?","text":"<p>LUMI is hosted in Finland but operated by a consortium of 11 countries, with Belgium being the third largest contributor to LUMI and the second largest in the consortium of countries. The Belgian contribution is brought together by 4 entities:</p> <ol> <li>BELSPO, the science agency of the Federal government, invested 5M EURO in the project.</li> <li>The SPW \u00c9conomie, Emploi, Recherche from Wallonia also invested 5M EURO in the project.</li> <li>The Department of Economy, Science and Innovation (EWI) of the Flemish government invested 3.5M EURO in the project.</li> <li>Innoviris (Brussels) invested 2M EURO.</li> </ol> <p>The resources of LUMI are allocated proportional to the investments. As a result EuroHPC can allocate half of the resources. The Belgian share is approximately 7.4%.</p> <p>Each LUMI consortium country can set its own policies for a national access program, within the limits of what the supercomputer can technically sustain. In Belgium, the 4 entities that invested in LUMI do so together via a competition (3 calls a year for regular projects). The access conditions for projects in the Belgian share are advertised via the  EuroCC Belgium National Competence Centre. As Belgium is fully responsible for the allocation of its share, the central LUMI User Support Team (LUST) cannot manage allocations. It makes no sense to send them requests for a larger allocation or project extension.</p> <p>Web links:</p> <ul> <li>EuroHPC JU supercomputers</li> <li>EuroCC Belgium National Competence Centre     with the specifics of LUMI access via the Belgian share.</li> </ul>"},{"location":"intro-evolving/03-Access/#users-and-projects","title":"Users and projects","text":"<p>LUMI works like most European large supercomputers: Users are members of projects.</p> <p>Projects are also the basis for most research allocations on LUMI. In LUMI there are three types of resource allocations, and each project needs at least two of them:</p> <ol> <li>A compute budget for the CPU nodes of LUMI (LUMI-C and the CPU-only large memory nodes),      expressed in core-hours.</li> <li>A compute budget for the GPU nodes of LUMI (LUMI-G and the visualisation nodes),      expressed in GPU-hours. As the mechanism was     already fixed before it became publicly known that for all practical purposes one AMD MI250X GPU     should really be treated as 2 GPUs, one GPU-hour is one hour on a full MI250X, so computing for one     hour on a full LUMI-G GPU node costs 4 GPU-hours.</li> <li>A storage budget which is expressed in TB-hours. Only storage that is actually being used is charged     on LUMI, to encourage users to clean up temporary storage. The rate at which storage is charged depends     on the file system, but more about that later when we discuss the available file spaces.</li> </ol> <p>These budgets are assigned and managed by the resource allocators, not by the LUMI User Support Team. For Belgium the VSC and C\u00c9CI both have the role of resource allocator, but both use a common help desk.</p> <p>LUMI projects will typically have multiple project numbers which may be a bit confusing:</p> <ol> <li>Each RA may have its own numbering system, often based on the numbering used for the project     requests. Note that the LUMI User Support Team is not aware of that numbering as it is purely     internal to the RA.</li> <li> <p>Each project on LUMI also gets a LUMI project ID which also corresponds to a Linux group to manage     access to the project resources. These project IDs are of the form <code>project_465XXXXXX</code> for most     projects but <code>project_462XXXXXX</code> for projects that are managed by the internal system of      CSC Finland. </p> <p>This is also the project number that you should mention when contacting the  central LUMI User Support.</p> </li> </ol> <p></p> <p>Besides projects there are also user accounts.  Each user account on LUMI corresponds to a physical person, and user accounts should not be shared. Some physical persons have more than one user account but this is an unfortunate consequence of decisions made very early in the LUMI project about how projects on LUMI would be managed. Users themselves cannot do a lot without a project as all a user has on LUMI is a small personal disk space which is simply a Linux requirement.  To do anything useful on LUMI users need to be member of a project. There are also \"robot accounts\" for special purposes  that would not correspond to a physical person but have a specific goal  (like organising data ingestion from an external source) but few projects are granted such an account.</p> <p>There is a many-to-many mapping between projects and user accounts. Projects can of course have multiple users who collaborate in the project, but a user account can also be part of multiple projects. The latter is more common than you may think, as. e.g., you may become member of a training project when you take a LUMI training.</p> <p>Most resources are attached to projects. The one resource that is attached to a user account is a small home directory to store user-specific configuration files. That home directory is not billed but can also not be extended. For some purposes you may have to store things that would usually automatically be placed in the home directory in a separate directory, e.g., in the project scratch space, and link to it. This may be the case when you try to convert big docker containers into singularity containers as the singularity cache can eat a lot of disk space. (Or sometimes setting an environment variable is enough to redirect to a different directory.)</p>"},{"location":"intro-evolving/03-Access/#project-management","title":"Project management","text":"<p>A large system like LUMI with many entities giving independent access to the system to users  needs an automated system to manage those projects and users. There are two such systems for LUMI. CSC, the hosting institution from Finland, uses its own internal system to manage projects allocated on the Finnish national share. This system manages the \"642\"-projects. The other system is called Puhuri and is developed in a collaboration between the Nordic countries to manage more than just LUMI projects. It can be used to manage multiple supercomputers but also to manage access to other resources such as experimental equipment.  Puhuri projects can span multiple resources (e.g., multiple supercomputers so that you can create a workflow involving Tier-2, Tier-1 and Tier-0 resources).</p> <p>In Belgium two entities manage projects for the Belgian LUMI organisation: VSC and C\u00c9CI. These entities are called the resource allocators.</p> <p>All projects allocated by Belgium ara managed through the Puhuri system, and VSC and C\u00c9CI  both have their own zone in that system. For Belgium it is only used to manage access to LUMI, not to any of the VSC, C\u00c9CI or Cenaero systems or other infrastructure. Belgian users log in to the Puhuri portal via MyAccessID, which is a G\u00c9ANT service. G\u00c9ANT is  the international organisation that manages the research network in Europe.  MyAccessID then in turn connects to your institute identity provider and a number of alternatives. It is important that you always use the same credentials to log in via MyAccessID, otherwise you create another user in MyAccessID that is unknown to Puhuri and get all kinds of strange error messages.</p> <p>The URL to the Puhuri portal is: puhuri-portal.neic.no.</p> <p>Puhuri can be used to check your remaining project resources, but once your user account  on LUMI is created, it is very easy to do this on the command line with the <code>lumi-workspaces</code> command.</p> <p>Web links</p> <ul> <li> <p>Puhuri documentation, look for the \"User Guides\".</p> </li> <li> <p>The <code>lumi-workspaces</code> command is provided through the <code>lumi-tools</code> module     which is loaded by default. The command will usually give the output you need when used     without any argument.</p> </li> </ul>"},{"location":"intro-evolving/03-Access/#file-spaces","title":"File spaces","text":"<p>LUMI has file spaces that are linked to a user account and file spaces that are linked to projects.</p>"},{"location":"intro-evolving/03-Access/#per-user-file-spaces","title":"Per-user file spaces","text":"<p>The only permanent file space linked to a user account is the home directory which is of the form <code>/users/&lt;my_uid&gt;</code>. It is limited in both size and number of files it can contain, and neither limit can be expanded. It should only be used for things that are not project-related and first and foremost for those things that Linux and software automatically stores in a home directory like user-specific software configuration files. It is not billed as users can exist temporarily without an active project but therefore is also very limited in size.</p>"},{"location":"intro-evolving/03-Access/#per-project-file-spaces","title":"Per-project file spaces","text":"<p>Each project also has 4 permanent or semi-permanent file spaces that are all billed against the storage budget of the project.</p> <ol> <li> <p>Permanent (for the duration of the project) storage on a hard disk based Lustre filesystem     accessed via <code>/project/project_46YXXXXXX</code>. This is the place to perform the software installation     for the project (as it is assumed that a project is a coherent amount of work it is only      natural to assume that everybody in the project needs the same software), or to store input data     etc. that will be needed for the duration of the project.</p> <p>In some cases (like in the Open OnDemand \"Home Directory\" app, see later in this session), you will see the name <code>/projappl/project_46YXXXXXX</code> instead. This is for historical reasons. The directory was first called <code>/projappl</code>, but that gave too much the  impression that it should only be used for applications, and it was later changed into <code>/project</code> which is more common on other systems. To avoid having to rework too many management scripts, both now exist next to one another on  LUMI, but it is the same file space.</p> <p>Storing one TB for one hour on the disk based Lustre file systems costs 1 TB-hour. As would storing 10 GB for 100 hours.</p> </li> <li> <p>Semi-permanent scratch storage on a hard disk based Lustre filesystem accessed via     <code>/scratch/project_46YXXXXXX</code>. Files in this storage space can in principle be erased      automatically after 90 days. This is not happening yet on LUMI, but will be activated if     the storage space starts to fill up.</p> <p>Storing one TB for one hour on the disk based Lustre file systems costs 1 TB-hour.</p> </li> <li> <p>Semi-permanent scratch storage on an SSD based Lustre filesystem accessed via     <code>/flash/project_46YXXXXXX</code>. Files in this storage space can in principle be erased     automatically after 30 days. This is not happening yet on LUMI, but will be activated if     the scratch storage space starts to fill up.</p> <p>Storing one TB for one hour on the flash based Lustre file system costs 3 TB-hour. Before 2025 this was even 10 TB-hour to reflect the cost difference for this type of  storage, but the rate has been reduced a bit to make the flash storage more attractive.</p> </li> <li> <p>Permanent (for the duration of the project) storage on the hard disk based     object filesystem.</p> <p>Storing one TB for one hour on the object based file system costs 0.25 TB-hour. Before 2025 this was 0.5 TB-hour as that reflected the cost difference with  Lustre hard-disk based software, but the billing rate has been reduced to encourage better use of this storage type.</p> </li> </ol>"},{"location":"intro-evolving/03-Access/#quota","title":"Quota","text":"<p>The slide above also shows the quota on each volume. This information is also available in the LUMI docs.</p> <p>The use of space in each file space is limited by block and file quota. Block quota limit the capacity you can use, while file quota limit the number of so-called inodes you can use. Each file, each subdirectory and each link use an inode. As we shall see later in this course (in the section on Lustre) or as you may have seen in other HPC courses already (e.g., the VSC Supercomputers for Starters  course organised by UAntwerpen), most parallel file systems are not built to deal with hundreds of thousands of small files and are very inefficient at that. This is certainly true for Lustre, the system that we have on LUMI. Therefore block quota on LUMI tend to be rather flexible (except for the home directory) but file quota are rather strict and will not easily get extended. Software installations that require tens of thousands of small files should be done in  containers (e.g., conda installations or any big Python installation) while data should also be organised in proper file formats rather than being dumped on the file system abusing the file system as a database.</p> <p>In the above slide, the \"Capacity\" column shows the block quota and the \"Files\" column show the total number of so-called inodes available in the file space.</p> <p>The project file spaces can be expanded in capacity within the limits specified. However, due to the many small files problem on Lustre, the files quota (or more accurately inode quota) are rather strict and not easily raised (and if raised, not by an order of magnitude).</p> <p>So storage use on LUMI is limited in two independent ways:</p> <ul> <li> <p>Traditional Linux block and file quota limit the maximum capacity you can use (in volume and number of     inodes, roughly the number of files and directories combined).</p> </li> <li> <p>But actual storage use is also \"billed\" on a use-per-hour basis. The idea behind this is that a user may     run a program that generates a lot of data, but after some post-processing much of the data can be deleted     so that other users can use that capacity again, and to encourage that behaviour you are billed based not     on peak use, but based on the combination of the volume that you use and the time you use it for. </p> <p>Storage use is monitored hourly for the billing process.  If you run out of storage billing units, you will not be able to run jobs anymore.</p> <p>Storage in your home directory is not billed but that should not mean that you should  abuse your home directory for other purposes then a home directory is meant to be used,  and an extension of the home directory will never be granted. If you run out of space for, e.g., caches, you should relocate them to, e.g., your scratch space, which can sometimes be done by setting an environment variable and in other cases by just using symbolic links to preserve the structure of the caching subdirectories in your home directory while storing data elsewhere.</p> </li> </ul> <p>Quota extensions are currently handled by the central LUMI User Support Team.  But storage billing units, just as any billing unit, comes from your resource allocator (so LUMI-BE for projects assigned through the Belgian channel and EuroHPC themselves for their projects), and the LUMI User Support Team cannot give you any storage billing units.</p>"},{"location":"intro-evolving/03-Access/#some-additional-information","title":"Some additional information","text":"<p>LUMI has four disk based Lustre file systems that house <code>/users</code>, <code>/project</code> and <code>/scratch</code>. The <code>/project</code> and <code>/scratch</code> directories of your project will always be on the same parallel file system, but your home directory may be on a different one. Both are assigned automatically during project and account creation and these assignments cannot be changed by the LUMI User Support Team. As there is a many-to-many mapping between user accounts and projects it is not possible to ensure that user accounts are on the same file system as their main project. In fact, many users enter LUMI for the first time through a course project and not through one of their main compute projects...</p> <p>It is important to note that even though <code>/flash</code> is SSD based storage, it is still a parallel file  system and will not behave the way an SSD in your PC does. The cost of opening and closing a file is still very high due to it being both a networked and a parallel file system rather than a local drive. In fact, the cost for metadata operations is not always that much lower as on the hard disk based parallel file systems as both use SSDs to store the metadata (but some metadata operations on Lustre involve both the metadata and object servers and the latter are faster on <code>/flash</code>).  Once a file is opened and with a proper data access pattern (big accesses, properly striped files which we will discuss later in this course) the flash file system can give a lot more bandwidth than the disk based ones.</p> <p>It is important to note that LUMI is not a data archiving service or a data publishing service. \"Permanent\" in the above discussion only means \"for the duration of the project\". There is no backup, not even of the home directory. And 90 days after the end of the project all data from the project is irrevocably deleted from the system. User accounts without project will also be closed, as will user accounts that remain inactive for several months, even if an active project is still attached to them.</p> <p>If you run out of storage billing units, access to the job queues or even to the storage  can be blocked and you should contact your resource allocator for extra billing units. Our experience within Belgium is that projects tend to heavily under-request storage billing units. It is important that you clean up after a run as LUMI is not meant for long-term data archiving. But at the same time it is completely normal that you cannot do  so right after a run, or as a job may not launch immediately, that you need to put input data on the system long before a run starts.  So data needed for or resulting from a run has to stay on the system for a few days or weeks, and you need to budget for that in your project request.</p> <p>Web links:</p> <ul> <li>Overview of storage systems on LUMI</li> <li>Billing policies (includes those for storage)</li> <li>Example for data archiving services: In the Netherlands, SURF provides the     SURF Data Archive and      SURF Data Repository services.</li> </ul>"},{"location":"intro-evolving/03-Access/#access","title":"Access","text":"<p>LUMI currently has 4 login nodes through which users can enter the system via key-based ssh. The generic name of those login nodes is <code>lumi.csc.fi</code>. Using the generic names will put you onto one of the available nodes more or less at random and will avoid contacting a login node that is down for maintenance. However, in some cases one needs to enter a specific login node.  E.g., tools for remote editing or remote file synchronisation such as Visual Studio Code or Eclipse  usually don't like it if they get a different node every time they try to connect, e.g., because  they may start a remote server and try to create multiple connections to that server. In that case you have to use a specific login node, which you can do through the names <code>lumi-uan01.csc.fi</code> up to <code>lumi-uan04.csc.fi</code>.  (UAN is the abbreviation for User Access Node, the term Cray uses for login nodes.)</p> <p>It is entirely normal that one of the login nodes is down and not functioning. It will then be taken out of the <code>lumi.csc.fi</code> list. But don't send support tickets about it. Sometimes a node may even be unavailable for several weeks as it may be in use by the sysadmins for other purposes. If you're connecting to a specific login node and it doesn't work anymore, simply try a different one.</p> <p>Key management is for most users done via MyAccessID: mms.myaccessid.org. This is the case for all user accounts who got their first project on LUMI via Puhuri, which is the case for almost all Belgian users. User accounts that were created via the My CSC service have to use the my.csc.fi portal to manage their keys. It recently became possible to link your account in My CSC to MyAccessID so that you do not get a second account on LUMI ones you join a  Puhuri-managed project, and in this case your keys are still managed through the My CSC service. But this procedure is only important for those LUMI-BE users who may have gotten their first access  to LUMI via a project managed by CSC.</p> <p>LUMI also provides a web interface via Open OnDemand. The URL is <code>https://www.lumi.csc.fi/</code>. It also offers a number of tools that can be useful for visualisation via a web browser via the \"Desktop\" app, which will give you a VNC session with the rather lightweight Xfce desktop environment. Note that the supercomputer is not your primary graphics workstation and is a shared resource, so a more lightweight solution is chosen to be able to serve more users on the available hardware. It makes no sense to request much richer environments such as Gnome or KDE; they are simply too heavyweight and would require to dedicate too much of the budget to hardware for something that is not the core task of a supercomputer.</p> <p>There is currently not much support for other technologies for GUI applications on LUMI.  Running X11 over ssh (via <code>ssh -X</code>) is unbearably slow for users located in Belgium.  The X11 protocol is really made for local area network connections, not for wide area network connections. The alternative is some support for VNC outside Open OnDemand, though the window manager and fonts used by that server do look a little dated. Access is possible via a browser or VNC client.  On the system, check for the  <code>lumi-vnc</code> module.</p> <p>Web links:</p> <ul> <li>LUMI documentation on logging in to LUMI and creating suitable SSH keys</li> <li>CSC documentation on linking My CSC to MyAccessID</li> </ul> <p>A walk through the Open OnDemand interface</p> <p>To enter the LUMI OpenOndemand interface, point your browser to www.lumi.csc.fi. You will get the screen:</p> <p> </p> <p>Most likely you just want to log on, so click on \"Go to login\" and the \"Select authentication provider\" screen should appear, with a link to give you more information about which authentication method to use:</p> <p> </p> <p>Belgian users typically need the \"Puhuri\" option, but users who came in via the myCSC platform need to use \"Haka\" or \"CSC\" depending on how they enter the myCSC platform.</p> <p>The whole login process is not shown, but after successful authentication, you end up at the main screen (that you can also go back to by clicking the LUMI-logo in the upper left corner):</p> <p> </p> <p>The list of pinned apps may change over time, and more apps are available via the menu at the top. Most apps will run in the context of a job, so you will need billing units, and those apps will also present you with a form to chose the resources you want to use, but that will only be discussed in the session on Slurm.</p> <p>Two apps don't run in the context of a job: The \"Login node shell\" and \"Home Directory\" apps, and we'll first have a look at those.</p> <p> </p> <p>The \"Login node shell\" does just what you expect from it\": It opens a tab in the browser with a shell on one of the login nodes. Open OnDemand uses its own set of login nodes, as you can see from the name of the node, but these nodes are otherwise identical to the login nodes that you access via an ssh client on your laptop, and the same policies apply. They should not be used for running applications and only be used for small work or not too heavy compilations.</p> <p> </p> <p>Let's now select the \"Home Directory\" app. We get:</p> <p> </p> <p>The \"Home Directory\" app presents you with an interface through which you cannot only browse your home directory, but also the project, scratch and flash directories of all your projects. It can be used for some elementary file access and also to upload and download files.</p> <p>It is not suitable though to upload or download very big files, or download large subdirectories (multiple files will be packed in a ZIP archive) as browsers may not be reliable enough and as there are also restrictions on how big an archive Open OnDemand can create.</p> <p>For transferring lots of data, transfer via LUMI-O is certainly the better option at the moment.</p> <p> </p> <p>Finally, let's have a look at the \"Desktop\" app.</p> <p>The \"Desktop\" app will present you with a simple GUI desktop based on the xfce desktop environment. After opening this app, one gets:</p> <p> </p> <p>This app needs to run in the context of a job and although it can run on several partitions on LUMI, its main use is to be able to use some visualisation applications, so your best choice is likely to use the partition <code>lumid</code> with visualisation GPUs (see the session on Slurm). As we have not discussed jobs yet, we will not give more details now on how to fill in the form that is presented to you.</p> <p>Now click \"Launch\" to launch the job, and after a while, you'll see a screen similar to</p> <p> </p> <p>The desktop is basically run in a VNC session, a popular protocol for remote desktop support in Linux. It can be used through a web browser, which is what you get if you click the \"Launch Desktop\" button, but there are other choices also.</p> <p>After launching/connecting to the desktop you get:</p> <p> </p> <p>There is a small settings menu hidden at the left to do some settings of the web interface that we are using here. Right-clicking with the mouse on the desktop gives you a menu with a number of applications.</p> <p>Note that using \"Logout\" in this menu is not enough to kill the job. You can do so though with the  <code>Cancel</code> button in the tab where we clicked the \"Launch Desktop\" button.</p> <p>This is in no means meant to be a replacement of your own workstation, so the software choice is limited and will remain limited. It should never be your main environment for all your work. LUMI is not meant to simply provide small workstations to all of Europe. And it will also react a lot slower than what you are used to from a workstation in front of you. This is 100% normal and simply the result of using a computer which is far away so there is a high network latency.</p>"},{"location":"intro-evolving/03-Access/#data-transfer","title":"Data transfer","text":"<p>There are currently two main options to transfer data to and from LUMI.</p> <p>The first one is to use sftp to the login nodes (or any other protocol that goes over the ssh daemon, like <code>rsync</code> over an ssh connection), authenticating via your ssh key.  There is a lot of software available for all major operating systems, both command line based and GUI based. The sftp protocol can be very slow over high latency connections. This is because it is a protocol that opens only a single stream for communication with the remote host, and the bandwidth one can reach via a single stream in the  TCP network protocol used for such connections, is limited not only by the bandwidth of all links involved but also by the latency. After sending a certain amount of data, the sender will wait for a confirmation that the data has arrived, and if the latency is high, that confirmation takes more time to reach the sender, limiting the effective bandwidth that can be reached over the connection. LUMI is not to blame for that; the whole path from the system from which you initiate the connection to LUMI is responsible and every step adds to the latency. We've seen many cases where the biggest contributor to the latency was actually the campus network of the user. And it is the very nature of a TCP connection that the effective bandwidth over a link becomes lower the higher the latency is. So the solution is to go to data transfer methods that use multiple TCP connections to get the full bandwidth of the connection.</p> <p>The second important option is to transfer data via the object storage system LUMI-O. To transfer data to LUMI, you'd first push the data to LUMI-O and then on LUMI pull it  from LUMI-O. When transferring data to your home institute, you'd first push it onto LUMI-O from LUMI and then pull the data from LUMI-O to your work machine.  LUMI offers some support for various tools, including  rclone and S3cmd. There also exist many GUI clients to access object storage.  Even though in principle any tool that can connect via the S3 protocol can work, the LUMI User Support Team nor the local support in Belgium can give you instructions for every possible tool.  Those tools for accessing object storage tend to set up multiple data streams and hence will offer a much higher effective bandwidth, even on high latency connections.</p> <p>Alternatively, you can also chose to access external servers from LUMI if you have client software that runs on LUMI (or if that software is already installed on LUMI, e.g., <code>rclone</code> and <code>S3cmd</code>), but neither the LUMI User Support Team not the Belgian local support teams can tell you how to configure tools to use an external service that they don't have access to. But, e.g., the VSC Tier-0 support team might be able to help you to configure a tool to access the data services that VSC offers.</p> <p>Using the LUMI-O object storage will be further discussed in the  \"LUMI-O Object Storage\" session of this course.</p> <p>Unfortunately there is no support yet for Globus or other forms of gridFTP, another technology that sets up multiple connections to improve the effective bandwidth.  LUST offers a recipe to install the UNICORE UFTP client if you have access to a UFTP server (still popular in Germany), but neither LUST nor local  Belgian support teams can offer true support for that either as they have no access to such server. How to work with these recipes, is discussed in the \"LUMI Software Stacks\" session of this course.</p>"},{"location":"intro-evolving/03-Access/#local-trainings-in-belgium","title":"Local trainings in Belgium","text":"<p>Any HPC introductory training in Belgium covers logging in via ssh and transferring files. Such a course is a prerequisite for this course.</p>"},{"location":"intro-evolving/04-Modules/","title":"Modules","text":"<p>Last update of this page: October 3, 2025</p>"},{"location":"intro-evolving/04-Modules/#modules-on-lumi","title":"Modules on LUMI","text":"<p>Intended audience</p> <p>As this course is designed for people already familiar with HPC systems and as virtually any cluster nowadays uses some form of module environment, this section assumes that the reader is already familiar with a module environment but not necessarily the one used on LUMI.</p> <p>However, even if you are very familiar with Lmod it makes sense to go through these notes as not every Lmod configuration is the same.</p>"},{"location":"intro-evolving/04-Modules/#module-environments","title":"Module environments","text":"<p>An HPC cluster is a multi-user machine. Different users may need different  versions of the same application, and each user has their own preferences for the environment. Hence there is no \"one size fits all\" for HPC and mechanisms are needed to support the diverse requirements of multiple users on a single machine. This is where modules play an important role. They are commonly used on HPC systems to enable users to create  custom environments and select between multiple versions of applications. Note that this also implies that applications on HPC systems are often not installed in the regular directories one would expect from the documentation of some packages, as that location may not even always support proper multi-version installations and as system administrators prefer to have a software stack which is as isolated as possible from the system installation to keep the image that has to be loaded on the compute nodes small.</p> <p>Another use of modules is to configure the programs that are being activated. E.g., some packages expect certain additional environment variables to be set and modules can often take care of that also.</p> <p>There are 3 systems in use for module management. The oldest is a C implementation of the commands using module files written in Tcl. The development of that system stopped around 2012, with version 3.2.10.  This system is supported by the HPE Cray Programming Environment. A second system builds upon the C implementation but now uses Tcl also for the module command and not only for the module files. It is developed in France at the C\u00c9A compute centre. The version numbering was continued from the C implementation, starting with version 4.0.0.  The third system and currently probably the most popular one is Lmod, a version written in Lua with module files also written in Lua. Lmod also supports most Tcl module files. It is also supported by HPE Cray. HPE used to be a bit slow in following versions, but since 2024 they stay close to the current version.  The original developer of Lmod, Robert McLay, retired  at the end of August 2023, but TACC, the centre where he worked, is committed to at least maintain Lmod though it may not see as much new development anymore as before (and currently he's still working on it part-time).</p> <p>On LUMI we have chosen to use Lmod. As it is very popular, many users may already be familiar with it, though it does make sense to revisit some of the commands that are specific for Lmod and differ from those in the two other implementations.</p> <p>It is important to realise that each module that you see in the overview corresponds to a module file that contains the actual instructions that should be executed when loading  or unloading a module, but also other information such as some properties of the module, information for search and help information.</p> Links <ul> <li>Old-style environment modules on SourceForge</li> <li>TCL Environment Modules home page on SourceForge and the     development on GitHub</li> <li>Lmod documentation and      Lmod development on GitHub</li> </ul> <p>I know Lmod, should I continue?</p> <p>Lmod is a very flexible tool. Not all sites using Lmod use all features, and Lmod can be configured in different ways to the extent that it may even look like a very different module system for people coming from another cluster. So yes, it makes sense to continue reading as Lmod on LUMI may have some tricks that are not available on your home cluster. E.g., several of the features that  we rely upon on LUMI are disabled on the UGhent clusters to make them more similar to clusters running the C/Tcl module implementation which was used in the early days of the VSC.</p> <p>Standard OS software</p> <p>Most large HPC systems use enterprise-level Linux distributions: derivatives of the stable Red Hat or SUSE distributions. Those distributions typically have a life span of 5 years or even more during which they receive security updates and ports of some newer features, but some of the core elements of such a distribution stay at the same version to break as little as possible between minor version updates. Python and the system compiler are typical examples of those. Red Hat 8 and SUSE Enterprise Linux 15 both came with Python 3.6 in their first version, and  keep using this version as the base version of Python even though official support from the Python Software Foundation has long ended. Similarly, the default GNU compiler version offered on those system also remains the same. The compiler may not even fully support some of the newer CPUs the code is running on. E.g., the system compiler of SUSE Enterprise Linux 15, GCC 7.5, does not support the zen2 \"Rome\" or zen3 \"Milan\" CPUs on LUMI. </p> <p>HPC systems will usually offer newer versions of those system packages through modules and users should always use those. The OS-included tools are really only for system management and system related tasks and serve a different purpose which actually requires a version that remains stable across a number of updates to not break things at the core of the OS. Users however will typically have a choice between several newer versions through modules, which also enables them to track the evolution and transition to a new version at the best suited moment.</p>"},{"location":"intro-evolving/04-Modules/#exploring-modules-with-lmod","title":"Exploring modules with Lmod","text":"<p>Contrary to some other module systems, or even some other Lmod installations, not all modules are immediately available for loading. So don't be disappointed by the few modules you will see with <code>module available</code> right after login. Lmod has a so-called hierarchical setup that tries to protect you from being confronted with all modules at the same time, even those that may conflict with  each other, and we use that to some extent on LUMI. Lmod distinguishes between installed modules and available modules. Installed modules are all modules on the system that can be loaded one way or another, sometimes through loading other modules first. Available modules are all those modules that can be loaded at a given point in time without first loading other modules.</p> <p>The HPE Cray Programming Environment also uses a hierarchy though it is not fully implemented in the way the Lmod developer intended so that some features do not function as they should.</p> <ul> <li>For example, the <code>cray-mpich</code> module can only be loaded if both a network target module and a     compiler module are loaded (and that is already the example that is implemented differently from     what the Lmod developer had in mind). </li> <li>Another example is the performance monitoring tools. Many of those     tools only become available after loading the <code>perftools-base</code> module. </li> <li>Another example is the     <code>cray-fftw</code> module which requires a processor target module to be loaded first.</li> </ul> <p>Lmod has several tools to search for modules. </p> <ul> <li>The <code>module avail</code> command is one that is also     present in the various Environment Modules implementations and is the command to search in the     available modules. </li> <li> <p>But Lmod also has other commands, <code>module spider</code> and <code>module keyword</code>, to      search in the list of installed modules.</p> <p>On LUMI, we had to restrict the search space of <code>module spider</code>. By default, <code>module spider</code> will only search in the Cray PE modules, the CrayEnv stack and the LUMI stacks. This is done  for performance reasons. However, as we shall discuss later, you can load a module or set an environment variable to enable searching all installed modules. The behaviour is also not fully consistent. Lmod uses a cache which it refreshes once every 24 hours, or after manually clearing the cache. If a rebuild happens while modules from another software stack are available, that stack will also be indexed and results for that stack shown in the results of  <code>module spider</code>. It is a price we had to pay though as due to the large number of modules and the many organisations managing modules, the user cache rebuild time became too long and system caches are hard to manage also.</p> </li> </ul>"},{"location":"intro-evolving/04-Modules/#benefits-of-a-hierarchy","title":"Benefits of a hierarchy","text":"<p>When the hierarchy is well designed, you get some protection from loading modules that do not work together well. E.g., in the HPE Cray PE it is not possible to load the MPI library built for another compiler than your current main compiler. This is currently not exploited as much as we could on LUMI, mainly because we realised at the start that too many users are not familiar enough with hierarchies and would get confused more than the hierarchy helps them.</p> <p>Another benefit is that when \"swapping\" a module that makes other modules available with a different one, Lmod will try to look for equivalent modules in the list of modules made available by the newly loaded module.</p> <p>An easy example (though a tricky one as there are other mechanisms at play also) it to load a different programming environment in the default login environment right after login:</p> <pre><code>$ module load PrgEnv-gnu\n</code></pre> <p>which results in the next slide:</p> <p></p> <p>The first two lines of output are due to to other mechanisms that are at work here,  and the order of the lines may seem strange but that has to do with the way Lmod works internally. Each of the PrgEnv modules hard loads a compiler module which is why Lmod tells you that it is loading <code>gcc-native/13.2</code>. However, there is also another mechanism at work that causes <code>cce/17.0.1</code> and <code>PrgEnv-cray/8.5.0</code> to be unloaded, but more about that in the next subsection (next slide).</p> <p>The important line for the hierarchy in the output are the lines starting with  \"<code>Due to MODULEPATH changes...</code>\". Remember that we said that each module has a corresponding module file. Just as binaries on a system, these are organised in a directory structure, and there is a path, in this case <code>MODULEPATH</code>, that determines where Lmod will look for module files. The hierarchy is implemented with a directory structure and the environment variable <code>MODULEPATH</code>, and when the <code>cce/17.0.1</code> module was unloaded and <code>gcc-native/13.2</code> module was loaded, that  <code>MODULEPATH</code> was changed. As a result, the version of the cray-mpich module for the  <code>cce/17.0.1</code> compiler became unavailable, but one with the same module name for the <code>gcc-native/13.2</code> compiler became available and hence Lmod unloaded the version for the <code>cce/17.0.1</code> compiler as it is no longer available but loaded the matching one for the <code>gcc-native/13.2</code> compiler. </p>"},{"location":"intro-evolving/04-Modules/#about-module-names-and-families","title":"About module names and families","text":"<p>In Lmod you cannot have two modules with the same name loaded at the same time. On LUMI, when you load a module with the same name as an already loaded module, that other module will be unloaded automatically before loading the new one. There is  even no need to use the <code>module swap</code> command for that (which in Lmod corresponds to a <code>module unload</code> of the first module and a <code>module load</code> of the second). This gives you an automatic protection against some conflicts if the names of the modules are properly chosen. </p> <p>Remark</p> <p>Some clusters do not allow the automatic unloading of a module with the same name as the one you're trying to load, but on LUMI we felt that this is a  necessary feature to fully exploit a hierarchy.</p> <p>Lmod goes further also. It also has a family concept: A module can belong to a family (and at most 1) and no two modules of the same family can be loaded together.  The family property is something that is defined in the module file. It is commonly  used on systems with multiple compilers and multiple MPI implementations to ensure  that each compiler and each MPI implementation can have a logical name without  encoding that name in the version string (like needing to have <code>compiler/gcc-13.2</code> or <code>compiler/gcc/13.2</code> rather than <code>gcc-native/13.2</code>), while still having an easy way to avoid having two  compilers or MPI implementations loaded at the same time.  On LUMI, the conflicting module of the same family will be unloaded automatically when loading another module of that particular family.</p> <p>This is shown in the example in the previous subsection (the <code>module load PrgEnv-gnu</code> in  a fresh long shell) in two places. It is the mechanism that unloaded <code>PrgEnv-cray</code> when loading <code>PrgEnv-gnu</code> and that then unloaded <code>cce/17.0.1</code> when the  <code>PrgEnv-gnu</code> module loaded the <code>gcc-native/13.2</code> module.</p> <p>Remark</p> <p>Some clusters do not allow the automatic unloading of a module of the same family as the one you're trying to load and produce an error message instead. On LUMI, we felt that this is a necessary feature to fully exploit the  hierarchy and the HPE Cray Programming Environment also relies very much on this feature being enabled to make live easier for users.</p>"},{"location":"intro-evolving/04-Modules/#extensions","title":"Extensions","text":"<p>It would not make sense to have a separate module for each of the hundreds of R packages or tens of Python packages that a software stack may contain. In fact, as the software for each module is installed in a separate directory it would also create a performance problem due to excess directory accesses simply to find out where a command is located, and very long search path environment variables such as <code>PATH</code> or the various variables packages such as Python, R or Julia use to find extension packages. On LUMI related packages are often bundled in a single module. </p> <p>Now you may wonder: If a module cannot be simply named after the package it contains as it contains several ones, how can I then find the appropriate module to load? Lmod has a solution for that through the so-called extension mechanism. An Lmod module can define extensions, and some of the search commands for modules will also search in the extensions of a module. Unfortunately, the HPE Cray PE cray-python and cray-R modules do not provide that  information at the moment as they too contain several packages that may benefit from linking to optimised math libraries.</p>"},{"location":"intro-evolving/04-Modules/#searching-for-modules-the-module-spider-command","title":"Searching for modules: The module spider command","text":"<p>There are three ways to use <code>module spider</code>, discovering software in more and more detail. All variants however will by default only check the Cray PE, the CrayEnv stack and the LUMI stacks, unless another software stack is loaded through a module or <code>module use</code> statement and the cache is regenerated during that period.</p> <ol> <li> <p><code>module spider</code> by itself will show a list of all installed software with a short description.     Software is bundled by name of the module, and it shows the description taken from the default     version. <code>module spider</code> will also look for \"extensions\" defined in a module and show those also     and mark them with an \"E\". Extensions are a useful Lmod feature to make clear that a module offers     features that one would not expect from its name. E.g., in a Python module the extensions could be     a list of major Python packages installed in the module which would allow you to find <code>NumPy</code> if     it were hidden in a module with a different name. This is also a very useful feature to make     tools that are bundled in one module to reduce the module clutter findable.</p> </li> <li> <p><code>module spider</code> with the name of a package will show all versions of that package installed on     the system. This is also case-insensitive.      The spider command will not only search in module names for the package, but also in extensions     of the modules and so will be able to tell you that a package is delivered by another module. See      Example 4 below where we will search for the CMake tools.</p> </li> <li> <p>The third use of <code>module spider</code> is with the full name of a module.      This shows two kinds of information. First it shows which combinations of other modules one     might have to load to get access to the package. That works for both modules and extensions     of modules. In the latter case it will show both the module, and other modules that you might     have to load first to make the module available.     Second it will also show help information for the module if the module file provides      such information. </p> </li> </ol>"},{"location":"intro-evolving/04-Modules/#example-1-running-module-spider-on-lumi","title":"Example 1: Running <code>module spider</code> on LUMI","text":"<p>Let's first run the <code>module spider</code> command. The output varies over time, but at the time of writing, and leaving out a lot of the output, one would have gotten:</p> <p></p> <p></p> <p>In the above display, the <code>ARMForge</code> module is currently available in only one version. The <code>Autoconf</code> package is offered in two versions, but in both cases as an extension of another module as the blue <code>(E)</code> in the output shows. The <code>Blosc</code> package is available in many versions, but they are not all shown as the <code>...</code> suggests.</p> <p>After a few more screens, we get the last one:</p> <p></p> <p>On the second screen we see, e.g., the ARMForge module which was available in just a single version at that time, and then Autoconf where the version is in blue and followed by <code>(E)</code>. This denotes that the Autoconf package is actually provided as an extension of another module, and one of the next examples will tell us how to figure out which one.</p> <p>The third screen shows the last few lines of the output, which actually also shows some help information for the command.</p>"},{"location":"intro-evolving/04-Modules/#example-2-searching-for-the-fftw-module-which-happens-to-be-provided-by-the-pe","title":"Example 2: Searching for the FFTW module which happens to be provided by the PE","text":"<p>Next let us search for the popular FFTW library on LUMI:</p> <pre><code>$ module spider FFTW\n</code></pre> <p>produces</p> <p></p> <p>This shows that the FFTW library is actually provided by the <code>cray-fftw</code> module and was at the time that this was tested available in 4 versions.  Note that (a) it is not case sensitive as FFTW is not in capitals in the module name and (b) it also finds modules where the argument of module spider is only part of the name.</p> <p>The output also suggests us to dig a bit deeper and  check for a specific version, so let's run</p> <pre><code>$ module spider cray-fftw/3.3.10.7\n</code></pre> <p>This produces:</p> <p></p> <p></p> <p>We now get a long list of possible combinations of modules that would enable us to load this module. What these modules are will be explained in the next session of this course. However, it does show a weakness when module spider is used with the HPE Cray PE. In some cases, not all possible combinations are shown (and this is the case here as the module is actually available directly after login and also via some other combinations of modules that are not shown). This is because the HPE Cray Programming Environment is system-installed and sits next to the application software stacks that are managed differently, but in some cases also because the HPE Cray PE uses Lmod in a different way than intended by the Lmod developers, causing the spider command to not find some combinations that would actually work. The command does work well with the software managed by the LUMI User Support Team as the next two examples will show.</p>"},{"location":"intro-evolving/04-Modules/#example-3-searching-for-gnuplot","title":"Example 3: Searching for GNUplot","text":"<p>To see if GNUplot is available, we'd first search for the name of the package:</p> <pre><code>$ module spider gnuplot\n</code></pre> <p>This produces:</p> <p></p> <p></p> <p>We see that there are a lot of versions installed on the system and that the version actually contains more  information (e.g., <code>-cpeGNU-24.03</code>) that we will explain in the next part of this course. But you might of course guess that it has to do with the compilers that were used. It may look strange to you to have the same software built with different compilers. However, mixing compilers is sometimes risky as a library compiled with one compiler may not work in an executable compiled with another one, so to enable workflows that use multiple tools we try  to offer many tools compiled with multiple compilers (as for most software we don't use rpath linking which could help to solve that problem). So you want to chose the appropriate line in terms of the other software that you will be using.</p> <p>The output again suggests to dig a bit further for more information, so let's try</p> <pre><code>$ module spider gnuplot/5.4.10-cpeGNU-24.03\n</code></pre> <p>This produces:</p> <p></p> <p></p> <p>In this case, this module is provided by 3 different combinations of modules that also will be explained in the next part of this course. Furthermore, the output of the command now also shows some help information about the module, with some links to further documentation available on the system or on the web. The format of the output is generated automatically by the software installation tool that we use and we sometimes have to do some effort to fit all information in there.</p> <p>For some packages we also have additional information in our LUMI Software Library web site so it is often worth looking there also.</p>"},{"location":"intro-evolving/04-Modules/#example-4-searching-for-an-extension-of-a-module-cmake","title":"Example 4: Searching for an extension of a module: CMake.","text":"<p>The <code>cmake</code> command on LUMI is available in the operating system image, but as is often the case with such tools distributed with the OS, it is a rather old version and you may want to use a newer one.</p> <p>If you would just look through the list of available modules, even after loading some other modules to activate a larger software stack, you will not find any module called <code>CMake</code> though. But let's use the powers of <code>module spider</code> and try</p> <pre><code>$ module spider CMake\n</code></pre> <p>which produces</p> <p></p> <p>the output above shows us that there are actually 4 other versions of CMake on the system, but their version is followed by <code>(E)</code> which says that they are extensions of other modules.</p> <p>Most users would have gotten the same output from</p> <pre><code>$ module spider cmake\n</code></pre> <p></p> <p>However, if you've recently used one of the <code>spack</code> modules and the Lmod cache was last created with one of those modules loaded, </p> <pre><code>$ module spider CMake\n</code></pre> <p>may show you something like</p> <p></p> <p>This shows a number of alternative modules also called <code>cmake</code>, but with sometimes strange looking strings at the end of the version. These are modules that were installed on the system using Spack, a tool for HPC software management that we provide as our secondary tool for users familiar with that tool. More about it also in the presentation on software stack.</p> <p>With a <code>spack</code> module loaded, </p> <pre><code>$ module spider cmake\n</code></pre> <p>would give you something similar to </p> <p></p> <p>and we no longer see the CMake versions provided as extensions (and our main CMake instances).</p> <p>So there is no module called <code>CMake</code> on the system (well, there may be one for Spack users but then with lowercase name). But Lmod already tells us how to find out which module actually provides the CMake tools. So let's try</p> <pre><code>$ module spider CMake/3.29.3\n</code></pre> <p>which produces</p> <p></p> <p>This shows us that the version is provided by a number of <code>buildtools</code> modules, and for each of those modules also shows us which other modules should be loaded to get access to the commands. E.g., the first line tells us that there is a module <code>buildtools/24.03</code> that provides that version of CMake, but that we first need to load some other modules, with <code>LUMI/24.03</code> and <code>partition/L</code> (in that order)  one such combination.</p> <p>So in this case, after</p> <pre><code>$ module load LUMI/24.03 partition/L buildtools/24.03\n</code></pre> <p>the <code>cmake</code> command would be available.</p> <p>And you could of course also use</p> <pre><code>$ module spider buildtools/24.03\n</code></pre> <p>to get even more information about the buildtools module, including any help included in the module.</p>"},{"location":"intro-evolving/04-Modules/#alternative-search-the-module-keyword-command","title":"Alternative search: the module keyword command","text":"<p>Lmod has a second way of searching for modules: <code>module keyword</code>. It searches  in some of the information included in module files for the given keyword, and shows in which modules the keyword was found. We do an effort to put enough information in the modules to make this a suitable additional way to discover software that is installed on the system.</p> <p>Let us look for packages that allow us to download software via the <code>https</code> protocol. One could try</p> <pre><code>$ module keyword https\n</code></pre> <p>which produces the following output:</p> <p></p> <p></p> <p>The first option is misleading and is shown because it contains a URL in the module information that is used by <code>module keyword</code>.  But <code>cURL</code> and <code>wget</code> are indeed  two tools that can be used to fetch files from the internet.</p> <p>LUMI Software Library</p> <p>The LUMI Software Library also has a search box in the upper right. We will see in the next section of this course that much of the software of LUMI is managed through a tool called EasyBuild, and each module file corresponds to an EasyBuild recipe which is a file with the <code>.eb</code> extension. Hence the keywords can also be found in the EasyBuild recipes which are included in this web site, and from a page with an EasyBuild recipe (which may not mean much for you) it is easy to go back to the software package page itself for more information. Hence you can use the search box to search for packages that may not be installed on the system.</p> <p>The example given above though, searching for <code>https</code>, would not work via that box as most EasyBuild recipes include https web links to refer to, e.g., documentation and would be  shown in the result.</p> <p>The LUMI Software Library site includes both software installed in our central software stack and software for which we make customisable build recipes available for user installation, but more about that in the tutorial section on LUMI software stacks.</p>"},{"location":"intro-evolving/04-Modules/#sticky-modules-and-the-module-purge-command","title":"Sticky modules and the module purge command","text":"<p>On some systems you will be taught to avoid <code>module purge</code> as many HPC systems do their default user configuration also through modules. This advice is often given on Cray systems as it is a common practice to preload a suitable set of target modules and a programming environment. On LUMI both are used. A default programming environment and set of target modules suitable for the login nodes is preloaded when you log in to the system, and next the <code>init-lumi</code> module is loaded which in turn makes the LUMI software stacks available that we will discuss in the next session.</p> <p>Lmod however has a trick that helps to avoid removing necessary modules and it is called sticky modules. When issuing the <code>module purge</code> command these modules are automatically reloaded. It is very important to realise that those modules will not just be kept \"as is\" but are in fact unloaded and loaded again as we shall see later that this may have unexpected side effects.  It is still possible to force unload all these modules using <code>module --force purge</code> or selectively unload those using <code>module --force unload</code>.</p> <p>The sticky property is something that is defined in the module file and not used by the module files ot the HPE Cray Programming Environment, but we shall see that there is a partial workaround for this in some of the LUMI software stacks. The <code>init-lumi</code> module mentioned above though is a sticky module, as are the modules that activate a software stack so that you don't have to start from scratch if you have already chosen a software stack but want to clean up your environment.</p> <p>Let us look at the output of the <code>module avail</code> command, taken just after login on the system at the time of writing of these notes (the exact list of modules shown is a bit fluid):</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Next to the names of modules you sometimes see one or more letters. The <code>(D)</code> means that that is currently the default version of the module, the one that will be loaded if you do not specify a version. Note that the default version may depend on other modules that are already loaded as we have seen in the discussion of the programming environment.</p> <p>The <code>(L)</code> means that a module is currently loaded.</p> <p>The <code>(S)</code> means that the module is a sticky module.</p> <p>Next to the <code>rocm</code> module (on the fourth screen) you see  <code>(5.0.2:5.1.0:5.2.0:5.2.3:5.5.1:5.7.0:6.0.0)</code>. This shows that the <code>rocm/6.0.3</code> module can also  be loaded as <code>rocm/5.0.2</code> or any of the other versions in that list. Some of them were old versions that have been removed from the system in later updates, and others are versions that are hard-coded some of the Cray PE modules and other files but have never been on the system as we had an already patched version. (E.g., the 24.03 version of the Cray PE will sometimes try to load <code>rocm/6.0.0</code>  while we have immediate had <code>rocm/6.0.3</code> on the system which corrects some bugs).</p> <p>At the end of the overview the extensions are also shown. If this would be fully implemented on LUMI, the list could become very long. However, as we shall see next, there is an easy way to hide those from view. We haven't used extensions very intensely so far as there was a bug in older versions of Lmod so that turning off the view didn't work and so that extensions that were not in available modules, were also shown. But that is fixed in current versions.</p>"},{"location":"intro-evolving/04-Modules/#changing-how-the-module-list-is-displayed","title":"Changing how the module list is displayed","text":"<p>You may have noticed in the above example that we don't show directories of module files in the overview (as is the case on most clusters) but descriptive texts about the module group. This is just one view on the module tree though, and it can be changed easily by loading a  version of the <code>ModuleLabel</code> module.</p> <ul> <li><code>ModuleLabel/label</code> produces the default view of the previous example. </li> <li><code>ModuleLabel/PEhierarchy</code> still uses descriptive texts but will show the whole      module hierarchy of the HPE Cray Programming Environment.</li> <li><code>ModuleLabel/system</code> does not use the descriptive texts but shows module directories instead.</li> </ul> <p>When using any kind of descriptive labels, Lmod can actually bundle module files from different  directories in a single category and this is used heavily when <code>ModuleLabel/label</code> is loaded  and to some extent also when <code>ModuleLabel/PEhierarchy</code> is loaded.</p> <p>It is rather hard to provide multiple colour schemes in Lmod, and as we do not know how your  terminal is configured it is also impossible to find a colour scheme that works for all users. Hence we made it possible to turn on and off the use of colours by Lmod through the <code>ModuleColour/on</code> and <code>ModuleColour/off</code> modules.</p> <p>As the module extensions list in the output of <code>module avail</code> could potentially become very long over time (certainly if there would be Python or R modules installed with EasyBuild that show all included Python or R packages in that list) you may want to hide those. You can do this by loading the <code>ModuleExtensions/hide</code> module and undo this again by loading <code>ModuleExtensions/show</code>.</p> <p>There are two ways to tell <code>module spider</code> to search in all installed modules. One is more meant as a  temporary solution: Load</p> <pre><code>module load ModuleFullSpider/on\n</code></pre> <p>and this is turned off again by force-unloading this module or loading</p> <pre><code>module load ModuleFullSpider/off\n</code></pre> <p>The second and permanent way is to set add the line</p> <pre><code>export LUMI_FULL_SPIDER=1\n</code></pre> <p>to your <code>.profile</code> file and from then on, <code>module spider</code> will index all modules on the system. Note that this can have a large impact on the performance of the <code>module spider</code> and <code>module avail</code> commands that can easily \"hang\" for a minute or more if a cache rebuild is needed, which is the case after installing software with EasyBuild or once every 24 hours.</p> <p>We also hide some modules from regular users because we think they are not useful at all for regular users or not useful in the context you're in at the moment.  You can still load them if you know they exist and specify the full version but  you cannot see them with <code>module available</code>. It is possible though to still show most if not all of  them by loading <code>ModulePowerUser/LUMI</code>. Use this at your own risk however, we will not help you to make things work if you use modules that are hidden in the context you're in or if you try to use any module that was designed for us to maintain the system and is therefore hidden  from regular users.</p> <p>Another way to show hidden modules also, is to use the <code>--show_hidden</code> flag of the module command with the <code>avail</code> subcommand: <code>module --show_hidden avail</code>. </p> <p>With <code>ModulePowerUser</code>, all modules will be displayed as if they are regular modules, while <code>module --show_hidden avail</code> will still grey the hidden modules and add an <code>(H)</code> to them so that they are easily recognised.</p> <p>Example</p> <p>An example that will only become clear in the next session: When working with the software stack called <code>LUMI/24.03</code>, which is built upon the HPE Cray Programming Environment version 24.03, all (well, most) of the modules corresponding to other versions of the Cray PE are hidden.</p> <p>Just try</p> <pre><code>$ module load LUMI/24.03\n$ module avail\n</code></pre> <p>and you'll see a lot of new packages that have become available, but will also see less Cray PE  modules.</p>"},{"location":"intro-evolving/04-Modules/#getting-help-with-the-module-help-command","title":"Getting help with the module help command","text":"<p>Lmod has the <code>module help</code> command to get help on modules</p> <pre><code>$ module help\n</code></pre> <p>without further arguments will show some help on the <code>module</code> command. </p> <p>With the name of a module specified, it will show the help information for the default version of that module, and with a full name and version specified it will show this information specifically for that version of the module. But note that <code>module help</code> can only show help for currently available modules.</p> <p>Try, e.g., the following commands:</p> <pre><code>$ module help cray-mpich\n$ module help cray-python/3.11.7\n$ module help buildtools/24.03\n</code></pre> <p>Lmod also has another command that produces more limited information (and is currently not fully exploited on LUMI): <code>module whatis</code>. It is more a way to tag a module with different kinds of information, some of  which has a special meaning for Lmod and is used at some places, e.g., in the output of <code>module spider</code> without arguments.</p> <p>Try, e.g.,:</p> <pre><code>$ module whatis Subversion\n$ module whatis Subversion/1.14.3\n</code></pre>"},{"location":"intro-evolving/04-Modules/#a-note-on-caching","title":"A note on caching","text":"<p>Modules are stored as (small) files in the file system. Having a large module system with much software preinstalled for everybody means a lot of small files which will make our Lustre file system very unhappy. Fortunately Lmod does use caches by default. On LUMI we currently have no  system cache and only a user cache. That cache can be found in <code>$HOME/.cache/lmod</code> (and in older versions of LMOD in <code>$HOME/.lmod.d/.cache</code>). </p> <p>That cache is also refreshed automatically every 24 hours. You'll notice when this happens as, e.g., the <code>module spider</code> and <code>module available</code> commands will be slow during the rebuild. you may need to clean the cache after installing new software as on LUMI Lmod does not always detect changes to the installed software,</p> <p>Sometimes you may have to clear the cache also if you get very strange answers from  <code>module spider</code>. It looks like the non-standard way in which the HPE Cray Programming Environment does certain things in Lmod can cause inconsistencies in the cache. This is also one of the reasons why we do not yet have a central cache for that  software that is installed in the central stacks as we are not sure when that cache is in good shape.</p>"},{"location":"intro-evolving/04-Modules/#a-note-on-other-commands","title":"A note on other commands","text":"<p>As this tutorial assumes some experience with using modules on other clusters, we haven't paid much attention to some of the basic commands that are mostly the same across all three module environments implementations.  The <code>module load</code>, <code>module unload</code> and <code>module list</code> commands work largely as you would expect, though the output style of <code>module list</code> may be a little different from what you expect. The latter may show some inactive modules. These are modules that were loaded at some point, got unloaded when a module closer to the root of the hierarchy of the module system got unloaded, and they will be reloaded automatically when that module or an equivalent (family or name) module is loaded that makes this one or an equivalent module available again.</p> <p>Example</p> <p>To demonstrate this, try in a fresh login shell (with the lines starting with a <code>$</code> the commands that you should enter at the command prompt):</p> <pre><code>$ module unload craype-network-ofi\n\nInactive Modules:\n  1) cray-mpich\n\n$ module load craype-network-ofi\n\nActivating Modules:\n  1) cray-mpich/8.1.29\n</code></pre> <p>The <code>cray-mpich</code> module needs both a valid network architecture target module to be loaded (not <code>craype-network-none</code>) and a compiler module. Here we remove the network target module which inactivates the <code>cray-mpich</code> module, but the module gets reactivated again as soon as the network target module is reloaded.</p> <p>The <code>module swap</code> command is basically equivalent to a <code>module unload</code> followed by a <code>module load</code>.  With one argument it will look for a module with the same name that is loaded and unload that one  before loading the given module. With two modules, it will unload the first one and then load the second one. The <code>module swap</code> command is not really needed on LUMI as loading a conflicting module (name or family) will automatically unload the previously loaded one. However, in case of replacing  a module of the same family with a different name, <code>module swap</code> can be a little faster than just a <code>module load</code> as that command will need additional operations as in the first step it will  discover the family conflict and then try to resolve that in the following steps (but explaining that in detail would take us too far in the internals of Lmod).</p>"},{"location":"intro-evolving/04-Modules/#links","title":"Links","text":"<p>These links were OK at the time of the course. This tutorial will age over time though and is not maintained but may be replaced with evolved versions when the course is organised again, so links may break over time.</p> <ul> <li>Lmod documentation and more specifically     the User Guide for Lmod which is the part specifically for regular users who do not     want to design their own modules.</li> <li>Information on the module environment in the LUMI documentation</li> </ul>"},{"location":"intro-evolving/04-Modules/#local-materials","title":"Local materials","text":"<ul> <li> <p>VSC</p> <ul> <li> <p>VSC@UAntwerpen: Modules are covered in the HPC@UAntwerp introduction</p> </li> <li> <p>VSC@VUB: Modules are covered in the HPC Introduction course,</p> <p>with local documentation in the \"Module System\" section of the documentation</p> </li> <li> <p>VSC@UGent: Modules are briefly coverd in the \"Introduction to HPC-UGent\" course. The Lmod setup on the     clusters at UGent is more restrictive than on LUMI with several usefull features disabled.</p> <ul> <li>Slides and YouTube recording     of the September 20, 2024 session.</li> </ul> </li> <li> <p>VSC@KULeuven: Modules are discussed in the Linux for HPC course</p> </li> </ul> </li> <li> <p>C\u00c9CI: </p> <ul> <li> <p>Choosing and activating software with system modules on C\u00c9CI clusters     from the fall 2022 introductory courses.</p> </li> <li> <p>YouTube video: Modules: How to find/use software on cluster     from the fall 2020 introductory courses.</p> </li> </ul> </li> </ul>"},{"location":"intro-evolving/05-SoftwareStacks/","title":"Software stacks","text":"<p>Last update of this page: October 3, 2025</p>"},{"location":"intro-evolving/05-SoftwareStacks/#lumi-software-stacks","title":"LUMI Software Stacks","text":"<p>In this section we discuss</p> <ul> <li>Several of the ways in which we offer software on LUMI</li> <li>Managing software in our primary software stack which is based on EasyBuild</li> </ul>"},{"location":"intro-evolving/05-SoftwareStacks/#the-software-stacks-on-lumi","title":"The software stacks on LUMI","text":""},{"location":"intro-evolving/05-SoftwareStacks/#design-considerations","title":"Design considerations","text":"<ul> <li> <p>LUMI is a very leading edge and also an inhomogeneous machine. Leading edge often implies     teething problems and inhomogeneous doesn't make life easier either.</p> <ol> <li>It uses a novel interconnect which is an extension of Ethernet      (and a precursor of the upcoming Ultra Ethernet standard)     rather than being based on InfiniBand,      and that interconnect has a different software stack than your typical Mellanox InfiniBand cluster. </li> <li>It also uses a relatively new GPU architecture, AMD CDNA2, with a not fully mature software ecosystem.      The GPU nodes are really GPU-first, with the interconnect cards connected directly to the GPU packages      and only one CPU socket, and another feature which is relatively new: the option to use a partly coherent fully unified memory     space between the CPU and GPUs, though of course very NUMA. This is a feature that has previously     only been seen in supercomputers in some clusters with NVIDIA P100 and V100 GPUs and IBM Power 8 and 9 CPUs used     for some USA pre-exascale systems.</li> <li>LUMI is also inhomogeneous because some nodes have zen2 processors while the two main compute partitions     have zen3-based CPUs, and the compute GPU nodes have AMD GPUs while the visualisation nodes have     NVIDIA GPUs. </li> </ol> <p>Given the rather novel interconnect and GPU we cannot expect that all system and application software is already fully mature and we need to be prepared for fast evolution,  hence we needed a setup that enables us to remain very agile, which leads to different compromises compared to a software stack for a more conventional and mature system as an x86 cluster with NVIDIA GPUs and Mellanox InfiniBand.</p> </li> <li> <p>Users also come to LUMI from 12 different channels, not counting subchannels as some countries have     multiple organisations managing allocations, and those channels all have different expectations about     what LUMI should be and what kind of users should be served. For the major LUMI stakeholder, the EuroHPC JU,     LUMI is a pre-exascale system meant to prepare users and applications to make use of future even large     systems, while some of the LUMI consortium countries see LUMI more as an extension of their tier-1 or     even tier-2 machines.</p> </li> <li> <p>The central support team of LUMI is also relatively small compared to the nature of LUMI with its     many different partitions and storage services and the expected number of projects and users.      Support from users coming in via the national channels will rely a lot on efforts from local organisations     also. So we must set up a system so that they can support their users without breaking things on     LUMI, and to work with restricted rights. And in fact, LUMI User Support team members also have very limited additional     rights on the machine compared to regular users or support people from the local organisations.     LUST is currently 10 FTE. Compare this to 41 people in the J\u00fclich Supercomputer Centre for software     installation and support only... (I give this number because it was mentioned in a a talk in the     EasyBuild user meeting in 2022.)</p> </li> <li> <p>The Cray Programming Environment is also a key part of LUMI and the environment for which we get     support from HPE Cray. It is however different from more traditional environments such as a typical     Intel oneAPI installation of a typical installation build around the GNU Compiler Collection and Open MPI     or MPICH. The programming environment is installed with the operating system rather than through the     user application software stack hence not managed through the tools used for the application software     stack, and it also works differently with its universal compiler wrappers that are typically configured     through modules. </p> </li> <li> <p>There is an increasing need for customised setups. Everybody wants a central stack as long as their     software is in there but not much more as otherwise it is hard to find, and as long as software is      configured in the way they are used to. And everybody would like LUMI to look as much as possible      as their home system. But this is of course impossible. Moreover, there are more and more conflicts     between software packages and modules are only a partial solution to this problem. The success of     containers, conda and Python virtual environments is certainly to some extent explained by the      need for more customised setups and the need for multiple setups as it has become nearly impossible     to combine everything in a single setup due to conflicts between packages and the dependencies they need.</p> </li> </ul>"},{"location":"intro-evolving/05-SoftwareStacks/#the-lumi-solution","title":"The LUMI solution","text":"<p>The LUMI User Support Team (LUST) tried to take all these considerations into account  and came up with a solution that may look a little unconventional to many users.</p> <p>In principle there should be a high degree of compatibility between releases of the HPE Cray Programming Environment but LUST decided not to take the risk and build our software for a specific release of the  programming environment, which is also a better fit with the typical tools used to manage a scientific  software stack such as EasyBuild and Spack as they also prefer precise versions for all dependencies and compilers etc. The stack is also made very easy to extend. So LUMI has many base libraries and some packages already pre-installed but also provides an easy and very transparent way to install additional packages in your project space in exactly the same way as is done for the central stack, with the same performance but the benefit that the installation can be customised more easily to the needs of your project. Not everybody needs the same configuration of GROMACS or LAMMPS or other big packages, and in fact a one-configuration-that-works-for-everybody may even be completely impossible due to conflicting options that cannot be used together.</p> <p>For the module system a choice had to be made between two systems supported by HPE Cray. They support  Environment Modules with module files based on the TCL scripting language, but only the old version that is no longer really developed and not the newer versions 4 and 5 developed in France, and Lmod, a module system based on the LUA scripting language that also support many TCL module files through a translation layer. LUMI chose to go with Lmod as LUA is an easier and more modern language to work with and as Lmod is much more powerful than Environment Modules 3, certainly for searching modules.</p> <p>To manage the software installations there was a choice between EasyBuild, which is mostly developed in Europe and hence a good match with a EuroHPC project as EuroHPC wants to develop a European HPC technology stack from hardware to application software, and Spack, a package developed in the USA national labs. Both have their own strengths and weaknesses. LUMI chose to go with EasyBuild as the primary tool for which the LUST also does some development.  However, as we shall see, the EasyBuild installation is not your typical EasyBuild installation that you may be accustomed with from clusters at your home institution. It uses toolchains specifically for the HPE Cray programming environment so recipes need to be adapted. LUMI does offer a growing library of Cray-specific installation recipes though. The whole setup of EasyBuild is done such that you can build on top of the central software stack and such that your modules appear in your module view without having to add directories by hand to environment variables etc. You only need to point to the place where you want to install software for your project as LUMI cannot automatically determine a suitable place. </p> <p>The LUST does offer some help to set up Spack also but it is mostly offered \"as is\" and LUST will not do bug-fixing or development in Spack package files. Spack is very attractive for users who want to set up a personal environment with fully customised versions of the software rather than the rather fixed versions provided by EasyBuild for every version of the software stack. It is possible to specify versions for the main packages that you need and then let Spack figure out a minimal compatible set of dependencies to install  those packages.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#software-policies","title":"Software policies","text":"<p>As any site, LUMI also has a number of policies about software installation, and these policies are further developed as the LUMI team gains experience in what they can do with the amount of people  in LUST and what they cannot do.</p> <p>LUMI uses a bring-your-own-license model except for a selection of tools that are useful to a larger community. </p> <ul> <li>This is partly caused by the distributed user management as the LUST does not even have the necessary     information to determine if a particular user can use a particular license, so that      responsibility must be shifted to people who have that information, which is often the PI of your project.</li> <li>You also have to take into account that up to 20% of LUMI is reserved for industry use which makes      negotiations with software vendors rather difficult as they will want to push LUMI onto the industrial     rather than academic pricing as they have no guarantee that LUMI operations will obey      to the academic license restrictions. </li> <li>And lastly, the LUMI project doesn't have an infinite budget.      There was a questionnaire sent out to      some groups even before the support team was assembled and that contained a number of packages that     by themselves would likely consume the whole LUMI software budget for a single package if I look at the      size of the company that produces the package and the potential size of their industrial market.      So LUMI has to make choices and with any choice for a very specialised package you favour a few      groups. And there is also a political problem as without doubt the EuroHPC JU would prefer that LUMI     invests in packages that are developed by European companies or at least have large development     teams in Europe.</li> </ul> <p>Some restrictions coming from software licenses</p> <ul> <li> <p>Anaconda use is currently in a very grey area on LUMI, also after the     publication of the new      Anaconda Terms of Service     effective from July 15 2025. Users may assume to quickly that they     fall in one of the categories of free use if they are in academia,      but one should also check the      Academic Policy.     Read section 4 carefully and you see that the right to use is not automatic,     but you or the institute you work for must explicitly sign documents.</p> <p>Moreover, as the operator of LUMI, CSC, is not such an institution, they cannot assist users to install Anaconda. Just to remain on the safe side, even Miniconda, that can still be used even by companies as long as they don't download from the Anaconda repositories, has been removed from tools offered by CSC and replaced by miniforge.</p> <p>Downloading packages from conda-forge  is perfectly OK though.</p> </li> <li> <p>The LUMI support team cannot really help much with VASP as most people in the support      team are not covered by a valid VASP license. VASP licenses typically even contain a list     of people who are allowed to touch the source code, and one person per license who can     download the source code. We're trying to get better access with a support license     but it takes time.</p> </li> </ul> <p>The LUMI User Support Team tries to help with installations of recent software but porting or bug correction in software is not their task. In Flanders some help is possible by the VSC Tier-0 support team but do not expect that they will port your whole application. For very complicated installations the  EPICURE project may sometimes be able to help. As a user, you have to realise that not all Linux or even supercomputer software will work on LUMI. This holds even more for software that comes only as a binary. The biggest problems are the GPU and anything that uses distributed memory and requires high performance from the interconnect. For example,</p> <ul> <li>software that use NVIDIA proprietary programming models and     libraries needs to be ported. </li> <li>Binaries that do only contain NVIDIA code paths, even if the programming     model is supported on AMD GPUs, will not run on LUMI.</li> <li>Binaries for AMD GPUs must work with the ROCm versions that can be supported on the system.     There can be only one driver version and each driver version supports only a limited range     of ROCm versions.</li> <li>The LUMI interconnect requires libfabric, the Open Fabrics Interface (OFI) library,     using a specific provider for the NIC used on LUMI, the so-called Cassini provider (CXI),      so any software compiled with an MPI library that     requires UCX, or any other distributed memory model built on top of UCX, will not work on LUMI, or at     least not work efficiently as there might be a fallback path to TCP communications. </li> <li>Even intra-node interprocess communication can already cause problems     as there are three different kernel extensions     that provide more efficient interprocess messaging than the standard Linux mechanism. Many clusters     use knem for that but on LUMI xpmem is used. So software that is not build to support xpmem will     also fall back to the default mechanism or fail. </li> <li>Also, the MPI implementation needs to collaborate     with certain modules in our Slurm installation to start correctly and experience has shown that this     can also be a source of trouble as the fallback mechanisms that are often used do not work on LUMI. </li> <li>Containers solve none of these problems. There can be more subtle compatibility problems also.      As has been discussed earlier in the course, LUMI runs SUSE Linux and not Ubuntu which is popular on      workstations or a Red Hat-derived Linux popular on many clusters. Subtle differences between Linux      versions can cause compatibility problems that in some cases can be solved with containers. But containers     won't help you if they are build for different kernel extensions and hardware interfaces.</li> <li>The compute nodes also lack some Linux daemons that may be present on smaller clusters. HPE Cray use an     optimised Linux version called COS or Cray Operating System on the compute nodes. It is optimised to     reduce OS jitter and hence to enhance scalability of applications as that is after all the primary     goal of a pre-exascale machine. But that implies that certain Linux daemons that your software may      expect to find are not present on the compute nodes. D-Bus comes to mind.</li> </ul> <p>Also, the LUMI user support team is too small to do all software installations which is why LUMI currently states in its policy that a LUMI user should be capable of installing their software themselves or have another support channel. The LUST cannot install every single piece of often badly documented research-quality code that was never meant to be used by people who don't understand the code. Your national organisation may also be able to provide some help, but not all countries offer such service on LUMI.</p> <p>Another soft compatibility problem that I did not yet mention is that software that accesses tens of thousands of small files and abuses the file system as a database rather than using structured data formats designed to organise data on supercomputers is not welcome on LUMI. For that reason LUMI also requires to containerize conda and Python installations.  On LUMI three tools are offered for this. </p> <ol> <li>cotainr      is a tool developed by the Danish LUMI-partner DeIC that helps with building some types of      containers that can be built in user space. Its current version focusses on containerising      a conda-installation.</li> <li>The second tool is a container-based wrapper generator that offers      a way to install conda packages or to install Python packages with pip on top of      the Python provided by the <code>cray-python</code> module. On LUMI the tool is called     lumi-container-wrapper     but users of the CSC national systems will know it as Tykky. </li> <li>SingularityCE supports the so-called      unprivileged proot build process      to build containers. With this process, it is also possible to add additional OS packages, etc., to the container.</li> </ol> <p>Both cotainr and lumi-container-wrapper are pre-installed on the system as modules. Furthermore, there is also a module that provides the <code>proot</code> command needed by the SingularityCE unprivileged proot build process.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#organisation-of-the-software-in-software-stacks","title":"Organisation of the software in software stacks","text":"<p>LUMI offers several software stacks:</p> <p>CrayEnv is the minimal software stack for users who only need the  Cray Programming Environment but want a more recent set of build tools etc  than the OS provides. We also take care of a few issues that we will discuss on the next slide that are present right after login on LUMI.</p> <p>Next we have the stacks called \"LUMI\". Each one corresponds to a particular release of the HPE Cray Programming Environment. It is the stack in which the LUST installs software using that programming environment and mostly EasyBuild. The Cray Programming Environment modules are still used, but they are accessed through a replacement for the PrgEnv modules that is managed by EasyBuild. There are tuned versions for the 4 of hardware in the regular LUMI system: zen2 CPUs in the login nodes and large memory nodes, zen3 for the  LUMI-C compute nodes, zen3 + MI250X for the LUMI-G partition, and zen2 + the NVIDIA GPUs for visualisation.</p> <p>LUMI also offers an extensible software stack based on Spack which has been pre-configured to use the compilers from the Cray PE. This stack is offered as-is for users who know how to use Spack, but support is limited and no bug-fixing in Spack is done.</p> <p>Some partner organisations in the LUMI consortium also provide pre-installed software on LUMI. This software is not managed by the LUMI User Support Team and as a consequence of this, support is only provided through those organisations that manage the software. Though they did promise to offer some basic support for everybody, the level of support may be different depending on how your project ended up on LUMI as they receive no EuroHPC funding for this.  There is also no guarantee that software in those stacks is compatible with anything else on LUMI. The stacks are provided by modules whose name starts with <code>Local-</code>. Currently there are two such stacks on LUMI:</p> <ul> <li> <p><code>Local-CSC</code>: Enables software installed and maintained by CSC.      Most of that software is available to all users, though some packages are      restricted or only useful to users of other CSC services (e.g., the allas module).</p> <p>Some of that software builds on software in the LUMI stacks, some is based on  containers with wrapper scripts, and some is compiled outside of any software  management environment on LUMI.</p> <p>The names of the modules don't follow the conventions of the LUMI stacks, but those used on the Finnish national systems.</p> </li> <li> <p><code>Local-quantum</code> contains some packages of general use, but also some packages that      are only relevant to Finnish researchers with an account on the Helmi quantum computer.      Helmi is not a EuroHPC-JU computer so being eligible for an account on LUMI does      not mean that you are also eligible for an account on Helmi.</p> </li> </ul> <p>EuroHPC would like to see the EESSI stack stack on all their supercomputers as part of the EuroHPC Federation Platform early next year already. As currently AMD support is still missing, any initial offering will likely only be  for the CPU nodes. Moreover, there are several technical issues, including a choice of MPI implementation that is troublesome on LUMI and a preferred distribution mechanism that is not compatible with the setup of a Cray system and the currently available hardware. EESSI is built on one of the common toolchains of EasyBuild, so that would then be a way to offer that toolchain also, but with support coming from a separate help desk for  the Federation Platform.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#3-ways-to-access-the-cray-programming-environment-on-lumi","title":"3 ways to access the Cray Programming environment on LUMI.","text":""},{"location":"intro-evolving/05-SoftwareStacks/#bare-environment-and-crayenv","title":"Bare environment and CrayEnv","text":"<p>Right after login you have a very bare environment available with the Cray Programming Environment with the PrgEnv-cray module loaded. It gives you basically what you can expect on a typical Cray system. There aren't many tools available, basically mostly only the tools in the base OS image and some tools that will not impact software installed in one of the software stacks. The set of target modules loaded is the one for the login nodes and not tuned to any particular node type. As a user you're fully responsible for managing the target modules, reloading them when needed or loading the appropriate set for the hardware you're using or want to cross-compile for.</p> <p>The second way to access the Cray Programming Environment is through the CrayEnv software stack. This stack offers an \"enriched\" version of the Cray environment. It takes care of the target modules: Loading or reloading CrayEnv will reload an optimal set of target modules for the node you're on. It also provides some additional  tools like newer build tools than provided with the OS. They are offered here and not in the bare environment to be sure that those tools don't create conflicts with software in other stacks. But otherwise the Cray Programming  Environment works exactly as you'd expect from this course or the 4-day comprehensive courses that LUST organises.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#lumi-stack","title":"LUMI stack","text":"<p>The third way to access the Cray Programming Environment is through the LUMI software stacks, where each stack is based on a particular release of the HPE Cray Programming Environment. We advise against mixing with modules that came with other versions of the Cray PE, but they remain accessible although they are hidden from the default view for regular users. It is also better to not use the PrgEnv modules, but the equivalent LUMI EasyBuild  toolchains instead as indicated by the following table:</p> HPE Cray PE LUMI toolchain What? <code>PrgEnv-cray</code> <code>cpeCray</code> Cray Compiling Environment <code>PrgEnv-gnu</code> <code>cpeGNU</code> GNU C/C++ and Fortran <code>PrgEnv-aocc</code> <code>cpeAOCC</code> AMD CPU compilers (login nodes and LUMI-C only) <code>PrgEnv-amd</code> <code>cpeAMD</code> AMD ROCm GPU compilers (LUMI-G only) <p>The cpeCray etc modules also load the MPI libraries and Cray LibSci just as the PrgEnv modules do. And they are sometimes used to work around problems in Cray-provided modules that cannot changed easily due to the way system administration on a Cray system is done. </p> <p>This is also the environment in which the LUST installs most software,  and from the name of the modules you can see which compilers we used.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#lumi-stack-module-organisation","title":"LUMI stack module organisation","text":"<p>To manage the heterogeneity in the hardware, the LUMI software stack uses two levels of modules</p> <p>First there are the LUMI/24.03, LUMI/23.12, LUMI/23.09, LUMI/23.03, LUMI/22.12 and LUMI/22.08 modules.  Each of the LUMI modules loads a particular version of the LUMI stack.</p> <p>The second level consists of partition modules.  There is partition/L for the login and large memory nodes, partition/C for the regular compute nodes, partition/G for the AMD GPU nodes and partition/D for the visualisation nodes.</p> <p>There is also a hidden partition/common module in which software is installed that is available everywhere,  but we advise you to be careful to install software in there in your own installs as it is risky to rely on software in one of the regular partitions, and impossible in our EasyBuild setup.</p> <p>The LUMI module will automatically load the best partition module for the current hardware whenever it is loaded or reloaded. So if you want to cross-compile, you can do so by loading a different partition  module after loading the LUMI module, but you'll have to reload every time you reload the LUMI module.</p> <p>Hence you should also be very careful in your job scripts. On LUMI the environment from the login nodes is used when your job starts, so unless you switched to the suitable partition for the compute nodes, your job will start with the software stack for the login nodes. If in your job script you reload the  LUMI module it will instead switch to the software stack that corresponds to the type of compute node you're using and more optimised binaries can be available. If for some reason you'd like to use the same software on LUMI-C and on the login or large memory nodes and don't want two copies of locally installed software, you'll have to make sure that after reloading the LUMI module in your job script you explicitly load the partition/L module.</p> <p>Supported stacks after the August 2024 system update</p> <p>Since 24.03 is the only version of the Cray Programming Environment currently fully supported by HPE on LUMI, as it is the only version which is from the ground up built for ROCm/6.0,  SUSE Enterprise 15 SP5, and the current version of the SlingShot software, it is also the only fully supported version of the LUMI software stacks.</p> <p>The 23.12 and 23.09 version function reasonably well, but keep in mind that 23.09 was originally meant to be used with ROCm 5.2 or 5.5 depending on the SUSE version while you will now get a much newer version of the compilers that come with ROCm. You can expect issues with compiling GPU code with the Cray compilers, and the AMD module is also different from when the toolchain was installed as we had to redirect to a version of ROCm supported by the driver.</p> <p>The even older stacks are only there for projects that were using them. We've had problems with them already in the past and they currently don't work properly anymore for installing software via EasyBuild.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#easybuild-to-extend-the-lumi-software-stack","title":"EasyBuild to extend the LUMI software stack","text":""},{"location":"intro-evolving/05-SoftwareStacks/#installing-software-on-hpc-systems","title":"Installing software on HPC systems","text":"<p>Software on HPC systems is rarely installed from RPMs  (Red Hat Package Manager, a popular format to package Linux software distributed as binaries) or any other similar format for various reasons. Generic RPMs are rarely optimised for the specific CPU of the system as they have to work on a range of systems and including optimised code paths in a single executable for multiple architectures is hard to even impossible. Secondly generic RPMs might not even work with the specific LUMI environment. They may not fully support the SlingShot interconnect and hence run at reduced speed, or they may need particular kernel modules or daemons that are not present on the system or they may not work well with the resource manager on the system.  This is expected to happen especially with packages that require specific MPI versions or implementations. Moreover, LUMI is a multi-user system so there is usually no \"one version fits all\". And LUMI needs a small system image as nodes are diskless which means that RPMs need to be relocatable so that they can be installed elsewhere.</p> <p>Spack and EasyBuild are the two most popular HPC-specific software build and installation frameworks.  These two systems usually install packages from sources so that the software can be adapted to the underlying hardware and operating system. They do offer a mean to communicate and execute installation instructions easily so that in practice once a package is well supported by these tools a regular user can install them also. Both packages make software available via modules so that you can customise your environment and select appropriate versions for your work.  And they do take care of dependency handling in a way that is compatible with modules.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#extending-the-lumi-stack-with-easybuild","title":"Extending the LUMI stack with EasyBuild","text":"<p>On LUMI EasyBuild is the primary software installation tool.  EasyBuild was selected as there is already a lot of experience with EasyBuild in several LUMI consortium countries and as it is also a tool developed in Europe which makes it a nice fit with EuroHPC's goal of creating a fully European HPC ecosystem.</p> <p>EasyBuild is fully integrated in the LUMI software stack. Loading the LUMI module will not only make centrally installed packages available, but also packages installed in your personal or project stack. Installing packages in that space is done by loading the EasyBuild-user module that will load a suitable version of EasyBuild and configure it for installation in a way that is compatible with the LUMI stack. EasyBuild will then use existing modules for dependencies if those are already on the system or in your personal or project stack.</p> <p>Note however that the built-in easyconfig files that come with EasyBuild do not work on LUMI at the moment.</p> <ul> <li>For the GNU toolchain there would be problems with MPI. EasyBuild uses Open MPI and that     needs to be configured differently to work well on LUMI, and there are also still issues with     getting it to collaborate with the resource manager as it is installed on LUMI.</li> <li>The Intel-based toolchains have their problems also. At the moment, the Intel compilers with the     AMD CPUs are a problematic cocktail. There have recently been performance and correctness problems      with the MKL math library and also failures with some versions of Intel MPI,      and you need to be careful selecting compiler options and not use <code>-xHost</code>     or the classic Intel compilers will simply optimize for a two decades old CPU.     The situation is better with the new LLVM-based compilers though, and it looks like     very recent versions of MKL are less AMD-hostile. Problems have also been reported     with Intel MPI running on LUMI.</li> </ul> <p>Instead LUMI has its own EasyBuild build recipes that are also made available in the  LUMI-EasyBuild-contrib GitHub repository. The EasyBuild configuration done by the EasyBuild-user module will find a copy of that repository on the system or in your own install directory. The latter is useful if you always want the very latest, before it is even deployed on the system. </p> <p>LUMI also offers the LUMI Software Library which documents all software for which there are LUMI-specific EasyBuild recipes available.  This includes both the pre-installed software and the software for which recipes are provided in the LUMI-EasyBuild-contrib GitHub repository, and even instructions for some software that is not suitable for installation through EasyBuild or Spack, e.g., because it likes to write in its own directories while running.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#easybuild-recipes-easyconfigs","title":"EasyBuild recipes - easyconfigs","text":"<p>EasyBuild uses a build recipe for each individual package, or better said, each individual module as it is possible to install more than one software package in the same module. That installation description relies on either a generic or a specific installation process provided by an easyblock. The build recipes are called easyconfig files or simply easyconfigs and are Python files with  the extension <code>.eb</code>. </p> <p>The typical steps in an installation process are:</p> <ol> <li>Downloading and unpacking sources and applying patches.      For licensed software you may have to provide the sources as     often they cannot be downloaded automatically.</li> <li>A typical configure - build - test - install process, where the test process is optional and     depends on the package providing useable pre-installation tests.</li> <li>An extension mechanism can be used to install perl/python/R extension packages</li> <li>Then EasyBuild will do some simple checks (some default ones or checks defined in the recipe)</li> <li>And finally it will generate the module file using lots of information specified in the      EasyBuild recipe.</li> </ol> <p>Most or all of these steps can be influenced by parameters in the easyconfig.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#the-toolchain-concept","title":"The toolchain concept","text":"<p>EasyBuild uses the toolchain concept. A toolchain consists of compilers, an MPI implementation and some basic mathematics libraries. The latter two are optional in a toolchain. All these  components have a level of exchangeability as there are language standards, as MPI is standardised, and the math libraries that are typically included are those that provide a standard API for which several implementations exist. All these components also have in common that it is risky to combine  pieces of code compiled with different sets of such libraries and compilers because there can be conflicts in names in the libraries.</p> <p>LUMI doesn't use the standard EasyBuild toolchains but its own toolchains specifically for Cray and these are precisely the <code>cpeCray</code>, <code>cpeGNU</code>, <code>cpeAOCC</code> and <code>cpeAMD</code> modules already mentioned  before.</p> HPE Cray PE LUMI toolchain What? <code>PrgEnv-cray</code> <code>cpeCray</code> Cray Compiling Environment <code>PrgEnv-gnu</code> <code>cpeGNU</code> GNU C/C++ and Fortran <code>PrgEnv-aocc</code> <code>cpeAOCC</code> AMD CPU compilers (login nodes and LUMI-C only) <code>PrgEnv-amd</code> <code>cpeAMD</code> AMD ROCm GPU compilers (LUMI-G only) <p></p> <p>There is also a special toolchain called the SYSTEM toolchain that uses the compiler provided by the operating system. This toolchain does not fully function in the same way as the other toolchains when it comes to handling dependencies of a package and is therefore a bit harder to use. The EasyBuild designers had in mind that this compiler would only be used to bootstrap an EasyBuild-managed software stack, but on LUMI it is used for a bit more as it offers a relatively easy way to compile some packages also for the CrayEnv stack and do this in a way that they interact as little as possible with other software.</p> <p>It is not possible to load packages from different cpe toolchains at the same time. This is an EasyBuild restriction, because mixing libraries compiled with different compilers does not always work. This could happen, e.g., if a package compiled with the Cray Compiling Environment and one compiled with the GNU compiler collection would both use a particular  library, as these would have the same name and hence the last loaded one would be used by both executables (LUMI doesn't use rpath or runpath linking in EasyBuild for those familiar with that technique).</p> <p>However, as LUMI does not use hierarchy in the Lmod implementation of the software stack at the toolchain level, the module system will not protect you from these mistakes.  When the LUST set up the software stack, most people in the support team considered it too misleading and difficult to ask users to first select the toolchain they want to use and then see the  software for that toolchain.</p> <p>It is however possible to combine packages compiled with one CPE-based toolchain with packages compiled with the system toolchain, but you should avoid mixing those when linking as that may cause problems. The reason that it works when running software is because static linking is used as much as possible in the SYSTEM toolchain so that these packages are as independent as possible.</p> <p>And with some tricks it might also be possible to combine packages from the LUMI software stack with packages compiled with Spack, but one should make sure that no Spack packages are available when building as mixing libraries could cause problems. Spack uses rpath linking which is why this may work.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#easyconfig-names-and-module-names","title":"EasyConfig names and module names","text":"<p>There is a convention for the naming of an EasyConfig as shown on the slide. This is not mandatory, but EasyBuild will fail to automatically locate easyconfigs for dependencies  of a package that are not yet installed if the easyconfigs don't follow the naming convention. Each part of the name also corresponds to a parameter in the easyconfig  file.</p> <p>Consider, e.g., the easyconfig file <code>GROMACS-2024.3-cpeGNU-24.03-PLUMED-2.9.3-noPython-CPU.eb</code>.</p> <ol> <li>The first part of the name, <code>GROMACS</code>, is the name of the package, specified by the     <code>name</code> parameter in the easyconfig, and is after installation also the name of the     module.</li> <li>The second part, <code>2024.3</code>, is the version of GROMACS and specified by the     <code>version</code> parameter in the easyconfig.</li> <li> <p>The next part, <code>cpeGNU-24.03</code> is the name and version of the toolchain,     specified by the <code>toolchain</code> parameter in the easyconfig. The version of the     toolchain must always correspond to the version of the LUMI stack. So this is     an easyconfig for installation in <code>LUMI/24.03</code>.</p> <p>This part is not present for the SYSTEM toolchain</p> </li> <li> <p>The final part, <code>-PLUMED-2.9.3-noPython-CPU</code>, is the version suffix and used to provide     additional information and distinguish different builds with different options     of the same package. It is specified in the <code>versionsuffix</code> parameter of the     easyconfig.</p> <p>This part is optional.</p> </li> </ol> <p>The version, toolchain + toolchain version and versionsuffix together also combine to the version of the module that will be generated during the installation process. Hence this easyconfig file will generate the module  <code>GROMACS/2024.3-cpeGNU-24.03-PLUMED-2.9.3-noPython-CPU</code>.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#installing","title":"Installing","text":""},{"location":"intro-evolving/05-SoftwareStacks/#step-1-where-to-install","title":"Step 1: Where to install","text":"<p>Let's now discuss how you can extend the central LUMI software stack with packages that you need for your project.</p> <p>The default location for the EasyBuild user modules and software is in <code>$HOME/EasyBuild</code>. This is not the ideal place though as then the software is not available for other users in your project, and as the size of your home directory is also limited and cannot be expanded. The home file system on LUMI  is simply not meant to install software. However, as LUMI users can have multiple projects there is no easy way to figure out automatically where else to install software.</p> <p>The best place to install software is in your project directory so that it also becomes available for the whole project. After all, a project is meant to be a collaboration between all participants of the project to solve a scientific problem.  You'll need to point LUMI to the right location though and that has to be done by setting the environment variable <code>EBU_USER_PREFIX</code> to point to the location where you want to have your custom installation. Also don't forget to export that variable as otherwise the module system and EasyBuild will not find it when they need it. So a good choice would be  something like  <code>export EBU_USER_PREFIX=/project/project_465000000/EasyBuild</code>.  You have to do this before loading the <code>LUMI</code> module as it is then already used to ensure that user modules are included in the module search path. You can do this in your <code>.profile</code> or <code>.bashrc</code>.  This variable is not only used by EasyBuild-user to know where to install software, but also  by the <code>LUMI</code> - or actually the <code>partition</code> - module to find software so all users in your project who want to use the software should set that variable.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#step-2-configure-the-environment","title":"Step 2: Configure the environment","text":"<p>The next step is to configure your environment. First load the proper version of the LUMI stack for which you want to install software, and you may want to change to the proper partition also if you are cross-compiling.</p> <p>Once you have selected the software stack and partition, all you need to do to  activate EasyBuild to install additional software is to load the <code>LUMI</code> module, load a partition module if you want a different one from the default, and  then load the <code>EasyBuild-user</code> module. In fact, if you switch to a different <code>partition</code>  or <code>LUMI</code> module after loading <code>EasyBuild-user</code> EasyBuild will still be correctly reconfigured  for the new stack and new partition. </p> <p>Cross-compilation which is installing software for a different partition than the one you're working on does not always work since there is so much software around with installation scripts that don't follow good practices, but when it works it is easy to do on LUMI by simply loading a different partition module than the one that is auto-loaded by the <code>LUMI</code> module. It works  correctly for a lot of CPU-only software, but fails more frequently for GPU software as the installation scripts will try to run scripts that detect which GPU is present, or try to run tests on the GPU, even if you tell which GPU type to use, which does not work on the login nodes.</p> <p>Note that the <code>EasyBuild-user</code> module is only needed for the installation process. For using the software that is installed that way it is sufficient to ensure that <code>EBU_USER_PREFIX</code> has the proper value before loading the <code>LUMI</code> module.</p> <p></p>"},{"location":"intro-evolving/05-SoftwareStacks/#step-3-install-the-software","title":"Step 3: Install the software.","text":"<p>Let's look at GROMACS as an example. I will not try to do this completely live though as the  installation takes 15 or 20 minutes.</p> <p>First we need to figure out for which versions of GROMACS there is already support on LUMI. An easy way to do that is to simply check the LUMI Software Library. This web site lists all software that we manage via EasyBuild and make available either pre-installed on the system or as an EasyBuild recipe for user installation. Alternatively one can use <code>eb -S</code> or <code>eb --search</code> for that. So in our example this is <pre><code>eb --search GROMACS\n</code></pre></p> <p>Results of the searches:</p> <p>In the LUMI Software Library, after some scrolling through  the page for GROMACS,  the list of EasyBuild recipes is found in the  \"User-installable modules (and EasyConfigs)\" section:</p> <p> </p> <p><code>eb --search GROMACS</code> produces:</p> <p> </p> <p>while <code>eb -S GROMACS</code> produces:</p> <p> </p> <p>The information provided by both variants of the search command is the same, but <code>-S</code> presents the information in a more compact form.</p> <p>Now let's take the variant <code>GROMACS-2024.3-cpeGNU-24.03-PLUMED-2.9.3-noPython-CPU.eb</code>.  This is GROMACS 2024.3 with the PLUMED 2.9.3 plugin, built with the GNU compilers from <code>LUMI/24.03</code>, and a build meant for CPU-only systems. The <code>-CPU</code> extension is not always added for CPU-only system, but in case of GROMACS there already is a GPU version for AMD GPUs in active development so even before LUMI-G was active we chose to ensure that we could distinguish between GPU and CPU-only versions. To install it, we first run </p> <pre><code>eb GROMACS-2024.3-cpeGNU-24.03-PLUMED-2.9.3-noPython-CPU.eb \u2013D\n</code></pre> <p>The <code>-D</code> flag tells EasyBuild to just perform a check for the dependencies that are needed when installing this package.</p> <p>The output of this command looks like:</p> <p> </p> <p> </p> <p>Most lines start with <code>[x]</code> which means that the dependency is already installed. At the end of the list, we notice that  non only the GROMACS module is missing, but the <code>eb</code> command also failed to find a module for <code>PLUMED</code>, a dependency of this GROMACS configuration. So that module needs to be installed first, but EasyBuild can take care of that for us...</p> <p>To install GROMACS and also automatically install missing dependencies (only PLUMED in this case), we run</p> <pre><code>eb GROMACS-2024.3-cpeGNU-24.03-PLUMED-2.9.3-noPython-CPU.eb -r\n</code></pre> <p>The <code>-r</code> argument tells EasyBuild to also look for dependencies in a preset search path and to install them. The installation of dependencies is not automatic since there are scenarios where this is not desired and it cannot be turned off as easily as it can be turned on.</p> <p>Running EasyBuild to install GROMACS and dependency</p> <p>The command</p> <pre><code>eb GROMACS-2024.3-cpeGNU-24.03-PLUMED-2.9.3-noPython-CPU.eb -r\n</code></pre> <p>results in:</p> <p> </p> <p>EasyBuild detects PLUMED is a dependency and because of the <code>-r</code> option, it first installs the required version of PLUMED.</p> <p> </p> <p>When the installation of PLUMED finishes, EasyBuild starts the installation of GROMACS. It mentions something we haven't seen when installing PLUMED:</p> <pre><code>== starting iteration #0\n</code></pre> <p>GROMACS can be installed in many configurations, and they generate executables with different names. Our EasyConfig combines 4 popular installations in one: Single and double precision and with and without MPI, so it will do 4 iterations. As EasyBuild is developed by geeks, counting starts from 0.</p> <p> </p> <p> </p> <p> </p> <p> </p> <p>This takes too long to wait for, but once it finished the software should be available and you should be able to see the module in the output of <pre><code>module avail\n</code></pre></p>"},{"location":"intro-evolving/05-SoftwareStacks/#step-3-install-the-software-note","title":"Step 3: Install the software - Note","text":"<p>Installing software this way is 99% equivalent to an installation in the central software tree. The application is compiled in exactly the same way as we would do and served from Lustre file systems in both cases. The one difference is that the central software stack is on all 4 hard disk based Lustre  filesystem for availability reasons if one of the file systems is taken down for maintanence  (and a bit for performance reasons as executable and shared library loading for a big multi-node job will likely be spread across 4 filesystems), but on the other hand, as long as the retention policy is not active, you could even use <code>/flash</code> for a software installation if you have enough storage billing units and get better startup performance for some packages, or even better runtime performance for those packages that keep opening files in the software installation. Furthermore, it helps keeping the output of <code>module avail</code> reasonably short and focused on your projects, and it puts you in control of installing updates. For instance, we may find out that something in a module does not work for some users and that it needs to be re-installed.  Do this in the central stack and either you have to chose a different name or risk breaking running jobs as the software would become unavailable during the re-installation and also jobs may get confused if they all of a sudden find different binaries. However, have this in your own stack extension and you can update whenever it suits your project best or even not update at all if  you figure out that the problem we discovered has no influence on your work.</p> <p>Lmod does keep a user cache of modules. EasyBuild will try to erase that cache after a software installation to ensure that the newly installed module(s) show up immediately. We have seen some very rare cases where clearing the cache did not help likely because some internal data structures in Lmod where corrupt. The easiest way to solve this is to simply log out and log in again and rebuild your environment.</p> <p>In case you see strange behaviour using modules you can also try to manually remove the Lmod user cache which is in <code>$HOME/.cache/lmod</code>. You can do this with  <pre><code>rm -rf $HOME/.cache/lmod\n</code></pre> (With older versions of Lmod the cache directory is <code>$HOME/.lmod.d/cache</code>.)</p>"},{"location":"intro-evolving/05-SoftwareStacks/#more-advanced-work","title":"More advanced work","text":"<p>You can also install some EasyBuild recipes that you got from support. For this it is best to create a subdirectory where you put those files, then go into that directory and run  something like <pre><code>eb my_recipe.eb -r . \n</code></pre> The dot after the <code>-r</code> is very important here as it does tell EasyBuild to also look for  dependencies in the current directory, the directory where you have put the recipes you got from support, but also in its subdirectories so for speed reasons you should not do this just in your home directory but in a subdirectory that only contains those files.</p> <p>In some cases you will have to download sources by hand as packages don't allow to download  software unless you sign in to their web site first. This is the case for a lot of licensed software, for instance, for VASP. LUMI would likely be in violation of the license if it would  offer the download somewhere where EasyBuild can find it, and it is also a way to ensure that you have a license for VASP. For instance,  <pre><code>eb --search VASP\n</code></pre> will tell you for which versions of VASP LUMI provides EasyBuild recipes, but you will still have to download the source file that the EasyBuild recipe expects.  Put it somewhere in a directory, and then from that directory run EasyBuild, for instance for VASP 6.5.0 with the GNU compilers: <pre><code>eb VASP-6.5.0-cpeGNU-24.03-build02.eb \u2013r . \n</code></pre></p>"},{"location":"intro-evolving/05-SoftwareStacks/#more-advanced-work-2-repositories","title":"More advanced work (2): Repositories","text":"<p>It is also possible to have your own clone of the <code>LUMI-EasyBuild-contrib</code> GitHub repository in your <code>$EBU_USER_PREFIX</code> subdirectory if you want the latest and greatest before it is in the centrally maintained clone of the repository. All you need to do is <pre><code>cd $EBU_USER_PREFIX\ngit clone https://github.com/Lumi-supercomputer/LUMI-EasyBuild-contrib.git\n</code></pre> and then of course keep the repository up to date.</p> <p>And it is even possible to maintain your own GitHub repository. The only restrictions are that it should also be in <code>$EBU_USER_PREFIX</code> and that the subdirectory should be called <code>UserRepo</code>, but that doesn't stop you from using a different name for the repository on GitHub. After cloning your GitHub version you can always change the name of the directory. The structure should also be compatible with the structure that EasyBuild uses, so easyconfig files go in <code>$EBU_USER_PREFIX/UserRepo/easybuild/easyconfigs</code>.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#more-advanced-work-3-reproducibility","title":"More advanced work (3): Reproducibility","text":"<p>EasyBuild also takes care of a high level of reproducibility of installations.</p> <p>It will keep a copy of all the downloaded sources in the <code>$EBU_USER_PREFIX/sources</code> subdirectory (unless the sources are already available elswhere where EasyBuild can find them, e.g., in the system EasyBuild sources directory),  and use that source file again rather than downloading it again. Of course in some cases those \"sources\" could be downloaded tar files with binaries instead as EasyBuild can install downloaded binaries or relocatable RPMs. And if you know the structure of those directories, this is also a place where you could manually put the downloaded installation files for licensed software.</p> <p>Moreover, EasyBuild also keeps copies of all installed easyconfig files in two locations.</p> <ol> <li>There is a copy in <code>$EBU_USER_PREFIX/ebfiles_repo</code>. And in fact, EasyBuild will use this version     first if you try to re-install and did not delete this version first. This is a policy     we set on LUMI which has both its advantages and disadvantages. The advantage is that it ensures     that the information that EasyBuild has about the installed application is compatible with what is     in the module files. But the disadvantage of course is that if you install an EasyConfig file     without being in the subdirectory that contains that file, it is easily overlooked that it     is installing based on the EasyConfig in the <code>ebfiles_repo</code> subdirectory and not based on the     version of the recipe that you likely changed and is in your user repository or one of the      other repositories that EasyBuild uses.</li> <li>The second copy is with the installed software in <code>$EBU_USER_PREFIX/SW</code> in a subdirectory     called <code>easybuild</code>. This subdirectory is meant to have all information about how EasyBuild     installed the application, also some other files that play a role in the installation process, and hence     to help in reproducing an installation or checking what's in an existing installation. It is     also the directory where you will find the extensive log file with all commands executed during     the installation and their output.</li> </ol>"},{"location":"intro-evolving/05-SoftwareStacks/#easybuild-tips-and-tricks","title":"EasyBuild tips and tricks","text":"<p>Updating the version of a package often requires only trivial changes in the easyconfig file. However, we do tend to use checksums for the sources so that we can detect if the available sources have changed. This may point to files being tampered with, or other changes that might need us to be a bit more careful when installing software and check a bit more again.  Should the checksum sit in the way, you can always disable it by using  <code>--ignore-checksums</code> with the <code>eb</code> command.</p> <p>Updating an existing recipe to a new toolchain might be a bit more involving as you also have to make build recipes for all dependencies. When a toolchain is updated on the system,  the versions of all installed libraries are often also bumped to one of the latest versions to have most bug fixes and security patches in the software stack, so you need to check for those versions also to avoid installing yet another unneeded version of a library.</p> <p>LUMI provides documentation on the available software that is either pre-installed or can be user-installed with EasyBuild in the  LUMI Software Library. For most packages this documentation does also contain information about the license. The user documentation for some packages gives more information about how to use the package on LUMI, or sometimes also about things that do not work. The documentation also shows all EasyBuild recipes, and for many packages there is  also some technical documentation that is more geared towards users who want to build or modify recipes. It sometimes also tells why things are done in a particular way.</p>"},{"location":"intro-evolving/05-SoftwareStacks/#easybuild-training-for-advanced-users-and-developers","title":"EasyBuild training for advanced users and developers","text":"<p>Pointers to all information about EasyBuild can be found on the EasyBuild web site  easybuild.io. This page also includes links to training materials, both written and as recordings on YouTube, and the EasyBuild documentation.</p> <p>Generic EasyBuild training materials are available on  tutorial.easybuild.io. The site also contains a LUST-specific tutorial oriented towards Cray systems.</p> <p>There is also a later course developed by LUST for developers of EasyConfigs for LUMI that can be found on  lumi-supercomputer.github.io/easybuild-tutorial. LUMI is also considering organising an EasyBuild training for users sometime in 2026.</p>"},{"location":"intro-evolving/06-Support/","title":"Support","text":"<p>Last update of this page: May 12, 2025</p>"},{"location":"intro-evolving/06-Support/#lumi-support-and-documentation","title":"LUMI Support and Documentation","text":""},{"location":"intro-evolving/06-Support/#distributed-nature-of-lumi-support","title":"Distributed nature of LUMI support","text":"<p>User support for LUMI comes from several parties. Unfortunately, as every participating consortium countries has some responsibilities also and solves things differently, there is no central point where you can go with all questions.</p> <p>Resource allocators work independently from each other and the central LUMI User Support Team. This also  implies that they are the only ones who can help you with questions regarding your allocation: How to apply for compute time on LUMI, add users to your project, running out of resources (billing units) for your project, failure to even get access to the portal managing the allocations given by your resource allocator (e.g., because you let expire an invite), ...  For projects allocated in the Belgian allocation, help can be requested via the email address lumi-be-support@enccb.be. However, we cannot help you with similar problems for compute time directly  obtained via EuroHPC. For granted EuroHPC projects, support is available via lumi-customer-accounts@csc.fi, but you will have to contact EuroHPC directly at access@eurohpc-ju.europa.eu if, e.g., you need more resources or an  extension to your project.</p> <p>The central LUMI User Support Team (LUST) offers L1 and basic L2 support.  Given that the LUST team is very small compared to the number of project granted annually on LUMI  (roughly 10 FTE for on the order of 700 projects per year, and support is not their only task), it is clear that the amount of support they can give is limited.  E.g., don't expect them to install all software you request for them. There is simply too much software and too much software with badly written install code to do that with that number of people. Nor should you expect domain expertise from them. Though several members of the LUST have been scientist before, it does not mean that they can understand all scientific problems thrown at them or all codes used by users. Also, the team cannot fix bugs for you in the codes that you use, and usually not in the system code either. For fixing bugs in HPE or AMD-provided software, they are backed by a team of experts from those companies. However, fixing bugs in compilers or libraries  and implementing those changes on the system takes time. The system software on a big shared machine cannot be upgraded as easily as on a personal workstation. Usually you will have to look for workarounds, or if they show up in a preparatory project, postpone applying for an allocation until all problems are fixed.</p> <p>In Flanders, the VSC has a Tier-0 support project to offer more advanced L2 and some L3 support. The project unfortunately is not yet fully staffed. VSC Tier-0 support can be contacted via the LUMI-BE help desk at lumi-be-support@enccb.be (the same help desk that you need to contact for allocation problems).</p> <p>In the Walloon region, there is some limited advanced support via Orian Louant from C\u00c9CI. However, this is only a part of all his tasks. Here also the lumi-be-support@enccb.be mail address can be used.</p> <p>EuroHPC has also started various other support initiatives:</p> <ul> <li> <p>The EPICURE project started in February 2024 to set up a network for     advanced L2 and L3 support across EuroHPC centres.      Belgium also participates in that project as a partner in the LUMI consortium.     This project is also so small that it has to select the problems they tackle.     Moreover, access is only for EuroHPC extreme scale, regular or development projects,     though development projects are relatively easy to get without too much administration.     Yet, this only makes sense for a sufficiently large project with a long enough duration.     So far, for many systems, there have been enough resources available for the support requests     received.</p> </li> <li> <p>MINERVA is a more recent project set up to support AI users on HPC systems specifically.     At the time of the course, the project was still starting up and not all services were available     already.</p> </li> <li> <p>The AI Factories are an initiative     that is more oriented towards the use of AI in industry, and especially for generative AI models.</p> </li> <li> <p>In principle the EuroHPC Centres of Excellence should also play a role in porting some applications in their     field of expertise and offer some support and training, but so far especially the support and training are     not yet what one would like to have.</p> </li> </ul> <p>Basically given the growing complexity of scientific computing and diversity in the software field, what one needs is the equivalent of the \"lab technician\" that many experimental groups have who can then work with  various support instances, a so-called Research Software Engineer...</p>"},{"location":"intro-evolving/06-Support/#support-level-0-help-yourself","title":"Support level 0: Help yourself!","text":"<p>Support starts with taking responsibility yourself and using the available sources of information before contacting support. Support is not meant to be a search assistant for already available  information.</p> <p>The LUMI User Support Team has prepared trainings and a lot of documentation about LUMI. Good software packages also come with documentation, and usually it is possible to find trainings for  major packages. And a support team is also not there to solve communication problems in the  team in which you collaborate on a project!</p>"},{"location":"intro-evolving/06-Support/#take-a-training","title":"Take a training!","text":"<p>There exist system-specific and application-specific trainings.  Ideally of course a user would want a one-step solution, having a training for a specific application on a specific system (and preferably with the workflow tools they will be using, if any), but that is simply not possible. The group that would be interested in such a training is for most packages too small, and it is nearly impossible to find suitable teachers for such course given the amount of expertise that is needed in both the specific application and the specific system. It would also be hard to repeat such a training with a high enough frequency to deal with the continuous inflow of new users.</p> <p>The LUMI User Support Team organises 2 system-specific trainings:</p> <ol> <li> <p>There is a 2-day introductory course entirely given by members of the LUST.     The training does assume familiarity with HPC systems, e.g., obtained from the introductory     courses taught by VSC and     C\u00c9CI.</p> <p>The course is basically equivalent to this training.</p> </li> <li> <p>And there is a 4-day advanced training or 5-day training that includes the introductory one     with more attention on how to run efficiently, and on the     development and profiling tools. Even if you are not a developer, you may benefit from more knowledge     about these tools as especially a profiler can give you insight in why your application does not run     as expected.</p> </li> </ol> <p>This particular training is similar to the 2-day LUMI training offered by the LUST (in fact, the LUST training borrowed a lot of materials from this one), but has been enriched with  links to the situation specifically in Belgium.</p> <p>Application-specific trainings should come from other instances though that have the necessary domain knowledge: Groups that develop the applications, user groups, the EuroHPC Centres of Excellence, ...</p> <p>What users really want is of course a training for a specific application on a specific system, but as most applications have a too small usergroup and within that usergroup often still a large variety of workflows, and teaching such a course also requires an instructor with a lot of domain knowledge in the science field of the application. EuroHPC has set up some support initiatives specifically for AI, including the AI factories, more oriented towards industry startups, and the MINERVA project.</p> <p>Currently the training landscape in Europe is not too well organised. EuroHPC is starting some new training initiatives to succeed the excellent PRACE trainings. Moreover, CASTIEL, the centre coordinating the National Competence Centres also  tries to maintain an overview of available trainings (and several National Competence Centres organise trainings open to others also).</p>"},{"location":"intro-evolving/06-Support/#readsearch-the-documentation","title":"Read/search the documentation","text":"<p>The LUST has developed extensive documentation for LUMI. That documentation is split in two parts:</p> <ol> <li> <p>The main documentation at docs.lumi-supercomputer.eu     covers the LUMI system itself and includes topics such as how to get on the      system, where to place your files, how to start jobs, how to use the programming environment,     how to install software, etc.</p> <p> </p> </li> <li> <p>The LUMI Software Library contains     an overview of software pre-installed on LUMI or for which we have install recipes to start from.     For some software packages, it also contains additional information on how to use the software     on LUMI.</p> <p> </p> <p>That part of the documentation is generated automatically from information in the various repositories that are used to manage those installation recipes. It is kept deliberately separate, partly to have a more focused search in both documentation systems and partly because it is managed and updated very differently.</p> </li> <li> <p>The LUMI AI Guide is a GitHub repository     with code samples and further documentation on using AI packages (mainly PyTorch and packages layered     on top of it) on LUMI.</p> </li> </ol> <p>The first two documentation systems contain a search box which may help you find pages if you cannot find them  easily navigating the documentation structure. E.g., you may use the search box in the LUMI Software Library to search for a specific package as it may be bundled with other packages in a single module with a  different name. </p> <p>Some examples:</p> <ol> <li> <p>Search in the main documentation at docs.lumi-supercomputer.eu      for \"quota\" and it will take you to pages that among other things     explain how much quota you have in what partition.</p> <p> </p> </li> <li> <p>Users of the Finnish national systems have been told to use a tool called \"Tykky\"     to pack conda and Python installations to reduce the stress on the filesystems and     wonder if that tool is also on LUMI. So let's search in the     LUMI Software Library:</p> <p> </p> <p>It is, but with a different name as foreigners can't pronounce those Finnish names anyway and as something more descriptive was needed.</p> </li> <li> <p>Try searching for the <code>htop</code> command in the      LUMI Software Library</p> <p> </p> <p>So yes, <code>htop</code> is on LUMI, but if you read the page you'll see it is in a module together with some other small tools.</p> </li> </ol>"},{"location":"intro-evolving/06-Support/#talk-to-your-colleagues","title":"Talk to your colleagues","text":"<p>A LUMI project is meant to correspond to a coherent research project in which usually multiple people collaborate. </p> <p>This implies that your colleagues may have run in the same problem and may already have a solution, or they didn't even experience it as a problem and know how to do it. So talk to your colleagues first.</p> <p>Support teams are not meant to deal with your team communication problems. There is nothing worse than having the same question asked multiple times from different people in the same project. As a project does not have a dedicated support engineer, the second time a question is asked it may land at a different person in the support team so that it is not recognized that the question has been asked already and the answer is readily available, resulting in a loss of time for the support team and other, maybe more important questions, remaining unanswered.  Similarly bad is contacting multiple help desks with the same question without telling them, as that will also duplicate efforts to solve a question. We've seen it often that users contact both a local help desk and the LUST help desk without telling.</p> <p>Resources on LUMI are managed on a project basis, not on a user-in-project basis, so if you want to know what other users in the same project are doing with the resources, you have to talk to them and not to the LUST. We do not have systems in place to monitor use on a per-user, per-project basis, only on a per-project basis, and also have no plans to develop such tools as a project is meant to be a close collaboration of all involved users. </p> <p>LUMI events and on-site courses are also an excellent opportunity to network with more remote colleagues and learn from them! Which is why we favour on-site participation for courses.  No video conferencing system can give you the same experience as being physically present at a course or event.</p>"},{"location":"intro-evolving/06-Support/#l1-and-basic-l2-support-lust","title":"L1 and basic L2 support: LUST","text":"<p>The LUMI User Support Team is responsible for providing L1 and basic L2 support to users of the system. Their work starts from the moment that you have userid on LUMI (the local Resource Allocator is responsible for ensuring that you get a userid when a project has been assigned).</p> <p>The LUST is a distributed team roughly 10 FTE strong, with people in all LUMI consortium countries, but they work as a team, coordinated by CSC. The Belgian contribution currently consists of two people both working half time for LUMI and half time for user support in their own organisation (VSC and C\u00c9CI). However, you will not necessarily be helped by one of the Belgian team members when you contact LUST, but by the team member who is most familiar with your problem. </p> <p>There are some problems that we need to pass on to HPE or AMD, particularly if it may be caused by  bugs in system software, but also because they have more experts with in-depth knowledge of very specific tools. </p> <p>The LUMI help desk is staffed from Monday till Friday between 8am and 6pm Brussels time (except on public holidays in Finland). You can expect a same day first response if your support query is well formulated and submitted long enough before closing time, but a full solution of your problem may of course take longer, depending on how busy the help desk is and the complexity of the problem.</p> <p>Data security on LUMI is very important. Some LUMI projects may host confidential data, and especially industrial LUMI users may have big concerns about who can access their data.  Therefore only very, very few people on LUMI have the necessary rights to access user data on the system, and those people even went through a background check. The LUST members do not have that level of access, so we cannot see your data and you will have to pass all relevant information to the LUST through other means!</p> <p>The LUST help desk should be contacted through  web forms in the \"User Support - Contact Us\" section of the main LUMI web site. The page is also linked from the \"Help Desk\" page in the LUMI documentation. These forms help you to provide more information that we need to deal with your support request. Please do not email directly to the support web address (that you will know as soon as we answer at ticket as that is done through e-mail). Also, separate issues should go in separate tickets so that separate people in the LUST can deal with them, and you should not reopen an old ticket for a new question, also because then only the person who dealt with the previous ticket gets notified, and they may be on vacation or even not work for LUMI anymore, so your new request may remain unnoticed for a long time.</p>"},{"location":"intro-evolving/06-Support/#tips-for-writing-good-tickets-that-we-can-answer-promptly","title":"Tips for writing good tickets that we can answer promptly","text":""},{"location":"intro-evolving/06-Support/#how-not-to-write-a-ticket","title":"How not to write a ticket","text":"<ul> <li> <p>Use a meaningful subject line. All we see in the ticket overview is a number and the subject     line, so we need to find back a ticket we're working on based on that information alone.</p> <p>Yes, we have a user on LUMI who managed to send 8 tickets in a short time with the subject line \"Geoscience\" but 8 rather different problems...</p> <p>Hints: </p> <ul> <li>For common problems, including your name in the subject may be a good idea.</li> <li>For software problems, including the name of the package helps a lot. So not     \"Missing software\" but \"Need help installing QuTiP 4.3.1 on CPU\".     Or not \"Program crashes\" but \"UppASD returns an MPI error when using more than 1000 ranks\".</li> </ul> </li> <li> <p>Be accurate when describing the problem. Support staff members are not clairvoyants with     mysterious superpowers who can read your mind across the internet. </p> <p>We'll discuss this a bit more further in this lecture.</p> </li> <li> <p>If you have no time to work with us on the problem yourself, then tell so.</p> <p>Note: The priorities added to the ticket are currently rather confusing. You have three choices in the forms: \"It affects severely my work\", \"It is annoying, but I can work\", and  \"I can continue to work normally\", which map to \"high\", \"medium\" and \"low\".  So tickets are very easily marked as high priority because you cannot work on LUMI, even though you have so much other work to do that it is really not that urgent or that you  don't even have time to answer quickly.</p> </li> </ul> <p>The improved version could be something like this:</p> <p></p> <ul> <li> <p>The subject line makes it stand out as the person doing login tickets can quickly find back     the ticket.</p> </li> <li> <p>There is already a lot of useful information in the ticket:</p> <ul> <li> <p>When did the user first notice the problem, and has it worked before (and when)?</p> </li> <li> <p>The user clearly tried to check if anything has changed on their side, and in this case at least     the configuration files of the ssh client have remained unchanged.</p> </li> <li> <p>We know what client the user is using and luckily it is a standard one so that we know ourselves     how to use it (apart from local permission problems, but since it still works on the local cluster,     permissions on files in the local .ssh directory cannot be the issue here). Hence we can tell the user     how to gather more information. (Unfortunately, some users use exotic ssh clients that we cannot even     have access to without taking licenses, but expect us to be able to support them...)</p> </li> <li> <p>It is also useful to know that it is not a one-time hiccup.</p> </li> </ul> </li> </ul>"},{"location":"intro-evolving/06-Support/#how-to-write-tickets","title":"How to write tickets","text":""},{"location":"intro-evolving/06-Support/#1-ticket-1-issue-1-ticket","title":"1 ticket = 1 issue = 1 ticket","text":"<ul> <li> <p>If you have multiple unrelated issues, submit them as multiple tickets. In a support team, each member     has their own specialisation so different issues may end up with different people. Tickets need to be     assigned to people who will deal with the problem, and it becomes very inefficient if multiple people      have to work on different parts of the ticket simultaneously.</p> <p>Moreover, the communication in a ticket will also become very confusing if multiple issues are discussed simultaneously.</p> </li> <li> <p>Conversely, don't submit multiple tickets for a single issue just because you are too lazy to     look for the previous e-mail if you haven't been able to do your part of the work for some days.     If you've really lost the email, at least tell us that it is related to a previous ticket so that     we can try to find it back.</p> <p>So keep the emails you get from the help desk to reply!</p> </li> <li> <p>Avoid reusing exactly the same subject line. Surely there must be something different for the new     problem?</p> </li> <li> <p>Avoid reopening old tickets that have been closed long ago.</p> <p>But if you run into an issue that you think is the same issue again as in an old ticket, it can help us if you point that out and even more if you can say which ticket it was.</p> </li> <li> <p>Certainly do not reopen old tickets with new issues. Apart from the fact that the person who did     the ticket before may not be around, they may also have no time to deal with the ticket quickly     or may not even be the right person to deal with it.</p> </li> </ul>"},{"location":"intro-evolving/06-Support/#the-subject-line-is-important","title":"The subject line is important!","text":"<ul> <li> <p>The support team has two identifiers in your ticket: Your mail address and the subject that you      specified in the form (LUST help desk) or email (LUMI-BE help desk). So:</p> <ul> <li> <p>Use consistently the same mail address for tickets. This helps us locate previous requests from     you and hence can give us more background about what you are trying to do. </p> <p>The help desk is a professional service, and you use LUMI for professional work, so use your company or university mail address and not some private one.</p> </li> <li> <p>Make sure your subject line is already descriptive and likely unique in our system.</p> <p>We use the subject line to distinguish between tickets we're dealing with so make sure that it can easily be distinguished from others and is easy to find back.</p> </li> </ul> </li> <li> <p>So include relevant keywords in your subject, e.g.,</p> <ul> <li> <p>The userid you were using and the way of logging in to the system for login problems.</p> </li> <li> <p>Name of software packages for software installations or crashes.</p> </li> </ul> </li> </ul> <p>Some proper examples are</p> <ul> <li> <p>User abcdefgh cannot log in via web interface</p> <p>So we know we may have to pass this to our Open OnDemand experts, and your userid makes the message likely unique. Moreover, after looking into account databases etc., we can immediately find back the ticket as the userid is in the subject.</p> </li> <li> <p>ICON installation needs libxml2</p> </li> <li> <p>VASP produces MPI error message when using more than 1024 ranks</p> </li> </ul>"},{"location":"intro-evolving/06-Support/#think-with-us","title":"Think with us","text":"<ul> <li> <p>Provide enough information for us to understand who you are:</p> <ul> <li> <p>Name: and the name as we would see it on the system, not some nickname.</p> </li> <li> <p>Userid: Important especially for login problems.</p> </li> <li> <p>Project number:</p> <ul> <li> <p>When talking to the LUST: they don't know EuroHPC or LUMI-BE project numbers, only     the 465xxxxxx numbers, and that is what they need.</p> </li> <li> <p>When talking to LUMI-BE: for LUMI-BE projects, the LUMI-BE project number may be useful.     It is the ticket number of you project submission. Not all LUMI-BE help team members have     access to the data associated with your LUMI project number 465xxxxxx, but some have and can      use this to diagnose some problems.</p> </li> </ul> </li> </ul> </li> <li> <p>For login and data transfer problems, your client environment is often also important to diagnose     the problem. </p> <ul> <li> <p>This does include your geographical location. Doing things from work may have different issues     then doing things from your home connection, and being abroad may be part of the problem!</p> <ul> <li>Extreme example: Iran blocks most encrypted internet traffic going abroad.     This is an extreme example though as you are not allowed to use LUMI while being in Iran as     it breaks the US export restrictions and hence the LUMI conditions of use.</li> </ul> </li> </ul> </li> <li> <p>What software are you using, and how was it installed or where did you find it?</p> <p>We know that certain installation procedures (e.g., simply downloading a binary) may cause certain problems on LUMI. Also, there are some software installations on LUMI for which neither LUST nor the LUMI-BE help desk is responsible, so we need to direct to to their support instances when problems occur that are likely related to that software.</p> </li> <li> <p>Describe your environment (though experience learns that some errors are caused by users     not even remembering they've changed things while those changes can cause problems)</p> <ul> <li> <p>Which modules are you using?</p> </li> <li> <p>Do you have special stuff in <code>.bashrc</code> or <code>.profile</code>? </p> </li> <li> <p>For problems with running jobs, the batch script that you are using can be very useful.</p> </li> </ul> </li> <li> <p>Describe what worked so far, and if it ever worked: when? E.g., was this before a system update?</p> <p>The LUST has had tickets were a user told that something worked before but as we questioned further it was long ago before a system update that we know broke some things that affects some programs...</p> </li> <li> <p>What did you change since then? Think carefully about that. When something worked some time ago but     doesn't work know the cause is very often something you changed as a user and not something going on     on the system.</p> </li> <li> <p>What did you already try to solve the problem?</p> </li> <li> <p>How can we reproduce the problem? A simple and quick reproducer speeds up the time to answer your ticket.     Conversely, if it takes a 24-hour run on 256 nodes to see the problem it is very, very likely that the      support team cannot help you.</p> <p>Moreover, if you are using licensed software with a license that does not cover the support team members, usually we cannot do much for you. Neither the LUMI-BE help desk team nor the LUST will knowingly violate software licenses only to solve your problems!</p> </li> <li> <p>The LUST and LUMI-BE help desk members know a lot about LUMI but they are (usually) not researchers in     your field so cannot help you with problems that require domain knowledge in your field. We can impossibly     know all software packages and tell you how to use them (and, e.g., correct errors in your input files).</p> <p>You as a user should be the domain expert, and since you are doing computational science, somewhat  multidisciplinary and know something about both the \"computational\" and the \"science\".</p> <p>We as the support team should be the expert in the \"computational\". Some of us where researchers in the past so have some domain knowledge about a the specific subfield we were working in, but there are simply too many scientific domains and subdomains to have full coverage of that in a central support team for a generic infrastructure.</p> <p>We do see that lots of crashes and performance problems with software are in fact caused by wrong use of the package!</p> <p>However, some users expect that we understand the science they are doing, find the errors in their model and run that on LUMI, preferably by the evening they submitted the ticket. If we could do that, then we could basically make a Ph.D that usually takes 4 years in 4 weeks and wouldn't need users anymore as it would be more fun to produce the science that our funding agencies expect ourselves.</p> </li> <li> <p>The LUST and LUMI-BE help desk members know a lot about LUMI but cannot know or solve everything and     may need to pass your problem to other instances, and in particular HPE or AMD.</p> <p>Debugging system software is not the task of the LUMI-BE help desk nor of the LUST.  Issues with compilers or libraries can only be solved by those instances that produce those compilers or libraries, and this takes time.</p> <p>We have a way of working that enables us to quickly let users test changes to software in the user software stack by making user installations relatively easy and reproducible using EasyBuild, but changing the  software installed in the system images - which includes the Cray programming environment - where changes  have an effect on how the system runs and can affect all users, are non-trivial and many of those changes can only be made during maintenance breaks.</p> </li> </ul>"},{"location":"intro-evolving/06-Support/#beware-of-the-xy-problem","title":"Beware of the XY-problem!","text":"<p>Partly quoting from xyproblem.info:  Users are often tempted to ask questions about the solution they have in mind and where they got stuck, while it may actually be the wrong solution to the actual problem. As a result one can waste a lot of time attempting to get the solution they have in mind to work, while at the end it turns out that that solution does not work. It goes as follows:</p> <ol> <li>The user wants to do X.</li> <li>The user doesn't really know how to do X. However, they think that doing Y first would     be a good step towards solving X.</li> <li>But the user doesn't really know how to do Y either and gets stuck there too.</li> <li>So the user contacts the help desk to help with solving problem Y.</li> <li>The help desk tries to help with solving Y, but is confused because Y seems a very strange and     unusual problem to solve.</li> <li>Once Y is solved with the help of the help desk, the user is still stuck and cannot solve X yet.</li> <li>User contacts the help desk again for further help and it turns out that Y wasn't needed in the      first place as it is not part of a suitable solution for X.</li> </ol> <p>Or as one of the colleagues of the author of these notes says: \"Often the help desk knows the solution, but doesn't know the problem so cannot give the solution.\"</p> <p>To prevent this, you as a user has to be complete in your description:</p> <ol> <li> <p>Give the broader problem and intent (so X), not just the small problem (Y) on which you got stuck.</p> </li> <li> <p>Promptly provide information when the help desk asks you, even if you think that information is     irrelevant. The help desk team member may have a very different look on the problem and come up     with a solution that you couldn't think of, and you may be too focused on the solution that you have     in mind to see a better solution.</p> </li> <li> <p>Being complete also means that if you ruled out some solutions, share with the help desk why you ruled     them out as it can help the help desk team member to understand what you really want.</p> </li> </ol> <p>After all, if your analysis of your problem was fully correct, you wouldn't need to ask for help, don't you?</p>"},{"location":"intro-evolving/06-Support/#what-support-can-we-offer","title":"What support can we offer?","text":""},{"location":"intro-evolving/06-Support/#restrictions","title":"Restrictions","text":"<p>Contrary to what you may be familiar with from your local Tier-2 system and support staff, team members of the LUMI help desks have no elevated privileges. This holds for both the LUST and LUMI-BE help desk.</p> <p>As a result,</p> <ul> <li> <p>We cannot access user files.     A specific person of the LUMI-BE help desk can access your project, scratch and flash folders     if you make them part of the project. This requires a few steps and therefore is only done     for a longer collaboration between a LUMI project and that help desk member.     The LUST members don't do that.</p> </li> <li> <p>Help desk team members cannot install or modify system packages or settings. </p> <p>A good sysadmin usually wouldn't do so either. You are working on a multi-user system and you  have to take into account that any change that is beneficial for you, may have adverse effects for other users or for the system as a whole.</p> <p>E.g., installing additional software in the images takes away from the available memory on each node, slows down the system boot slightly, and can conflict with software that is installed through other ways.</p> </li> <li> <p>The help desk cannot extend the walltime of jobs.</p> <p>Requests are never granted, even not if the extended wall time would still be within the limits of the partition. </p> </li> <li> <p>The LUST is in close contact with the sysadmins, but as the sysadmins are very busy people they will     not promptly deal with any problem. Any problem though endangering the stability of the system gets a     high priority.</p> </li> <li> <p>The help desk does not monitor running jobs. Sysadmins monitor the general health of the system, but will      not try to pick out inefficient jobs unless the job does something that has a very negative effect on     the system.</p> </li> </ul>"},{"location":"intro-evolving/06-Support/#what-support-can-and-cannot-do","title":"What support can and cannot do","text":"<ul> <li> <p>The LUST and LUMI-BE help desk do not replace a good introductory HPC course nor are they a      search engine for the documentation. L0 support is the responsibility of every user.</p> </li> <li> <p>Resource allocators are responsible for the first steps in getting a project and userid on     LUMI. For LUMI-BE project this support is offered through the LUMI-BE help desk at      lumi-be-support@enccb.be and for EuroHPC projects the support is offered through CSC,     the operator of LUMI, at lumi-customer-accounts@csc.fi, or by EuroHPC itself at     access@eurohpc-ju.europa.eu if you have not yet been granted a project by them.</p> <p>Once your project is created and accepted (and the resource allocator can confirm that you properly accepted the invitation), support for account problems (in particular login problems)  moves to the LUST.</p> </li> <li> <p>If you run out of block or file quota, the LUST can increase your quota within      the limits specified in the LUMI documentation.</p> <p>If you run out of billing units for compute or storage, only the instance that granted your project can help you, so contact the LUMI-BE help desk for LUMI-BE projects and access@eurohpc-ju.europa.eu  for EuroHPC projects (CSC EuroHPC support at lumi-customer-accounts@csc.fi cannot help you directly for project extensions and increase of billing units as they can only do so when EuroHPC instructs them to).</p> <p>Projects cannot be extended past one year unless the granting instance is willing to take a  charge on the annual budget for the remaining billing units.</p> <p>As a rule, for LUMI-BE projects we will not increase the CPU or GPU billing units. However, as  for now we have plenty of storage billing units compared to demand and as we understand that it  takes time for the user community to get used to working with storage billing units, we're  rather flexible in increasing those.</p> </li> <li> <p>The LUST cannot do much complete software installations but often can give useful advice and     do some of the work.</p> <p>In LUMI-BE we have some resources, though mostly for VSC users.</p> <p>Note however that the LUST or LUMI-BE team may not even be allowed to help you due to software license restrictions. Similarly, the LUMI-BE team can only produce an install recipe if we have no access to your project, and though the LUST has technically speaking a zone where they can install software on the system, this is only done for software that the LUST can properly support and that is of interest to a wide enough audience. It is also not done for software where many users may want a specifically customised installation. Neither is it done for software that LUST cannot sufficiently test themselves.</p> </li> <li> <p>Both the LUST and LUMI-BE team can help with questions regarding compute and storage use.     LUST provides L1 and basic L2 support. These are basically problems that can solved in hours rather than     days or weeks. LUMI-BE has some resources to offer more advanced L2 and some L3 support, but      most of these resources are for users eligible for VSC-use only currently.</p> </li> <li> <p>The LUST can help with analysing the source of crashes or poor performance (with the emphasis on      \"help\" as they rarely have all the application knowledge required to dig deep).     There are some resources within LUMI-BE also, again mostly on the VSC side.</p> </li> <li> <p>However, neither LUST nor the LUMI-BE help desk is a debugging service (though of course we do     take responsibility for code that we developed)</p> </li> <li> <p>The LUST has some resources for work on porting and optimising codes to/for AMD GPUs     via porting calls and hackathons respectively. </p> <p>VSC also has some resources, but not the same level of access to experts from HPE or AMD to assist.</p> </li> <li> <p>Neither help desk can do your science or solve your science problems though.</p> </li> </ul> <p>Remember:</p> <p>\"Supercomputer support is there to  support you in the computational aspects of your work  related to the supercomputer but not to take over your work.\"</p> <p>Any support will always be a collaboration where you may have to do most of the work. Supercomputer support services are not a free replacement of a research software engineer (the equivalent of the lab assistant that many experimental groups have).</p>"},{"location":"intro-evolving/06-Support/#links","title":"Links","text":"<ul> <li> <p>LUMI web sites</p> <ul> <li> <p>The main LUMI web site contains very general information about     LUMI and also has a section in which the trainings organised by the LUST and some other trainings     are announced. It is also the starting point to contact the LUST with your support questions via     web forms. The web forms assure that we have the necessary information to start investigating your      issues.</p> <p>Note that when the support form asks for a project number, this is the project number on LUMI  (of the form 465XXXXXX for most projects) and not the project number used in LUMI-BE or EuroHPC, as the LUST does not know these numbers.</p> </li> <li> <p>The LUMI documentation     covers the LUMI system itself and includes topics such as how to get on the      system, where to place your files, how to start jobs, how to use the programming environment,     how to install software, etc.</p> <p>This is your primary source of information when you are investigating if LUMI might be suitable for you, or once you have obtained a project on LUMI.</p> </li> <li> <p>The LUMI Software Library contains     an overview of software pre-installed on LUMI or for which we have install recipes to start from.     For some software packages, it also contains additional information on how to use the software     on LUMI.</p> </li> <li> <p>The LUMI AI Guide has a lot of code samples     and some documentation on doing AI on LUMI (mostly focusing on PyTorch).</p> </li> </ul> </li> <li> <p>Web sites in Belgium:</p> <ul> <li> <p>The EuroCC Belgium web site announced most local and LUST LUMI trainings     in the \"Trainings\" section     and also contains information on      how to apply for compute time on LUMI via the Belgian share.</p> </li> <li> <p>The VSC web site.     Several VSC trainings are also relevant for (future) LUMI users!</p> <p>The VSC Supercomputers for Starters course lectured at UAntwerpen also covers several topics that are even more relevant to LUMI than to the Tier-2 systems of VSC and C\u00c9CI.  Full course notes are available, so you can have a look at the material if you have no time to join the lectures (twice a year).</p> </li> <li> <p>The C\u00c9CI web site.     Several C\u00c9CI trainings are also relevant for (future) LUMI users!</p> </li> </ul> </li> <li> <p>These course notes also contain a     page with links into technical documentation     of the scheduler and the programming environments on LUMI, and links to the user documentation     of some similar systems.</p> </li> </ul>"},{"location":"intro-evolving/07-Slurm/","title":"Slurm on LUMI","text":"<p>Last update of this page: October 2, 2025</p>"},{"location":"intro-evolving/07-Slurm/#slurm-on-lumi","title":"Slurm on LUMI","text":"<p>Who is this for?</p> <p>We assume some familiarity with job scheduling in this section. The notes will cover some of the more basic aspects of Slurm also, though as this version of the notes is intended for Belgian users and since all but one HPC site in Belgium currently teaches Slurm to their users, some elements will be covered only briefly.</p> <p>Even if you have a lot of experience with Slurm, it is still very useful to have a quick look at this section as Slurm is not always configured in the same way.</p> <p>Links to Slurm material</p> <p>Links to Slurm material on this web page are all for the version on LUMI at the time of the course. Some of the links in the PDF of the slides however are to the newest version.</p>"},{"location":"intro-evolving/07-Slurm/#what-is-slurm","title":"What is Slurm","text":"<p>Slurm is both a resource manager and job scheduler for supercomputers in a single package.</p> <p>A resource manager manages all user-exposed resources on a supercomputer: cores, GPUs or other accelerators, nodes, ... It sets up the resources to run a job and cleans up after the job, and may also give additional facilities to start applications in a job. Slurm does all this.</p> <p>But Slurm is also a job scheduler. It will assign jobs to resources, following policies set by sysadmins to ensure a good use of the machine and a fair distribution of resources among projects.</p> <p>Slurm is the most popular resource manager and job scheduler at the moment and is used on more than 50% of all big supercomputers. It is an open source package with commercial support. Slurm is a very flexible and configurable tool with the help of tens or even hundreds of plugins. This also implies that Slurm installations on different machines can also differ a lot and that not all features available on one computer are also available on another. So do not expect that Slurm will behave the same on LUMI as on that other computer you're familiar with, even if that other computer may have hardware that is very similar to LUMI.</p> <p>Slurm is starting to show its age and has trouble dealing in an elegant and proper way with the deep hierarchy of resources in modern supercomputers. So Slurm will not always be as straightforward to use as we would like it, and some tricks will be needed on LUMI. Yet there is no better option at this moment that is sufficiently mature.</p> <p>Other systems in Belgium</p> <p>Previously at the VSC Torque was used as the resource manager and Moab as the scheduler. All VSC sites now use Slurm, though at UGent it is still hidden behind wrappers that mimic Torque/Moab. As we shall see in this and the next session, Slurm, which is a more modern resource manager and scheduler than the Torque-Moab combination, has  already trouble dealing well with the hierarchy of resources in a modern supercomputer. Yet it is still a lot better at it than Torque and Moab. So no, the wrappers used on the HPC systems managed by UGent will not be installed on LUMI.</p> <p>Nice to know...</p> <p>Lawrence Livermore National Laboratory, the USA national laboratory that  originally developed Slurm is now working on the  development of another resource and job management framework called  flux. It will is on the third USA exascale supercomputer El Capitan. </p>"},{"location":"intro-evolving/07-Slurm/#slurm-concepts-physical-resources","title":"Slurm concepts: Physical resources","text":"<p>The machine model of Slurm is bit more limited than what we would like for LUMI. </p> <p>On the CPU side it knows:</p> <ul> <li> <p>A node: The hardware that runs a single operating system image</p> </li> <li> <p>A socket: On LUMI a Slurm socket corresponds to a physical socket, so there are two sockets on the      CPU nodes and a single socket on a GPU node.</p> <p>Alternatively a cluster could be configured to let a Slurm socket correspond to a NUMA nore or  L3 cache region, but this is something that sysadmins need to do so even if this would be useful for your job, you cannot do so.</p> </li> <li> <p>A core is a physical core in the system</p> </li> <li> <p>A thread is a hardware thread in the system (virtual core)</p> </li> <li> <p>A CPU is a \"consumable resource\" and the unit at which CPU processing capacity is allocated to a job.     On LUMI a Slurm CPU corresponds to a physical core, but Slurm could also be configured to let it correspond     to a hardware thread.</p> </li> </ul> <p>The first four bullets already show the problem we have with Slurm on LUMI: For three levels in the hierarchy of CPU resources on a node: the socket, the NUMA domain and the L3 cache domain, there is only one concept in Slurm, so we are not able to fully specify the hierarchy in resources that we want when sharing nodes with  other jobs.</p> <p>A GPU in Slurm is an accelerator and on LUMI corresponds to one GCD of an MI250X, so one half of an MI250X.</p>"},{"location":"intro-evolving/07-Slurm/#slurm-concepts-logical-resources","title":"Slurm concepts: Logical resources","text":"<ul> <li> <p>A partition: is a job queue with limits and access control. Limits include maximum     wall time for a job, the maximum number of nodes a single job can use, or the maximum     number of jobs a user can run simultaneously in the partition. The access control      mechanism determines who can run jobs in the partition.</p> <p>It is different from what we call LUMI-C and LUMI-G, or the <code>partition/C</code> and <code>partition/G</code> modules in the LUMI software stacks.</p> <p>Each partition covers a number of nodes, but partitions can overlap. This is not the case for the partitions that are visible to users on LUMI. Each partition covers a disjunct set of nodes. There are hidden partitions though that overlap with other partitions, but they are not accessible to regular users.</p> </li> <li> <p>A job in Slurm is basically only a resource allocation request.</p> </li> <li> <p>A job step is a set of (possibly parallel) tasks within a job</p> <ul> <li> <p>Each batch job always has a special job step called the batch job step which runs     the job script on the first node of a job allocation.</p> </li> <li> <p>An MPI application will typically run in its own job step.</p> </li> <li> <p>Serial or shared memory applications are often run in the batch job step but there     can be good reasons to create a separate job step for those applications.</p> </li> </ul> </li> <li> <p>A task executes in a job step and corresponds to a Linux process (and possibly subprocesses)</p> <ul> <li> <p>A shared memory program is a single task</p> </li> <li> <p>In an MPI application: Each rank (so each MPI process) is a task</p> <ul> <li> <p>Pure MPI: Each task uses a single CPU (which is also a core for us)</p> </li> <li> <p>Hybrid MPI/OpenMP applications: Each task can use multiple CPUs</p> </li> </ul> </li> </ul> <p>Of course a task cannot use more CPUs than available in a single node as a process can only run within a single operating system image.</p> </li> </ul>"},{"location":"intro-evolving/07-Slurm/#slurm-is-first-and-foremost-a-batch-scheduler","title":"Slurm is first and foremost a batch scheduler","text":"<p>And LUMI is in the first place a batch processing supercomputer.</p> <p>A supercomputer like LUMI is a very large and very expensive machine. This implies that it also has to be used as efficiently as possible which in turn implies that we don't want to wast time waiting for input as is the case in an interactive program.</p> <p>On top of that, very few programs can use the whole capacity of the supercomputer, so in practice a supercomputer is a shared resource and each simultaneous user gets a fraction on the machine depending on the requirements that they specify. Yet, as parallel applications work best when performance is predictable, it is also important to isolate users enough from each other.</p> <p>Research supercomputers are also typically very busy with lots of users so one often has to wait a little  before resources are available. This may be different on some commercial supercomputers and is also different on commercial cloud infrastructures, but the \"price per unit of work done on the cluster\" is also very  different from an academic supercomputer and few or no funding agencies are willing to carry that cost.</p> <p>Due to all this the preferred execution model on supercomputer is via batch jobs as they don't have to wait for input from the user, specified via batch scripts with resource specification where the user asks  precisely the amount of resources needed for the job, submitted to a queueing system with a scheduler to select the next job in a fair way based on available resources and scheduling policies set by the  compute centre.</p> <p>LUMI does have some facilities for interactive jobs, and with the introduction of Open On Demand some more became available. But it is far from ideal, and you will also be billed for the idle time of the resources you request. In fact, if you only need some interactive resources for a quick 10-minute experiment and don't  need too many resources, the wait may be minimal thanks to a scheduler mechanism called backfill where the scheduler looks for small and short jobs to fill up the gaps left while the scheduler is collecting resources for a big job.</p>"},{"location":"intro-evolving/07-Slurm/#a-slurm-batch-script","title":"A Slurm batch script","text":"<p>Slurm batch scripts (also called job scripts) are conceptually not that different from batch scripts for other HPC schedulers. A typical batch script will have 4 parts:</p> <ol> <li> <p>The shebang line with the shell to use. We advise to use the bash shell (<code>/bin/bash</code> or <code>/usr/bin/bash</code>)     If omitted, a very restricted shell will be used and some commands (e.g., related to modules)     may fail. In principle any shell language that uses a hashtag to denote comments can be used, but     we would advise against experimenting and the LUMI User Support Team and VSC support teams will only     support bash.</p> </li> <li> <p>Specification of resources and some other instructions for the scheduler and resource manager. This part     is also optional as one can also pass the instructions via the command line of <code>sbatch</code>, the command to     submit a batch job. But again, we would advise against omitting this block as specifying all options on     the command line can be very tedious.</p> </li> <li> <p>Building a suitable environment for the job. This part is also optional as on LUMI, Slurm will copy the     environment from the node from which the job was submitted. This may not be the ideal environment for your job,     and if you later resubmit the job you may do so accidentally from a different environment so it is a good practice     to specify the environment.</p> </li> <li> <p>The commands you want to execute.</p> </li> </ol> <p>Blocks 3 and 4 can of course be mixed as you may want to execute a second command in a different environment.</p> <p>On the following slides we will explore in particular the second block and to some extent how to start programs (the fourth block).</p> <p>lumi-CPEtools module</p> <p>The <code>lumi-CPEtools</code> module will be used a lot in this session of the course and in the next one on binding. It contains among other things a number of programs to quickly visualise how a serial, OpenMP, MPI or hybrid OpenMP/MPI application would run on LUMI and which cores and GPUs would be used. It is a very useful tool to  discover how Slurm options work without using a lot of billing units and we would advise you to  use it whenever you suspect Slurm isn't doing what you meant to do.</p> <p>It has its documentation page in the LUMI Software Library.</p>"},{"location":"intro-evolving/07-Slurm/#partitions","title":"Partitions","text":"<p>Remark</p> <p>Jobs run in partitions so the first thing we should wonder when setting up a job is which partition to use for a job (or sometimes partitions in case of a heterogeneous job which will be discussed later).</p> <p>Slurm partitions are possibly overlapping groups of nodes with similar resources or associated limits.  Each partition typically targets a particular job profile. E.g., LUMI has partitions for large multi-node jobs, for smaller jobs that often cannot fill a node, for some quick debug work and for some special resources that  are very limited (the nodes with 4TB of memory and the nodes with GPUs for visualisation). The number of jobs a user can have running simultaneously in each partition or have in the queue, the maximum wall time for a job, the number of nodes a job can use are all different for different partitions.</p> <p>There are two types of partitions on LUMI:</p> <ul> <li> <p>Exclusive node use by a single job. This ensures that parallel jobs can have a clean environment     with no jitter caused by other users running on the node and with full control of how tasks and threads     are mapped onto the available resources. This may be essential for the performance of a lot of codes.</p> </li> <li> <p>Allocatable by resources (CPU and GPU). In these partitions nodes are shared by multiple users and     multiple jobs, though in principle it is possible to ask for exclusive use which will however increase     your waiting time in the queue. The cores you get are not always continuously numbered, nor do you      always get the minimum number of nodes needed for the number of tasks requested. A proper mapping      of cores onto GPUs is also not ensured at all. The fragmentation of resources is a real problem on     these nodes and this may be an issue for the performance of your code.</p> </li> </ul> <p>It is also important to realise that the default settings for certain Slurm parameters may differ between partitions and hence a node in a partition allocatable by resource but for which exclusive  access was requested may still react differently to a node in the exclusive partitions.</p> <p>In general it is important to use some common sense when requesting resources and to have some understanding of what each Slurm parameter really means. Overspecifying resources (using more parameters than needed for the desired effect) may result in unexpected conflicts between parameters and error messages.</p> <p></p> <p>For the overview of Slurm partitions, see the LUMI documentation, \"Slurm partitions\" page. In the overview on the slides we did not mention partitions that are hidden to regular users, restricted for very specific use by some selected projects, or meant to be used via Open OnDemand only.</p> <p>The policies for partitions and the available partitions may change over time to fine tune the operation of LUMI and depending on needs observed by the system administrators and LUMI User Support Team, so don't take the above tables in the slide for granted.</p> <p></p> <p></p> <p>Some useful commands with respect to Slurm partitions:</p> <ul> <li> <p>To request information about the available partitions, use <code>sinfo -s</code>: </p> <pre><code>$ sinfo -s\nPARTITION   AVAIL  TIMELIMIT   NODES(A/I/O/T) NODELIST\ndebug          up      30:00          1/7/0/8 nid[002500-002501,002504-002506,002595-002597]\ninteractive    up    8:00:00          2/2/0/4 nid[002502,002507,002594,002599]\nq_fiqci        up      15:00          0/1/0/1 nid002598\nq_industry     up      15:00          0/1/0/1 nid002598\nq_nordiq       up      15:00          0/1/0/1 nid002503\nsmall          up 3-00:00:00     281/8/17/306 nid[002280-002499,002508-002593]\nstandard       up 2-00:00:00  1612/1/115/1728 nid[001000-002279,002600-003047]\ndev-g          up    3:00:00        44/2/2/48 nid[005002-005025,007954-007977]\nsmall-g        up 3-00:00:00      191/2/5/198 nid[005026-005123,007852-007951]\nstandard-g     up 2-00:00:00 1641/749/338/272 nid[005124-007851]\nlargemem       up 1-00:00:00          0/5/1/6 nid[000101-000106]\nlumid          up    4:00:00          1/6/1/8 nid[000016-000023]\n</code></pre> <p>The fourth column shows 4 numbers: The number of nodes that are currently fully or partially allocated to jobs, the number of idle nodes, the number of nodes in one of the other possible states (and not user-accessible) and the total number of nodes in the partition. Sometimes a large number of nodes can be in the \"O\" column, e.g., when mechanical maintenance is needed (like problem with the cooling). Also note that the width of the <code>NODES</code> field is not enough as the total number of nodes for <code>standard-g</code> doesn't make sense, but this is easyly solved, e.g., using</p> <pre><code>sinfo -o \"%11P %.5a %.10l %.20F %N\"\n</code></pre> <p> Note that this overview may show partitions that are not hidden but also not accessible to everyone. E.g.,  the <code>q_nordic</code> and <code>q_fiqci</code> partitions are used to access experimental quantum computers that are only available to some users of those countries that paid for those machines, which does not include Belgium. </p> <p>The <code>interactive</code> partition is used by the Open OnDemand web interface.</p> </li> <li> <p>For technically-oriented people, some more details about a partition can be obtained with     <pre><code>scontrol show partition &lt;partition-name&gt;\n</code></pre></p> <p>Another interesting command that will show you how many jobs you can have running in each partition, is  <pre><code>sacctmgr show assoc where account=&lt;my project number&gt; user=$USER\n</code></pre></p> </li> </ul> Additional example with <code>sinfo</code> <p>Try</p> <p><pre><code>$ sinfo --format \"%4D %10P %25f %.4c %.8m %25G %N\"\nNODE PARTITION  AVAIL_FEATURES            CPUS   MEMORY GRES                      NODELIST\n2    debug      AMD_EPYC_7763,x1004        256   229376 (null)                    nid[002275-002276]\n5    debug      AMD_EPYC_7763,x1005        256   229376 (null)                    nid[002500-002501,002504-002506]\n3    debug      AMD_EPYC_7763,x1006        256   229376 (null)                    nid[002595-002597]\n2    interactiv AMD_EPYC_7763,x1004        256   229376 (null)                    nid[002278-002279]\n2    interactiv AMD_EPYC_7763,x1005        256   229376 (null)                    nid[002502,002507]\n2    interactiv AMD_EPYC_7763,x1006        256   229376 (null)                    nid[002594,002599]\n1    q_fiqci    AMD_EPYC_7763,x1005        256   491520 (null)                    nid002343\n1    q_industry AMD_EPYC_7763,x1005        256   491520 (null)                    nid002343\n1    q_nordiq   AMD_EPYC_7763,x1005        256   229376 (null)                    nid002503\n247  small      AMD_EPYC_7763,x1005        256  229376+ (null)                    nid[002280-002342,002344-002499,002508-002535]\n58   small      AMD_EPYC_7763,x1006        256   229376 (null)                    nid[002536-002593]\n256  standard   AMD_EPYC_7763,x1000        256   229376 (null)                    nid[001000-001255]\n256  standard   AMD_EPYC_7763,x1001        256   229376 (null)                    nid[001256-001511]\n256  standard   AMD_EPYC_7763,x1002        256   229376 (null)                    nid[001512-001767]\n256  standard   AMD_EPYC_7763,x1003        256   229376 (null)                    nid[001768-002023]\n251  standard   AMD_EPYC_7763,x1004        256   229376 (null)                    nid[002024-002274]\n192  standard   AMD_EPYC_7763,x1006        256   229376 (null)                    nid[002600-002791]\n256  standard   AMD_EPYC_7763,x1007        256   229376 (null)                    nid[002792-003047]\n24   dev-g      AMD_EPYC_7A53,x1405        128   491520 gpu:mi250:8(S:0)          nid[007954-007977]\n24   dev-g      AMD_EPYC_7A53,x1100        128   491520 gpu:mi250:8(S:0)          nid[005002-005025]\n98   small-g    AMD_EPYC_7A53,x1100        128   491520 gpu:mi250:8(S:0)          nid[005026-005123]\n100  small-g    AMD_EPYC_7A53,x1405        128   491520 gpu:mi250:8(S:0)          nid[007852-007951]\n124  standard-g AMD_EPYC_7A53,x1103        128   491520 gpu:mi250:8(S:0)          nid[005372-005495]\n2    standard-g AMD_EPYC_7A53,x1200        128   491520 gpu:mi250:8(S:0-7)        nid[005834-005835]\n1    standard-g AMD_EPYC_7A53,x1302        128   491520 gpu:mi250:8               nid006839\n1    standard-g AMD_EPYC_7A53,x1305        128   491520 gpu:mi250:8               nid007220\n124  standard-g AMD_EPYC_7A53,x1101        128   491520 gpu:mi250:8(S:0)          nid[005124-005247]\n124  standard-g AMD_EPYC_7A53,x1102        128   491520 gpu:mi250:8(S:0)          nid[005248-005371]\n124  standard-g AMD_EPYC_7A53,x1104        128   491520 gpu:mi250:8(S:0)          nid[005496-005619]\n124  standard-g AMD_EPYC_7A53,x1105        128   491520 gpu:mi250:8(S:0)          nid[005620-005743]\n122  standard-g AMD_EPYC_7A53,x1200        128   491520 gpu:mi250:8(S:0)          nid[005744-005833,005836-005867]\n124  standard-g AMD_EPYC_7A53,x1201        128   491520 gpu:mi250:8(S:0)          nid[005868-005991]\n124  standard-g AMD_EPYC_7A53,x1202        128   491520 gpu:mi250:8(S:0)          nid[005992-006115]\n124  standard-g AMD_EPYC_7A53,x1203        128   491520 gpu:mi250:8(S:0)          nid[006116-006239]\n124  standard-g AMD_EPYC_7A53,x1204        128   491520 gpu:mi250:8(S:0)          nid[006240-006363]\n124  standard-g AMD_EPYC_7A53,x1205        128   491520 gpu:mi250:8(S:0)          nid[006364-006487]\n124  standard-g AMD_EPYC_7A53,x1300        128   491520 gpu:mi250:8(S:0)          nid[006488-006611]\n124  standard-g AMD_EPYC_7A53,x1301        128   491520 gpu:mi250:8(S:0)          nid[006612-006735]\n123  standard-g AMD_EPYC_7A53,x1302        128   491520 gpu:mi250:8(S:0)          nid[006736-006838,006840-006859]\n124  standard-g AMD_EPYC_7A53,x1303        128   491520 gpu:mi250:8(S:0)          nid[006860-006983]\n124  standard-g AMD_EPYC_7A53,x1304        128   491520 gpu:mi250:8(S:0)          nid[006984-007107]\n123  standard-g AMD_EPYC_7A53,x1305        128   491520 gpu:mi250:8(S:0)          nid[007108-007219,007221-007231]\n124  standard-g AMD_EPYC_7A53,x1400        128   491520 gpu:mi250:8(S:0)          nid[007232-007355]\n124  standard-g AMD_EPYC_7A53,x1401        128   491520 gpu:mi250:8(S:0)          nid[007356-007479]\n124  standard-g AMD_EPYC_7A53,x1402        128   491520 gpu:mi250:8(S:0)          nid[007480-007603]\n124  standard-g AMD_EPYC_7A53,x1403        128   491520 gpu:mi250:8(S:0)          nid[007604-007727]\n124  standard-g AMD_EPYC_7A53,x1404        128   491520 gpu:mi250:8(S:0)          nid[007728-007851]\n6    largemem   AMD_EPYC_7742              256 4096000+ (null)                    nid[000101-000106]\n8    lumid      AMD_EPYC_7742              256  2048000 gpu:a40:8,nvme:40000      nid[000016-000023]    \n</code></pre> (Output may vary over time)</p> <p>This shows more information about the system. The <code>xNNNN</code> feature corresponds to groups in  the Slingshot interconnect and may be useful if you want to try to get a job running in a single group (which is too advanced for this course and may get you an angry mail from the sysadmins as when not done properly, it will block the job queue and stop other jobs from launching).</p> <p>The memory size is given in megabyte (MiB, multiples of 1024). The \"+\" in the second group of the small partition is because that partition also contains the 512 GB and 1 TB regular  compute nodes. The memory reported is always 32 GB less than you would expect from the  node specifications. This is because 32 GB on each node is reserved for the OS and the  RAM disk it needs.</p>"},{"location":"intro-evolving/07-Slurm/#accounting-of-jobs","title":"Accounting of jobs","text":"<p>The use of resources by a job is billed to projects, not users. All management is also done at the project level, not at the \"user-in-a-project\" level. As users can have multiple projects, the system cannot know to which project a job should be billed, so it is mandatory to specify a project account (of the form <code>project_46YXXXXXX</code>) with every command that creates an allocation.</p> <p>Billing on LUMI is not based on which resources you effectively use, but on the amount of resources that others cannot use well because of your allocation.  This assumes that you make proportional use of CPU cores, CPU memory and GPUs (actually GCDs). If you job makes a disproportionally high use of one of those resources, you will be billed based on that use. For the CPU nodes, the billing is based on both the number of cores you request in your allocation and the amount of memory compared to the amount of memory per core in a regular node, and the highest of the two numbers is used. For the GPU nodes, the formula looks at the number of cores compared to he number of cores per GPU, the amount of CPU memory compared  to the amount of memory per GCD (so 64 GB), and the amount of GPUs and the highest amount determines for how many GCDs you will be billed (with a cost of 0.5 GPU-hour per hour per GCD). For jobs in job-exclusive partitions you are automatically billed for the full node as no other job can use that node, so 128 core-hours per hour for the standard partition or 4 GPU-hours per hour for the standard-g partition.</p> <p>E.g., if you would ask for only one core but 128 GB of memory, half of what a regular LUMI-C node has, you'd be billed for the use of 64 cores. Or assume you want to use only one GCD but want to use 16 cores and 256 GB of system RAM with it, then you would be billed for 4 GPUs/GCDs: 256 GB of memory makes it impossible for other users to use 4 GPUs/GCDs in the system, and 16 cores make it impossible to use 2 GPUs/GCDs, so the highest number of those is 4, which means that you will pay 2 GPU-hours per hour that you use the allocation (as GPU-hours are based on a full MI250x and not on a GCD which is the GPU for Slurm).</p> <p>This billing policy is unreasonable!</p> <p>Users who have no experience with performance optimisation may think this way of billing is unfair. After all, there may be users who need far less than 2 GB of memory per core so they could still use the other cores on a node where I am using only one core but 128 GB of memory, right? Well, no, and this has everything to do with the very hierarchical nature of a modern compute node, with on LUMI-C 2 sockets, 4 NUMA domains per socket, and 2 L3 cache domains per NUMA domain. Assuming your job would get the first core on the first socket (called core 0 and socket 0 as computers tend to number from 0). Linux will then allocate the memory of the job as close as possible to that core, so it will fill up the 4 NUMA domains of that socket. It can migrate unused memory to the other socket, but let's assume your  code does not only need 128 GB but also accesses bits and pieces from it everywhere all the time. Another application running on socket 0 may then get part or all of its memory on socket 1, and the latency to access that memory is more than  3 times higher, so performance of that application will suffer. In other words, the other cores in socket 0 cannot be used with full efficiency.</p> <p>This is not a hypothetical scenario. The author of this text has seem benchmarks run on one of the largest systems in Flanders that didn't scale at all and for some core configuration ran at only 10% of the speed they should have been running at...</p> <p>Still, even with this billing policy Slurm on LUMI is a far from perfect scheduler and core, GPU and memory allocation on the non-exclusive partitions are far from optimal. Which is why we spend a section of the course on binding applications to resources.</p> <p>The billing is done in a postprocessing step in the system based on data from the Slurm  job database, but the Slurm accounting features do not produce the correct numbers.  E.g., Slurm counts the core hours based on the virtual cores so the numbers are double of what they should be. There are two ways to check the state of an allocation, though both work with some delay.</p> <ul> <li> <p>The <code>lumi-workspaces</code> and <code>lumi-allocations</code> commands show the total amount of      billing units consumed. In regular operation of the system these numbers are updated     approximately once an hour.</p> <p><code>lumi-workspaces</code> is the all-in command that intends to show all information that is  useful to a regular user, while <code>lumi-allocations</code> is a specialised tool that only shows billing units, but he numbers shown by both tools come from the same database and are identical.</p> </li> <li> <p>For projects managed via Puhuri, Puhuri can show billing unit use per month, but the     delay is larger than with the <code>lumi-workspaces</code> command.</p> </li> </ul> <p>Billing unit use per user in a project</p> <p>The current project management system in LUMI cannot show the use of billing units per person within a project.</p> <p>For storage quota this would be very expensive to organise as quota are managed by Lustre on a group basis. </p> <p>For CPU and GPU billing units it would in principle be possible as the Slurm database contains the necessary information, but there are no plans to implement such a feature. It is assumed that every PI makes sure that members of their  projects use LUMI in a responsible way and ensures that they have sufficient  experience to realise what they are doing.</p>"},{"location":"intro-evolving/07-Slurm/#queueing-and-fairness","title":"Queueing and fairness","text":"<p>Remark</p> <p>Jobs are queued until they can run so we should wonder how that system works.</p> <p>LUMI is a pre-exascale machine meant to foster research into exascale applications.  As a result the scheduler setup of LUMI favours large jobs (though some users with large jobs will claim that it doesn't do so enough yet). So most nodes are reserved for larger  jobs (in the <code>standard</code> and <code>standard-g</code> partitions), and the priority computation also favours larger jobs (in terms of number of nodes). The latter is also because those large jobs cannot benefit from a mechanism in the scheduler that allows smaller jobs to run when there is a suitable gap.</p> <p>When you submit a job, it will be queued until suitable resources are available for the requested time window. Each job has a priority attached to it which the scheduler computes based on a number of factors, such as size of the job, how much you have been running in the past days, and how long the job has been waiting already. LUMI is not a first come, first served system. Keep in mind that you may see a lot of free nodes on LUMI yet your  small job may not yet start immediately as the scheduler may be gathering nodes for a big job with a higher priority.</p> <p>The <code>sprio</code> command will list the different elements that determine the priority of your job but is basically a command for system administrators as users cannot influence those numbers nor do  they say a lot unless you understand all intricacies of the job policies chosen by the site, and those policies may be fine-tuned over time to optimise the operation of the cluster. The fairshare parameter influences the priority of jobs depending on how much users or projects (this is not clear to us at the moment) have been running jobs in the past few days and is a very dangerous parameter on a supercomputer where the largest project is over 1000 times the size of the smallest projects, as treating all projects equally for the fair share would make it impossible for big projects to consume all their CPU time.</p> <p>Another concept of the scheduler on LUMI is backfill. On a system supporting very large jobs as LUMI, the scheduler will often be collecting nodes to run those large jobs, and this may take a while, particularly since the maximal wall time for a job in the standard partitions is rather large for such a system. If you need one quarter of the nodes for a big job on a partition on which most  users would launch jobs that use the full two days of walltime, one can expect that it takes half a day to gather those nodes. However, the LUMI scheduler will schedule short jobs even though they have a lower priority on the nodes already collected if it expects that those jobs will be finished before it expects to have all nodes for the big job. This mechanism is called backfill and is the reason why short experiments of half an hour or so that also don't require many nodes often start quickly on LUMI even though the queue is very long.</p>"},{"location":"intro-evolving/07-Slurm/#managing-slurm-jobs","title":"Managing Slurm jobs","text":"<p>Before experimenting with jobs on LUMI, it is good to discuss how to manage those jobs. We will not discuss the commands in detail and instead refer to the pretty decent manual pages that in fact can also be found on the web.</p> <ul> <li> <p>The command to check the status of the queue is <code>squeue</code>. It is also a good command to find out     the job IDs of your jobs if you didn't write them down after submitting the job.</p> <p>Two command line flags are useful:</p> <ul> <li> <p><code>--me</code> will restrict the output to your jobs only</p> </li> <li> <p><code>--start</code> will give an estimated start time for each job. Note that this really doesn't say      much as the scheduler cannot predict the future. On one hand, other jobs that are running     already or scheduled to run before your job, may have overestimated the time they need and     end early. But on the other hand, the scheduler does not use a \"first come, first serve\" policy     so another user may submit a job that gets a higher priority than yours, pushing back the start     time of your job. So it is basically a random number generator.</p> </li> </ul> </li> <li> <p>To delete a job, use <code>scancel &lt;jobID&gt;</code></p> </li> <li> <p>An important command to manage jobs while they are running is      <code>sstat -j &lt;jobID&gt;</code>.     This command display real-time information directly gathered from the resource manager     component of Slurm and can also be used to show information about individual job steps using     the job step identifier (which is in most case <code>&lt;jobID&gt;.0</code> for the first regular job step and so on).     We will cover this command in more detail      further in the notes of this session.</p> </li> <li> <p>The <code>sacct -j &lt;jobID&gt;</code> command can be used both while the     job is running and when the job has finished. It is the main command to get information about a job     after the job has finished. All information comes from a database, also while the job is running, so      the information is available with some delay compared to the information obtained with <code>sstat</code> for     a running job. It will also produce information about individual job steps.      We will cover this command in more detail      further in the notes of this session.</p> </li> </ul> <p>The <code>sacct</code> command will also be used in various examples in this section of the tutorial to investigate the behaviour of Slurm.</p>"},{"location":"intro-evolving/07-Slurm/#creating-a-slurm-job","title":"Creating a Slurm job","text":"<p>Slurm has three main commands to create jobs and job steps.  Remember that a job is just a request for an allocation. Your applications always have to run inside a job step.</p> <p>The <code>salloc</code> command only creates an allocation but does not create a job step. The behaviour of <code>salloc</code> differs between clusters!  On LUMI, <code>salloc</code> will put you in a new shell on the node from which you issued the <code>salloc</code> command, typically the login node. Your allocation will exist until you exit that shell with the <code>exit</code> command or with the CONTROL-D key combination. Creating an allocation with <code>salloc</code> is good for interactive work.</p> <p>Differences in <code>salloc</code> behaviour.</p> <p>On some systems <code>salloc</code> does not only create a job allocation but will also create a job step, the so-called \"interactive job step\", on a node of the allocation, similar to the way that the <code>sbatch</code> command discussed later will create a so-called \"batch job step\".</p> <p>The main purpose of the <code>srun</code> command is to create a job step in an allocation. When run outside of a job (outside an allocation) it will also create a job allocation. However, be careful when using this command to also create the job in which the job step will run as some options work differently as for the commands meant to create an allocation. When creating a job with <code>salloc</code> you will have to use <code>srun</code> to start anything on the node(s) in the allocation as it is not possible to, e.g., reach the nodes with <code>ssh</code>.</p> <p>The <code>sbatch</code> command both creates a job and then starts a job step, the so-called batch job step, to run the job script on the first node of the job allocation. In principle it is possible to start both sequential and shared memory processes directly in the batch job step without creating a new job step with <code>srun</code>,  but keep in mind that the resources may be different from what you expect to see in some cases as some of the options given with the <code>sbatch</code> command will only be enforced when starting another job step from the batch job step. To run any multi-process job (e.g., MPI) you will have to use <code>srun</code> or a process starter that internally calls <code>srun</code> to start the job. When using Cray MPICH as the MPI implementation (and it is the only one that is fully supported on LUMI) you will have to use <code>srun</code> as the process starter.</p>"},{"location":"intro-evolving/07-Slurm/#passing-options-to-srun-salloc-and-sbatch","title":"Passing options to srun, salloc and sbatch","text":"<p>There are several ways to pass options and flags to the <code>srun</code>, <code>salloc</code> and <code>sbatch</code> command.</p> <p>The lowest priority way and only for the <code>sbatch</code> command is specifying the options (mostly resource-related) in the batch script itself on <code>#SBATCH</code> lines. These lines should not be interrupted by commands, and it is not possible to use environment variables to specify values of options. </p> <p>Higher in priority is specifying options and flags through environment variables.  For the <code>sbatch</code> command this are the <code>SBATCH_*</code> environment variables, for <code>salloc</code> the <code>SALLOC_*</code> environment variables and for <code>srun</code> the <code>SLURM_*</code> and some <code>SRUN_*</code> environment variables. For the <code>sbatch</code> command this will overwrite values on the <code>#SBATCH</code> lines. You can find lists in the manual pages of the  <code>sbatch</code>,  <code>salloc</code> and <code>srun</code> command. Specifying command line options via environment variables that are hidden in your <code>.profile</code> or <code>.bashrc</code> file or any script that you run before starting your work, is not free of risks. Users often forget that they set those environment variables and are then surprised that the Slurm commands act differently then expected. E.g., it is very tempting to set the project account to use in environment variables but if you  then get a second project you may be running inadvertently in the wrong project.</p> <p>The highest priority is for flags and options given on the command line. The position of  those options is important though. With the <code>sbatch</code> command they have to be specified before the batch script as otherwise they will be passed to the batch script as command line options for  that script. Likewise, with <code>srun</code> they have to be specified before the command you want to execute as otherwise they would be passed to that command as flags and options.</p> <p>Several options specified to <code>sbatch</code> or <code>salloc</code> are also forwarded to <code>srun</code> via <code>SLURM_*</code> environment variables set in the job by these commands. These may interfere with options specified on the <code>srun</code> command line and lead to unexpected behaviour. </p> Example: Conflict between <code>--ntasks</code> and <code>--ntasks-per-node</code> <p>We'll meet this example later on in these notes,when we discuss starting a job step in per-node allocations. You'll need some Slurm experience to understand this example at this point, but keep it in mind when you read further in these notes.</p> <p>Two different options to specify the number of tasks in a job step are  <code>--ntasks</code> and <code>--ntasks-per-node</code>. The <code>--ntasks</code> command line options specifies the total number of tasks for the job step, and these will be distributed across nodes according to rules we will discuss later. The <code>--ntasks-per-node</code> command line option on the other hand requests that that number of tasks is launched on each node of the job (which really only makes sense if you have entire nodes, e.g., in node-exclusive allocations) and is attractive as it is easy to scale your allocation by just changing the number of nodes. </p> <p>Checking the  srun manual page for the <code>--ntasks-per-node</code> option,  you read that the <code>--ntasks</code> option takes precedence and if present,  <code>--ntasks-per-node</code> will be interpreted as the maximum number of tasks per node.</p> <p>Now depending on how the allocation was made, Slurm may set the environment variables <code>SLURM_NTASKS</code> and <code>SLURM_NPROCS</code> (the latter for historical reasons) that have the same effect as specifying <code>--ntasks</code>. So if these environment variables are present in the environment of your job script, they have effectively the effect of specifying <code>--ntasks</code> even though you did not specify it explicitly on the <code>srun</code> command line and <code>--ntasks-per-node</code> will no longer be the exact number of tasks. And this may happen even if you have never specified any <code>--ntasks</code> flag anywhere simply because Slurm fills in some defaults for certain parameters when creating an allocation...</p>"},{"location":"intro-evolving/07-Slurm/#specifying-options","title":"Specifying options","text":"<p>Slurm commands have way more options and flags than we can discuss in this course or any other course organised by the LUMI User Support Team. Moreover, if and how they work may depend on the specific configuration of Slurm. Slurm has so many options that no two clusters are the same. </p> <p>Slurm command can exist in two variants:</p> <ol> <li> <p>The long variant, with a double dash, is of the form <code>--long-option=&lt;value&gt;</code> or      <code>--long-option &lt;value&gt;</code></p> </li> <li> <p>But many popular commands also have a single letter variant, with a single dash:     <code>-S &lt;value&gt;</code> or <code>-S&lt;value&gt;</code></p> </li> </ol> <p>This is no different from many popular Linux commands.</p> <p>Slurm commands for creating allocations and job steps have many different flags for specifying the allocation and the organisation of tasks in that allocation. Not all combinations are valid, and it is not possible to sum up all possible configurations for all possible scenarios. Use  common sense and if something does not work, check the manual page and try something different. Overspecifying options is not a good idea as you may very well create conflicts, and we will see some examples in this section and the next section on binding. However, underspecifying is not a good idea either as some defaults may be used you didn't think of. Some combinations also just  don't make sense, and we will warn for some on the following slides and try to bring some  structure in the wealth of options.</p>"},{"location":"intro-evolving/07-Slurm/#some-common-options-to-all-partitions","title":"Some common options to all partitions","text":"<p>For CPU and GPU requests, a different strategy should be used for \"allocatable by node\" and \"allocatable by resource\" partitions, and this will be discussed later. A number of options however are common to both strategies and will be discussed first. All are typically used on <code>#SBATCH</code> lines in job scripts, but can also be used on the command line and the first three are certainly needed with <code>salloc</code> also.</p> <ul> <li> <p>Specify the account to which the job should be billed with <code>--account=project_46YXXXXXX</code> or <code>-A project_46YXXXXXX</code>.     This is mandatory; without this your job will not run.</p> </li> <li> <p>Specify the partition: <code>--partition=&lt;partition&gt;</code>  or <code>-p &lt;partition&gt;</code>. This option is also necessary     on LUMI as there is currently no default partition.</p> </li> <li> <p>Specify the wall time for the job: <code>--time=&lt;timespec&gt;</code> or <code>-t &lt;timespec&gt;</code>. There are multiple formats for     the time specifications, but the most common ones are minutes (one number), minutes:seconds (two numbers separated     by a colon) and hours:minutes:seconds (three numbers separated by a column). If not specified, the partition-dependent     default time is used.</p> <p>It does make sense to make a reasonable estimate for the wall time needed. It does protect you a bit in case your application hangs for some reason, and short jobs that also don't need too many nodes have a high chance of running quicker as they can be used as backfill while the scheduler is gathering nodes for a big job.</p> </li> <li> <p>Completely optional: Specify a name for the job with <code>--job-name=&lt;name&gt;</code> or <code>-J &lt;name&gt;</code>. Short but clear     job names help to make the output of <code>squeue</code> easier to interpret, and the name can be used to generate      a name for the output file that captures output to stdout and stderr also.</p> </li> <li> <p>For courses or other special opportunities such as the \"hero runs\" (a system for projects that want to test     extreme scalability beyond the limits of the regular partitions), reservations are used. You can specify the     reservation (or even multiple reservations as a comma-separated list) with <code>--reservation=&lt;name&gt;</code>.</p> <p>In principle no reservations are given to regular users for regular work as this is unfair to other users. It would not be possible to do all work in reservations and bypass the scheduler as the scheduling would be extremely complicated and the administration enormous. And empty reservations do not lead to efficient machine use. Schedulers have been developed for a reason.</p> </li> <li> <p>Slurm also has options to send mail to a given address when a job starts or ends or some other job-related     events occur, but this is currently not configured on LUMI.</p> </li> </ul>"},{"location":"intro-evolving/07-Slurm/#redirecting-output","title":"Redirecting output","text":"<p>Slurm has two options to redirect stdout and stderr respectively: <code>--output=&lt;template&gt;</code> or <code>-o &lt;template&gt;</code> for stdout and <code>--error=&lt;template&gt;</code> or <code>-e &lt;template&gt;</code> for stderr. They work together in the following way:</p> <ul> <li> <p>If neither <code>--output</code> not <code>--error</code> is specified, then stdout and stderr are merged and redirected to the file <code>slurm-&lt;jobid&gt;.out</code>.</p> </li> <li> <p>If <code>--output</code> is specified but <code>--error</code> is not, then stdout and stderr are merged and redirected to the file given with <code>--output</code>.</p> </li> <li> <p>If <code>--output</code> is not specified but <code>--error</code>, then stdout will still be redirected to <code>slurm-&lt;jobid&gt;.out</code>, but     stderr will be redirected to the file indicated by the <code>--error</code> option.</p> </li> <li> <p>If both <code>--output</code> and <code>--error</code> are specified, then stdout is redirected to the file given by <code>--output</code> and     stderr is redirected to the file given by <code>--error</code>.</p> </li> </ul> <p>It is possible to insert codes in the filename that will be replaced at runtime with the corresponding Slurm  information. Examples are <code>%x</code> which will be replaced with the name of the job (that you can then best set with <code>--job-name</code>) and <code>%j</code> which will be replaced with the job ID (job number). It is recommended to always include  the latter in the template for the filename as this ensures unique names if the same job script would be run a  few times with different input files. Discussing all patterns that can be used for the filename is outside the scope of this tutorial, but you can find them all in the sbatch manual page in the \"filename pattern\" section.</p>"},{"location":"intro-evolving/07-Slurm/#requesting-resources-cpus-and-gpus","title":"Requesting resources: CPUs and GPUs","text":"<p>Slurm is very flexible in the way resources can be requested. Covering every scenario and every possible way to request CPUs and GPUs is impossible, so we will present a scheme that works for most users and jobs.</p> <p>First, you have to distinguish between two strategies for requesting resources, each with their own pros and cons. We'll call them \"per-node allocations\" and \"per-core allocations\":</p> <ol> <li> <p>\"Per-node allocations\": Request suitable nodes (number of nodes and partition) with <code>sbatch</code> or <code>salloc</code>     but postpone specifying the full structure of the job step (i.e., tasks, cpus per task, gpus per task, ...)     until you actually start the job step with <code>srun</code>.</p> <p>This strategy relies on job-exclusive nodes, so works on the <code>standard</code> and <code>standard-g</code> partitions that  are \"allocatable-by-node\" partitions, but can be used on the \"allocatable-by-resource\" partitions also it the <code>--exclusive</code> flag is used  with <code>sbatch</code> or <code>salloc</code> (on the command line or with and <code>#SBATCH --exclusive</code> line for <code>sbatch</code>).</p> <p>This strategy gives you the ultimate flexibility in the job as you can run multiple job steps with a different  structure in the same job rather than having to submit multiple jobs with job dependencies to ensure that they are started in the proper order. E.g., you could first have an initialisation step that generates input files in a multi-threaded shared memory program and then run a pure MPI job with a single-threaded process per rank. </p> <p>This strategy also gives you full control over how the application is mapped onto the available hardware: mapping of MPI ranks across nodes and within nodes, binding of threads to cores, and binding of GPUs to MPI ranks. This will be the topic of the next chapter of the course and is for some applications very important to get optimal performance on modern supercomputer nodes that have a strongly hierarchical architecture (which in fact is not only the case for AMD processors, but will likely be an issue on some Intel Sapphire Rapids processors also).</p> <p>The downside is that allocations and hence billing is always per full node, so if you need only half a node  you waste a lot of billing units. It shows that to exploit the full power of a supercomputer you really need to have problems and applications that can at least exploit a full node.</p> </li> <li> <p>\"Per-core allocations\": Specify the full job step structure when creating the job allocation and optionally     limit the choice fo Slurm for the resource allocation by specifying a number of nodes     that should be used. </p> <p>The problem is that Slurm cannot create a correct allocation on an \"allocatable by resource\" partition if it would only know the total number of CPUs and total number of GPUs that you need. Slurm does not automatically allocate the resources on the minimal number of nodes (and even then there could be problems) and cannot know how you intend to use the resources to ensure that the resources are actually useful for you job. E.g., if you ask for 16 cores and Slurm would spread them over two or more nodes, then they would not be useful to run a shared memory program as such a program cannot  span nodes. Or if you really want to run an MPI application that needs 4 ranks and 4 cores per rank, then those cores must be assigned in groups of 4 within nodes as an MPI rank cannot span nodes. The same holds for GPUs. If you would  ask for 16 cores and 4 GPUs you may still be using them in different ways. Most users will probably intend to start an MPI program with 4 ranks that each use 4 cores and one GPU, and in that case the allocation should be done in groups  that each contain 4 cores and 1 GPU but can be spread over up to 4 nodes, but you may as well intend to run  a 16-thread shared memory application that also needs 4 GPUs. </p> <p>The upside of this is that with this strategy you will only get what you really need when used in an \"allocatable-by-resources\" partition, so  if you don't need a full node, you won't be billed for a full node (assuming of course that you don't request that much memory that you basically need a full node's memory). </p> <p>One downside is that you are now somewhat bound to the job structure. You can run job steps with a different structure, but they may produce a warning or may not run at all if the job step cannot be mapped on the resources allocated to  the job.</p> <p>More importantly, most options to do binding (see the next chapter) cannot be used or don't make sense anyway as there is no guarantee your cores will be allocated in a dense configuration.</p> <p>However, if you can live with those restrictions and if your job size falls within the limits of the \"allocatable per  resource\" partitions, and cannot fill up the minimal number of nodes that would be used, then this strategy ensures you're only billed for the minimal amount of resources that are made unavailable by your job.</p> </li> </ol> <p>This choice is something you need to think about in advance and there are no easy guidelines. Simply say \"use the first  strategy if your job fills whole nodes anyway and the second one otherwise\" doesn't make sense as your job may be so sensitive to its mapping to resources that it could perform very badly in the second case. The real problem is that there is no good way in Slurm to ask for a number of L3 cache regions (CPU chiplets), a number of NUMA node or a number of sockets and also no easy way to always do the proper binding if you would get resources that way (but that is something that can only be understood after the next session). If a single job needs only a half  node and if all jobs take about the same time anyway, it might be better to bundle them by hand in jobs and do a proper mapping of each subjob on the available resources (e.g., in case of two jobs on a CPU node, map each on a socket).</p>"},{"location":"intro-evolving/07-Slurm/#resources-for-per-node-allocations","title":"Resources for per-node allocations","text":"<p>In a per-node allocation, all you need to specify is the partition and the number of nodes needed, and in some cases, the amount of memory. In this scenario, one should use those Slurm options that specify resources per node also.</p> <p>The partition is specified using <code>--partition=&lt;partition</code> or <code>-p &lt;partition&gt;</code>.</p> <p>The number of nodes is specified with <code>--nodes=&lt;number_of_nodes&gt;</code> or its short form  <code>-N &lt;number_of_nodes&gt;</code>.</p> <p>If you want to use a per-node allocation on a partition which is allocatable-by-resources such as <code>small</code> and <code>small-g</code>, you also need to specify the <code>--exclusive</code> flag. On LUMI this flag does not have the same effect as running on a partition that is allocatable-by-node. The <code>--exclusive</code> flag does allocate all cores and GPUs on the node to your job, but the memory use is still limited by other parameters in the Slurm configuration. In fact, this can also be the case for allocatable-by-node partitions, but there  the limit is set to allow the use of all available memory. Currently the interplay between various parameters in the Slurm configuration results in a limit of 112 GB of memory on the <code>small</code> partition and 64 GB on the <code>standard</code> partition when running in <code>--exclusive</code> mode. It is possible to change this with the <code>--mem</code> option.</p> <p>You can request all memory on a node by using <code>--mem=0</code>. This is currently the default behaviour on nodes in the <code>standard</code> and <code>standard-g</code> partition so not really needed there. It is needed on all of the partitions that are allocatable-by-resource.</p> <p>We've experienced that it may be a better option to actually specify the maximum amount of useable memory on a node which is the memory capacity of the node you want minus 32 GB, so you can use <code>--mem=224G</code> for a regular CPU node or <code>--mem=480G</code> for a GPU node. In the past we have had memory leaks on compute nodes that were not detected by the node health checks, resulting in users getting nodes with less available memory than expected, but specifying these amounts protected them against getting such nodes. (And similarly you could use <code>--mem=480G</code> and <code>--mem=992G</code> for the 512 GB and 1 TB compute nodes in the small  partition, but note that running on these nodes is expensive!)</p> Example jobscript (click to expand) <p>The following job script runs a shared memory program in the batch job step, which shows that it has access to all hardware threads and all GPUs in a node at that moment:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-perNode-minimal-small-g\n#SBATCH --partition=small-g\n#SBATCH --exclusive\n#SBATCH --nodes=1\n#SBATCH --mem=480G\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n\nmodule load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeCray-24.03\n\ngpu_check\n\nsleep 2\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p>As we are using <code>small-g</code> here instead of <code>standard-g</code>, we added the <code>#SBATCH --exclusive</code> and <code>#SBATCH --mem=480G</code> lines.</p> <p>A similar job script for a CPU-node in LUMI-C and now in the <code>standard</code> partition would look like:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-perNode-minimal-standard\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeCray-24.03\n\nomp_check\n\nsleep 2\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p><code>gpu_check</code> and <code>omp_check</code> are two programs provided by the <code>lumi-CPEtools</code> modules to check the allocations. Try <code>man lumi-CPEtools</code> after loading the module. The programs will be used extensively in the next section on binding also, and are written to check how your program would behave in the allocation without burning through tons of billing units.</p> <p></p> <p>By default you will get all the CPUs in each node that is allocated in a per-node allocation. The Slurm options to request CPUs on a per-node basis are not really useful on LUMI, but might be on clusters with multiple node types in a single partition as they enable you to specify the minimum number of sockets, cores and hardware threads a node should have.</p> <p>We advise against using the options to request CPUs on LUMI  because it is more likely to cause problems due to user error than to solve problems. Some of these options also conflict with options that will be used later in the course.</p> <p>There is no direct way to specify the number of cores per node. Instead one has to specify the number sockets and then the number of cores per socket and one can specify even the number of hardware threads per core, though we will favour another mechanism later in these course notes.</p> <p>The two options are:</p> <ol> <li> <p>Specify <code>--sockets-per-node=&lt;sockets</code> and <code>--cores-per-socket=&lt;cores&gt;</code>.     For LUMI-C the maximal specification is </p> <pre><code>--sockets-per-node=2 --cores-per-socket-64\n</code></pre> <p>and for LUMI-G</p> <pre><code>--sockets-per-node=1 --cores-per-socket=56\n</code></pre> <p>Note that on LUMI-G, nodes have 64 cores but one core is reserved for the operating system and drivers to  reduce OS jitter that limits the scalability of large jobs. Requesting 64 cores will lead to error messages or jobs getting stuck.</p> </li> <li> <p>There is a shorthand for those parameters: <code>--extra-node-info=&lt;sockets&gt;[:cores]</code> or     <code>-B --extra-node-info=&lt;sockets&gt;[:cores]</code> where the second and third number are optional.     The full maximal specification for LUMI-C would be <code>--extra-node-info=2:64</code> and for LUMI-G     <code>--extra-node-info=1:56</code>.</p> </li> </ol> What about <code>--threads-per-core</code>? <p>Slurm also has a <code>--threads-per-core</code> (or a third number with <code>--extra-node-info</code>) which is a somewhat misleading name. On LUMI, as hardware threads  are turned on, you would expect that you can use <code>--threads-per-core=2</code> but if you try, you will see that your job is not accepted. This because on LUMI, the smallest allocatable processor resource  (called the CPU in Slurm) is a core and not a hardware thread (or virtual core as they are also  called). There is another mechanism to enable or disable hyperthreading in regular job steps that we will discuss later.</p> <p></p> <p>By default you will get all the GPUs in each node that is allocated in a per-node allocation. The Slurm options to request GPUs on a per-node basis are not really useful on LUMI, but might be on clusters with multiple types of GPUs in a single partition as they enable you to specify which type of node you want. If you insist, slurm has several options to specify the number of GPUs for this scenario:</p> <ol> <li> <p>The most logical one to use for a per-node allocation is <code>--gpus-per-node=8</code> to request 8 GPUs per node.     You can use a lower value, but this doesn't make much sense as you will be billed for the full node anyway.</p> <p>It also has an option to also specify the type of the GPU but that doesn't really make sense on LUMI.  On LUMI, you could in principle use <code>--gpus-per-node=mi250:8</code>.</p> </li> <li> <p><code>--gpus=&lt;number&gt;</code> or <code>-G &lt;number&gt;</code> specifies the total number of GPUs needed for the job. In our opinion     this is a dangerous option to use as when you change the number of nodes, you likely also want to change     the number of GPUs for the job and you may overlook this. Here again it is possible to specify the type of     the GPU also. Moreover, if you ask for fewer GPUs than are present in the total number of nodes you request,     you may get a very strange distribution of the available GPUs across the nodes.</p> Example of an unexpected allocation <p>Assuming <code>SLURM_ACCOUNT</code> is set to a valid project with access to the partition used: </p> <pre><code>module load LUMI/24.03 partition/G lumi-CPEtools\nsrun --partition standard-g --time 5:00 --nodes 2 --tasks-per-node 1 --gpus 8 gpu_check\n</code></pre> <p>returns</p> <pre><code>MPI 000 - OMP 000 - HWT 001 - Node nid007264 - RT_GPU_ID 0,1,2,3,4,5,6 - GPU_ID 0,1,2,3,4,5,6 - Bus_ID c1,c9,ce,d1,d6,d9,dc\nMPI 001 - OMP 000 - HWT 001 - Node nid007265 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1\n</code></pre> <p>So 7 GPUs were allocated on the first node and 1 on the second.</p> </li> <li> <p>A GPU belongs to the family of \"generic consumable resources\" (or GRES) in Slurm and there is an option to request     any type of GRES that can also be used. Now you also need to specify the type of the GRES. The number you      have to specify if on a per-node basis, so on LUMI you can use  <code>--gres=gpu:8</code> or <code>--gres=gpu:mi250:8</code>.</p> </li> </ol> <p>As these options are also forwarded to <code>srun</code>, it will save you from specifying them there if you specified them already at the time of the job allocation.</p>"},{"location":"intro-evolving/07-Slurm/#per-node-allocations-starting-a-job-step","title":"Per-node allocations: Starting a job step","text":"<p>Serial or shared-memory multithreaded programs in a batch script can in principle be run in  the batch job step. As we shall see though the effect may be different from what you expect.  However, if you are working interactively via <code>salloc</code>, you are in a shell on the node on which you called <code>salloc</code>, typically a login node, and to run anything on the compute nodes you  will have to start a job step.</p> <p>The command to start a new job step is <code>srun</code>. But it needs a number of arguments in most cases. After all, a job step consists of a number of equal-sized tasks (considering only homogeneous job steps at the moment, the typical case for most users) that each need a number of cores or hardware threads and, in case of GPU compute, access to a number of GPUs.</p> <p></p> <p>There are several ways telling Slurm how many tasks should be created and what the  resources are for each individual task, but this scheme is an easy scheme:</p> <ol> <li> <p>Specifying the number of tasks: You can specify per node or the total number:</p> <ol> <li> <p>Specifying the total number of tasks:      <code>--ntasks=&lt;ntasks</code> or <code>-n ntasks</code>.     There is a risk associated to this approach which is the same as when specifying the     total number of GPUs for a job: If you change the number of nodes, then you should     change the total number of tasks also. However, it is also very useful in certain cases.     Sometimes the number of tasks cannot be easily adapted and does not fit perfectly into     your allocation (cannot be divided by the number of nodes). In that case, specifying the     total number of nodes makes perfect sense.</p> </li> <li> <p>Specifying on a per node basis:      <code>--ntasks-per-node=&lt;ntasks&gt;</code>      is possible in combination with <code>--nodes</code> according to the Slurm manual.      In fact, this would be a logical thing to do in a per node allocation.      However, we see it fail on LUMI when it is used as an option for <code>srun</code> and not with <code>sbatch</code>,      even though it should work     according to the documentation.</p> <p>The reason for the failure is that Slurm when starting a batch job defines a large number of <code>SLURM_*</code> and <code>SRUN_*</code> variables. Some only give information about the allocation, but others are picked up by <code>srun</code> as options and some of those options have a higher priority than <code>--ntasks-per-node</code>. So the trick is to  unset both <code>SLURM_NTASKS</code> and <code>SLURM_NPROCS</code>. The <code>--ntasks</code> option triggered by <code>SLURM_NTASKS</code> has a higher priority than <code>--ntasks-per-node</code>.  <code>SLURM_NPROCS</code> was used in older versions of Slurm as with the same function as the current environment variable <code>SLURM_NTASKS</code> and therefore also implicitly specifies  <code>--ntasks</code> if <code>SLURM_NTASKS</code> is removed from the environment.</p> <p>The option is safe to use with <code>sbatch</code> though.</p> </li> </ol> <p>Lesson: If you want to play it safe and not bother with modifying the environment that Slurm creates, use the total number of tasks <code>--ntasks</code> if you want to specify the number of tasks with <code>srun</code>.</p> </li> <li> <p>Specifying the number of CPUs (cores on LUMI) for each task. The easiest way to do this is by     using <code>--cpus-per-task=&lt;number_CPUs&gt;</code> or <code>-c &lt;number_CPUs&gt;</code>.</p> </li> <li> <p>Specifying the number of GPUs per task. Following the Slurm manuals, the following     seems the easiest way:</p> <ol> <li> <p>Use <code>--gpus-per-task=&lt;number_GPUs&gt;</code> to bind one or more GPUs to each task.     This is probably the most used option in this scheme.</p> </li> <li> <p>If however you want multiple tasks to share a GPU, then you should use      <code>--ntasks-per-gpu=&lt;number_of_tasks&gt;</code>. There are use cases where this makes sense.     We have also had issues in the past with tasks sharing a GPU on LUMI.</p> </li> </ol> <p>This however does not always work and you should not use this approach in job-exclusive node allocations... A proper solution will be discussed in the  \"Bindings\" chapter of these notes.</p> </li> </ol> <p>The job steps created in this simple scheme do not always run the programs at optimal efficiency. Slurm has various strategies to assign tasks to nodes, and there is an option which we will discuss in the next session of the course (binding) to change that. Moreover, not all clusters use the same default setting for this strategy. Cores and GPUs are assigned in order and this is not always the best order.</p> <p>It is also possible to specify these options already on <code>#SBATCH</code> lines. Slurm will transform those options into <code>SLURM_*</code> environment variables that will then be picked up by <code>srun</code>. However, this  behaviour has changed in more recent versions of Slurm. E.g., <code>--cpus-per-task</code> is no longer  automatically picked up by <code>srun</code> as there were side effects with some MPI implementations on some clusters. CSC has modified the configuration to again forward that option (now via an <code>SRUN_*</code>  environment variable) but certain use cases beyond the basic one described above are not covered. And take into account that not all cluster operators will do that as there are also good reasons not to do so. Otherwise the developers of Slurm wouldn't have changed that behaviour in the first place.</p> Demonstrator for the problems with <code>--tasks-per-node</code> (click to expand) <p>Try the batch script:</p> <p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-perNode-jobstart-standard-demo1\n#SBATCH --partition=standard\n#SBATCH --nodes=2\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeCray-24.03\n\necho \"Submitted from $SLURM_SUBMIT_HOST\"\necho \"Running on $SLURM_JOB_NODELIST\"\necho\necho -e \"Job script:\\n$(cat $0)\\n\"\necho \"SLURM_* and SRUN_* environment variables:\"\nenv | egrep ^SLURM\nenv | egrep ^SRUN\n\nset -x\n# This works\nsrun --ntasks=32 --cpus-per-task=8 hybrid_check -r\n\n# This does not work\nsrun --ntasks-per-node=16 --cpus-per-task=8 hybrid_check -r\n\n# But this works again\nunset SLURM_NTASKS\nunset SLURM_NPROCS\nsrun --ntasks-per-node=16 --cpus-per-task=8 hybrid_check -r\nset +x\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre></p>"},{"location":"intro-evolving/07-Slurm/#a-warning-for-gpu-applications","title":"A warning for GPU applications","text":"<p>Allocating GPUs with <code>--gpus-per-task</code> or <code>--tasks-per-gpu</code> may seem the most logical thing to do when reading the Slurm manual pages. It does come with a problem though resulting of how Slurm currently manages the AMD GPUs, and now the discussion becomes more technical.</p> <p>Slurm currently uses a separate control group per task for the GPUs. Now control groups are a mechanism in Linux for restricting resources available to a process and its childdren. Putting the GPUs in a separate control group per task limits the ways in intra-node communication can be done between GPUs, and this in turn is incompatible with some software.</p> <p>The solution is to ensure that all tasks within a node see all GPUs in the node and then to manually perform the binding of each task to the GPUs it needs using a different mechanism more like affinity masks for CPUs. It can be tricky to do though as many options for <code>srun</code> do a mapping under the hood.</p> <p>As we need a mechanisms that are not yet discussed yet in this chapter, we refer to the chapter \"Process and thread distribution and binding\" for a more ellaborate discussion and a solution.</p> <p>Unfortunately using AMD GPUs in Slurm is more complicated then it should be (and we will see even more problems).</p>"},{"location":"intro-evolving/07-Slurm/#turning-simultaneous-multithreading-on-or-off","title":"Turning simultaneous multithreading on or off","text":"<p>Hardware threads are enabled by default at the operating system level. In Slurm however, regular job steps start by default with hardware threads disabled. This is not true though for the  batch job step as the example below will show.</p> <p>Hardware threading for a regular job step can be turned on explicitly with <code>--hint=multhithread</code> and turned off explicitly with <code>--hint=nomultithread</code>,  with the latter the default on LUMI. The hint should be given as an option to <code>sbatch</code>(e.g., as a line <code>#SBATCH --hint=multithread</code>) and not as an option of <code>srun</code>. </p> <p>The way it works is a bit confusing though. We've always told, and that is also what the Slurm manual tells, that a CPU is the  smallest allocatable unit and that on LUMI, Slurm is set to use the core as the smallest allocatable unit. So you would expect that <code>srun --cpus-per-task=4</code> combined with <code>#SBATCH --hint=multithread</code> would give you 4 cores with in total 8 threads, but instead you will get 2 cores with 4 hardware threads. In other words, it looks like (at least with the settings on LUMI) <code>#SBATCH --hint=multithread</code> changes the meaning of CPU in the context of an <code>srun</code> command to a hardware thread instead of a  core. This is illustrated with the example below.</p> Use of <code>--hint=(no)multithread</code> (click to expand) <p>We consider the job script </p> <p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-HWT-standard-multithread\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --hint=multithread\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeGNU-24.03\n\necho -e \"Job script:\\n$(cat $0)\\n\"\n\nset -x\nsrun -n 1 -c 4 omp_check -r\nset +x\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre></p> <p>We consider three variants of this script:</p> <ol> <li> <p>Without the <code>#SBATCH --hint=multithread</code> line to see the default behaviour of Slurm on LUMI.     The relevant lines of the output are:</p> <pre><code>+ srun -n 1 -c 4 omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001847 mask 0-3\n\n+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4238727      slurm-HWT+   standard project_4+        256    RUNNING      0:0 \n4238727.bat+      batch            project_4+        256    RUNNING      0:0 \n4238727.0     omp_check            project_4+          8    RUNNING      0:0 \n</code></pre> <p>The <code>omp_check</code> program detects that it should run 4 threads (we didn't even need to help by setting <code>OMP_NUM_THREADS</code>) and uses cores 0 till 3 which are the first 4 physical cores on the processor.</p> <p>The output of the <code>sacct</code> command claims that the job (which is the first line of the table) got allocated 256 CPUs. This is a confusing feature of <code>sacct</code>: it shows  the number of hardware threads even though the Slurm CPU on LUMI is defined as a core. The next line shows the batch job step which actually does see all hardware threads of all cores (and in general, all hardware threads of all allocated cores of the first node of the job). The final line, with the '.0' job step, shows that that core was using 8 hardware threads, even though <code>omp_check</code> only saw 4. This is because the default  behaviour (as the next test will confirm) is <code>--hint=nomultithread</code>.</p> <p>Note that <code>sacct</code> shows the last job step as running even though it has finished. This is because <code>sacct</code> gets the information not from the compute node but from a database, and it  looks like the full information has not yet derived in the database. A short sleep before the <code>sacct</code> call would cure this problem.</p> </li> <li> <p>Now replace the <code>#SBATCH --hint=multithread</code>  with <code>#SBATCH --hint=nomultithread</code>.     The relevant lines of the output are now</p> <pre><code>+ srun -n 1 -c 4 omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001847 mask 0-3\n\n+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4238730      slurm-HWT+   standard project_4+        256    RUNNING      0:0 \n4238730.bat+      batch            project_4+        256    RUNNING      0:0 \n4238730.0     omp_check            project_4+          8    RUNNING      0:0 \n</code></pre> <p>The output is no different from the previous case which confirms that this is the default behaviour.</p> </li> <li> <p>Lastly, we run the above script unmodified, i.e., with <code>#SBATCH --hint=multithread</code>      Now the relevant lines of the output are:</p> <pre><code>+ srun -n 1 -c 4 omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001847 mask 0-1, 128-129\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001847 mask 0-1, 128-129\n++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001847 mask 0-1, 128-129\n++ omp_check: OpenMP thread   3/4   on cpu 129/256 of nid001847 mask 0-1, 128-129\n\n+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4238728      slurm-HWT+   standard project_4+        256    RUNNING      0:0 \n4238728.bat+      batch            project_4+        256    RUNNING      0:0 \n4238728.0     omp_check            project_4+          4  COMPLETED      0:0 \n</code></pre> <p>The <code>omp_check</code> program again detects only 4 threads but now runs them on the first two physical cores and the corresponding second hardware thread for these cores.  The output of <code>sacct</code> now shows 4 in the \"AllocCPUS\" command for the <code>.0</code> job step, which confirms that indeed only 2 cores with both hardware threads were allocated instead of 4 cores.</p> </li> </ol> Buggy behaviour when used with <code>srun</code> <p>Consider the following job script:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-HWT-standard-bug2\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n#SBATCH --hint=multithread\n#SBATCH --account=project_46YXXXXXX\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeGNU-24.03\n\nset -x\nsrun -n 1 -c 4 --hint=nomultithread omp_check -r\n\nsrun -n 1 -c 4 --hint=multithread omp_check -r\n\nOMP_NUM_THREADS=8 srun -n 1 -c 4 --hint=multithread omp_check -r\n\nsrun -n 1 -c 4 omp_check -r\nset +x\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n\nset -x\nsrun -n 1 -c 256 --hint=multithread omp_check -r\n</code></pre> <p>The relevant lines of the output are:</p> <pre><code>+ srun -n 1 -c 4 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001246 mask 0-3\n\n+ srun -n 1 -c 4 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   1/4   on cpu 129/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   3/4   on cpu   1/256 of nid001246 mask 0-1, 128-129\n\n+ OMP_NUM_THREADS=8\n+ srun -n 1 -c 4 --hint=multithread omp_check -r\n\nRunning 8 threads in a single process\n\n++ omp_check: OpenMP thread   0/8   on cpu   0/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   1/8   on cpu 128/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   2/8   on cpu   0/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   3/8   on cpu   1/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   4/8   on cpu 129/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   5/8   on cpu 128/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   6/8   on cpu 129/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   7/8   on cpu   1/256 of nid001246 mask 0-1, 128-129\n\n+ srun -n 1 -c 4 omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001246 mask 0-3\n\n+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4238801      slurm-HWT+   standard project_4+        256    RUNNING      0:0 \n4238801.bat+      batch            project_4+        256    RUNNING      0:0 \n4238801.0     omp_check            project_4+          8  COMPLETED      0:0 \n4238801.1     omp_check            project_4+          8  COMPLETED      0:0 \n4238801.2     omp_check            project_4+          8  COMPLETED      0:0 \n4238801.3     omp_check            project_4+          8  COMPLETED      0:0 \n\n+ srun -n 1 -c 256 --hint=multithread omp_check -r\nsrun: error: Unable to create step for job 4238919: More processors requested than permitted\n</code></pre> <p>The first <code>omp_check</code> runs as expected. The second one uses only 2 cores but all 4 hyperthreads on those cores. This is also not unexpected. In the third case we force the use of 8 threads, and they all land on the 4 hardware threads of 2 cores. Again, this is not unexpected. And neither is the output of the last  run of <code>omp_check</code> which is again with multithreading disabled as requested in the <code>#SBATCH</code> lines. What is surprising though is the output of <code>sacct</code>:  It claims there were 8 hardware threads, so 4 cores, allocated to the second  (the <code>.1</code>) and third (the <code>.2</code>) job step while whatever we tried, <code>omp_check</code> could only see 2 cores and 4 hardware threads. Indeed, if we would try to run with <code>-c 256</code> then <code>srun</code> will fail.</p> <p>But now try the reverse: we turn multithreading on in the <code>#SBATCH</code> lines and try to turn it off again with <code>srun</code>:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-HWT-standard-bug2\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n#SBATCH --hint=multithread\n#SBATCH --account=project_46YXXXXXX\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeGNU-24.03\n\nset -x\nsrun -n 1 -c 4 --hint=nomultithread omp_check -r\n\nsrun -n 1 -c 4 --hint=multithread omp_check -r\n\nsrun -n 1 -c 4 omp_check -r\nset +x\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p>The relevant part of the output is now</p> <pre><code>+ srun -n 1 -c 4 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   1/256 of nid001460 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   2/256 of nid001460 mask 0-3\n++ omp_check: OpenMP thread   2/4   on cpu   3/256 of nid001460 mask 0-3\n++ omp_check: OpenMP thread   3/4   on cpu   0/256 of nid001460 mask 0-3\n\n+ srun -n 1 -c 4 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   1/4   on cpu 129/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   3/4   on cpu   1/256 of nid001460 mask 0-1, 128-129\n\n++ srun -n 1 -c 4 omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   1/4   on cpu 129/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   3/4   on cpu   1/256 of nid001460 mask 0-1, 128-129\n\n+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4238802      slurm-HWT+   standard project_4+        256    RUNNING      0:0 \n4238802.bat+      batch            project_4+        256    RUNNING      0:0 \n4238802.0     omp_check            project_4+          8  COMPLETED      0:0 \n4238802.1     omp_check            project_4+          4  COMPLETED      0:0 \n4238802.2     omp_check            project_4+          4  COMPLETED      0:0 \n</code></pre> <p>And this is fully as expected. The first <code>srun</code> does not use hardware threads as requested by <code>srun</code>, the second run does use hardware threads and only 2 cores which is also what we requested with the <code>srun</code> command, and the last one also uses hardware threads. The output of <code>sacct</code> (and in particular the <code>AllocCPUS</code> comumn) not fully confirms that indeed there were only 2 cores allocated to the second and third run.</p> <p>So turning hardware threads on in the <code>#SBATCH</code> lines and then off again with <code>srun</code> works as expected, but the opposite, explicitly turning it off in the <code>#SBATCH</code> lines (or relying on the default which is off) and then trying to turn it on again, does not work.</p>"},{"location":"intro-evolving/07-Slurm/#per-core-allocations","title":"Per-core allocations","text":""},{"location":"intro-evolving/07-Slurm/#when-to-use","title":"When to use?","text":"<p>Not all jobs can use entire nodes efficiently, and therefore the LUMI setup does provide some partitions that enable users to define jobs that use only a part of a node. This scheme enables the user to only request the resources that are really needed for the job (and only get billed for those at least if they are proportional to the resources that a node provides), but also comes with the disadvantage that it is not possible to control how cores and GPUs are allocated within a node. Codes that depend on proper mapping of threads and processes on L3 cache regions, NUMA nodes or sockets, or on shortest paths between cores in a task and the associated GPU(s)  may see an unpredictable performance loss as the mapping (a) will rarely be optimal unless you are very lucky (and always be suboptimal for GPUs in the current LUMI setup) and (b) will also depend on other jobs already running on the set of nodes assigned to your job.</p> <p>Unfortunately, </p> <ol> <li> <p>Slurm does not seem to fully understand the GPU topology on LUMI and cannot take that properly into     account when assigning resources to a job or task in a job, and</p> </li> <li> <p>Slurm does not support the hierarchy in the compute nodes of LUMI. There is no way to specifically      request all cores in a socket, NUMA node or L3 cache region. It is only possible on a per-node level      which is the case that we already discussed.</p> </li> </ol> <p>Instead, you have to specify the task structure in the <code>#SBATCH</code> lines of a job script or as the command line arguments of <code>sbatch</code> and <code>salloc</code> that you will need to run the job.</p>"},{"location":"intro-evolving/07-Slurm/#resource-request","title":"Resource request","text":"<p>To request an allocation, you have to specify the task structure of the job step you want to run using mostly the same options that we have discussed on the slides \"Per-node allocations: Starting a job step\": </p> <ol> <li> <p>Now you should specify just the total amount of tasks needed using     <code>--ntasks=&lt;number&gt;</code> or <code>-n &lt;number&gt;</code>. As the number of nodes is not fixed     in this allocation type, <code>--ntasks-per-node=&lt;ntasks&gt;</code> does not make much sense.</p> <p>It is possible to request a number of nodes using <code>--nodes</code>, and it can even take two arguments: <code>--nodes=&lt;min&gt;-&lt;max&gt;</code> to specify the minimum and maximum number of nodes that Slurm should use rather than the exact number (and there are even more options),  but really the only case where it makes sense to use <code>--nodes</code> with <code>--ntasks-per-node</code> in this case, is if all tasks would fit on a single node and you also want to force them on a single node so that all MPI communications are done through shared memory rather than via the Slingshot interconnect.</p> <p>Restricting the choice of resources for the scheduler may increase your waiting time in the queue though.</p> </li> <li> <p>Specifying the number of CPUs (cores on LUMI) for each task. The easiest way to do this is by     using <code>--cpus-per-task=&lt;number&gt;</code> or <code>-c &lt;number</code>.</p> <p>Note that as has been discussed before, the standard behaviour of recent versions of Slurm is to no longer forward <code>--cpus-per-task</code> from the <code>sbatch</code> or <code>salloc</code> level to the <code>srun</code> level though CSC has made a configuration change in Slurm that will still try to do this though with some limitations.</p> </li> <li> <p>Specifying the number of GPUs per task. The easiest way here is:</p> <ol> <li> <p>Use <code>--gpus-per-task=&lt;number_GPUs&gt;</code> to bind one or more GPUs to each task.     This is probably the most used option in this scheme.</p> </li> <li> <p>If however you want multiple tasks to share a GPU, then you should use      <code>--ntasks-per-gpu=&lt;number_of_tasks&gt;</code>. There are use cases where this makes sense.     We have experienced issues with this though on LUMI, see below.</p> </li> </ol> <p>While this does ensure a proper distribution of GPUs across nodes compatible with the  distributions of cores to run the requested tasks, we will again run into binding issues when these options are propagated to <code>srun</code> to create the actual job steps, and here this is even more tricky to solve. It will stop some of the more efficient modes of MPI  and RCCL communications from working properly.</p> <p>We will again discuss a solution in the  Chapter \"Process and thread distribution and binding\"</p> </li> <li> <p>CPU memory. By default you get less than the memory per core on the node type. To change:</p> <ol> <li> <p>Against the logic there is no <code>--mem-per-task=&lt;number&gt;</code>, instead memory needs to be specified in     function of the other allocated resources.</p> </li> <li> <p>Use <code>--mem-per-cpu=&lt;number&gt;</code> to request memory per CPU (use k, m, g to specify kilobytes, megabytes or gigabytes)</p> </li> <li> <p>Alternatively on a GPU allocation <code>--mem-per-gpu=&lt;number&gt;</code>.     This is still CPU memory and not GPU memory!</p> </li> <li> <p>Specifying memory per node with <code>--mem</code> doesn't make much sense unless the number of nodes is fixed.</p> </li> </ol> </li> </ol> <p><code>--ntasks-per-gpu=&lt;number&gt;</code> does not work</p> <p>At the time of writing there were several problems when using <code>--ntasks-per-gpu=&lt;number&gt;</code> in combination with <code>--ntasks=&lt;number&gt;</code>. While according to the Slurm documentation this is a valid request and Slurm should automatically determine the right number of GPUs to allocate, it turns out that instead you need to specify the number of GPUs with <code>--gpus=&lt;number&gt;</code> together with <code>--ntasks-per-gpu=&lt;number&gt;</code> and let Slurm compute the number of tasks.</p> <p>Moreover, we've seen cases where the final allocation was completely wrong, with tasks ending up with the wrong number of GPUs or on the wrong node (like too many tasks on one and too little on another compared to the number of GPUs set aside in each of these nodes).</p> <p></p> <p><code>--sockets-per-node</code> and <code>--ntasks-per-socket</code></p> <p>If you don't read the manual pages of Slurm carefully enough you may have the impression that you can use parameters like <code>--sockets-per-node</code> and <code>--ntasks-per-socket</code> to force all tasks on a single socket (and get a single socket), but these options will not work as you expect.</p> <p>The <code>--sockets-per-node</code> option is not used to request an exact resource, but to specify a  type of node by specifying the minimal number of sockets a node should have.It is an irrelevant option on LUMI as each partition does have only a single node type.</p> <p>If you read the manual carefully, you will also see that there is a subtle difference between <code>--ntasks-per-node</code> and <code>--ntasks-per-socket</code>: With <code>--ntasks-per-node</code> you specify the exact number of tasks for each node while with <code>--tasks-per-socket</code> you specify the  maximum number of tasks for each socket. So all hope that something like</p> <pre><code>--ntasks=8 --ntasks-per-socket=8 --cpus-per-task=8\n</code></pre> <p>would always ensure that you get a socket for yourself with each task nicely assigned to a single L3 cache domain, is futile.</p>"},{"location":"intro-evolving/07-Slurm/#different-job-steps-in-a-single-job","title":"Different job steps in a single job","text":"<p>It is possible to have an <code>srun</code> command with a different task structure in your job script. This will work if no task requires more CPUs or GPUs than in the original request, and if there are either not more tasks either or if an entire number of tasks in the new structure fits in a task in the structure from the allocation and the total number of tasks does not exceed the original number multiplied with that entire number. Other cases may work randomly, depending on how Slurm did the actual allocation. In fact, this may even be abused to ensure that all tasks are allocated to a single node, though this is done more elegantly by just specifying <code>--nodes=1</code>.</p> <p>With GPUs though it can become very complicated to avoid binding problems if the Slurm way of implementing GPU binding does not work for you.</p> Some examples that work and don't work (click to expand) <p>Consider the job script:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-small-multiple-srun\n#SBATCH --partition=small\n#SBATCH --ntasks=4\n#SBATCH --cpus-per-task=4\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n#SBATCH --output %x-%j.txt\n#SBATCH --acount=project_46YXXXXXX\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeCray-24.03\n\necho \"Running on $SLURM_JOB_NODELIST\"\n\nset -x\n\nomp_check\n\nsrun --ntasks=1 --cpus-per-task=3 omp_check\n\nsrun --ntasks=2 --cpus-per-task=4 hybrid_check\n\nsrun --ntasks=4 --cpus-per-task=1 mpi_check\n\nsrun --ntasks=16 --cpus-per-task=1 mpi_check\n\nsrun --ntasks=1 --cpus-per-task=16 omp_check\n\nset +x\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p>In the first output example (with lots of output deleted) we got the full allocation of 16 cores on a single node, and in fact, even 16 consecutive cores though spread across 3 L3 cache domains. We'll go over the output in steps:</p> <pre><code>Running on nid002154\n\n+ omp_check\n\nRunning 32 threads in a single process\n\n++ omp_check: OpenMP thread   0/32  on cpu  20/256 of nid002154\n++ omp_check: OpenMP thread   1/32  on cpu 148/256 of nid002154\n...\n</code></pre> <p>The first <code>omp_check</code> command was started without using <code>srun</code> and hence ran on all hardware cores allocated to the job. This is why hardware threading is enabled and why the executable sees 32 cores.</p> <pre><code>+ srun --ntasks=1 --cpus-per-task=3 omp_check\n\nRunning 3 threads in a single process\n\n++ omp_check: OpenMP thread   0/3   on cpu  20/256 of nid002154\n++ omp_check: OpenMP thread   1/3   on cpu  21/256 of nid002154\n++ omp_check: OpenMP thread   2/3   on cpu  22/256 of nid002154\n</code></pre> <p>Next <code>omp_check</code> was started via <code>srun --ntasks=1 --cpus-per-task=3</code>. One task instead of 4, and the task is also smaller in terms of number of nodes as the tasks requested in <code>SBATCH</code> lines, and Slurm starts the executable without problems. It runs on three cores, correctly detects that number, and also correctly does not use hardware threading.</p> <pre><code>+ srun --ntasks=2 --cpus-per-task=4 hybrid_check\n\nRunning 2 MPI ranks with 4 threads each (total number of threads: 8).\n\n++ hybrid_check: MPI rank   0/2   OpenMP thread   0/4   on cpu  23/256 of nid002154\n++ hybrid_check: MPI rank   0/2   OpenMP thread   1/4   on cpu  24/256 of nid002154\n++ hybrid_check: MPI rank   0/2   OpenMP thread   2/4   on cpu  25/256 of nid002154\n++ hybrid_check: MPI rank   0/2   OpenMP thread   3/4   on cpu  26/256 of nid002154\n++ hybrid_check: MPI rank   1/2   OpenMP thread   0/4   on cpu  27/256 of nid002154\n++ hybrid_check: MPI rank   1/2   OpenMP thread   1/4   on cpu  28/256 of nid002154\n++ hybrid_check: MPI rank   1/2   OpenMP thread   2/4   on cpu  29/256 of nid002154\n++ hybrid_check: MPI rank   1/2   OpenMP thread   3/4   on cpu  30/256 of nid002154\n</code></pre> <p>Next we tried to start 2 instead of 4 MPI processes with 4 cores each which also works without problems. The allocation now starts on core 23 but that is because Slurm was still finishing the job step on cores 20 till 22 from the previous <code>srun</code> command. This may or may not happen and is also related to a remark we made before about using <code>sacct</code> at the end of the job where the last job step may still be shown as running instead of completed.</p> <pre><code>+ srun --ntasks=4 --cpus-per-task=1 mpi_check\n\nRunning 4 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/4   on cpu  20/256 of nid002154\n++ mpi_check: MPI rank   1/4   on cpu  21/256 of nid002154\n++ mpi_check: MPI rank   2/4   on cpu  22/256 of nid002154\n++ mpi_check: MPI rank   3/4   on cpu  23/256 of nid002154\n</code></pre> <p>Now we tried to start 4 tasks with 1 core each. This time we were lucky and the system  considered the previous <code>srun</code> completely finished and gave us the first 4 cores of the  allocation.</p> <pre><code>+ srun --ntasks=16 --cpus-per-task=1 mpi_check\nsrun: Job 4268529 step creation temporarily disabled, retrying (Requested nodes are busy)\nsrun: Step created for job 4268529\n\nRunning 16 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/16  on cpu  20/256 of nid002154\n++ mpi_check: MPI rank   1/16  on cpu  21/256 of nid002154\n++ mpi_check: MPI rank   2/16  on cpu  22/256 of nid002154\n++ mpi_check: MPI rank   3/16  on cpu  23/256 of nid002154\n++ mpi_check: MPI rank   4/16  on cpu  24/256 of nid002154\n++ mpi_check: MPI rank   5/16  on cpu  25/256 of nid002154\n...\n</code></pre> <p>With the above <code>srun</code> command we try to start 16 single-threaded MPI processes. This fits  perfectly in the allocation as it simply needs to put 4 of these tasks in the space reserved  for one task in the <code>#SBATCH</code> request. The warning at the start may or may not happen. Basically Slurm was still freeing up the cores from the previous run and therefore the new <code>srun</code> dind't  have enough resources the first time it tried to, but it automatically tried a second time.</p> <pre><code>+ srun --ntasks=1 --cpus-per-task=16 omp_check\nsrun: Job step's --cpus-per-task value exceeds that of job (16 &gt; 4). Job step may never run.\nsrun: Job 4268529 step creation temporarily disabled, retrying (Requested nodes are busy)\nsrun: Step created for job 4268529\n\nRunning 16 threads in a single process\n\n++ omp_check: OpenMP thread   0/16  on cpu  20/256 of nid002154\n++ omp_check: OpenMP thread   1/16  on cpu  21/256 of nid002154\n++ omp_check: OpenMP thread   2/16  on cpu  22/256 of nid002154\n...\n</code></pre> <p>In the final <code>srun</code> command we try to run a single 16-core OpenMP run. This time Slurm produces a warning as it would be impossible to fit a 16-cpre shared memory run in the space of 4 4-core  tasks if the resources for those tasks would have been spread across multiple nodes. The next warning is again for the same reason as in the previous case, but ultimately the command does run on all 16 cores allocated and without using hardware threading.</p> <pre><code>+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4268529      slurm-sma+      small project_4+         32    RUNNING      0:0 \n4268529.bat+      batch            project_4+         32    RUNNING      0:0 \n4268529.0     omp_check            project_4+          6  COMPLETED      0:0 \n4268529.1    hybrid_ch+            project_4+         16  COMPLETED      0:0 \n4268529.2     mpi_check            project_4+          8  COMPLETED      0:0 \n4268529.3     mpi_check            project_4+         32  COMPLETED      0:0 \n4268529.4     omp_check            project_4+         32    RUNNING      0:0 \n</code></pre> <p>The output of <code>sacct</code> confirms what we have been seeing. The first <code>omp_check</code> was run without srun and ran in the original batch step which had all hardware threads of all 16 allocated cores available. The next <code>omp_check</code> ran on 3 cores but 6 is shwon in this scheme which is normal as the \"other\" hardware thread on each core is implicitly also reserved. And the same holds for all other numbers in that column.</p> <p>At another time I was less lucky and got the tasks spread out across 4 nodes, each  running a single 4-core task. Let's go through the output again:</p> <pre><code>Running on nid[002154,002195,002206,002476]\n\n+ omp_check\n\nRunning 8 threads in a single process\n\n++ omp_check: OpenMP thread   0/8   on cpu  36/256 of nid002154\n++ omp_check: OpenMP thread   1/8   on cpu 164/256 of nid002154\n++ omp_check: OpenMP thread   2/8   on cpu  37/256 of nid002154\n++ omp_check: OpenMP thread   3/8   on cpu 165/256 of nid002154\n++ omp_check: OpenMP thread   4/8   on cpu  38/256 of nid002154\n++ omp_check: OpenMP thread   5/8   on cpu 166/256 of nid002154\n++ omp_check: OpenMP thread   6/8   on cpu  39/256 of nid002154\n++ omp_check: OpenMP thread   7/8   on cpu 167/256 of nid002154\n</code></pre> <p>The first <code>omp_check</code> now uses all hardware threads of the 4 cores allocated in the first node of the job (while using 16 cores/32 threads in the configuration where all cores were allocated on a single node).</p> <pre><code>+ srun --ntasks=1 --cpus-per-task=3 omp_check\n\nRunning 3 threads in a single process\n\n++ omp_check: OpenMP thread   0/3   on cpu  36/256 of nid002154\n++ omp_check: OpenMP thread   1/3   on cpu  37/256 of nid002154\n++ omp_check: OpenMP thread   2/3   on cpu  38/256 of nid002154\n</code></pre> <p>Running a three core OpenMP job goes without problems as it nicely fits within the space of a single task of the <code>#SBATCH</code> allocation.</p> <pre><code>+ srun --ntasks=2 --cpus-per-task=4 hybrid_check\n\nRunning 2 MPI ranks with 4 threads each (total number of threads: 8).\n\n++ hybrid_check: MPI rank   0/2   OpenMP thread   0/4   on cpu  36/256 of nid002195\n++ hybrid_check: MPI rank   0/2   OpenMP thread   1/4   on cpu  37/256 of nid002195\n++ hybrid_check: MPI rank   0/2   OpenMP thread   2/4   on cpu  38/256 of nid002195\n++ hybrid_check: MPI rank   0/2   OpenMP thread   3/4   on cpu  39/256 of nid002195\n++ hybrid_check: MPI rank   1/2   OpenMP thread   0/4   on cpu  46/256 of nid002206\n++ hybrid_check: MPI rank   1/2   OpenMP thread   1/4   on cpu  47/256 of nid002206\n++ hybrid_check: MPI rank   1/2   OpenMP thread   2/4   on cpu  48/256 of nid002206\n++ hybrid_check: MPI rank   1/2   OpenMP thread   3/4   on cpu  49/256 of nid002206\n</code></pre> <p>Running 2 4-thread MPI processes also goes without problems. In this case we got the second and third task from the original allocation, likely because Slurm was still freeing up the first node after the previous <code>srun</code> command.</p> <pre><code>+ srun --ntasks=4 --cpus-per-task=1 mpi_check\nsrun: Job 4268614 step creation temporarily disabled, retrying (Requested nodes are busy)\nsrun: Step created for job 4268614\n\nRunning 4 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/4   on cpu  36/256 of nid002154\n++ mpi_check: MPI rank   1/4   on cpu  36/256 of nid002195\n++ mpi_check: MPI rank   2/4   on cpu  46/256 of nid002206\n++ mpi_check: MPI rank   3/4   on cpu   0/256 of nid002476\n</code></pre> <p>Running 4 single threaded processes also goes without problems (but the fact that they are scheduled on 4 different nodes here is likely an artifact of the way we had to force to get more than one node as the small partition on LUMI was not very busy at that time).</p> <pre><code>+ srun --ntasks=16 --cpus-per-task=1 mpi_check\n\nRunning 16 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/16  on cpu  36/256 of nid002154\n++ mpi_check: MPI rank   1/16  on cpu  37/256 of nid002154\n++ mpi_check: MPI rank   2/16  on cpu  38/256 of nid002154\n++ mpi_check: MPI rank   3/16  on cpu  39/256 of nid002154\n++ mpi_check: MPI rank   4/16  on cpu  36/256 of nid002195\n++ mpi_check: MPI rank   5/16  on cpu  37/256 of nid002195\n++ mpi_check: MPI rank   6/16  on cpu  38/256 of nid002195\n++ mpi_check: MPI rank   7/16  on cpu  39/256 of nid002195\n++ mpi_check: MPI rank   8/16  on cpu  46/256 of nid002206\n++ mpi_check: MPI rank   9/16  on cpu  47/256 of nid002206\n++ mpi_check: MPI rank  10/16  on cpu  48/256 of nid002206\n++ mpi_check: MPI rank  11/16  on cpu  49/256 of nid002206\n++ mpi_check: MPI rank  12/16  on cpu   0/256 of nid002476\n++ mpi_check: MPI rank  13/16  on cpu   1/256 of nid002476\n++ mpi_check: MPI rank  14/16  on cpu   2/256 of nid002476\n++ mpi_check: MPI rank  15/16  on cpu   3/256 of nid002476\n</code></pre> <p>16 single threaded MPI processes also works without problems.</p> <pre><code>+ srun --ntasks=1 --cpus-per-task=16 omp_check\nsrun: Job step's --cpus-per-task value exceeds that of job (16 &gt; 4). Job step may never run.\nsrun: Warning: can't run 1 processes on 4 nodes, setting nnodes to 1\nsrun: error: Unable to create step for job 4268614: More processors requested than permitted\n...\n</code></pre> <p>However, trying to run a single 16-thread process now fails. Slurm first warns us that it might fail, then tries and lets it fail.</p>"},{"location":"intro-evolving/07-Slurm/#the-job-environment","title":"The job environment","text":"<p>On LUMI, <code>sbatch</code>, <code>salloc</code> and <code>srun</code> will all by default copy the environment in which they run to the job step they start (the batch job step for <code>sbatch</code>, an interactive job step for <code>salloc</code> and a regular job step for <code>srun</code>). For <code>salloc</code> this is normal behaviour as it also starts an interactive shell on the login nodes (and it cannot be changed with a command line parameter). For <code>srun</code>, any other behaviour would be a pain as each job step would need to set up an environment. But for <code>sbatch</code> this may be surprising to some as the environment on the login nodes may not be the best environment for the compute nodes. Indeed, we do recommend to reload, e.g., the LUMI modules to use software optimised specifically for the compute nodes or to have full support of ROCm.</p> <p>It is possible to change this behaviour or to define extra environment variables with <code>sbatch</code> and <code>srun</code> using the command line option <code>--export</code>: </p> <ul> <li> <p><code>--export=NONE</code> will start the job (step) in a clean environment. The environment will not be inherited,     but Slurm will attempt to re-create the user environment even if no login shell is called or used in     the batch script. (<code>--export=NIL</code> would give you a truly empty environment.)</p> </li> <li> <p>To define extra environment variables, use <code>--export=ALL,VAR1=VALUE1</code> which would pass all existing      environment variables and define a new one, <code>VAR1</code>, with the value <code>VALUE1</code>. It is of course also possible     to define more environment variables using a comma-separated list (without spaces).      With <code>sbatch</code>, specifying <code>--export</code> on the command line that way is a way to parameterise a batch script.     With <code>srun</code> it can be very useful with heterogeneous jobs if different parts of the job need a different      setting for an environment variable (e.g., <code>OMP_NUM_THREADS</code>).</p> <p>Note however that <code>ALL</code> in the above <code>--export</code> option is essential as otherwise only the environment  variable <code>VAR1</code> would be defined.</p> <p>It is in fact possible to pass only select environment variables by listing them without assigning a new  value and omitting the <code>ALL</code> but we see no practical use of that on LUMI as the list of environment variables that is needed to have a job script in which you can work more or less normally is rather long.</p> </li> </ul> <p></p> <p>Passing arguments to a batch script</p> <p>With the Slurm <code>sbatch</code> command, any argument passed after the name of the job script is passed to the job script as an argument, so you can use regular bash shell argument processing to pass arguments to the bash script and do not necessarily need to use <code>--export</code>. Consider the following job script to demonstrate both options:</p> <pre><code>! /usr/bin/bash\n#SBATCH --job-name=slurm-small-parameters\n#SBATCH --partition=small\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n#SBATCH --output %x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n\necho \"Batch script parameter 0: $0\"\necho \"Batch script parameter 1: $1\"\necho \"Environment variable PAR1: $PAR1\"\n</code></pre> <p>Now start this with (assuming the job script is saved as <code>slurm-small-parameters.slurm</code>)</p> <pre><code>$ sbatch --export=ALL,PAR1=\"Hello\" slurm-small-parameters.slurm 'Wow, this works!'\n</code></pre> <p>and check the output file when the job is completed:</p> <pre><code>Batch script parameter 0: /var/spool/slurmd/job4278998/slurm_script\nBatch script parameter 1: Wow, this works!\nEnvironment variable PAR1: Hello\n</code></pre> <p>You see that you do not get the path to the job script as it was submitted (which you may expect  to be the value of <code>$0</code>). Instead the job script is buffered when you execute <code>sbatch</code> and started from a different directory. <code>$1</code> works as expected, and <code>PAR1</code> is also defined.</p> <p>In fact, passing arguments through command line arguments of the bash script is a more robust mechanism than using <code>--export</code> as can be seen from the bug discussed below...</p> <p>Fragile behaviour of <code>--export</code></p> <p>One of the problems with <code>--export</code> is that you cannot really assign any variable to a new environment variable the way you would do it on the bash command line. It is not clear what internal processing is going on, but the value is not always what you would expect.  In particular, problems can be expected when the value of the variable contains a semicolon.</p> <p>E.g., try the command from the previous example with <code>--export=ALL,PAR1='Hello, world'</code>  and it turns out that only <code>Hello</code> is passed as the value of the variable.</p> <p>Differences with some VSC systems</p> <p>The job environment in Slurm is different from that of some other resource managers, and in paritcular  Torque which was in use on VSC clusters and whose behaviour is still emulated on some.  LUMI uses the default settings of Slurm when it comes to environment management which is to start a job or job step in the environment from which the Slurm command was called.</p>"},{"location":"intro-evolving/07-Slurm/#automatic-requeueing","title":"Automatic requeueing","text":"<p>LUMI has the Slurm automatic requeueing of jobs upon node failure enabled. So jobs will be automatically resubmitted when one of the allocated nodes fails. For this an identical job ID is used and by default the prefious output will be truncated when the requeueed job starts.</p> <p>There are some options to influence this behaviour:</p> <ul> <li> <p>Automatic requeueing can be disabled at job submission with the <code>--no-requeue</code> option     of the <code>sbatch</code> command.</p> </li> <li> <p>Truncating of the output files can be avoided by specifying <code>--open-mode=append</code>.</p> </li> <li> <p>It is also possible to detect in a job script if a job has been restarted or not. For this     Slurm sets the environment variable <code>SLURM_RESTART_COUNT</code> which is 0 the first time a job      script runs and augmented by one at every restart.</p> </li> </ul>"},{"location":"intro-evolving/07-Slurm/#job-dependencies","title":"Job dependencies","text":"<p>The maximum wall time that a job can run on LUMI is fairly long for a Tier-0 system. Many other big systems in  Europe will only allow a maximum wall time of 24 hours. Despite this, this is not yet enough for some users. One way to deal with this is ensure that programs end in time and write the necessary restart information in a file, then start a new job that continues from that file. </p> <p>You don't have to wait to submit that second job. Instead, it is possible to tell Slurm that the second job should not start before the first one has ended (and ended successfully). This is done through job dependencies. It would take us too far to discuss all possible cases in this tutorial.</p> <p>One example is</p> <pre><code>$ sbatch --dependency=afterok:&lt;jobID&gt; jobdepend.slurm \n</code></pre> <p>With this statement, the job defined by the job script <code>jobdpend.slurm</code> will not start until the job with the given jobID has ended successfully (and you may have to clean up the queue if it never ends successfully). But  there are other possibilities also, e.g., start another job after a list of jobs has ended, or after a job has failed. We refer to the  sbatch manual page where you should  look for <code>--dependency</code> on the page.</p> <p>It is also possible to automate the process of submitting a chain of dependent jobs. For this the <code>sbatch</code> flag <code>--parsable</code> can be used which on LUMI will only print the job number of the job being submitted. So to  let the job defined by <code>jobdepend.slurm</code> run after the job defined by <code>jobfirst.slurm</code> while  submitting both at the same time, you can use something like</p> <pre><code>first=$(sbatch --parsable jobfirst.slurm)\nsbatch --dependency=afterok:$first jobdepend.slurm\n</code></pre>"},{"location":"intro-evolving/07-Slurm/#interactive-jobs","title":"Interactive jobs","text":"<p>Interactive jobs can have several goals, e.g.,</p> <ol> <li> <p>Simply testing a code or steps to take to get a code to run while developing a job script.     In this case you will likely want an allocation in which you can also easily run parallel MPI     jobs.</p> </li> <li> <p>Compiling a code usually works better interactively, but here you only need an allocation for     a single task supporting multiple cores if your code supports a parallel build process.     Building on the compute nodes is needed if architecture-specific optimisations are desired     while the code building process does not support cross-compiling (e.g., because the build process     adds <code>-march=native</code> or a similar compiler switch even if it is told not to do so) or ie you want     to compile software for the GPUs that during the configure or build process needs a GPU to be      present in the node to detect its features.</p> </li> <li> <p>Attaching to a running job to inspect how it is doing.</p> </li> </ol>"},{"location":"intro-evolving/07-Slurm/#interactive-jobs-with-salloc","title":"Interactive jobs with salloc","text":"<p>This is a very good way of working for the first scenario described above, testing steps for a future job script with parallel MPI jobs. </p> <p>Using <code>salloc</code> will create a pool of resources reserved for interactive execution, and will start a new shell on the node where you called <code>salloc</code>(usually a login node). As such it does not take resources away from other job steps that you will create so the shell is a good environment to test most stuff that you would execute in the  batch job step of a job script.</p> <p>To execute any code on one of the allocated compute nodes, be it a large sequential program, a shared memory program, distributed memory program or hybrid code, you can use <code>srun</code> in the same way as we have discussed for job scripts.</p> <p>It is possible to obtain an interactive shell on the first allocated compute node with</p> <pre><code>srun --pty $SHELL\n</code></pre> <p>(which if nothing more is specified would give you a single core for the shell), but keep in mind that this takes away resources from other job steps so if you try to start further job steps from that interactive shell you will note that you have fewer  resources available, and will have to force overlap (with <code>--overlap</code>), so it is not very practical to work that way.</p> <p>To terminate the allocation, simply exit the shell that was created by <code>salloc</code> with <code>exit</code> or  the CTRL-D key combination (and the same holds for the interactive shell in the previous paragraph).</p> Example with <code>salloc</code> and a GPU code (click to expand) <pre><code>$ salloc --account=project_46YXXXXXX --partition=standard-g --nodes=2 --time=15\nsalloc: Pending job allocation 4292946\nsalloc: job 4292946 queued and waiting for resources\nsalloc: job 4292946 has been allocated resources\nsalloc: Granted job allocation 4292946\n$ module load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeCray-24.03\n\n...\n\n$ srun -n 16 -c 2 --gpus-per-task 1 gpu_check\nMPI 000 - OMP 000 - HWT 001 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 001 - HWT 002 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 001 - OMP 000 - HWT 003 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6\nMPI 001 - OMP 001 - HWT 004 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6\nMPI 002 - OMP 000 - HWT 005 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 002 - OMP 001 - HWT 006 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 007 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce\nMPI 003 - OMP 001 - HWT 008 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 009 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1\nMPI 004 - OMP 001 - HWT 010 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1\nMPI 005 - OMP 000 - HWT 011 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6\nMPI 005 - OMP 001 - HWT 012 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6\nMPI 006 - OMP 000 - HWT 013 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9\nMPI 006 - OMP 001 - HWT 014 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9\nMPI 007 - OMP 000 - HWT 015 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc\nMPI 007 - OMP 001 - HWT 016 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc\nMPI 008 - OMP 000 - HWT 001 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 008 - OMP 001 - HWT 002 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 009 - OMP 000 - HWT 003 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6\nMPI 009 - OMP 001 - HWT 004 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6\nMPI 010 - OMP 000 - HWT 005 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 010 - OMP 001 - HWT 006 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 007 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce\nMPI 011 - OMP 001 - HWT 008 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 009 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1\nMPI 012 - OMP 001 - HWT 010 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1\nMPI 013 - OMP 000 - HWT 011 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6\nMPI 013 - OMP 001 - HWT 012 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6\nMPI 014 - OMP 000 - HWT 013 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9\nMPI 014 - OMP 001 - HWT 014 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9\nMPI 015 - OMP 000 - HWT 015 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc\nMPI 015 - OMP 001 - HWT 016 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc\n</code></pre>"},{"location":"intro-evolving/07-Slurm/#interactive-jobs-with-srun","title":"Interactive jobs with srun","text":"<p>Starting an interactive job with <code>srun</code> is good to get an interactive shell in which you want to do some work without starting further job steps, e.g., for compilation on the compute nodes or to run an interactive shared memory program such as R. It is not ideal if you want to spawn further job steps with <code>srun</code> within the same allocation as the interactive shell already fills a task slot, so you'd have to overlap if you want to use all resources of the job in the  next job step. </p> <p>For this kind of work you'll rarely need a whole node so <code>small</code>, <code>small-g</code>, <code>debug</code> or <code>dev-g</code> will likely be your partitions of choice.</p> <p>To start such a job, you'd use </p> <pre><code>srun --account=project_46YXXXXXX --partition=&lt;partition&gt; --ntasks=1 --cpus-per-task=&lt;number&gt; --time=&lt;time&gt; --pty=$SHELL\n</code></pre> <p>or with the short options</p> <pre><code>srun -A project_46YXXXXXX -p &lt;partition&gt; -n 1 -c &lt;number&gt; -t &lt;time&gt; --pty $SHELL\n</code></pre> <p>For the GPU nodes you'd also add a <code>--gpus-per-task=&lt;number&gt;</code> to request a number of GPUs.</p> <p>To end the interactive job, all you need to do is to leave the shell with <code>exit</code> or the CTRL-D key combination.</p>"},{"location":"intro-evolving/07-Slurm/#inspecting-a-running-job","title":"Inspecting a running job","text":"<p>On LUMI it is not possible to use <code>ssh</code> to log on to a compute node in use by one of your jobs. Instead you need to use Slurm to attach a shell to an already running job. This can be done with <code>srun</code>, but there are two differences with the previous scenario. First, you do not need a new allocation but need to tell <code>srun</code> to use an existing allocation. As there is already an allocation, <code>srun</code> does not need your project account in this case. Second, usually the job will be using all its resources so there is no room in the allocation to create another job step with the interactive shell. This is solved by telling <code>srun</code> that the resources should overlap with those already in use.</p> <p>To start an interactive shell on the first allocated node of a specific job/allocation, use</p> <pre><code>srun --jobid=&lt;jobID&gt; --overlap --pty $SHELL\n</code></pre> <p>and to start an interactive shell on another node of the jobm simply add a <code>-w</code> or <code>--nodelist</code> argument:</p> <pre><code>srun --jobid=&lt;jobID&gt; --nodelist=nid00XXXX --overlap --pty $SHELL\nsrun --jobid=&lt;jobID&gt; -w nid00XXXX --overlap --pty $SHELL\n</code></pre> <p>Instead of starting a shell, you could also just run a command, e.g., <code>top</code>, to inspect what the nodes are doing.</p> <p>Note that you can find out the nodes allocated to your job using <code>squeue</code> (probably the easiest as the nodes are shown by default), <code>sstat</code> or <code>salloc</code>.</p>"},{"location":"intro-evolving/07-Slurm/#job-arrays","title":"Job arrays","text":"<p>Job arrays is a mechanism to submit a large number of related jobs with the same batch script in a single <code>sbatch</code> operation.</p> <p>As an example, consider the job script <code>job_array.slurm</code></p> <pre><code>#!/bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --partition=small\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=1G\n#SBATCH --time=15:00\n\nINPUT_FILE=\"input_${SLURM_ARRAY_TASK_ID}.dat\"\nOUTPUT_FILE=\"output_${SLURM_ARRAY_TASK_ID}.dat\"\n\n./test_set -input ${INPUT_FILE} -output ${OUTPUT_FILE}\n</code></pre> <p>Note that Slurm defines the environment variable <code>SLURM_ARRAY_TASK_ID</code> which will have a unique value for each job of the job array, varying in the range given at job submission. This enables to distinguish between the different runs and can be used to generate names of input and output files.</p> <p>Submitting this job script and running it for values of <code>SLURM_ARRAY_TASK_ID</code> going from 1 to 50 could be done with </p> <pre><code>$ sbatch --array 1-50 job_array.slurm\n</code></pre> <p>Note that this will count for 50 Slurm jobs so the size of your array jobs on LUMI is limited by the rather strict limit on job size. LUMI is made as a system for big jobs, and is a system with a lot of users, and there are only that many simultaneous jobs that a scheduler can deal with. Users doing  throughput computing should do some kind of hierarchical scheduling, running a subscheduler in the  job that then further start subjobs.</p>"},{"location":"intro-evolving/07-Slurm/#heterogeneous-jobs","title":"Heterogeneous jobs","text":"<p>A heterogeneous job is one in which multiple executables run in a single <code>MPI_COMM_WORLD</code>, or a single executable runs in different combinations (e.g., some multithreaded and some single-threaded MPI ranks where the latter take a different code path from the former and do a different task). One example is large  simulation codes that use separate I/O servers to take care of the parallel I/O ot the file system.</p> <p>There are two ways to start such a job:</p> <ol> <li> <p>Create groups in the <code>SBATCH</code> lines, separated by <code>#SBATCH hetjob</code> lines, and then recall these groups with     <code>srun</code>. This is the most powerful mechanism as in principle one could use nodes in different partitions     for different parts of the heterogeneous job.</p> </li> <li> <p>Request the total number of nodes needed with the <code>#SBATCH</code> lines and then do the rest entirely with     <code>srun</code>, when starting the heterogeneous job step. The different blocks in <code>srun</code> are separated by a colon.     In this case we can only use a single partition.</p> </li> </ol> <p>The Slurm support for heterogeneous jobs is not very good and problems to often occur, or new bugs are being introduced.</p> <ul> <li> <p>The different parts of heterogeneous jobs in the first way of specifying them, are treated as different     jobs which may give problems with the scheduling.</p> </li> <li> <p>When using the <code>srun</code> method, these are still separate job steps and it looks like a second job is created     internally to run these, and on a separate set of nodes.</p> </li> </ul> <p></p> <p></p> Let's show with an example (worked out more in the text than in the slides) <p>Consider the following case of a 2-component job:</p> <ul> <li> <p>Part 1: Application A on 1 node with 32 tasks with 4 OpenMP threads each</p> </li> <li> <p>Part 2: Application B on 2 nodes with 4 tasks per node with 32 OpenMP threads each</p> </li> </ul> <p>We will simulate this case with the <code>hybrid_check</code> program from the <code>lumi-CPEtools</code> module  that we have used in earlier examples also.</p> <p>The job script for the first method would look like:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-herterogeneous-sbatch\n#SBATCH --time=5:00\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --cpus-per-task=4\n#SBATCH hetjob\n#SBATCH --partition=standard\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=32\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeCray-24.03\n\nsrun --het-group=0 --cpus-per-task=$SLURM_CPUS_PER_TASK_HET_GROUP_0 --export=ALL,OMP_NUM_THREADS=4  hybrid_check -l app_A : \\\n     --het-group=1 --cpus-per-task=$SLURM_CPUS_PER_TASK_HET_GROUP_1 --export=ALL,OMP_NUM_THREADS=32 hybrid_check -l app_B\n\nsrun --het-group=0 --cpus-per-task=$SLURM_CPUS_PER_TASK_HET_GROUP_0 hybrid_check -l hybrid_check -l app_A : \\\n     --het-group=1 --cpus-per-task=$SLURM_CPUS_PER_TASK_HET_GROUP_1 hybrid_check -l hybrid_check -l app_B\n\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p>There is a single <code>srun</code> command. <code>--het-group=0</code> tells <code>srun</code> to pick up the settings for the first heterogeneous group (before the <code>#SBATCH hetjob</code> line), and use that to start the <code>hybrid_check</code> program with the command line arguments <code>-l app_A</code>.  Next we have the column to tell <code>srun</code> that we start with the  second group, which is done in the same way. Note that since recent versions of Slurm do no longer  propagate the value for <code>--cpus-per-task</code>, we need to specify the value here explicitly which we can do via an environment variable. This is one of the cases where the patch to work around this new behaviour on LUMI does not work.</p> <p>This job script shows also demonstrates how a different value of a variable can be passed to each component using <code>--export</code>, even though this was not needed as the second case would show.</p> <p>The output of this job script would look lik (with a lot omitted):</p> <pre><code>srun: Job step's --cpus-per-task value exceeds that of job (32 &gt; 4). Job step may never run.\n\nRunning 40 MPI ranks with between 4 and 32 threads each (total number of threads: 384).\n\n++ app_A: MPI rank   0/40  OpenMP thread   0/4   on cpu   0/256 of nid001083\n++ app_A: MPI rank   0/40  OpenMP thread   1/4   on cpu   1/256 of nid001083\n...\n++ app_A: MPI rank  31/40  OpenMP thread   2/4   on cpu 126/256 of nid001083\n++ app_A: MPI rank  31/40  OpenMP thread   3/4   on cpu 127/256 of nid001083\n++ app_B: MPI rank  32/40  OpenMP thread   0/32  on cpu   0/256 of nid001544\n++ app_B: MPI rank  32/40  OpenMP thread   1/32  on cpu   1/256 of nid001544\n...\n++ app_B: MPI rank  35/40  OpenMP thread  30/32  on cpu 126/256 of nid001544\n++ app_B: MPI rank  35/40  OpenMP thread  31/32  on cpu 127/256 of nid001544\n++ app_B: MPI rank  36/40  OpenMP thread   0/32  on cpu   0/256 of nid001545\n++ app_B: MPI rank  36/40  OpenMP thread   1/32  on cpu   1/256 of nid001545\n...\n++ app_B: MPI rank  39/40  OpenMP thread  30/32  on cpu 126/256 of nid001545\n++ app_B: MPI rank  39/40  OpenMP thread  31/32  on cpu 127/256 of nid001545\n... (second run produces identical output)\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4285795+0    slurm-her+   standard project_4+        256    RUNNING      0:0 \n4285795+0.b+      batch            project_4+        256    RUNNING      0:0 \n4285795+0.0  hybrid_ch+            project_4+        256  COMPLETED      0:0 \n4285795+0.1  hybrid_ch+            project_4+        256  COMPLETED      0:0 \n4285795+1    slurm-her+   standard project_4+        512    RUNNING      0:0 \n4285795+1.0  hybrid_ch+            project_4+        512  COMPLETED      0:0 \n4285795+1.1  hybrid_ch+            project_4+        512  COMPLETED      0:0 \n</code></pre> <p>The warning at the start can be safely ignored. It just shows how heterogeneous job were an afterthought in Slurm and likely implemented in a very dirty way. We see that we get what we expected: 32 MPI ranks on the first node of the allocation, then 4 on  each of the other two nodes.</p> <p>The output of <code>sacct</code> is somewhat surprising. Slurm has essnetially started two jobs, with jobIDs that end with <code>+0</code> and <code>+1</code>, and it first shows all job steps for the first job, which is the batch job step and the first group of both <code>srun</code> commands, and then shows the second job and its job steps, again indicating that heterogeneous jobs are not really treated as a single job.</p> <p>The same example can also be done by just allocating 3 nodes and then using more arguments with <code>srun</code> to start the application:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-herterogeneous-srun\n#SBATCH --time=5:00\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard\n#SBATCH --nodes=3\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeCray-24.03\n\nsrun --ntasks=32 --cpus-per-task=4  --export=ALL,OMP_NUM_THREADS=4  hybrid_check -l app_A : \\\n     --ntasks=8  --cpus-per-task=32 --export=ALL,OMP_NUM_THREADS=32 hybrid_check -l app_B\n\nsrun --ntasks=32 --cpus-per-task=4  hybrid_check -l app_A : \\\n     --ntasks=8  --cpus-per-task=32 hybrid_check -l app_B\n\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p>The output of the two <code>srun</code> commands is essentially the same as before, but the output  of <code>sacct</code> is different:</p> <pre><code>sacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4284021      slurm-her+   standard project_4+        768    RUNNING      0:0 \n4284021.bat+      batch            project_4+        256    RUNNING      0:0 \n4284021.0+0  hybrid_ch+            project_4+        256  COMPLETED      0:0 \n4284021.0+1  hybrid_ch+            project_4+        512  COMPLETED      0:0 \n4284021.1+0  hybrid_ch+            project_4+        256  COMPLETED      0:0 \n4284021.1+1  hybrid_ch+            project_4+        512  COMPLETED      0:0 \n</code></pre> <p>We now get a single job ID but the job step for each of the <code>srun</code> commands is split  in two separate job steps, a <code>+0</code> and a <code>+1</code>. </p> <p>Erratic behaviour of <code>--nodes=&lt;X&gt; --ntasks-per-node=&lt;Y&gt;</code> </p> <p>One can wonder if in the second case we could still specify resources on a per-node basis in the <code>srun</code> command:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-herterogeneous-srun\n#SBATCH --time=5:00\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard\n#SBATCH --nodes=3\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeCray-24.03\n\nsrun --nodes=1 --ntasks-per-node=32 --cpus-per-task=4  hybrid_check -l hybrid_check -l app_A : \\\n     --nodes=2 --ntasks-per-node=4  --cpus-per-task=32 hybrid_check -l hybrid_check -l app_B\n</code></pre> <p>It turns out that this does not work at all. Both components get the wrong number of tasks. For some reason only 3 copies were started of the first application on the first node of the allocation, the 2 32-thread processes on the second node and one 32-thread process on the third node, also with an unexpected thread distribution.</p> <p>This shows that before starting a big application it may make sense to check with the tools from the <code>lumi-CPEtools</code> module if the allocation would be what you expect as Slurm is definitely not free of problems when it comes to hetereogeneous jobs.</p>"},{"location":"intro-evolving/07-Slurm/#simultaneous-job-steps","title":"Simultaneous job steps","text":"<p>It is possible to run multiple job steps in parallel on LUMI. The core of your job script would look something like:</p> <pre><code>#! /usr/bin/bash\n...\n#SBATCH partition=standard\n...\nsrun -n4 -c16 exe1 &amp;\nsleep 2\nsrun -n8 -c8 exe2 &amp;\nwait\n</code></pre> <p>The first <code>srun</code> statement will start a hybrid job of 4 tasks with 16 cores each on the first 64 cores of the node, the second <code>srun</code> statement would start a hybrid job of 8 tasks with 8 cores each on the remaining 64 cores. The <code>sleep 2</code> statement is used because we have experienced that from time to time even though the second <code>srun</code> statement cannot be executed immediately as the resource manager is busy with the first one. The <code>wait</code> command at the end is essential, as otherwise the batch job step would end without waiting for the two <code>srun</code> commands to finish the work they started, and the whole job would be killed.</p> <p>Running multiple job steps in parallel in a single job can be useful if you want to ensure a proper binding and hence do not want to use the \"allocate by resources\" partition, while a single job step is not enough to fill an exclusive node. It does turn out to be tricky though, especially when GPU nodes are being used, and with proper binding of the resources. In some cases the <code>--overlap</code> parameter of <code>srun</code> may help a bit. (And some have reported that in some cases <code>--exact</code> is needed instead, but this parameter is already implied if <code>--cpus-per-task</code> can be used.)</p> <p>Note that we have observed unexpected behaviour when using this on nodes that were not job-exclusive, likely caused by Slurm bugs. It also doesn't make much sense to use this feature in that case. The main reason to use it, is to be able to do proper mapping/binding of resources even when a single job step cannot fill a whole node. In other cases it may simply be easier to have  multiple jobs, one for each job step that you would run simultaneously.</p> A longer example <p>Consider the bash job script for an exclusive CPU node:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-simultaneous-CPU-1\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --hint=nomultithread\n#SBATCH --time=2:00\n#SBATCH --output %x-%j.txt\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeCray-24.03\n\necho \"Submitted from $SLURM_SUBMIT_HOST\"\necho \"Running on $SLURM_JOB_NODELIST\"\necho\necho -e \"Job script:\\n$(cat $0)\\n\"\necho \"SLURM_* environment variables:\"\nenv | egrep ^SLURM\n\nfor i in $(seq 0 7)\ndo \n    srun --ntasks=1 --cpus-per-task=16 --output=\"slurm-simultaneous-CPU-1-$SLURM_JOB_ID-$i.txt\" \\\n        bash -c \"export ROCR_VISIBLE_DEVICES=${GPU_BIND[$i]} &amp;&amp; omp_check -w 30\" &amp;\n\n    sleep 2\ndone\n\nwait\n\nsleep 2\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID --format JobID%-13,Start,End,AllocCPUS,NCPUS,TotalCPU,MaxRSS --units=M )\\n\"\n</code></pre> <p>It will start 8 parallel job steps and in total create 9 files: One file with the output of the job script itself, and then one file for each job step with the output specific to that job step. the <code>sacct</code> command at the end shows that the 8 job parallel job steps indeed overlap, as can be seen from the start and end time of each, with the <code>TotalCPU</code> column confirming that they are also consuming CPU time during that time. The last bit of the output of the main batch file looks like:</p> <pre><code>sacct for the job:\nJobID                       Start                 End  AllocCPUS      NCPUS   TotalCPU     MaxRSS \n------------- ------------------- ------------------- ---------- ---------- ---------- ---------- \n6849913       2024-04-09T16:15:45             Unknown        256        256   01:04:07            \n6849913.batch 2024-04-09T16:15:45             Unknown        256        256   00:00:00            \n6849913.0     2024-04-09T16:15:54 2024-04-09T16:16:25         32         32  08:00.834      6.92M \n6849913.1     2024-04-09T16:15:56 2024-04-09T16:16:26         32         32  08:00.854      6.98M \n6849913.2     2024-04-09T16:15:58 2024-04-09T16:16:29         32         32  08:00.859      6.76M \n6849913.3     2024-04-09T16:16:00 2024-04-09T16:16:30         32         32  08:00.793      6.76M \n6849913.4     2024-04-09T16:16:02 2024-04-09T16:16:33         32         32  08:00.870      6.59M \n6849913.5     2024-04-09T16:16:04 2024-04-09T16:16:34         32         32  08:01.046      8.57M \n6849913.6     2024-04-09T16:16:06 2024-04-09T16:16:36         32         32  08:01.133      6.76M \n6849913.7     2024-04-09T16:16:08 2024-04-09T16:16:39         32         32  08:00.793      6.57M \n</code></pre> <p>Obviously as we execute the <code>sacct</code> command in the job the end time of the batch job step and hence the job as a whole are still unknown. We ask <code>omp_check</code> to do some computations during 30 seconds on  each thread, and so we see that the CPU time consumed by each 16-core job is indeed around 8 minutes, while start and end time of each job step showed that they executed for roughly 30s each and nicely  overlapped.</p>"},{"location":"intro-evolving/07-Slurm/#slurm-job-monitoring-commands","title":"Slurm job monitoring commands","text":"<p>Slurm has two useful commands to monitor jobs that we want to discuss a bit further:</p> <ul> <li> <p><code>sstat</code> is a command to monitor jobs that are currently running. It gets its information     directly from the resource manager component of Slurm.</p> </li> <li> <p><code>sacct</code> is a command to get information about terminated jobs. It gets its information from     the Slurm accounting database. As that database is not continuously updated, information about     running jobs may already be present but is far from real-time.</p> </li> </ul> <p>Some users may also be familiar with the <code>sreport</code> command, but it is of limited use on LUMI.</p>"},{"location":"intro-evolving/07-Slurm/#the-sstat-command","title":"The <code>sstat</code> command","text":"<p>The <code>sstat</code> command is a command to get real-time information about a running job. That information is obtained from the resource manager components in Slurm and not from the  accounting database. The command can only produce information about job steps that are currently  being executed and cannot be used to get information about jobs tha thave already been terminated, or job steps that have terminated from jobs that are still running.</p> <p>In its most simple form, you'd likely use the <code>-j</code> (or <code>--jobs</code>) flag to specify the job for which you want information:</p> <pre><code>sstat -j 1234567\n</code></pre> <p>and you may like to add the <code>-a</code> flag to get information about all job steps for which information is available. You can also restrict to a single job step, e.g.,</p> <pre><code>sstat -j 1234567.0\n</code></pre> <p>The command produces a lot of output though and it is nearly impossible to interpret the output, even on a very wide monitor.</p> <p>To restrict that output to something that can actually be handled, you can use the <code>-o</code> or <code>--format</code> flag to specify the columns that you want to see.</p> <p>E.g., the following variant would show for each job step the minimum amount of CPU time that a task has consumed, and the average across all tasks. These numbers should be fairly close if the job has a good load balance.</p> <pre><code>$ sstat -a -j 1234567 -o JobID,MinCPU,AveCPU\nJobID            MinCPU     AveCPU\n------------ ---------- ----------\n1234567.bat+   00:00:00   00:00:00\n1234567.1      00:23:44   00:26:02\n</code></pre> <p>The above output is from an MPI job that has two job steps in it. The first step was a quick initialisation step and that one has terminated already, so we get no information about that step. The <code>1234567.1</code> step is the currently executing one, and we do note a slight load inbalance in this case. No measurable amount of time has been consumed running the batch script itself outside the <code>srun</code> commands in this case.</p> <p>It can also be used to monitor memory use of the application. E.g.,</p> <pre><code>$ sstat -a -j 1234567 -o JobID,MaxRSS,MaxRSSTask,MaxRSSNode\nJobID            MaxRSS MaxRSSTask MaxRSSNode\n------------ ---------- ---------- ----------\n1234567.bat+     25500K          0  nid001522\n1234567.1       153556K          0  nid001522\n</code></pre> <p>will show the maximum amount of resident memory used by any of the tasks, and also tell you which task that is and on which node it is running.</p> <p>You can get a list of output fields using <code>sstat -e</code> or <code>sstat --helpformat</code>.  Or check the  \"Job Status Fields\" section in the <code>sstat</code> manual page. That page also contains further examples.</p>"},{"location":"intro-evolving/07-Slurm/#the-sacct-command","title":"The <code>sacct</code> command","text":"<p>The <code>sacct</code> command shows information kept in the Slurm job accounting database. Its main use is to extract information about jobs or job steps that have already  terminated. It will however also provide information about running jobs and job steps, but that information if not real-time and only pushed periodically to the accounting database.</p> <p>If you know the job ID of the job you want to investigate, you can specify it directly using the <code>-j</code> or <code>--jobs</code> flag. E.g.,</p> <pre><code>$ sacct -j 1234567\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n1234567      healthy_u+   standard project_4+        512  COMPLETED      0:0\n1234567.bat+      batch            project_4+        256  COMPLETED      0:0\n1234567.0     gmx_mpi_d            project_4+          2  COMPLETED      0:0\n1234567.1     gmx_mpi_d            project_4+        512  COMPLETED      0:0\n</code></pre> <p>This report is for a GROMACS job that ran on two nodes. The first line gives the data for the overall job. The second line is for the batch job step that ran the batch script. That job got access to all resources on the first node of the job which is why 256 is shown in the <code>AllocCPUS</code> column (as that data is reported using the number of virtual cores). Job step <code>.0</code> was really an initialisation step that ran as a single task on a single physical core of the node, while the <code>.1</code> step was running on both nodes (as 256 tasks each on a physical core but that again cannot be directly derived from the output shown here).</p> <p>You can also change the amount of output that is shown using either <code>--brief</code> (which will show a lot less) or <code>--long</code> (which shows an unwieldly amount of information similar to <code>sstat</code>), and just as with <code>sstat</code>, the information can be fully customised using <code>-o</code> or <code>--format</code>, but as there is a lot more information in the accounting database, the format options are different.</p> <p></p> <p>As an example, let's check the CPU time and memory used by a job:</p> <pre><code>$ sacct -j 1234567 --format JobID%-13,AllocCPUS,MinCPU%15,AveCPU%15,MaxRSS,AveRSS --units=M\nJobID          AllocCPUS          MinCPU          AveCPU     MaxRSS     AveRSS\n------------- ---------- --------------- --------------- ---------- ----------\n1234567              512\n1234567.batch        256        00:00:00        00:00:00     25.88M     25.88M\n1234567.0              2        00:00:00        00:00:00      5.05M      5.05M\n1234567.1            512        01:20:02        01:26:19    173.08M    135.27M\n</code></pre> <p>This is again the two node MPI job that we've used in the previous example. We used <code>--units=M</code> to get the memory use per task in megabytes, which is the proper option here as tasks are relatively small (but not uncommonly small for an HPC system when a  properly scaling code is used). The <code>%15</code> is used to specify the width of the field as otherwise some of that information could be truncated (and the width of 15 would have been needed if this were a shared memory program or a program that ran for longer than a day). By default, specifying the field width will right justify the information in  the columns. The <code>%-13</code> tells to use a field width of 13 and to left-justify the data in that column.</p> <p>You can get a list of output fields using <code>sacct -e</code> or <code>sacct --helpformat</code>.  Or check the  \"Job Accounting Fields\" section in the <code>sacct</code> manual page. That page also contains further examples.</p> <p></p> <p>Using <code>sacct</code> is a bit harder if you don't have the job ID of the job for which you want information. You can run <code>sacct</code> without any arguments, and in that case it will produce output for your jobs that  have run since midnight. It is also possible to define the start time (with <code>-S</code> or <code>--starttime</code>) and the end time (with <code>-E</code> or <code>--endtime</code>) of the time window for which job data should be shown, and there are even more features to filter jobs, though some of them are really more useful for  administrators.</p> <p>This is only a very brief introduction to <code>sacct</code>, basically so that you know that it exists and what its main purpose is. But you can find more information in the <code>sacct</code> manual page</p>"},{"location":"intro-evolving/07-Slurm/#the-sreport-command","title":"The <code>sreport</code> command","text":"<p>The <code>sreport</code> command is a command to create summary reports from data in the Slurm accounting database. Its main use is to track consumed resources in a project.</p> <p>On LUMI it is of little use as as the billing is not done by Slurm but by a script that runs outside of Slurm that uses data from the Slurm accounting database. That data is gathered in a different  database though with no direct user access, and only some summary reports are brought back to the system (and used by the <code>lumi-workspaces</code> command and some other tools for user and project monitoring). So the correct billing information is not available in the Slurm accounting database, nor can it be easily derived from data in the summary reports as the billing is more complicated than some billing for individual elements such as core use, memory use and accelerator use. E.g., one can get summary reports mentioning the amount of core hours used per user for a project, but that is reported for all partitions together and hence  irrelevant to get an idea of how the CPU billing units were consumed.</p> <p>This section is mostly to discourage you to use <code>sreport</code> as its information is often misleading and certainly it it is used to follow up your use of billing units on LUMI, but should you insist, there is more information in the <code>sreport</code> manual page.</p>"},{"location":"intro-evolving/07-Slurm/#local-trainings-and-materials","title":"Local trainings and materials","text":"<ul> <li> <p>Docs VSC:</p> <ul> <li> <p>Running jobs in Slurm in the VSC general documentation</p> </li> <li> <p>Slurm in the VUB-specific documentation</p> </li> </ul> </li> <li> <p>Docs C\u00c9CI: Extensive documentation on the use of Slurm</p> </li> <li> <p>VSC training materials</p> <ul> <li> <p>Slurm Lunchbox training KU Leuven</p> </li> <li> <p>VSC@KULeuven HPC-intro training     covers Slurm in the \"Starting to Compute\" section.</p> </li> <li> <p>VSC@UAntwerpen covers Slurm in the \"HPC@UAntwerp introduction\" training</p> </li> </ul> </li> <li> <p>C\u00c9CI training materials: Slurm is covered in the \"Learning how to use HPC infrastructure\" training.</p> <ul> <li>2022 session: Lecture \"Preparing, submitting and managing jobs with Slurm\" \u2192</li> </ul> </li> </ul>"},{"location":"intro-evolving/08-Binding/","title":"Binding","text":"<p>Last update of this page: October 1, 2025</p>"},{"location":"intro-evolving/08-Binding/#process-and-thread-distribution-and-binding","title":"Process and Thread Distribution and Binding","text":""},{"location":"intro-evolving/08-Binding/#what-are-we-talking-about-in-this-session","title":"What are we talking about in this session?","text":"<p>Distribution is the process of distributing processes and threads across the available resources of the job (nodes, sockets, NUMA nodes, cores, ...), and binding is the process of ensuring they stay there as naturally processes and threads are only bound to a node  (OS image) but will migrate between cores. Binding can also ensure that processes cannot use resources they shouldn't use.</p> <p>When running a distributed memory program, the process starter - <code>mpirun</code> or <code>mpiexec</code> on many clusters, or <code>srun</code> on LUMI - will distribute  the processes over the available nodes. Within a node, it is possible to pin or attach processes or even individual threads in processes to one or more cores (actually hardware threads) and other resources,  which is called process binding.</p> <p>The system software (Linux, ROCm\u2122 and Slurm)  has several mechanisms for that. Slurm uses Linux cgroups or control groups to limit the  resources that a job can use within a node and thus to isolate jobs from one another on a node so that one job cannot deplete the resources of another job, and sometimes even uses control groups at the task level to restrict some resources for a task (currently when  doing task-level GPU binding via Slurm). The second mechanism is processor affinity which works at the process and thread level and is used by Slurm at the task level and  can be used by the OpenMP runtime to further limit thread migration. It works through affinity masks which indicate the hardware threads that a thread or process can use. There is also a third mechanism provided by the ROCm\u2122) runtime to control which GPUs can be used.</p> <p>Some of the tools in the <code>lumi-CPEtools</code> module can show the affinity mask for each thread (or effectively the process for single-threaded processes) so you can use these tools to study the affinity masks and check the distribution and binding of processes and threads. The <code>serial_check</code>, <code>omp_check</code>, <code>mpi_check</code> and <code>hybrid_check</code> programs can be used to study thread binding. In fact, <code>hybrid_check</code> can be used in all cases, but the other three show more compact output for serial, shared memory OpenMP and single-threaded MPI processes  respectively. The <code>gpu_check</code> command can be used to study the steps in GPU binding. From version 1.2 onwards, the module also contains the <code>hpcat</code> command, which stands for HPC Affinity Tracker. This tool can only be compiled with Cray MPICH, but it shows core affinity, GPUs used and, when running on more than one node, the network adapter used by each MPI rank. It shows the NUMA node for all these resources so that it is easy to check if the binding is OK.</p> Credits for these programs (click to expand) <p>The <code>hybrid_check</code> program and its derivatives <code>serial_check</code>, <code>omp_check</code> and <code>mpi_check</code> are similar to the <code>xthi</code> program used in the 4-day comprehensive LUMI course organised by the LUST in collaboration with  HPE Cray and AMD. Its main source of inspiration is a very similar program, <code>acheck</code>, written by Harvey Richardson of HPE Cray and used in an earlier course, but it is a complete rewrite of that application.</p> <p>One of the advantages of <code>hybrid_check</code> and its derivatives is that the output is  sorted internally already and hence is more readable. The tool also has various extensions, e.g., putting some load on the CPU cores so that you can in some cases demonstrate thread migration as the Linux scheduler tries to distribute the load in a good way.</p> <p>The <code>gpu_check</code> program builds upon the  <code>hello_jobstep</code> program from ORNL with several extensions implemented by the LUST.</p> <p>(ORNL is the national lab that operates Frontier, an exascale supercomputer based on the same node type as LUMI-G.)</p> <p>The HPC Affinity Tracker tool is developed by HPE and made  publicly available in GitHub.</p> <p></p> <p>In this section we will consider process and thread distribution and binding at several levels:</p> <ul> <li> <p>When creating an allocation, Slurm will already reserve resources at the node level, but this     has been discussed already in the Slurm session of the course.</p> <p>It will also already employ control groups to restrict the access to those resources on a per-node per-job basis.</p> </li> <li> <p>When creating a job step, Slurm will distribute the tasks over the available resources,     bind each task to CPUs allocated to that task, and depending on how the job step was started, bind each task to the GPUs allocated to the task or to the subset of the     GPUs available to the job step on the node the task is running on.</p> </li> <li> <p>With Cray MPICH, you can change the binding between MPI ranks and Slurm tasks. Normally MPI rank i     would be assigned to task i in the job step, but sometimes there are reasons to change this.     The mapping options offered by Cray MPICH are more powerful than what can be obtained with the      options to change the task distribution in Slurm.</p> </li> <li> <p>The OpenMP runtime also uses library calls and environment variables to redistribute and pin threads     within the subset of hardware threads available to the process. Note that different compilers     use different OpenMP runtimes so the default behaviour will not be the same for all compilers,     and on LUMI is different for the Cray compiler compared to the GNU and AMD compilers.</p> </li> <li> <p>Finally, the ROCm\u2122 runtime also can limit the use of GPUs by a process to a subset of the ones that Slurm allocated      to the task through the use of the <code>ROCR_VISIBLE_DEVICES</code> environment variable.</p> </li> </ul> <p>Binding almost only makes sense on job-exclusive nodes as only then you have full control over all available  resources. On \"allocatable by resources\" partitions  you usually do not know which resources are available. The advanced Slurm binding options that we will discuss do not work in those cases, and the options offered by the MPICH, OpenMP and ROCm\u2122 runtimes may work very unpredictable, though OpenMP thread binding may still  help a bit with performance in some cases.</p> <p>Warning</p> <p>Note also that some <code>srun</code> options that we have seen (sometimes already given at the <code>sbatch</code> or <code>salloc</code> level but picket up by <code>srun</code>) already do a simple binding, so those options cannot be combined with the options that we will discuss in this session. This is the case for <code>--cpus-per-task</code>, <code>--gpus-per-task</code> and <code>--ntasks-per-gpu</code>.  In fact, the latter two options will also change the numbering of the GPUs visible to the ROCm runtime, so  using <code>ROCR_VISIBLE_DEVICES</code> may also lead to surprises!</p>"},{"location":"intro-evolving/08-Binding/#why-do-i-need-this","title":"Why do I need this?","text":"<p>As we have seen in the \"LUMI Architecture\" session of this course and is discussed into even more detail in some other courses lectures in Belgium (in particular the \"Supercomputers for Starters\" course  given twice a year at VSC@UAntwerpen), modern supercomputer nodes have increasingly a very hierarchical architecture.  This hierarchical architecture is extremely pronounced on the AMD EPYC architecture used in LUMI but is also increasingly showing up with Intel processors and the ARM server processors, and is also relevant but often ignored in GPU clusters.</p> <p>A proper binding of resources to the application is becoming more and more essential for good performance and  scalability on supercomputers. </p> <ul> <li> <p>Memory locality is very important, and even if an application would be written to take the NUMA character     properly into account at the thread level, a bad mapping of these threads to the cores may result into threads     having to access memory that is far away (with the worst case on a different socket) extensively.</p> <p>Memory locality at the process level is easy as usually processes share little or no memory. So if you would have an MPI application where each rank needs 14 GB of memory and so only 16 ranks can run on a regular node, then it is essential to ensure that these ranks are spread out nicely over the whole node, with one rank per CCD. The default of  Slurm when allocating 16 single-thread tasks on a node would be to put them all on the first two CCDs, so the first NUMA-domain, which would give very poor performance as a lot of memory accesses would have to go across sockets.</p> </li> <li> <p>If threads in a process don't have sufficient memory locality it may be very important to run all threads      in as few L3 cache domains as possible, ideally just one, as otherwise you risk having a lot of conflicts     between the different L3 caches that require resolution and can slow down the process a lot.</p> <p>This already shows that there is no single works-for-all solution, because if those threads would use all memory on a  node and each have good memory locality then it would be better to spread them out as much possible. You really need to understand your application to do proper resource mapping, and the fact that it can be so application-dependent is  also why Slurm and the various runtimes cannot take care of it automatically.</p> </li> <li> <p>In some cases it is important on the GPU nodes to ensure that tasks are nicely spread out over CCDs with each task     using the GPU (GCD) that is closest to the CCD the task is running on. This is certainly the case if the application     would rely on cache-coherent access to GPU memory from the CPU.</p> </li> <li> <p>With careful mapping of MPI ranks on nodes you can often reduce the amount of inter-node data transfer in favour of the     faster intra-node transfers. This requires some understanding of the communication pattern of your MPI application,     but there are profiling tools for that that are discussed in the advanced courses.</p> </li> <li> <p>For GPU-aware MPI: Check if the intra-node communication pattern can map onto the links between the GCDs.</p> </li> </ul>"},{"location":"intro-evolving/08-Binding/#core-numbering","title":"Core numbering","text":"<p>Linux core numbering is not hierarchical and may look a bit strange. This is because Linux core numbering was fixed before hardware threads were added, and later on hardware threads were simply added to the numbering scheme.</p> <p>As is usual with computers, numbering starts from 0. Core 0 is the first hardware thread (or we could say the actual core) of the first core of the first CCD (CCD 0) of the first NUMA domain (NUMA domain 0) of the first socket (socket 0). Core 1 is then the first hardware thread of the second core of the same CCD, and so on, going over all cores in a CCD, then NUMA domain and then socket. So on LUMI-C, core 0 till 63 are on the first socket and core 64 till 127 on the second one. The numbering of the second hardware thread of each core - we could say the virtual core - then starts where the numbering of the actual cores ends, so 64 for LUMI-G (which has only one socket per node) or 128 for LUMI-C. This has the advantage that if hardware threading is turned off at the BIOS/UEFI level, the numbering of the actual  cores does not change. </p> <p>On LUMI G, the first core of each CCD, so core 0, 8, ... and the corresponding second hardware threads 64, 72, ...,  is not available for user threads. Basically, physical core 0 had to be reserved for OS processes  to help reduce OS jitter which can kill scalability of large parallel applications.  But as only reserving physical core 0 creates an asymmetry in the node which is difficult to deal with for users, the decision was made to - similar as on Frontier - reserve the first core of each CCD. Don't be surprised if when running a GPU code you see a lot of activity on core 0. It is caused by the ROCm\u2122) driver and often also by Lustre processes, and is precisely the reason why that core is reserved, as that activity would break scalability of applications that expect to have the same amount of available compute power on each core.</p> <p>Note that even with <code>--hint=nomultithread</code> the hardware threads will still be turned on at the hardware level and be visible in the  OS (e.g., in <code>/proc/cpuinfo</code>). In fact, the batch job step will use them, but they will not be used by applications in job steps started with subsequent <code>srun</code> commands.</p> Slurm under-the-hoods example (click to expand) <p>We will use the Linux <code>lstopo</code> and <code>taskset</code> commands to study how a job step sees the system and how task affinity is used to manage the CPUs for a task. Consider the job script:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=cpu-numbering-demo1\n#SBATCH --output %x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --partition=small\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=16\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeGNU-24.03\n\ncat &lt;&lt; EOF &gt; task_lstopo_$SLURM_JOB_ID\n#!/bin/bash\necho \"Task \\$SLURM_LOCALID\"                            &gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"Output of lstopo:\"                              &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nlstopo -p                                             &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"Taskset of current shell: \\$(taskset -p \\$\\$)\"  &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nEOF\n\nchmod +x ./task_lstopo_$SLURM_JOB_ID\n\necho -e \"\\nFull lstopo output in the job:\\n$(lstopo -p)\\n\\n\"\necho -e \"Taskset of the current shell: $(taskset -p $$)\\n\"\n\necho \"Running two tasks on 4 cores each, extracting parts from lstopo output in each:\"\nsrun -n 2 -c 4 ./task_lstopo_$SLURM_JOB_ID\necho\ncat output-$SLURM_JOB_ID-0\necho\ncat output-$SLURM_JOB_ID-1\n\necho -e \"\\nRunning hybrid_check in the same configuration::\"\nsrun -n 2 -c 4 hybrid_check -r\n\n/bin/rm task_lstopo_$SLURM_JOB_ID output-$SLURM_JOB_ID-0 output-$SLURM_JOB_ID-1\n</code></pre> <p>It creates a small test program that we will use to run lstopo and gather its output on two tasks with 4 cores each. All this is done in a job allocation with 16 cores on the  <code>small</code> partition.</p> <p>The results of this script will differ strongly between runs as Slurm can give different valid configurations for this request. Below is one possible output we got.</p> <p>Let's first look at the output of the <code>lstopo</code> and <code>taskset</code> commands run in the batch job step:</p> <pre><code>Full lstopo output in the job:\nMachine (251GB total)\n  Package P#0\n    Group0\n      NUMANode P#0 (31GB)\n    Group0\n      NUMANode P#1 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 41:00.0 (Ethernet)\n            Net \"nmn0\"\n    Group0\n      NUMANode P#2 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 21:00.0 (Ethernet)\n            Net \"hsn0\"\n    Group0\n      NUMANode P#3 (31GB)\n  Package P#1\n    Group0\n      NUMANode P#4 (31GB)\n    Group0\n      NUMANode P#5 (31GB)\n    Group0\n      NUMANode P#6 (31GB)\n      L3 P#12 (32MB)\n        L2 P#100 (512KB) + L1d P#100 (32KB) + L1i P#100 (32KB) + Core P#36\n          PU P#100\n          PU P#228\n        L2 P#101 (512KB) + L1d P#101 (32KB) + L1i P#101 (32KB) + Core P#37\n          PU P#101\n          PU P#229\n        L2 P#102 (512KB) + L1d P#102 (32KB) + L1i P#102 (32KB) + Core P#38\n          PU P#102\n          PU P#230\n        L2 P#103 (512KB) + L1d P#103 (32KB) + L1i P#103 (32KB) + Core P#39\n          PU P#103\n          PU P#231\n      L3 P#13 (32MB)\n        L2 P#104 (512KB) + L1d P#104 (32KB) + L1i P#104 (32KB) + Core P#40\n          PU P#104\n          PU P#232\n        L2 P#105 (512KB) + L1d P#105 (32KB) + L1i P#105 (32KB) + Core P#41\n          PU P#105\n          PU P#233\n        L2 P#106 (512KB) + L1d P#106 (32KB) + L1i P#106 (32KB) + Core P#42\n          PU P#106\n          PU P#234\n        L2 P#107 (512KB) + L1d P#107 (32KB) + L1i P#107 (32KB) + Core P#43\n          PU P#107\n          PU P#235\n        L2 P#108 (512KB) + L1d P#108 (32KB) + L1i P#108 (32KB) + Core P#44\n          PU P#108\n          PU P#236\n        L2 P#109 (512KB) + L1d P#109 (32KB) + L1i P#109 (32KB) + Core P#45\n          PU P#109\n          PU P#237\n        L2 P#110 (512KB) + L1d P#110 (32KB) + L1i P#110 (32KB) + Core P#46\n          PU P#110\n          PU P#238\n        L2 P#111 (512KB) + L1d P#111 (32KB) + L1i P#111 (32KB) + Core P#47\n          PU P#111\n          PU P#239\n    Group0\n      NUMANode P#7 (31GB)\n      L3 P#14 (32MB)\n        L2 P#112 (512KB) + L1d P#112 (32KB) + L1i P#112 (32KB) + Core P#48\n          PU P#112\n          PU P#240\n        L2 P#113 (512KB) + L1d P#113 (32KB) + L1i P#113 (32KB) + Core P#49\n          PU P#113\n          PU P#241\n        L2 P#114 (512KB) + L1d P#114 (32KB) + L1i P#114 (32KB) + Core P#50\n          PU P#114\n          PU P#242\n        L2 P#115 (512KB) + L1d P#115 (32KB) + L1i P#115 (32KB) + Core P#51\n          PU P#115\n          PU P#243\n\nTaskset of the current shell: pid 81788's current affinity mask: ffff0000000000000000000000000000ffff0000000000000000000000000\n</code></pre> <p>Note the way the cores are represented.  There are 16 lines the lines <code>L2 ... + L1d ... + L1i ... + Core ...</code> that represent the 16 cores requested. We have used the <code>-p</code> option of <code>lstopo</code> to ensure that <code>lstopo</code> would show us the physical number as seen by the bare OS. The numbers indicated after each core are within the socket but the number indicated right after <code>L2</code> is the global core numbering within the node as seen by the bare OS. The two <code>PU</code> lines (Processing Unit) after each core are correspond to the  hardware threads and are also the numbers as seen by the bare OS.</p> <p>We see that in this allocation the cores are not spread over the minimal number of L3 cache domains that would be possible, but across three domains. In this particular allocation the cores are still consecutive cores, but even that is not guaranteed in an \"Allocatable by resources\" partition. Despite <code>--hint=nomultithread</code> being the default behaviour, at this level we still see both hardware threads for each physical core in the taskset. </p> <p>Next look at the output printed by lines 29 and 31:</p> <pre><code>Task 0\nOutput of lstopo:\nMachine (251GB total)\n  Package P#0\n    Group0\n      NUMANode P#0 (31GB)\n    Group0\n      NUMANode P#1 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 41:00.0 (Ethernet)\n            Net \"nmn0\"\n    Group0\n      NUMANode P#2 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 21:00.0 (Ethernet)\n            Net \"hsn0\"\n    Group0\n      NUMANode P#3 (31GB)\n  Package P#1\n    Group0\n      NUMANode P#4 (31GB)\n    Group0\n      NUMANode P#5 (31GB)\n    Group0\n      NUMANode P#6 (31GB)\n      L3 P#12 (32MB)\n        L2 P#100 (512KB) + L1d P#100 (32KB) + L1i P#100 (32KB) + Core P#36\n          PU P#100\n          PU P#228\n        L2 P#101 (512KB) + L1d P#101 (32KB) + L1i P#101 (32KB) + Core P#37\n          PU P#101\n          PU P#229\n        L2 P#102 (512KB) + L1d P#102 (32KB) + L1i P#102 (32KB) + Core P#38\n          PU P#102\n          PU P#230\n        L2 P#103 (512KB) + L1d P#103 (32KB) + L1i P#103 (32KB) + Core P#39\n          PU P#103\n          PU P#231\n      L3 P#13 (32MB)\n        L2 P#104 (512KB) + L1d P#104 (32KB) + L1i P#104 (32KB) + Core P#40\n          PU P#104\n          PU P#232\n        L2 P#105 (512KB) + L1d P#105 (32KB) + L1i P#105 (32KB) + Core P#41\n          PU P#105\n          PU P#233\n        L2 P#106 (512KB) + L1d P#106 (32KB) + L1i P#106 (32KB) + Core P#42\n          PU P#106\n          PU P#234\n        L2 P#107 (512KB) + L1d P#107 (32KB) + L1i P#107 (32KB) + Core P#43\n          PU P#107\n          PU P#235\n    Group0\n      NUMANode P#7 (31GB)\nTaskset of current shell: pid 82340's current affinity mask: f0000000000000000000000000\n\nTask 1\nOutput of lstopo:\nMachine (251GB total)\n  Package P#0\n    Group0\n      NUMANode P#0 (31GB)\n    Group0\n      NUMANode P#1 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 41:00.0 (Ethernet)\n            Net \"nmn0\"\n    Group0\n      NUMANode P#2 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 21:00.0 (Ethernet)\n            Net \"hsn0\"\n    Group0\n      NUMANode P#3 (31GB)\n  Package P#1\n    Group0\n      NUMANode P#4 (31GB)\n    Group0\n      NUMANode P#5 (31GB)\n    Group0\n      NUMANode P#6 (31GB)\n      L3 P#12 (32MB)\n        L2 P#100 (512KB) + L1d P#100 (32KB) + L1i P#100 (32KB) + Core P#36\n          PU P#100\n          PU P#228\n        L2 P#101 (512KB) + L1d P#101 (32KB) + L1i P#101 (32KB) + Core P#37\n          PU P#101\n          PU P#229\n        L2 P#102 (512KB) + L1d P#102 (32KB) + L1i P#102 (32KB) + Core P#38\n          PU P#102\n          PU P#230\n        L2 P#103 (512KB) + L1d P#103 (32KB) + L1i P#103 (32KB) + Core P#39\n          PU P#103\n          PU P#231\n      L3 P#13 (32MB)\n        L2 P#104 (512KB) + L1d P#104 (32KB) + L1i P#104 (32KB) + Core P#40\n          PU P#104\n          PU P#232\n        L2 P#105 (512KB) + L1d P#105 (32KB) + L1i P#105 (32KB) + Core P#41\n          PU P#105\n          PU P#233\n        L2 P#106 (512KB) + L1d P#106 (32KB) + L1i P#106 (32KB) + Core P#42\n          PU P#106\n          PU P#234\n        L2 P#107 (512KB) + L1d P#107 (32KB) + L1i P#107 (32KB) + Core P#43\n          PU P#107\n          PU P#235\n    Group0\n      NUMANode P#7 (31GB)\nTaskset of current shell: pid 82341's current affinity mask: f00000000000000000000000000\n</code></pre> <p>The output of <code>lstopo -p</code> is the same for both: we get the same 8 cores. This is because all cores for all tasks on a node are gathered in a single control group. Instead,  affinity masks are used to ensure that both tasks of 4 threads are scheduled on different cores. If we have a look at booth taskset lines:</p> <pre><code>Taskset of current shell: pid 82340's current affinity mask: 0f0000000000000000000000000\nTaskset of current shell: pid 82341's current affinity mask: f00000000000000000000000000\n</code></pre> <p>we see that they are indeed different (a zero was added to the front of the first to make the difference clearer). The first task got cores 100 till 103 and the second task got cores 104 till 107. This also shows an important property: Tasksets are defined based on the bare OS numbering of the cores, not based on a numbering relative to the control group, with cores numbered from 0 to 15 in this example. It also implies that it is not possible to set a taskset manually without knowing which physical cores can be used!</p> <p>The output of the <code>srun</code> command on line 34 confirms this:</p> <pre><code>Running 2 MPI ranks with 4 threads each (total number of threads: 8).\n\n++ hybrid_check: MPI rank   0/2   OpenMP thread   0/4   on cpu 101/256 of nid002040 mask 100-103\n++ hybrid_check: MPI rank   0/2   OpenMP thread   1/4   on cpu 102/256 of nid002040 mask 100-103\n++ hybrid_check: MPI rank   0/2   OpenMP thread   2/4   on cpu 103/256 of nid002040 mask 100-103\n++ hybrid_check: MPI rank   0/2   OpenMP thread   3/4   on cpu 100/256 of nid002040 mask 100-103\n++ hybrid_check: MPI rank   1/2   OpenMP thread   0/4   on cpu 106/256 of nid002040 mask 104-107\n++ hybrid_check: MPI rank   1/2   OpenMP thread   1/4   on cpu 107/256 of nid002040 mask 104-107\n++ hybrid_check: MPI rank   1/2   OpenMP thread   2/4   on cpu 104/256 of nid002040 mask 104-107\n++ hybrid_check: MPI rank   1/2   OpenMP thread   3/4   on cpu 105/256 of nid002040 mask 104-107\n</code></pre> <p>Note however that this output will depend on the compiler used to compile <code>hybrid_check</code>. The Cray compiler will produce different output as it has a different default strategy for OpenMP threads  and will by default pin each thread to a different hardware thread if possible.</p>"},{"location":"intro-evolving/08-Binding/#gpu-numbering","title":"GPU numbering","text":"<p>The numbering of the GPUs is a very tricky thing on LUMI.</p> <p>The only way to reliably identify the physical GPU is through the PCIe bus ID. This does not change over time  or in an allocation where access to some resources is limited through cgroups. It is the same on all nodes.</p> <p>Based on these PICe bus IDs, the OS will assign numbers to the GPU. It are those numbers that are shown in the figure in the  Architecture chapter - \"Building LUMI: What a LUMI-G node really looks like\". We will call this the bare OS numbering or global numbering in these notes.</p> <p>Slurm manages GPUs for jobs through the control group mechanism. Now if a job requesting 4 GPUs would get the GPUs that are numbered 4 to 7 in bare OS numbering,  it would still see them as GPUs 0 to 3, and this is the numbering that one would have to use for the <code>ROCR_VISIBLE_DEVICES</code> environment variable that is used to further limit the GPUs that the ROCm runtime will use in an application. We will call this the job-local numbering.</p> <p>Inside task of a regular job step, Slurm can further restrict the GPUs that are visible through control groups at the task level, leading to yet another numbering that starts from 0 which we will call the  task-local numbering.  That kind of control group should be avoided though at any price as it stops direct  communication between GPUs with MPI or RCCL (a ROCm communication library popular in  AI applications).</p> <p>Note also that Slurm does take care of setting the <code>ROCR_VISIBLE_DEVICES</code> environment variable. It will be set at the start of a batch job step giving access to all GPUs that are available in the allocation, and will also be set by <code>srun</code> for each task. But you don't need to know in your application which numbers these are as, e.g., the HIP runtime will number the GPUs that are available from 0 on.</p> <p></p> A more technical example demonstrating what Slurm does (click to expand) <p>We will use the Linux <code>lstopo</code>command and the <code>ROCR_VISIBLE_DEVICES</code> environment variable to study how a job step sees the system and how task affinity is used to manage the CPUs for a task.  Consider the job script:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=gpu-numbering-demo1\n#SBATCH --output %x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --partition=standard-g\n#SBATCH --nodes=1\n#SBATCH --hint=nomultithread\n#SBATCH --time=15:00\n\nmodule load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeCray-24.03\n\ncat &lt;&lt; EOF &gt; task_lstopo_$SLURM_JOB_ID\n#!/bin/bash\necho \"Task \\$SLURM_LOCALID\"                                                   &gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"Relevant lines of lstopo:\"                                             &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nlstopo -p | awk '/ PCI.*Display/ || /GPU/ || / Core / || /PU L/ {print \\$0}' &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"ROCR_VISIBLE_DEVICES: \\$ROCR_VISIBLE_DEVICES\"                          &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nEOF\nchmod +x ./task_lstopo_$SLURM_JOB_ID\n\necho -e \"\\nFull lstopo output in the job:\\n$(lstopo -p)\\n\\n\"\necho -e \"Extract GPU info:\\n$(lstopo -p | awk '/ PCI.*Display/ || /GPU/ {print $0}')\\n\" \necho \"ROCR_VISIBLE_DEVICES at the start of the job script: $ROCR_VISIBLE_DEVICES\"\n\necho \"Running two tasks with 4 GPUs each, extracting parts from lstopo output in each:\"\nsrun -n 2 -c 1 --gpus-per-task=4 ./task_lstopo_$SLURM_JOB_ID\necho\ncat output-$SLURM_JOB_ID-0\necho\ncat output-$SLURM_JOB_ID-1\n\necho -e \"\\nRunning gpu_check in the same configuration::\"\nsrun -n 2 -c 1 --gpus-per-task=4 gpu_check -l\n\n/bin/rm task_lstopo_$SLURM_JOB_ID output-$SLURM_JOB_ID-0 output-$SLURM_JOB_ID-1\n</code></pre> <p>It creates a small test program that is run on two tasks and records some information on the system. The output is not sent to the screen directly as it could end up mixed between the tasks which is far  from ideal. </p> <p>Let's first have a look at the first lines of the <code>lstopo -p</code> output:</p> <pre><code>Full lstopo output in the job:\nMachine (503GB total) + Package P#0\n  Group0\n    NUMANode P#0 (125GB)\n    L3 P#0 (32MB)\n      L2 P#1 (512KB) + L1d P#1 (32KB) + L1i P#1 (32KB) + Core P#1\n        PU P#1\n        PU P#65\n      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2\n        PU P#2\n        PU P#66\n      L2 P#3 (512KB) + L1d P#3 (32KB) + L1i P#3 (32KB) + Core P#3\n        PU P#3\n        PU P#67\n      L2 P#4 (512KB) + L1d P#4 (32KB) + L1i P#4 (32KB) + Core P#4\n        PU P#4\n        PU P#68\n      L2 P#5 (512KB) + L1d P#5 (32KB) + L1i P#5 (32KB) + Core P#5\n        PU P#5\n        PU P#69\n      L2 P#6 (512KB) + L1d P#6 (32KB) + L1i P#6 (32KB) + Core P#6\n        PU P#6\n        PU P#70\n      L2 P#7 (512KB) + L1d P#7 (32KB) + L1i P#7 (32KB) + Core P#7\n        PU P#7\n        PU P#71\n      HostBridge\n        PCIBridge\n          PCI d1:00.0 (Display)\n            GPU(RSMI) \"rsmi4\"\n    L3 P#1 (32MB)\n      L2 P#9 (512KB) + L1d P#9 (32KB) + L1i P#9 (32KB) + Core P#9\n        PU P#9\n        PU P#73\n      L2 P#10 (512KB) + L1d P#10 (32KB) + L1i P#10 (32KB) + Core P#10\n        PU P#10\n        PU P#74\n      L2 P#11 (512KB) + L1d P#11 (32KB) + L1i P#11 (32KB) + Core P#11\n        PU P#11\n        PU P#75\n      L2 P#12 (512KB) + L1d P#12 (32KB) + L1i P#12 (32KB) + Core P#12\n        PU P#12\n        PU P#76\n      L2 P#13 (512KB) + L1d P#13 (32KB) + L1i P#13 (32KB) + Core P#13\n        PU P#13\n        PU P#77\n      L2 P#14 (512KB) + L1d P#14 (32KB) + L1i P#14 (32KB) + Core P#14\n        PU P#14\n        PU P#78\n      L2 P#15 (512KB) + L1d P#15 (32KB) + L1i P#15 (32KB) + Core P#15\n        PU P#15\n        PU P#79\n      HostBridge\n        PCIBridge\n          PCI d5:00.0 (Ethernet)\n            Net \"hsn2\"\n        PCIBridge\n          PCI d6:00.0 (Display)\n            GPU(RSMI) \"rsmi5\"\n    HostBridge\n      PCIBridge\n        PCI 91:00.0 (Ethernet)\n          Net \"nmn0\"\n...\n</code></pre> <p>We see only 7 cores in the each block (the lines <code>L2 ... + L1d ... + L1i ... + Core ...</code>) because the first physical core on each CCD is reserved for the OS. </p> <p>The <code>lstopo -p</code> output also clearly suggests that each GCD has a special link to a particular CCD</p> <p>Next check the output generated by lines 22 and 23 where we select the lines that show information about the GPUs and print some more information:</p> <pre><code>Extract GPU info:\n          PCI d1:00.0 (Display)\n            GPU(RSMI) \"rsmi4\"\n          PCI d6:00.0 (Display)\n            GPU(RSMI) \"rsmi5\"\n          PCI c9:00.0 (Display)\n            GPU(RSMI) \"rsmi2\"\n          PCI ce:00.0 (Display)\n            GPU(RSMI) \"rsmi3\"\n          PCI d9:00.0 (Display)\n            GPU(RSMI) \"rsmi6\"\n          PCI de:00.0 (Display)\n            GPU(RSMI) \"rsmi7\"\n          PCI c1:00.0 (Display)\n            GPU(RSMI) \"rsmi0\"\n          PCI c6:00.0 (Display)\n            GPU(RSMI) \"rsmi1\"\n\nROCR_VISIBLE_DEVICES at the start of the job script: 0,1,2,3,4,5,6,7\n</code></pre> <p>All 8 GPUs are visible and note the numbering on each line below the line with the PCIe bus ID.  We also notice that <code>ROCR_VISIBLE_DEVICES</code> was set by Slurm and includes all 8 GPUs.</p> <p>Next we run two tasks requesting 4 GPUs and a single core without hardware threading each.  The output of those two tasks is gathered in files that are then sent to the standard  output in lines 28 and 30:</p> <pre><code>Task 0\nRelevant lines of lstopo:\n      L2 P#1 (512KB) + L1d P#1 (32KB) + L1i P#1 (32KB) + Core P#1\n      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2\n          PCI d1:00.0 (Display)\n          PCI d6:00.0 (Display)\n          PCI c9:00.0 (Display)\n            GPU(RSMI) \"rsmi2\"\n          PCI ce:00.0 (Display)\n            GPU(RSMI) \"rsmi3\"\n          PCI d9:00.0 (Display)\n          PCI de:00.0 (Display)\n          PCI c1:00.0 (Display)\n            GPU(RSMI) \"rsmi0\"\n          PCI c6:00.0 (Display)\n            GPU(RSMI) \"rsmi1\"\nROCR_VISIBLE_DEVICES: 0,1,2,3\n\nTask 1\nRelevant lines of lstopo:\n      L2 P#1 (512KB) + L1d P#1 (32KB) + L1i P#1 (32KB) + Core P#1\n      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2\n          PCI d1:00.0 (Display)\n            GPU(RSMI) \"rsmi0\"\n          PCI d6:00.0 (Display)\n            GPU(RSMI) \"rsmi1\"\n          PCI c9:00.0 (Display)\n          PCI ce:00.0 (Display)\n          PCI d9:00.0 (Display)\n            GPU(RSMI) \"rsmi2\"\n          PCI de:00.0 (Display)\n            GPU(RSMI) \"rsmi3\"\n          PCI c1:00.0 (Display)\n          PCI c6:00.0 (Display)\nROCR_VISIBLE_DEVICES: 0,1,2,3\n</code></pre> <p>Each task sees GPUs named 'rsmi0' till 'rsmi3', but look better and you see that these are not the same. If you compare with the first output of <code>lstopo</code> which we ran in the batch job step, we notice that task 0 gets the first 4 GPUs in the node while task 1 gets the next 4, that were named <code>rsmi4</code> till <code>rsmi7</code> before.  The other 4 GPUs are invisible in each of the tasks. Note also that in both tasks  <code>ROCR_VISIBLE_DEVICES</code> has the same value <code>0,1,2,3</code> as the numbers detected by <code>lstopo</code> in that task are used. </p> <p>The <code>lstopo</code> command does see two cores though for each task (but they are the same) because the cores are not isolated by cgroups on a per-task level, but on a per-job level.</p> <p>Finally we have the output of the <code>gpu_check</code> command run in the same configuration. The <code>-l</code> option that was used prints some extra information that makes it easier to check the mapping: For the hardware threads it shows the CCD and for each GPU it shows the GCD number based on the physical order of the GPUs and the corresponding CCD that should be used for best performance:</p> <pre><code>MPI 000 - OMP 000 - HWT 001 (CCD0) - Node nid005163 - RT_GPU_ID 0,1,2,3 - GPU_ID 0,1,2,3 - Bus_ID c1(GCD0/CCD6),c6(GCD1/CCD7),c9(GCD2/CCD2),cc(GCD3/CCD3)\nMPI 001 - OMP 000 - HWT 002 (CCD0) - Node nid005163 - RT_GPU_ID 0,1,2,3 - GPU_ID 0,1,2,3 - Bus_ID d1(GCD4/CCD0),d6(GCD5/CCD1),d9(GCD6/CCD4),dc(GCD7/CCD5)\n</code></pre> <p><code>RT_GPU_ID</code> is the numbering of devices used in the program itself, <code>GPU_ID</code> is essentially the value of <code>ROCR_VISIBLE_DEVICES</code>, the logical numbers of the GPUs in the control group and <code>Bus_ID</code> shows the relevant part of the PCIe bus ID.</p> <p>The above example is very technical and not suited for every reader. One important conclusion though that is of use when running on LUMI is that Slurm works differently with CPUs and GPUs on LUMI.  Cores and GPUs are treated differently. Core access is controlled by control groups at the job step level on each node and at the task level by affinity masks.  The equivalent for GPUs would be to also use control groups at the job step level and then <code>ROCR_VISIBLE_DEVICES</code> to further set access to GPUs for each task, but this is not what  is currently happening in Slurm on LUMI. Instead it is using control groups at the  task level. </p> Playing with control group and <code>ROCR_VISIBLE_DEVICES</code> (click to expand) <p>Consider the following (tricky and maybe not very realistic) job script.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=gpu-numbering-demo2\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard-g\n#SBATCH --nodes=1\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeCray-24.03\n\ncat &lt;&lt; EOF &gt; select_1gpu_$SLURM_JOB_ID\n#!/bin/bash\nexport ROCR_VISIBLE_DEVICES=\\$SLURM_LOCALID\nexec \\$*\nEOF\nchmod +x ./select_1gpu_$SLURM_JOB_ID\n\ncat &lt;&lt; EOF &gt; task_lstopo_$SLURM_JOB_ID\n#!/bin/bash\nsleep \\$((SLURM_LOCALID * 5))\necho \"Task \\$SLURM_LOCALID\"                                                   &gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"Relevant lines of lstopo:\"                                             &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nlstopo -p | awk '/ PCI.*Display/ || /GPU/ || / Core / || /PU L/ {print \\$0}' &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"ROCR_VISIBLE_DEVICES: \\$ROCR_VISIBLE_DEVICES\"                          &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nEOF\nchmod +x ./task_lstopo_$SLURM_JOB_ID\n\n# Start a background task to pick GPUs with global numbers 0 and 1\nsrun -n 1 -c 1 --gpus=2 sleep 60 &amp;\nsleep 5\n\nset -x\nsrun -n 4 -c 1 --gpus=4 ./task_lstopo_$SLURM_JOB_ID\nset +x\n\ncat output-$SLURM_JOB_ID-0\n\nset -x\nsrun -n 4 -c 1 --gpus=4 ./select_1gpu_$SLURM_JOB_ID gpu_check -l\nset +x\n\nwait\n\n/bin/rm select_1gpu_$SLURM_JOB_ID task_lstopo_$SLURM_JOB_ID output-$SLURM_JOB_ID-*\n</code></pre> <p>We create two small programs that we will use in here. The first one is used to set <code>ROCR_VISIBLE_DEVICES</code> to the value of <code>SLURM_LOCALID</code> which is the local task number within a node of a Slurm task (so always numbered starting from 0 per node). We will use this to tell the <code>gpu_check</code> program that we will run which GPU should be used by which task. The second program is one we have seen before already and just shows some relevant output of <code>lstopo</code> to see which GPUs are in principle available to the task and then also prints the value of <code>ROCR_VISIBLE_DEVICES</code>. We did have to put in some task-dependent delay  as it turns out that running multiple <code>lstopo</code> commands on a node together can cause problems.</p> <p>The tricky bit is line 29. Here we start an <code>srun</code> command on the background that steals two GPUs. In this way, we ensure that the next <code>srun</code> command will not be able to get the GCDs 0 and 1 from the regular full-node numbering. The delay is again to ensure that the next <code>srun</code> works without conflicts as internally Slurm is still finishing steps from the first <code>srun</code>.</p> <p>On line 33 we run our command that extracts info from <code>lstopo</code>. As we already know from the more technical example above the output will be the same for each task so in line 36 we only look at the output of the first task:</p> <pre><code>Relevant lines of lstopo:\n      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2\n      L2 P#3 (512KB) + L1d P#3 (32KB) + L1i P#3 (32KB) + Core P#3\n      L2 P#4 (512KB) + L1d P#4 (32KB) + L1i P#4 (32KB) + Core P#4\n      L2 P#5 (512KB) + L1d P#5 (32KB) + L1i P#5 (32KB) + Core P#5\n          PCI d1:00.0 (Display)\n            GPU(RSMI) \"rsmi2\"\n          PCI d6:00.0 (Display)\n            GPU(RSMI) \"rsmi3\"\n          PCI c9:00.0 (Display)\n            GPU(RSMI) \"rsmi0\"\n          PCI ce:00.0 (Display)\n            GPU(RSMI) \"rsmi1\"\n          PCI d9:00.0 (Display)\n          PCI de:00.0 (Display)\n          PCI c1:00.0 (Display)\n          PCI c6:00.0 (Display)\nROCR_VISIBLE_DEVICES: 0,1,2,3\n</code></pre> <p>If you'd compare with output from a full-node <code>lstopo -p</code> shown in the previous example, you'd see that we actually got the GPUs with regular full node numbering 2 till 5, but they have been renumbered from  0 to 3. And notice that <code>ROCR_VISIBLE_DEVICES</code> now also refers to this numbering and not the  regular full node numbering when setting which GPUs can be used. </p> <p>The <code>srun</code> command on line 40 will now run <code>gpu_check</code> through the <code>select_1gpu_$SLURM_JOB_ID</code> wrapper that gives task 0 access to GPU 0 in the \"local\" numbering, which should be GPU2/CCD2 in the regular full node numbering, etc. Its output is</p> <pre><code>MPI 000 - OMP 000 - HWT 002 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)\nMPI 001 - OMP 000 - HWT 003 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID ce(GCD3/CCD3)\nMPI 002 - OMP 000 - HWT 004 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID d1(GCD4/CCD0)\nMPI 003 - OMP 000 - HWT 005 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID d6(GCD5/CCD1)\n</code></pre> <p>which confirms that out strategy worked. So in this example we have 4 tasks running in a control group that in principle gives each task access to all 4 GPUs, but with actual access further restricted to a different GPU per task via <code>ROCR_VISIBLE_DEVICES</code>.</p> <p>This again rather technical example demonstrates another difference between the way one works with  CPUs and with GPUs. Affinity masks for CPUs refer to the \"bare OS\" numbering of hardware threads, while the numbering used for <code>ROCR_VISIBLE_DEVICES</code> which determines which GPUs the ROCm runtime can use, uses the numbering within the current control group.</p> <p>Running GPUs in a different control group per task has consequences for the way inter-GPU communication within a node can be organised so the above examples are important. It is essential to run MPI applications with optimal efficiency.</p>"},{"location":"intro-evolving/08-Binding/#task-distribution-with-slurm","title":"Task distribution with Slurm","text":"<p>The Slurm <code>srun</code> command offers the <code>--distribution</code> option to influence the distribution of  tasks across nodes (level 1), sockets or NUMA domains (level 2 and sockets or NUMA) or  even across cores in the socket or NUMA domain (third level). The first level is the most useful level, the second level is sometimes used but the third level is very tricky and both the second and third level are often better replaced with other mechanisms that will also be discussed in this chapter on distribution and binding.</p> <p>The general form of the <code>--distribution</code> option is </p> <pre><code>--distribution={*|block|cyclic|arbitrary|plane=&lt;size&gt;}[:{*|block|cyclic|fcyclic}[:{*|block|cyclic|fcyclic}]][,{Pack|NoPack}]\n</code></pre> <ul> <li> <p>Level 1: Distribution across nodes. There are three useful options for LUMI:</p> <ul> <li> <p><code>block</code> which is the default: A number of consecutive tasks is allocated on the first     node, then another number of consecutive tasks on the second node, and so on till the last     node of the allocation. Not all nodes may have the same number of tasks and this is determined     by the optional  <code>pack</code> or <code>nopack</code> parameter at the end.</p> <ul> <li> <p>With <code>pack</code> the first node in the allocation is first filled up as much as possible, then the     second node, etc.</p> <p>E.g., with 10 quarter node sized tasks (32 cores on LUMI-C) spread across 3 nodes, this option would put tasks 0, 1, 2 and 3 on the first node, tasks 4, 5, 6 and 7 on  the second node, and tasks 8 and 9 on the third (tasks are numbered from 0).</p> </li> <li> <p>With <code>nopack</code> a more balanced approach is taken filling up all nodes as equally as possible.     In fact, the number of tasks on each node will correspond to that of the <code>cyclic</code> distribution,     but the task numbers will be different.</p> <p>E.g., with 10 quarter node tasks (32 cores on LUMI-C) spread across 3 nodes, this option would put tasks 0, 1, 2 and 3 on the first node, 4, 5 and 6 on the second and  7, 8 and 9 on the third.</p> </li> </ul> </li> <li> <p><code>cyclic</code> assigns the tasks in a round-robin fashion to the nodes of the allocation. The first task     is allocated to the first node, then the second one to the second node, and so on, and when all nodes     of the allocation have received one task, the next one will be allocated again on the first node.</p> <p>E.g., with 10 quarter node tasks (32 cores on LUMI-C) spread across 3 nodes, task 0 would be put on the first node, task 1 on the second, task 2 on the third,  task 3 again on the first node, etc., resulting in tasks 0, 3, 6 and 9 on the first node, tasks 1, 4 and 7 on the second node and tasks 2, 5 and 8 on the third node.</p> </li> <li> <p><code>plane=&lt;size&gt;</code> is a combination of both of the former methods: Blocks of <code>&lt;size&gt;</code> consecutive tasks     are allocated in a cyclic way. </p> <p>E.g., with 10 quarter node tasks (32 cores on LUMI-C) spread across 3 nodes, task 0 and 1 would be put on the first node, then tasks 2 and 3 on the second, tasks 4 and 5 on the third, tasks 6 and 7 again on the first and finally tasks 8 and 9 on the second node, resulting in tasks 0, 1, 6 and 7 on the first node, tasks 2, 3, 8 and 9 on the second node and tasks 4 and 5 on the third node.</p> </li> </ul> </li> <li> <p>Level 2: Here we are distributing and pinning the tasks assigned to a node at level 1 across the sockets     and cores of that node.</p> <p>As this option already does a form of binding, it may conflict with other options that we will discuss later that also perform binding. In practice, this second level is less useful as often other mechanisms will be  preferred for doing a proper binding, or the default behaviour is OK for simple distribution problems.</p> <ul> <li> <p><code>block</code> will assign whole tasks to consecutive sets of cores on the node. On LUMI-C, it will first fill up     the first socket before moving on to the second socket.</p> </li> <li> <p><code>cyclic</code> assigns the first task of a node to a set of consecutive cores on the first socket, then the second task to a set      of cores on the second socket, etc., in a round-robin way. It will do its best to not allocate tasks across sockets.</p> </li> <li> <p><code>fcyclic</code> is a very strange distribution, where tasks requesting more than 1 CPU per task will see those      spread out across sockets. </p> <p>We cannot see how this is useful on an AMD CPU except for cases where we have only one task per node which accesses a lot of memory (more than offered by a single socket) but does so in a very NUMA-aware way.</p> </li> </ul> </li> <li> <p>Level 3 is beyond the scope of an introductory course and rarely used.</p> </li> </ul> <p>The default behaviour of Slurm depends on LUMI seems to be <code>block:block,nopack</code> if <code>--distribution</code> is not specified, though it is best to always verify as it can change over time and as the manual indicates that the default differs according to the number of tasks compared to the number of nodes. The defaults are also very tricky if a binding option at level 2 (or 3) is replaced with a <code>*</code> to mark the default behaviour, e.g., <code>--distribution=\"block:*\"</code> gives the result of <code>--distribution=block:cyclic</code> while <code>--distribution=block</code> has the same effect as <code>--distribution=block:block</code>.</p> <p>This option only makes sense on job-exclusive nodes.</p>"},{"location":"intro-evolving/08-Binding/#task-to-cpu-binding-with-slurm","title":"Task-to-CPU binding with Slurm","text":"<p>In the chapter on the LUMI architecture, we've seen that a LUMI compute node has a very hierarchical structure with 8 L3 cache domains per socket (the CCDs), each socket basically subdivided in 4 NUMA domains, and 2 sockets in the CPU nodes. The GPU nodes added further complexity: Each GPU is really two GCDs that should be treated as independent GPUs for most practical issues, except that, e.g., power cap is per package and not per GCD. Moreover, each GCD has its favoured CCD to which it is connected, and these connections can be important to get good performance of CPU-to-GPU communication. And each LUMI-G node also has 4 network interface cards, each connected to a separate GPU, so each CCD and each GCD also has a \"best network interface\" to maximise inter-node  communication.</p> <p>For these reasons, it is often important to distribute tasks and threads correctly over the system. And unfortunately, there is no way that works best for every application and every scenario. </p> <p>For example, for memory performance reasons, you may want to bundle all threads of a task in a single L3 cache domain, a single NUMA domain or a single socket. In a hybrid MPI/OpenMP application it may be interesting to size each task to an L3 cache domain or a NUMA node for optimal performance. And for very memory bandwidth intensive applications, underpopulating cores may actually give you better performance for the same number of nodes, but then you need to carefully select the cores that will remain unpopulated to benefit most from cache and memory bandwidth.</p> <p>In some cases, you may have a shared memory code that is very NUMA-aware but cannot use all cores efficiently because of memory bandwidth requirements or memory capacity requirements (simply needing more GB per core than LUMI can provide). In this case, you may  want to spread out the threads as much as possible to have a maximal memory bandwidth.</p> <p>On LUMI-G, proper mapping of CCDs, GCDs and network interfaces can be very important for good performance. The easiest way here is often to reorder the tasks across the CCDs  in a non-trivial way to have an easy way to select the GPU for each task.</p> <p></p> <p></p> <p></p> <p>The level 2 and 3 options from the previous section already do some binding. But we will now  discuss a different option that enables very precise binding of tasks to hardware threads in Slurm.</p> <p>The mechanism does conflict with some Slurm options that implicitly already do some binding, e.g.,  it will not always work together with <code>--cpus-per-task</code> and <code>--hint=[no]multithread</code> may also not  act as expected depending on how the options are used.  Level 2 and 3 control via <code>--distribution</code> sometimes also makes no sense when this option is used (and will be ignored).</p> <p>Task-to-CPU binding is controlled through the Slurm option </p> <pre><code>--cpu-bind=[{quiet|verbose},]&lt;type&gt;\n</code></pre> <p>We'll describe a few of the possibilities for the <code>&lt;type&gt;</code> parameter but for a more concrete overview we refer to the Slurm <code>srun</code> manual page</p> <ul> <li> <p><code>--cpu-bind=threads</code> is the default behaviour on LUMI. Tasks will be allocated a number     of hardware threads and be bound to them.</p> </li> <li> <p><code>--cpu-bind=map_cpu:&lt;cpu_id_for_task_0&gt;,&lt;cpu_id_for_task_1&gt;, ...</code> is used when tasks are bound to single     cores. The first number is the number of the hardware thread for the task with local task ID 0, etc.      In other words, this option at the same time also defines the slots that can be used by the      <code>--distribution</code> option above and replaces level 2 and level 3 of that option. </p> <p>E.g.,</p> <pre><code>salloc --nodes=1 --partition=standard-g\nmodule load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeGNU-24.03\nsrun --ntasks=8 --cpu-bind=map_cpu:49,57,17,25,1,9,33,41 mpi_check -r\n</code></pre> <p>will run the first task on hardware threads 49, the second task on 57, third on 17, fourth on  25, fifth on 1, sixth on 9, seventh on 33 and eight on 41.</p> <p>This may look like a very strange numbering, but we will see an application for it further in this chapter.</p> </li> <li> <p><code>--cpu-bind=mask_cpu:&lt;mask_for_task_0&gt;,&lt;mask_for_task_1&gt;,...</code> is similar to <code>map_cpu</code>, but now multiple     hardware threads can be specified per task through a mask. The mask is a hexadecimal number and leading      zeros can be omitted. The least significant bit in the mask corresponds to HWT 0, etc. </p> <p>Masks can become very long, but we shall see that this option is very useful on the nodes of the  <code>standard-g</code> partition. Just as with <code>map_cpu</code>, this option replaces level 2 and 3 of the <code>--distribution</code> option. </p> <p>E.g.,</p> <pre><code>salloc --nodes=1 --partition=standard-g\nmodule load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeGNU-24.03\nsrun --ntasks=8 --cpu-bind=mask_cpu:7e000000000000,7e00000000000000,7e0000,7e000000,7e,7e00,7e00000000,7e0000000000 hybrid_check -r\n</code></pre> <p>will run the first task on hardware threads 49-54, the second task on 57-62, third on 17-22, fourth on  25-30, fifth on 1-6, sixth on 9-14, seventh on 33-38 and eight on 41-46.</p> </li> </ul> <p>The <code>--cpu-bind=map_cpu</code> and <code>--cpu-bind=mask_gpu</code> options also do not go together with <code>-c</code> / <code>--cpus-per-task</code>. Both commands define a binding (the latter in combination with the default <code>--cpu-bind=threads</code>)  and these will usually conflict.</p> <p>There are more options, but these are currently most relevant ones on LUMI. That may change in the future as LUMI User Support is investigating whether it isn't better to change the concept of \"socket\" in Slurm given how important it sometimes is to carefully map onto L3 cache domains for performance.</p> How to understand masks? (Click to expand) <p>Masks indicate which (virtual) cores can be used. A mask is really a series of bits with  each bit corresponding to a virtual core. The least significant bit corresponds to core 0, the next one to core 1, etc. A 1-bit indicates that the corresponding virtual core can be used while a 0-bit indicates that it cannot be used. </p> <p>Consider the mask <code>1111000001011010</code>. For readability, we split it up in groups of 4 from right to left (and read vertically for the core numbers): </p> <p> Core               1111 1100 0000 0000         5432 1098 7654 3210 Mask        1111 0000 0101 1010 </p> <p>Hence this masks indicates that cores 1, 3, 4, 6, 12, 13, 14 and 15 can be used.</p> <p>Now using such long bit strings is awkward. There is a long tradition among computer scientists to represent such bit strings instead as hexadecimal numbers: numbers base 16, instead of as bit strings, numbers base 2. Each hexadecimal digit then corresponds with  4 bits, and we start assigning those again from the 4 least significant bits to the  most significant bits, adding 0s at the front to get a multiple of 4 bits. The conversion is given by the following table.</p> <p> decimal binary hexadecimal 0 0000 0 1 0001 1 2 0010 2 3 0011 3 4 0100 4 5 0101 5 6 0110 6 7 0111 7 8 1000 8 9 1001 9 10 1010 a 11 1011 b 12 1100 c 13 1101 d 14 1110 e 15 1111 f <p>So our mask <code>1111000001011010</code> is more conveniently written as <code>f05a</code> or <code>F05A</code>:</p> <p> Core               1111 1100 0000 0000         5432 1098 7654 3210 Mask        1111 0000 0101 1010 Hexadecimal        \u00a0\u00a0f\u00a0 \u00a0\u00a00\u00a0 \u00a0\u00a05\u00a0 \u00a0\u00a0a  </p> <p>To indicate that a number is a hexadecimal number, there are several conventions, but the one which is usually used, is preceding the number with <code>0x</code>, so our mask then becomes  <code>0xf05a</code>.  This convention is used by bash and can also be used in Slurm masks.</p> <p>Since CCDs on the zen3-based LUMI CPUs have 8 cores, they correspond to exactly 2 hexadecimal digits. This also implies that if we want to use the same cores on each CCD, building the mask becomes simple as we only need to look at CCD 0 and then shift the pattern two positions for each subsequent CCD. </p> <p>The primary use case for all this will be core mapping on the GPU nodes to get an optimal binding between the cores and GCDs used by a Slurm task / MPI rank. On each CCD of a LUMI-G processor, core 0 cannot be used by the user. So building a mask that includes the first hardware thread of all available cores (cores 1-7) of a CCD is done as follows:</p> <p> Core 7654 3210 Mask        1111 1110 Hexadecimal        \u00a0\u00a0f\u00a0 \u00a0\u00a0e </p> <p>so the mask is <code>0xfe</code>. Suppose that we want to use all 7 available cores on CCD 1, i.e., cores 9 till 15, we get</p> <p> Core               1111 1100 0000 0000         5432 1098 7654 3210 Mask        1111 1110 0000 0000 Hexadecimal        \u00a0\u00a0f\u00a0 \u00a0\u00a0e\u00a0 \u00a0\u00a00\u00a0 \u00a0\u00a00  </p> <p>of <code>0xfe00</code>, so really just our pattern for CCD 0 shifted by two positions by adding two  zeros at the end. </p> <p>For the example above, with  <code>--cpu-bind=mask_cpu:7e000000000000,7e00000000000000,7e0000,7e000000,7e,7e00,7e00000000,7e0000000000</code>, the basic building block is <code>0x7e</code>, so</p> <p> Hexadecimal        \u00a0\u00a07\u00a0 \u00a0\u00a0e Mask        0111 1110 Core 7654 3210 </p> <p>or cores 1 till 6. For the whole mask, we get: </p> <p> CCD        \u00a07 \u00a06 \u00a05 \u00a04 \u00a03 \u00a02 \u00a01 \u00a00 using Element 1        \u00a0\u00a0 7e 00 00 00 00 00 00 CCD 6, cores 49-54 Element 2        7e 00 00 00 00 00 00 00 CCD 7, cores 57-62 Element 3        \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 7e 00 00 CCD 2, cores 17-22 Element 4        \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 7e 00 00 00 CCD 3, cores 25-30 Element 5        \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 7e CCD 0, cores 1-6 Element 6        \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 7e 00 CCD 1, cores 9-14 Element 7        \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 7e 00 00 00 00 CCD 4, cores 33-38 Element 1        \u00a0\u00a0 \u00a0\u00a0 7e 00 00 00 00 00 CCD 5, cores 41-46 </p> <p>So basically this mask means that we are creating slots for 8 tasks that each use 6 cores on a single  CCD (cores 1 till 6), in the order CCD 6, CCD 7, CCD 2, CCD 3, CCD 0, CCD 1, CCD 4 and CCD 5.</p>"},{"location":"intro-evolving/08-Binding/#task-to-gpu-binding-with-slurm","title":"Task-to-GPU binding with Slurm","text":"<p>Doing the task-to-GPU binding fully via Slurm is currently not recommended on LUMI.  The problem is that Slurm uses control groups at the task level rather than just <code>ROCR_VISIBLE_DEVICES</code> with the latter being more or less the equivalent of affinity masks. When using control groups this way, the other GPUs in a job step on a node become completely invisible to a task, and the Peer2Peer IPC mechanism for communication cannot be used anymore.</p> <p>We present the options for completeness, and as it may still help users if the control group setup is not a problem for the application.</p> <p>Task-to-GPU binding is done with</p> <pre><code>--gpu-bind=[verbose,]&lt;type&gt;\n</code></pre> <p>(see the Slurm manual) which is somewhat similar to <code>--cpu-binding</code> (to the extent that that makes sense).</p> <p>Some options for the <code>&lt;type&gt;</code> parameter that are worth considering:</p> <ul> <li> <p><code>--gpu-bind=closest</code>: This currently does not work well on LUMI. The problem is being investigated     so the situation may have changed by the time you read this.</p> </li> <li> <p><code>--gpu-bind=none</code>: Turns off the GPU binding of Slurm. This can actually be useful on shared node     jobs where doing a proper allocation of GPUs is difficult. You can then first use Slurm options such      as <code>--gpus-per-task</code> to get a working allocation of GPUs and CPUs, then un-bind and rebind using a      different mechanism that we will discuss later.</p> </li> <li> <p><code>--gpu-bind=map_gpu:&lt;list&gt;</code> is the equivalent of <code>--cpu-bind=map_cpu:&lt;list&gt;</code>.     This option only makes sense on a job-exclusive node and is for jobs that need a single      GPU per task. It defines the list of GPUs that should be used, with the task with local ID 0     using the first one in the list, etc.     The numbering and topology was already discussed in the \"LUMI Architecture\" chapter, section     \"Building LUMI: What a LUMI-G really looks like.</p> </li> <li> <p><code>--gpu-bind=mask_gpu:&lt;list&gt;</code> is the equivalent of <code>--cpu-bind=mask_cpu:&lt;list&gt;</code>.      Now the bits in the mask correspond to individual GPUs, with GPU 0 the least significant bit.      This option again only makes sense on a job-exclusive node.</p> </li> </ul> <p>Though <code>map_gpu</code> and <code>mask_gpu</code> could be very useful to get a proper mapping taking the topology of the  node into account, due to the current limitation of creating a control group per task it can not often be used as it breaks some efficient communication mechanisms between tasks, including the GPU Peer2Peer  IPC used by Cray MPICH for intro-node MPI transfers if GPU aware MPI support is enabled.</p> What do the HPE Cray manuals say about this? (Click to expand) <p>From the HPE Cray CoE:  \"Slurm may choose to use cgroups to implement the required affinity settings. Typically, the use of cgroups has the downside of preventing the use of  GPU Peer2Peer IPC mechanisms. By default Cray MPI uses IPC for implementing intra-node, inter-process MPI data movement operations that involve GPU-attached user buffers.  When Slurm\u2019s cgroups settings are in effect, users are advised to set <code>MPICH_SMP_SINGLE_COPY_MODE=NONE</code> or <code>MPICH_GPU_IPC_ENABLED=0</code> to disable the use of IPC-based implementations.  Disabling IPC also has a noticeable impact on intra-node MPI performance when  GPU-attached memory regions are involved.\"</p> <p>This is exactly what Slurm does on LUMI.</p>"},{"location":"intro-evolving/08-Binding/#mpi-rank-redistribution-with-cray-mpich","title":"MPI rank redistribution with Cray MPICH","text":"<p>By default MPI rank i will use Slurm task i in a parallel job step.  With Cray MPICH this can be changed via the environment variable  <code>MPICH_RANK_REORDER_METHOD</code>. It provides an even more powerful way of reordering MPI ranks than the Slurm <code>--distribution</code> option as one can define fully custom orderings.</p> <p>Rank reordering is an advanced topic that is discussed in more detail in the 4 or 5-day Advanced LUMI course or the \"Performance Analysis and Optimization\" workshop organised by the LUMI User Support Team. The material of the latest advanced course can be found via the course archive web page and is discussed in the  \"MPI Topics on the HPE Cray EX Supercomputer\" which is often given on day 3.</p> <p>Rank reordering can be used to reduce the number of inter-node messages or to spread those ranks that do parallel I/O over more nodes to increase the I/O bandwidth that can be obtained in the application.</p> <p>Possible values for <code>MPICH_RANK_REORDER_METHOD</code> are:</p> <ul> <li> <p><code>export MPICH_RANK_REORDER_METHOD=0</code>: Round-robin placement of the MPI ranks.     This is the equivalent of the cyclic ordering in Slurm.</p> </li> <li> <p><code>export MPICH_RANK_REORDER_METHOD=1</code>: This is the default and it preserves the     ordering of Slurm, and the only one that makes sense with other L1 Slurm distributions     than <code>block</code>.</p> <p>The Cray MPICH manual confusingly calls this \"SMP-style ordering\".</p> </li> <li> <p><code>export MPICH_RANK_REORDER_METHOD=2</code>: Folded rank placement. This is somewhat similar      to round-robin, but when the last node is reached, the node list is transferred in the      opposite direction.</p> </li> <li> <p><code>export MPICH_RANK_REORDER_METHOD=3</code>: Use a custom ordering, given by the      <code>MPICH_RANK_ORDER</code> file which gives a comma-separated list of the MPI ranks     in the order they should be assigned to slots on the nodes. The default filename      <code>MPICH_RANK_ORDER</code> can be overwritten through the environment variable      <code>MPICH_RANK_REORDER_FILE</code>.</p> </li> </ul> <p>Rank reordering does not always work well if Slurm is not using the (default) block ordering.  As the <code>lumi-CPEtools</code> <code>mpi_check</code>, <code>hybrid_check</code> and <code>gpu_check</code> commands use Cray MPICH they can be used to test the Cray MPICH rank reordering also. The MPI ranks that are  displayed are the MPI ranks as seen through MPI calls and not the value of <code>SLURM_PROCID</code> which is the Slurm task number.</p> <p>The HPE Cray Programming Environment actually has profiling tools that help you determine the optimal rank ordering for a particular run, which is useful if you do a lot of runs with the same problem size (and hence same number of nodes and tasks).</p> Try the following job script (click to expand) <p> <pre><code>#!/bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --job-name=renumber-demo\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard\n#SBATCH --nodes=2\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeGNU-24.03\n\nset -x\necho -e \"\\nSMP-style distribution on top of block.\"\nexport MPICH_RANK_REORDER_METHOD=1\nsrun -n 8 -c 32 -m block mpi_check -r\necho -e \"\\nSMP-style distribution on top of cyclic.\"\nexport MPICH_RANK_REORDER_METHOD=1\nsrun -n 8 -c 32 -m cyclic mpi_check -r\necho -e \"\\nRound-robin distribution on top of block.\"\nexport MPICH_RANK_REORDER_METHOD=0\nsrun -n 8 -c 32 -m block mpi_check -r\necho -e \"\\nFolded distribution on top of block.\"\nexport MPICH_RANK_REORDER_METHOD=2\nsrun -n 8 -c 32 -m block mpi_check -r\necho -e \"\\nCustom distribution on top of block.\"\nexport MPICH_RANK_REORDER_METHOD=3\ncat &gt;MPICH_RANK_ORDER &lt;&lt;EOF\n0,1,4,5,2,3,6,7\nEOF\ncat MPICH_RANK_ORDER\nsrun -n 8 -c 32 -m block mpi_check -r\n/bin/rm MPICH_RANK_ORDER\nset +x\n</code></pre></p> <p>Ths script starts 8 tasks that each take a quarter node. </p> <ol> <li> <p>The first <code>srun</code> command (on line 15) is just the block distribution.      The first 4 MPI ranks are     on the first node, the next 4 on the second node.</p> <pre><code>+ export MPICH_RANK_REORDER_METHOD=1\n+ MPICH_RANK_REORDER_METHOD=1\n+ srun -n 8 -c 32 -m block mpi_check -r\n\nRunning 8 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/8   on cpu  17/256 of nid001804 mask 0-31\n++ mpi_check: MPI rank   1/8   on cpu  32/256 of nid001804 mask 32-63\n++ mpi_check: MPI rank   2/8   on cpu  65/256 of nid001804 mask 64-95\n++ mpi_check: MPI rank   3/8   on cpu 111/256 of nid001804 mask 96-127\n++ mpi_check: MPI rank   4/8   on cpu   0/256 of nid001805 mask 0-31\n++ mpi_check: MPI rank   5/8   on cpu  32/256 of nid001805 mask 32-63\n++ mpi_check: MPI rank   6/8   on cpu  64/256 of nid001805 mask 64-95\n++ mpi_check: MPI rank   7/8   on cpu 120/256 of nid001805 mask 96-127\n</code></pre> </li> <li> <p>The second <code>srun</code> command, on line 18, is an example where the Slurm cyclic     distribution is preserved. MPI rank 0 now lands on the first     32 cores of node 0 of the allocation, MPI rank 1 on the first 32 cores of node 1 of the allocation,     then task 2 on the second 32 cores of node 0, and so on:</p> <pre><code>+ export MPICH_RANK_REORDER_METHOD=1\n+ MPICH_RANK_REORDER_METHOD=1\n+ srun -n 8 -c 32 -m cyclic mpi_check -r\n\nRunning 8 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31\n++ mpi_check: MPI rank   1/8   on cpu   1/256 of nid001805 mask 0-31\n++ mpi_check: MPI rank   2/8   on cpu  32/256 of nid001804 mask 32-63\n++ mpi_check: MPI rank   3/8   on cpu  33/256 of nid001805 mask 32-63\n++ mpi_check: MPI rank   4/8   on cpu  79/256 of nid001804 mask 64-95\n++ mpi_check: MPI rank   5/8   on cpu  64/256 of nid001805 mask 64-95\n++ mpi_check: MPI rank   6/8   on cpu 112/256 of nid001804 mask 96-127\n++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001805 mask 96-127\n</code></pre> </li> <li> <p>The third <code>srun</code> command, on line 21, uses Cray MPICH rank reordering instead to get a round-robin ordering     rather than using the Slurm <code>--distribution=cyclic</code> option. The result is the same     as in the previous case:</p> <pre><code>+ export MPICH_RANK_REORDER_METHOD=0\n+ MPICH_RANK_REORDER_METHOD=0\n+ srun -n 8 -c 32 -m block mpi_check -r\n\nRunning 8 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31\n++ mpi_check: MPI rank   1/8   on cpu   1/256 of nid001805 mask 0-31\n++ mpi_check: MPI rank   2/8   on cpu  32/256 of nid001804 mask 32-63\n++ mpi_check: MPI rank   3/8   on cpu  47/256 of nid001805 mask 32-63\n++ mpi_check: MPI rank   4/8   on cpu  64/256 of nid001804 mask 64-95\n++ mpi_check: MPI rank   5/8   on cpu  64/256 of nid001805 mask 64-95\n++ mpi_check: MPI rank   6/8   on cpu 112/256 of nid001804 mask 96-127\n++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001805 mask 96-127\n</code></pre> </li> <li> <p>The fourth <code>srun</code> command, on line 24, demonstrates the folded ordering: Rank 0 runs on the first 32      cores of node 0 of the allocation, rank 1 on the first 32 of node 1, then rank 2 runs on      the second set of 32 cores again on node 1, with rank 3 then running on the second 32 cores     of node 0, rank 4 on the third group of 32 cores of node 0, rank 5 on the third group of     32 cores on rank 1, and so on. So the nodes are filled in the order 0, 1, 1, 0, 0, 1, 1, 0.</p> <pre><code>+ export MPICH_RANK_REORDER_METHOD=2\n+ MPICH_RANK_REORDER_METHOD=2\n+ srun -n 8 -c 32 -m block mpi_check -r\n\nRunning 8 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31\n++ mpi_check: MPI rank   1/8   on cpu  17/256 of nid001805 mask 0-31\n++ mpi_check: MPI rank   2/8   on cpu  32/256 of nid001805 mask 32-63\n++ mpi_check: MPI rank   3/8   on cpu  32/256 of nid001804 mask 32-63\n++ mpi_check: MPI rank   4/8   on cpu  64/256 of nid001804 mask 64-95\n++ mpi_check: MPI rank   5/8   on cpu  64/256 of nid001805 mask 64-95\n++ mpi_check: MPI rank   6/8   on cpu 112/256 of nid001805 mask 96-127\n++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001804 mask 96-127\n</code></pre> </li> <li> <p>The fifth example ('srun' on line 31) demonstrate a custom reordering.      Here we face a 4x2-grid which we want     to split in 2 2x2 groups. So where the ranks in our grid are numbered as</p> <pre><code>0 1 2 3\n4 5 6 7\n</code></pre> <p>we really want the left half of the grid on the first node of the allocation and the right half on the second node as this gives us less inter-node communication than when we would put the first line on the first node and the second line on the second. So basically we want ranks 0, 1, 4 and 5 on  the first node and ranks 2, 3, 6 and 7 on the second node, which is done by creating the reorder file with content</p> <pre><code>0,1,4,5,2,3,6,7\n</code></pre> <p>The resulting output is</p> <pre><code>+ export MPICH_RANK_REORDER_METHOD=3\n+ MPICH_RANK_REORDER_METHOD=3\n+ cat\n+ srun -n 8 -c 32 -m block mpi_check -r\n\nRunning 8 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31\n++ mpi_check: MPI rank   1/8   on cpu  32/256 of nid001804 mask 32-63\n++ mpi_check: MPI rank   2/8   on cpu   1/256 of nid001805 mask 0-31\n++ mpi_check: MPI rank   3/8   on cpu  32/256 of nid001805 mask 32-63\n++ mpi_check: MPI rank   4/8   on cpu  64/256 of nid001804 mask 64-95\n++ mpi_check: MPI rank   5/8   on cpu 112/256 of nid001804 mask 96-127\n++ mpi_check: MPI rank   6/8   on cpu  64/256 of nid001805 mask 64-95\n++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001805 mask 96-127\n</code></pre> </li> </ol>"},{"location":"intro-evolving/08-Binding/#mpi-network-adapter-binding-with-cray-mpich","title":"MPI network adapter binding with Cray MPICH","text":"<p>Binding network interface controllers (NICs) to MPI ranks can help to improve  inter-node communication on nodes with more than one NIC for the high-speed interconnect. On LUMI, this is the case for the GPU nodes, but there are also many Cray systems out in the world with multiple NICs in CPU nodes, usually one per socket. </p> <p>On LUMI, this matters only on LUMI-G, and in fact, the default binding is usually not optimal so it is useful to change the default NIC binding.</p> <p>This is done through the environment variable <code>MPICH_OFI_NIC_POLICY</code>.  We did already briefly discuss this environment variable when discussing GPU-aware MPI in the  HPE Cray Programming Environment chapter.</p> <p>Several values are possible, but the first two are the most relevant ones for LUMI:</p> <ul> <li> <p><code>MPICH_OFI_NIC_POLICY=GPU</code>: Cray MPICH will try to use the NIC that is closest to the     GPU used by the MPI rank. This is the most useful value if most MPI communications are done     directly from or to GPU-attached memory regions. Obviously this only makes sense if GPU-aware MPI     is enabled through <code>MPICH_GPU_SUPPORT_ENABLED=1</code>.</p> </li> <li> <p><code>MPICH_OFI_NIC_POLICY=NUMA</code>: Cray MPICH will try to use the NIC that is closest to the NUMA domain     of the CPU cores assigned to an MPI rank. This value is useful if most MPI operations are done     to or from buffers in CPU-attached memory.</p> <p>Note that if an optimal mapping between GCDs and CCDs is done (discussed later in this chapter), both values are equivalent. However, on shared nodes where a proper mapping between CPU cores and GCDs is not possible, selecting the right value for a particular code may help a lot to improve  performance.</p> </li> <li> <p><code>MPICH_OFI_NIC_POLICY=BLOCK</code>: Consecutive local ranks are equally distributed among NICs. E.g.,     with 8 ranks and 4 NICs, rank 0 an 1 would use NIC 0, rank 1 and 2 NIC 1, etc. </p> <p>This is the default value but on the LUMI-G nodes often not the proper one.</p> </li> <li> <p><code>MPICH_OFI_NIC_POLICY=ROUND-ROBIN</code>: With 4 NICs: rank 0 goes to NIC 0, rank 1 to NIC 1, rank 2 to NIC 2,     rank 3 to NIC 3, rank 4 again to NIC 0, etc. So basically rank i goes to NIC i mod #NICs. </p> </li> <li> <p>User defined mappings are also possible by combining <code>MPICH_OFI_NIC_POLICY=USER</code> with defining a mapping     through <code>MPICH_OFI_NIC_MAPPING</code>. </p> </li> </ul> <p>More information on these environment variables can be found in the <code>intro_mpi</code> man page on the system or the  <code>intro_mpi</code> page in the online CPE documentation.</p>"},{"location":"intro-evolving/08-Binding/#refining-core-binding-in-openmp-applications","title":"Refining core binding in OpenMP applications","text":"<p>In a Slurm batch job step, threads of a shared memory process will be contained to all  hardware threads of all available cores on the first node of your allocation. To contain a shared memory program to the hardware threads asked for in the allocation (i.e., to ensure that <code>--hint=[no]multithread</code> has effect) you'd have to start the shared memory program with <code>srun</code> in a regular job step.</p> <p>Any multithreaded executable run as a shared memory job or ranks in a hybrid MPI/multithread job, will - when started properly via <code>srun</code> - get access to a group of cores via an affinity mask. In some cases you will want to manually refine the way individual threads of each process are mapped onto the available hardware threads.</p> <p>In OpenMP, this is usually done through environment variables (it can also be done partially in the program through library calls). A number of environment variables is standardised in the  OpenMP standard, but some implementations offer some additional non-standard ones, or non-standard values for the standard environment variables.  Below we discuss the more important of the standard ones:</p> <ul> <li> <p><code>OMP_NUM_THREADS</code> is used to set the number of CPU threads OpenMP will use. In its most basic     form this is a single number (but you can give multiple comma-separated numbers for nested     parallelism). </p> <p>OpenMP programs on LUMI will usually correctly detect how many hardware threads are available to the task and use one OpenMP thread per hardware thread. There are cases where you may want to ask for a certain number of hardware threads when allocating resources, e.g., to easily get a good mapping of tasks on cores, but do not want to use them all, e.g., because your application is too memory bandwidth or cache constrained and using fewer threads actually gives better overall performance on a per-node basis.</p> </li> <li> <p><code>OMP_PLACES</code> is used to restrict each OpenMP thread to a group of hardware threads. Possible values     include: </p> <ul> <li><code>OMP_PLACES=threads</code> to restrict OpenMP threads to a single hardware thread</li> <li><code>OMP_PLACES=cores</code> to restrict each OpenMP threads to a single core (but all hardware threads associated with that core)</li> <li><code>OMP_PLACES=sockets</code> to restrict each OpenMP thread to the hardware threads of a single socket</li> <li> <p>And it is possible to give a list with explicit values, e.g.,</p> <pre><code>export OMP_PLACES=\"{0:4}:3:8\"\n</code></pre> <p>which is also equivalent to</p> <pre><code>export OMP_PLACES=\"{0,1,2,3},{8,9,10,11},{16,17,18,19}\"\n</code></pre> <p>so each OpenMP thread is restricted to a different group of 4 hardware threads. The numbers in the list are not the physical Linux hardware thread numbers, but are relative to the hardware threads available in the  affinity mask of the task. </p> <p>More general, <code>{a:b}:c:d</code> means b numbers starting from a (so a, a+1, ..., a+b-1), repeated c times, at every repeat shifted by d. There are more variants to generate lists of places and we show another one in the example below. But in all the syntax may look strange and there are manuals that give the wrong information (including some versions of the manual for the GNU OpenMP runtime).</p> <p>Note that this is different from the core numbers that would be used in <code>--cpu-bind=map_cpu</code> or <code>--gpu-bind=mask_cpu</code> which sets the CPUs or groups of CPUs available to each thread and which always use the physical numbering and not a numbering that is local to the job allocation.</p> </li> </ul> </li> <li> <p><code>OMP_PROC_BIND</code>: Sets how threads are distributed over the places. Possible values are:</p> <ul> <li> <p><code>OMP_PROC_BIND=false</code>: Turn off OpenMP thread binding. Each thread will get access to all hardware threads     available in to the task (and defined by a Linux affinity mask in Slurm).</p> </li> <li> <p><code>OMP_PROC_BIND=close</code>: If more places are available than there are OpenMP threads, then try     to put the OpenMP threads in different places as close as possible to the master thread.      In general, bind as close as possible     to the master thread while still distributing for load balancing.</p> </li> <li> <p><code>OMP_PROC_BIND=spread</code>: Spread threads out as evenly as possible over the places available     to the task.</p> </li> <li> <p><code>OMP_PROC_BIND=master</code>: Bind threads to the same place as the master thread. The place is determined by the     <code>OMP_PLACES</code> environment variable and it is clear this makes no sense if that place is just a single hardware     thread or single core as all threads would then be competing for the resources of a single core.</p> </li> </ul> <p>Multiple values of <code>close</code>, <code>spread</code> and <code>master</code> in a comma-separated list are possible to organise nested OpenMP parallelism, but this is outside of the scope of this tutorial. </p> <p>The Cray Compilation Environment also has an additional non-standard option <code>auto</code>,  which is actually the default and tries to do a reasonable job for most cases.  On the other compilers on LUMI, the default behaviour is <code>false</code> unless the next environment variable, <code>OMP_PLACES</code>, is specified.</p> </li> <li> <p><code>OMP_DISPLAY_AFFINITY</code>: When set tot <code>TRUE</code> information about the affinity binding of each thread will be       shown which is good for debugging purposes.</p> </li> </ul> <p>For single-level OpenMP parallelism, the <code>omp_check</code> and <code>hybrid_check</code> programs from the <code>lumi-CPEtools</code> modules can also be used to check the OpenMP thread binding.</p> Some examples (click to expand) <p>Consider the following job script:</p> <p> <pre><code>#!/bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --job-name=omp-demo\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --hint=multithread\n#SBATCH --time=5:00\n\nmodule load LUMI/24.03 partition/C lumi-CPEtools/1.2a-cpeCray-24.03\n\nset -x\nexport OMP_NUM_THREADS=4\nexport OMP_PROC_BIND=false\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nsrun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nexport OMP_NUM_THREADS=4\nunset OMP_PROC_BIND\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nsrun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nexport OMP_NUM_THREADS=4\nexport OMP_PROC_BIND=close\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nsrun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nexport OMP_NUM_THREADS=4\nexport OMP_PROC_BIND=spread\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nsrun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nexport OMP_NUM_THREADS=4\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=threads\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nexport OMP_PLACES=cores\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\n\nexport OMP_NUM_THREADS=4\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=\"{0:8}:4:8\"\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\n\nexport OMP_PLACES=\"{0:4,16:4}:4:4\"\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nset +x\n</code></pre></p> <p>Let's check the output step by step:</p> <p>In the first block we run 2 <code>srun</code> commands that actually both use 16 cores, but first with hardware threading enabled in Slurm and then with multithread mode off in Slurm:</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ export OMP_PROC_BIND=false\n+ OMP_PROC_BIND=false\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-15, 128-143\n++ omp_check: OpenMP thread   1/4   on cpu 137/256 of nid001077 mask 0-15, 128-143\n++ omp_check: OpenMP thread   2/4   on cpu 129/256 of nid001077 mask 0-15, 128-143\n++ omp_check: OpenMP thread   3/4   on cpu 143/256 of nid001077 mask 0-15, 128-143\n\n+ srun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-15\n++ omp_check: OpenMP thread   1/4   on cpu  15/256 of nid001077 mask 0-15\n++ omp_check: OpenMP thread   2/4   on cpu   1/256 of nid001077 mask 0-15\n++ omp_check: OpenMP thread   3/4   on cpu  14/256 of nid001077 mask 0-15\n</code></pre> <p><code>OMP_PROC_BIND</code> was explicitly set to false to disable the Cray Compilation Environment default behaviour. The masks reported by <code>omp_check</code> cover all hardware threads available to the task in Slurm: Both hardware threads for the 16 first cores in the multithread case and just the primary hardware thread on the first 16 cores in the second case. So each OpenMP thread can in principle migrate over all available hardware threads.</p> <p>In the second block we unset the <code>PROC_BIND</code> environment variable to demonstrate the behaviour of the Cray Compilation Environment. The output would be different had we used the cpeGNU or cpeAOCC version.</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ unset OMP_PROC_BIND\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   1/256 of nid001077 mask 0-3, 128-131\n++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4-7, 132-135\n++ omp_check: OpenMP thread   2/4   on cpu   8/256 of nid001077 mask 8-11, 136-139\n++ omp_check: OpenMP thread   3/4   on cpu 142/256 of nid001077 mask 12-15, 140-143\n\n+ srun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4-7\n++ omp_check: OpenMP thread   2/4   on cpu   9/256 of nid001077 mask 8-11\n++ omp_check: OpenMP thread   3/4   on cpu  15/256 of nid001077 mask 12-15\n</code></pre> <p>The default behaviour of the CCE is very nice: Threads are nicely spread out over the available cores and then all get access to their own group of hardware threads that in this case with 4 threads for 16 cores spans 4 cores for each thread. In fact, also in other cases the default behaviour of CCE will be a binding  that works well for many cases. </p> <p>In the next experiment we demonstrate the <code>close</code> binding:</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ export OMP_PROC_BIND=close\n+ OMP_PROC_BIND=close\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0\n++ omp_check: OpenMP thread   1/4   on cpu 128/256 of nid001077 mask 128\n++ omp_check: OpenMP thread   2/4   on cpu   1/256 of nid001077 mask 1\n++ omp_check: OpenMP thread   3/4   on cpu 129/256 of nid001077 mask 129\n\n+ srun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001077 mask 1\n++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001077 mask 2\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001077 mask 3\n</code></pre> <p>In the first case, with Slurm multithreading mode on, we see that the 4 threads are now concentrated on only 2 cores but each gets pinned to its own hardware thread. In general  this behaviour is not what one wants if more cores are available as on each core two threads will now be competing for available resources. In the second case, with Slurm multithreading  disabled, the threads are bound to the first 4 cores, with one core for each thread.</p> <p>Next we demonstrate the <code>spread</code> binding:</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ export OMP_PROC_BIND=spread\n+ OMP_PROC_BIND=spread\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0\n++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4\n++ omp_check: OpenMP thread   2/4   on cpu   8/256 of nid001077 mask 8\n++ omp_check: OpenMP thread   3/4   on cpu  12/256 of nid001077 mask 12\n\n+ srun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0\n++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4\n++ omp_check: OpenMP thread   2/4   on cpu   8/256 of nid001077 mask 8\n++ omp_check: OpenMP thread   3/4   on cpu  12/256 of nid001077 mask 12\n</code></pre> <p>The result is now the same in both cases as we have fewer threads than physical cores. Each OpenMP thread is bound to a single core, but these cores are spread out over the first 16 cores of the node. </p> <p>Next we return to the <code>close</code> binding but try both <code>threads</code> and <code>cores</code> as places with Slurm multithreading turned on for both cases:</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ export OMP_PROC_BIND=close\n+ OMP_PROC_BIND=close\n+ export OMP_PLACES=threads\n+ OMP_PLACES=threads\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0\n++ omp_check: OpenMP thread   1/4   on cpu 128/256 of nid001077 mask 128\n++ omp_check: OpenMP thread   2/4   on cpu   1/256 of nid001077 mask 1\n++ omp_check: OpenMP thread   3/4   on cpu 129/256 of nid001077 mask 129\n\n+ export OMP_PLACES=cores\n+ OMP_PLACES=cores\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0, 128\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001077 mask 1, 129\n++ omp_check: OpenMP thread   2/4   on cpu 130/256 of nid001077 mask 2, 130\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001077 mask 3, 131\n</code></pre> <p>With <code>threads</code> as places we get again the distribution with two OpenMP threads on each physical core, each with their own hardware thread. With cores as places, we get only one thread per physical core, but each thread has access to both hardware threads of that physical core.</p> <p>And lastly we play a bit with custom placements:</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ export OMP_PROC_BIND=close\n+ OMP_PROC_BIND=close\n+ export 'OMP_PLACES={0:8}:4:8'\n+ OMP_PLACES='{0:8}:4:8'\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-7\n++ omp_check: OpenMP thread   1/4   on cpu   8/256 of nid001077 mask 8-15\n++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001077 mask 128-135\n++ omp_check: OpenMP thread   3/4   on cpu 136/256 of nid001077 mask 136-143\n</code></pre> <p><code>OMP_PLACES='{0:8}:4:8</code> means: Take starting from core with logical number 0 8 cores and  repeat this 4 times, shifting by 8 each time, so effectively</p> <pre><code>OMP_PLACES=\"{ 0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15},{16,17,18,19,20,21,22,23},{24,25,26,27,27,28,30,31}\"\n</code></pre> <p><code>omp_check</code> however shows the OS numbering for the hardware threads so we can see what this places variable means: the first thread can get scheduled on the first hardware thread of the first 8 cores, the second thread on the first hardware thread of the next 8 cores, the third OpenMP thread on the second thread of the first 8 cores, and the  fourth OpenMP thread on the second hardware thread of the next 8 cores. In other words, the logical numbering of the  threads follows the same ordering as at the OS level: First the first hardware thread of each core, then the second  hardware thread.</p> <p>When trying another variant with</p> <pre><code>OMP_PACES={0:4,16:4}:4:4\n</code></pre> <p>which is equivalent to</p> <pre><code>OMP_PLACES={0,1,2,3,16,17,18,19},{4,5,6,7,20,21,22,23},{8,9,10,11,24,25,26,27},{12,13,14,15,28,29,30,31}\"\n</code></pre> <p>we get a much nicer distribution:</p> <pre><code>+ export 'OMP_PLACES={0:4,16:4}:4:4'\n+ OMP_PLACES='{0:4,16:4}:4:4'\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-3, 128-131\n++ omp_check: OpenMP thread   1/4   on cpu 132/256 of nid001077 mask 4-7, 132-135\n++ omp_check: OpenMP thread   2/4   on cpu 136/256 of nid001077 mask 8-11, 136-139\n++ omp_check: OpenMP thread   3/4   on cpu 140/256 of nid001077 mask 12-15, 140-143\n</code></pre> <p>We only discussed a subset of the environment variables defined in the OpenMP standard. Several implementations also offer additional environment variables, e.g.,  a number of <code>GOMP_*</code> environment variables in the GNU Compiler Collection implementation or <code>KMP_*</code> variables in the Intel compiler (not available on LUMI).</p> <p>Some further documentation:</p> <ul> <li> <p>The <code>OMP_*</code> environment variables and a number of environment variables specific for the runtime libraries     of the Cray Compiling Environment are discussed in the      <code>intro_openmp</code> manual page, section \"Environment variables\".</p> </li> <li> <p>A list of OMP_ environment variables in the OpenMP 5.1 standard      (as the current list in the HTML version of the 5.2 standard has some problems).</p> </li> </ul>"},{"location":"intro-evolving/08-Binding/#gpu-binding-with-rocr_visible_devices","title":"GPU binding with ROCR_VISIBLE_DEVICES","text":"<p>The <code>ROCR_VISIBLE_DEVICES</code> environment variable restricts access to GPUs at the ROCm platform runtime  level. Contrary to control groups however this mechanism is compatible with the Peer2Peer IPC used by GPU-aware Cray MPI for intra-node communication.</p> <p>The value of the <code>ROCR_VISIBLE_DEVICES</code> environment variable is a list of device indices that will be exposed to the applications. The device indices do depend on the control group. Visible devices in a control group are always numbered from 0.</p> <p>So though <code>ROCR_VISIBLE_DEVICES</code> has the same function as affinity masks for CPUs, it is different in many respects.</p> <ol> <li> <p>Affinity masks are part of the Linux kernel and fully OS-controlled, while      <code>ROCR_VISIBLE_DEVICES</code> is interpreted in the ROCm\u2122 stack.</p> </li> <li> <p>Affinity masks are set through an OS call and that call can enforce that the new     mask cannot be less restrictive than the parent mask. <code>ROCR_VISIBLE_DEVICES</code> is just     an environment variable, so at the time that you try to set it to a value that you      shouldn't use, there is no check.</p> </li> <li> <p>Affinity masks always use the global numbering of hardware threads while      <code>ROCR_VISIBLE_DEVICES</code> uses the local numbering in the currently active control group.     So the GPU that corresponds to 0 in <code>ROCR_VISIBLE_DEVICES</code> is not always the same GPU.</p> </li> </ol> Alternative values for <code>ROCR_VISIBLE_DEVICES</code> (click to expand) <p>Instead of device indices, <code>ROCR_VISIBLE_DEVICES</code> also accepts GPU UUIDs that are unique to each GPU. This is less practical then it seems as the UUIDs of GPUs are different on each node so one would need to discover them first before they can be used.</p>"},{"location":"intro-evolving/08-Binding/#combining-slurm-task-binding-with-rocr_visible_devices","title":"Combining Slurm task binding with ROCR_VISIBLE_DEVICES","text":"<p>In the chapter on the architecture of LUMI we discussed  what a LUMI-G really looks like.</p> <p>The full topology of a LUMI-G compute node is shown in the figure:</p> <p>Note that the numbering of GCDs does not correspond to the numbering of CCDs/cores. However, for optimal memory transfers (and certainly if cache-coherent memory access from CPU to GPU would be used) it is  better to ensure that each GCD collaborates with the matched CCD in an MPI rank. So we have the mapping:</p> CCD HWTs Available HWTs CPU mask (without HWTs) GCD 0 0-7, 64-71 1-7, 65-71 0xfe 4 1 8-15, 72-79 9-15, 73-79 0xfe00 5 2 16-23, 80-87 17-23, 81-87 0xfe0000 2 3 24-32, 88-95 25-32, 89-95 0xfe000000 3 4 32-39, 96-103 33-39, 97-103 0xfe00000000 6 5 40-47, 104-111 41-47, 105-111 0xfe0000000000 7 6 48-55, 112-119 49-55, 113-119 0xfe000000000000 0 7 56-63, 120-127 57-63, 121-127 0xfe00000000000000 1 <p>or the reverse mapping</p> GCD CCD HWTs Available HWTs CPU mask (without HWTs) 0 6 48-55, 112-119 49-55, 113-119 0xfe000000000000 1 7 56-63, 120-127 57-63, 121-127 0xfe00000000000000 2 2 16-23, 80-87 17-23, 81-87 0xfe0000 3 3 24-32, 88-95 25-32, 89-95 0xfe000000 4 0 0-7, 64-71 1-7, 65-71 0xfe 5 1 8-15, 72-79 9-15, 73-79 0xfe00 6 4 32-39, 96-103 33-39, 97-103 0xfe00000000 7 5 40-47, 104-111 41-47, 105-111 0xfe0000000000 <p></p> <p>Moreover, if you look more carefully at the topology, you can see that the connections between the  GCDs contain a number of rings:</p> <ol> <li> <p>Green ring: 0 - 1 - 3 - 2 - 4 - 5 - 7 - 6 - 0</p> </li> <li> <p>Red ring:   0 - 1 - 5 - 4 - 6 - 7 - 3 - 2 - 0</p> </li> <li> <p>Sharing some connections with the previous ones, but can be combined with the green ring:      0 - 1 - 5 - 4 - 2 - 3 - 7 - 6 - 0</p> </li> </ol> <p>So if your application would use a ring mapping for communication and use communication from GPU buffers  for that, than it may be advantageous to map the MPI ranks on one of those rings which would mean that neither the order of the CCDs nor the order of the GCDs is trivial.</p> <p>Some other topologies can also be mapped on these connections (but unfortunately not a 3D cube).</p> <p>Note: The red ring and green ring correspond to the red and green rings on page 6 of the \"Introducing AMD CDNA<sup>TM</sup> 2 Architecture\" whitepaper.</p> <p></p> <p>To implement a proper CCD-to-GCD mapping we will use three mechanisms:</p> <ul> <li> <p>On the CPU side we'll use Slurm <code>--cpu-bind</code>. Sometimes we can also simply use <code>-c</code> or      <code>--cpus-per-task</code> (in particular in the case below with linear ordering of the CCDs and      7 cores per task)</p> </li> <li> <p>On the GPU side we will manually assign GPUs via a different value of <code>ROCR_VISIBLE_DEVICES</code> for each     thread. To accomplish this we will have to write a wrapper script which we will generate in the job script.</p> </li> <li> <p>On the NIC side, one can then ensure that the proper NIC will be used by each MPI rank by setting     <code>MPICH_OFI_NIC_POLICY</code> to either <code>GPU</code> or <code>NUMA</code>. Which one does not matter as when the proper mappings     are used, the CPU cores and GCD used by an MPI rank are in the same NUMA node.</p> </li> </ul> <p>Let us start with the simplest case:</p>"},{"location":"intro-evolving/08-Binding/#linear-assignment-of-gcd-then-match-the-cores","title":"Linear assignment of GCD, then match the cores","text":"<p>In this scenario, task 0 uses GCD 0 and the matching CCD, CCD 6, task 1 uses GCD 1 and the matching CCD 7, etc. So the following table is the relevant one to use:</p> GCD CCD HWTs Available HWTs CPU mask (without HWTs) 0 6 48-55, 112-119 49-55, 113-119 0xfe000000000000 1 7 56-63, 120-127 57-63, 121-127 0xfe00000000000000 2 2 16-23, 80-87 17-23, 81-87 0xfe0000 3 3 24-32, 88-95 25-32, 89-95 0xfe000000 4 0 0-7, 64-71 1-7, 65-71 0xfe 5 1 8-15, 72-79 9-15, 73-79 0xfe00 6 4 32-39, 96-103 33-39, 97-103 0xfe00000000 7 5 40-47, 104-111 41-47, 105-111 0xfe0000000000 <p>Mapping each task to a GCD is easy in this case, as the Slurm task with local task ID $SLURM_LOCALID is mapped on GCD $SLURM_LOCALID. However, the CPU mapping is more complex, but to build the CPU mask or list, we simply  have to read the 4<sup>th</sup> or 5<sup>th</sup> column from the above table from the first to the last line.</p> <p>One possible job script to implement this order is:</p> <pre><code>#!/bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --job-name=map-linear-GCD\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard-g\n#SBATCH --gpus-per-node=8\n#SBATCH --nodes=2\n#SBATCH --time=5:00\n\nmodule load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeCray-24.03\n\n# Mapping:\n# | Task | GCD | CCD | HWTs           | Available HWTs | CPU mask (w/o HWTs) |\n# |-----:|----:|----:|:---------------|:---------------|--------------------:|\n# |    0 |   0 |   6 | 48-55, 112-119 | 49-55, 113-119 |   0xfe000000000000  |\n# |    1 |   1 |   7 | 56-63, 120-127 | 57-63, 121-127 | 0xfe00000000000000  |\n# |    2 |   2 |   2 | 16-23, 80-87   | 17-23, 81-87   |           0xfe0000  |\n# |    3 |   3 |   3 | 24-32, 88-95   | 25-32, 89-95   |         0xfe000000  |\n# |    4 |   4 |   0 | 0-7, 64-71     | 1-7, 65-71     |               0xfe  |\n# |    5 |   5 |   1 | 8-15, 72-79    | 9-15, 73-79    |             0xfe00  |\n# |    6 |   6 |   4 | 32-39, 96-103  | 33-39, 97-103  |       0xfe00000000  |\n# |    7 |   7 |   5 | 40-47, 104-111 | 41-47, 105-111 |     0xfe0000000000  |\n\n# Mapping from column 2 (GCD)\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nexport ROCR_VISIBLE_DEVICES=\\$SLURM_LOCALID\nexec \\$*\nEOF\nchmod +x select_gpu_$SLURM_JOB_ID\n\n# Mapping from column 5 (Available HWTs)\nCPU_BIND1=\"map_cpu:49,57,17,25,1,9,33,41\"\n\n# Mapping from column 6 (CPU mask)\nCPU_BIND2=\"mask_cpu:0xfe000000000000,0xfe00000000000000\"\nCPU_BIND2=\"$CPU_BIND2,0xfe0000,0xfe000000\"\nCPU_BIND2=\"$CPU_BIND2,0xfe,0xfe00\"\nCPU_BIND2=\"$CPU_BIND2,0xfe00000000,0xfe0000000000\"\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport MPICH_OFI_NIC_POLICY=GPU\n\necho -e \"\\nPure MPI:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID mpi_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\necho -e \"\\nHybrid:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID hybrid_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID\n</code></pre> <p>To select the GPUs we either use a map with numbers of cores (ideal for pure MPI programs) or masks (the only option for hybrid programs). The mask that we give in the example uses 7 cores per CCD and always skips the first core, as is required on LUMI as the first core of each chiplet is reserved and not available to Slurm jobs. To select the right GPU for <code>ROCR_VISIBLE_DEVICES</code>  we can use the Slurm local task ID which is  also what the MPI rank will be.  We use a so-called \"bash here document\"  to generate the script. Note that in the bash here document we needed to protect the <code>$</code> with a backslash (so use <code>\\$</code>) as otherwise the variables would already be expanded when generating the script file.</p> <p>Instead of the somewhat complicated <code>--ntasks</code> with <code>srun</code> we could have specified <code>--ntasks-per-node=8</code> on a <code>#SBATCH</code> line which would have fixed the structure for all <code>srun</code> commands. Even though we want to use all GPUs in the node, <code>--gpus-per-node</code> or an equivalent option has to be specified either as an <code>#SBATCH</code> line or with each <code>srun</code> command or no GPUs will be made available to the tasks  started by the <code>srun</code> command.</p> <p>Note the output of the second <code>srun</code> command:</p> <pre><code>MPI 000 - OMP 000 - HWT 049 (CCD6) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 001 - OMP 000 - HWT 057 (CCD7) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 002 - OMP 000 - HWT 017 (CCD2) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 003 - OMP 000 - HWT 025 (CCD3) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\nMPI 004 - OMP 000 - HWT 001 (CCD0) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)\nMPI 005 - OMP 000 - HWT 009 (CCD1) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)\nMPI 006 - OMP 000 - HWT 033 (CCD4) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)\nMPI 007 - OMP 000 - HWT 041 (CCD5) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)\n</code></pre> <p>With the <code>-l</code> option of <code>gpu_check</code>, we also print some information about the CCD that a core belongs to and the  GCD and corresponding optimal CCD for each PCIe bus ID, which makes it very easy to check if the mapping is as intended. Note that the GCDs are indeed in the linear order starting with GCD0.</p>"},{"location":"intro-evolving/08-Binding/#linear-assignment-of-the-ccds-then-match-the-gcd","title":"Linear assignment of the CCDs, then match the GCD","text":"<p>Modifying the order of the GPUs to match the CCDs which are assigned as CCD i for local task i, is a bit more complicated as we still need to ensure that each task lands on a different CCD while assigning the GPU is more difficult. The relevant table for this example is:</p> CCD HWTs Available HWTs CPU mask (without HWTs) GCD 0 0-7, 64-71 1-7, 65-71 0xfe 4 1 8-15, 72-79 9-15, 73-79 0xfe00 5 2 16-23, 80-87 17-23, 81-87 0xfe0000 2 3 24-32, 88-95 25-32, 89-95 0xfe000000 3 4 32-39, 96-103 33-39, 97-103 0xfe00000000 6 5 40-47, 104-111 41-47, 105-111 0xfe0000000000 7 6 48-55, 112-119 49-55, 113-119 0xfe000000000000 0 7 56-63, 120-127 57-63, 121-127 0xfe00000000000000 1 <p>To modify the order of the GPUs, we now use an array with the desired order in the <code>select_gpu</code> script, and that order can be read from the last column in the above table. With the current setup of LUMI, with one core reserved on each chiplet, there are now two options to get the proper CPUs:</p> <ol> <li> <p>We can use masks to define the cores for each slot, but they will now look more regular, or</p> </li> <li> <p>we can simply use <code>--cpus-per-task=7</code> and then further restrict the number of threads per task     with <code>OMP_NUM_THREADS</code>.</p> </li> </ol> <p>The job script (for option 1) now becomes:</p> <pre><code>#!/bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --job-name=map-linear-CCD\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard-g\n#SBATCH --gpus-per-node=8\n#SBATCH --nodes=2\n#SBATCH --time=5:00\n\nmodule load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeCray-24.03\n\n# Mapping:\n# | Task | CCD | HWTs           | Available HWTs | CPU mask (w/o HWTs) | GCD |\n# |-----:|----:|:---------------|:---------------|--------------------:|----:|\n# |    0 |   0 | 0-7, 64-71     | 1-7, 65-71     |               0xfe  |   4 |\n# |    1 |   1 | 8-15, 72-79    | 9-15, 73-79    |             0xfe00  |   5 |\n# |    2 |   2 | 16-23, 80-87   | 17-23, 81-87   |           0xfe0000  |   2 |\n# |    3 |   3 | 24-32, 88-95   | 25-32, 89-95   |         0xfe000000  |   3 |\n# |    4 |   4 | 32-39, 96-103  | 33-39, 97-103  |       0xfe00000000  |   6 |\n# |    5 |   5 | 40-47, 104-111 | 41-47, 105-111 |     0xfe0000000000  |   7 |\n# |    6 |   6 | 48-55, 112-119 | 49-55, 113-119 |   0xfe000000000000  |   0 |\n# |    7 |   7 | 56-63, 120-127 | 57-63, 121-127 | 0xfe00000000000000  |   1 |\n\n# Mapping from column 6 (GCD)\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nGPU_ORDER=(4 5 2 3 6 7 0 1)\nexport ROCR_VISIBLE_DEVICES=\\${GPU_ORDER[\\$SLURM_LOCALID]}\nexec \\$*\nEOF\nchmod +x select_gpu_$SLURM_JOB_ID\n\n# Mapping from column 4 (Available HWTs)\nCPU_BIND1=\"map_cpu:1,9,17,25,33,41,49,57\"\n\n# Mapping from column 5 (CPU masks)\nCPU_BIND2=\"mask_cpu\"\nCPU_BIND2=\"$CPU_BIND2:0x00000000000000fe,0x000000000000fe00\"\nCPU_BIND2=\"$CPU_BIND2,0x0000000000fe0000,0x00000000fe000000\"\nCPU_BIND2=\"$CPU_BIND2,0x000000fe00000000,0x0000fe0000000000\"\nCPU_BIND2=\"$CPU_BIND2,0x00fe000000000000,0xfe00000000000000\"\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport MPICH_OFI_NIC_POLICY=GPU\n\necho -e \"\\nPure MPI:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID mpi_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\necho -e \"\\nHybrid:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID hybrid_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID\n</code></pre> <p>The order of the elements in <code>GPU_ORDER</code> comes from the last column of the table, while the values for <code>map_cpu</code> or <code>mask_cpu</code> can be read from the 3<sup>rd</sup> or 4<sup>th</sup> column respectively.</p> <p>The leading zeros in the masks in the <code>CPU_BIND2</code> environment variable are not needed but we added them as it makes it easier to see which chiplet is used in what position.</p>"},{"location":"intro-evolving/08-Binding/#the-green-ring","title":"The green ring","text":"<p>As a final example for whole node allocations, lets bind tasks such that the MPI ranks are mapped upon the green ring which is GCD 0 - 1 - 3 - 2 - 4 - 5 - 7 - 6 - 0. In other words, we want to create the mapping</p> Task GCD CCD Available cores CPU mask (without HWTs) 0 0 6 49-55, 113-119 0xfe000000000000 1 1 7 57-63, 121-127 0xfe00000000000000 2 3 3 25-32, 89-95 0xfe000000 3 2 2 17-23, 81-87 0xfe0000 4 4 0 1-7, 65-71 0xfe 5 5 1 9-15, 73-79 0xfe00 6 7 5 41-47, 105-111 0xfe0000000000 7 6 4 33-39, 97-103 0xfe00000000 <p>This mapping would be useful when using GPU-to-GPU communication in a scenario where task i only communicates with tasks i-1 and i+1 (module 8), so the communication pattern is a ring.</p> <p>Now we need to reorder both the cores and the GCDs, so we basically combine the approach taken in the two scripts above:</p> <pre><code>#!/bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --job-name=map-ring-green\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard-g\n#SBATCH --gpus-per-node=8\n#SBATCH --nodes=2\n#SBATCH --time=5:00\n\nmodule load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeCray-24.03\n\n# Mapping:\n# | Task | GCD | CCD | Available cores | CPU mask (w/o HWTs) |\n# |-----:|----:|----:|:----------------|--------------------:|\n# |    0 |   0 |   6 | 49-55, 113-119  |   0xfe000000000000  |\n# |    1 |   1 |   7 | 57-63, 121-127  | 0xfe00000000000000  |\n# |    2 |   3 |   3 | 25-32, 89-95    |         0xfe000000  |\n# |    3 |   2 |   2 | 17-23, 81-87    |           0xfe0000  |\n# |    4 |   4 |   0 | 1-7, 65-71      |               0xfe  |\n# |    5 |   5 |   1 | 9-15, 73-79     |             0xfe00  |\n# |    6 |   7 |   5 | 41-47, 105-111  |     0xfe0000000000  |\n# |    7 |   6 |   4 | 33-39, 97-103   |       0xfe00000000  |\n\n# Mapping from column 2 (GCD)\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\n# GPU order: Column 2 of the table\nGPU_ORDER=(0 1 3 2 4 5 7 6)\nexport ROCR_VISIBLE_DEVICES=\\${GPU_ORDER[\\$SLURM_LOCALID]}\nexec \\$*\nEOF\nchmod +x select_gpu_$SLURM_JOB_ID\n\n# Core order: Column 4 (Available cores)\nCPU_BIND1=\"map_cpu:49,57,25,17,1,9,41,33\"\n\n# Mask: Column 5 of the table, top to bottom, or column 3 (CCD) for the trick we use here.\nCCD_MASK=( 0x00000000000000fe \\\n           0x000000000000fe00 \\\n           0x0000000000fe0000 \\\n           0x00000000fe000000 \\\n           0x000000fe00000000 \\\n           0x0000fe0000000000 \\\n           0x00fe000000000000 \\\n           0xfe00000000000000 )\nCPU_BIND2=\"mask_cpu\"\nCPU_BIND2=\"$CPU_BIND2:${CCD_MASK[6]},${CCD_MASK[7]}\"\nCPU_BIND2=\"$CPU_BIND2,${CCD_MASK[3]},${CCD_MASK[2]}\"\nCPU_BIND2=\"$CPU_BIND2,${CCD_MASK[0]},${CCD_MASK[1]}\"\nCPU_BIND2=\"$CPU_BIND2,${CCD_MASK[5]},${CCD_MASK[4]}\"\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport MPICH_OFI_NIC_POLICY=GPU\n\necho -e \"\\nPure MPI:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID mpi_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\necho -e \"\\nHybrid:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID hybrid_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID\n</code></pre> <p>The values for <code>GPU_ORDER</code> are easily read from the second column of the table with the mapping that we prepared. The cores to use for the pure MPI run are easily read from the fourth column of the table: simply take the first core of each line. Finally, to build the mask, we used some bash trickery. We first define the bash array <code>CCD_MASK</code> with the mask for each chiplet. As this has a regular structure, this is easy to build. Then we compose the mask list for the CPUs by indexing in that array, where the indices are easily read from the third column in the mapping table.</p> <p>The alternative code to build <code>CPU_BIND2</code> is</p> <pre><code>CPU_BIND2=\"mask_cpu\"\nCPU_BIND2=\"$CPU_BIND2:0x00fe000000000000,0xfe00000000000000\"\nCPU_BIND2=\"$CPU_BIND2,0x00000000fe000000,0x0000000000fe0000\"\nCPU_BIND2=\"$CPU_BIND2,0x00000000000000fe,0x000000000000fe00\"\nCPU_BIND2=\"$CPU_BIND2,0x0000fe0000000000,0x000000fe00000000\"\n</code></pre> <p>which may be shorter, but requires some puzzling to build and hence is more prone to error. These values can be read from the last column of the table.</p> <p>The output of the second <code>srun</code> command is now</p> <pre><code>MPI 000 - OMP 000 - HWT 049 (CCD6) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 001 - OMP 000 - HWT 057 (CCD7) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 002 - OMP 000 - HWT 025 (CCD3) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\nMPI 003 - OMP 000 - HWT 017 (CCD2) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 004 - OMP 000 - HWT 001 (CCD0) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)\nMPI 005 - OMP 000 - HWT 009 (CCD1) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)\nMPI 006 - OMP 000 - HWT 041 (CCD5) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)\nMPI 007 - OMP 000 - HWT 033 (CCD4) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)\n</code></pre> <p>Checking the last column, we see that the GCDs are indeed in the desired order for the green ring, and  is is also easy to check that each task is also mapped on the optimal CCD for the GCD.</p> Job script with some more advanced bash (click to expand) <p> <pre><code>#!/bin/bash\n#SBATCH --job-name=map-advanced-multiple\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard-g\n#SBATCH --gpus-per-node=8\n#SBATCH --nodes=1\n#SBATCH --time=5:00\n\nmodule load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeCray-24.03\n\n#\n# Define the order of the GPUs and the core mask for CCD0\n# It is important that the order of the GPUs is a string with the numbers separated by spaces.\n#\nGCD_ORDER=\"0 1 5 4 6 7 3 2\"\ncoremask='2#00000010'  # Can use the binary representation, hexadecimal with 0x, or decimal\n\n#\n# run_gpu script, takes the string with GCDs as the first argument.\n#\ncat &lt;&lt; EOF &gt; run_gpu_$SLURM_JOB_ID\n#!/bin/bash\nGCD_ORDER=( \\$1 )\nshift\nexport ROCR_VISIBLE_DEVICES=\\${GCD_ORDER[\\$SLURM_LOCALID]}\nexec \"\\$@\"\nEOF\nchmod +x run_gpu_$SLURM_JOB_ID\n\n#\n# Build the CPU binding\n# Argument one is mask, all other arguments are treated as an array of GCD numbers.\n#\n\nfunction generate_mask {\n\n    # First argument is the mask for CCD0\n    mask=$1\n\n    # Other arguments are either a string already with the GCDs, or just one GCD per argument.\n    shift\n    GCDs=( \"$@\" )\n    # Fully expand (doesn't matter as the loop can deal with it, but good if we want to check the number)\n    GCDs=( ${GCDs[@]} )\n\n    # For each GCD, the corresponding CCD number in the optimal mapping.\n    MAP_to_CCD=( 6 7 2 3 0 1 4 5 )\n\n    CPU_BIND=\"\"\n\n    # Loop over the GCDs in the order of the list to compute the corresponding\n    # CPU mask.\n    for GCD in ${GCDs[@]}\n    do\n        # Get the matching CCD for this GCD\n        CCD=${MAP_to_CCD[$GCD]}\n\n        # Shift the mask for CCD0 to the position for CCD $CCD\n        printf -v tmpvar \"0x%016x\" $((mask &lt;&lt; $((CCD*8))))\n\n        # Add to CPU_BIND. We'll remove the leading , this creates later.\n        CPU_BIND=\"$CPU_BIND,$tmpvar\"\n    done\n\n    # Strip the leading ,\n    CPU_BIND=\"${CPU_BIND#,}\"\n\n    # Return the result by printing to stdout\n    printf \"$CPU_BIND\"\n\n}\n\n#\n# Running the check programs\n#\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport MPICH_OFI_NIC_POLICY=GPU\n\n# Some mappings:\nlinear_CCD=\"4 5 2 3 6 7 0 1\"\nlinear_GCD=\"0 1 2 3 4 5 6 7\" \nring_green=\"0 1 3 2 4 5 7 6\"\nring_red=\"0 1 5 4 6 7 3 2\"\n\necho -e \"\\nTest runs:\\n\"\n\n# Use either \"hpcat\" (once bugs are fixed) or \"gpu_check -l\"\nexe=\"gpu_check -l\"\n\necho -e \"\\nConsecutive CCDs:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) \\\n     --cpu-bind=mask_cpu:$(generate_mask $coremask $linear_CCD) \\\n     ./run_gpu_$SLURM_JOB_ID \"$linear_CCD\" $exe\n\necho -e \"\\nConsecutive GCDs:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) \\\n     --cpu-bind=mask_cpu:$(generate_mask $coremask $linear_GCD) \\\n     ./run_gpu_$SLURM_JOB_ID \"$linear_GCD\" $exe\n\necho -e \"\\nGreen ring:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) \\\n     --cpu-bind=mask_cpu:$(generate_mask $coremask $ring_green) \\\n     ./run_gpu_$SLURM_JOB_ID \"$ring_green\" $exe\n\necho -e \"\\nRed ring:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) \\\n     --cpu-bind=mask_cpu:$(generate_mask $coremask $ring_red) \\\n     ./run_gpu_$SLURM_JOB_ID \"$ring_red\" $exe\n\necho -e \"\\nFirst two CPU NUMA domains (assuming one node in the allocation):\\n\"\nhalf=\"4 5 2 3\"\nsrun --ntasks=4 \\\n     --cpu-bind=mask_cpu:$(generate_mask $coremask $half) \\\n     ./run_gpu_$SLURM_JOB_ID \"$half\" $exe\n\n/bin/rm -f run_gpu_$SLURM_JOB_ID\n</code></pre></p> <p>In this script, we have modified the and renamed the usual <code>select_gpu</code> script (renamed to <code>run_cpu</code>) to take as the first argument a string with a space-separated list of the GCDs to use. This has been combined with the bash function <code>generate_mask</code> (which could have been transformed in a script as well) that computes the CPU mask starting from the mask for CCD0 and shifting that mask as needed. The input is the mask to use and then the GCDs to use, either as a single string or as a series of arguments (e.g., resulting from an array expansion).</p> <p>Both commands are then combined in the <code>srun</code> command. The <code>generate_mask</code> function is used to generate the mask for <code>--gpu-bind</code> while the <code>run_gpu</code> script is used to set <code>ROCR_VISIBLE_DEVICES</code> for each task. The examples also show how easy it is to experiment with different mappings. The one limitation of the  script and function is that there can be only 1 GPU per task and one task per GPU, and the CPU mask is also limited to a single CCD (which makes sense with the GPU restriction). Generating masks that also include the second hardware thread is not supported yet. (We use bash arithmetic internally which is limited to 64-bit integers).</p>"},{"location":"intro-evolving/08-Binding/#what-about-allocate-by-resources-partitions","title":"What about \"allocate by resources\" partitions?","text":"<p>On partitions that are \"allocatable by resource\", e.g., <code>small-g</code>, you are never guaranteed that tasks  will be spread in a reasonable way over the CCDs and that the matching GPUs will be available to your job. Creating an optimal mapping or taking the topology into account is hence impossible. </p> <p>What is possible though is work around the fact that with the usual options for such resource allocations, Slurm will lock up the GPUs for individual tasks in control groups so that the Peer2Peer IPC intra-node communication mechanism has to be turned off. We can do this for job steps that follow the pattern of  resources allocated via the <code>sbatch</code> arguments (usually <code>#SBATCH</code> lines), and rely on three elements for that:</p> <ol> <li> <p>We can turn off the Slurm GPU binding mechanism with <code>--gpu-bind=none</code>.</p> </li> <li> <p>Even then, the GPUs will still be locked up in a control group on each node for the job and hence on each node     be numbered starting from zero.</p> </li> <li> <p>And each task also has a local ID that can be used to map the appropriate number of GPUs to each task.</p> </li> </ol> <p>This can be demonstrated with the following job script:</p> <p> <pre><code>#! /bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --job-name=map-smallg-1gpt\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=small-g\n#SBATCH --ntasks=12\n#SBATCH --cpus-per-task=2\n#SBATCH --gpus-per-task=1\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeCray-24.03\n\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nexport ROCR_VISIBLE_DEVICES=\\$SLURM_LOCALID\nexec \\$*\nEOF\nchmod +x ./select_gpu_$SLURM_JOB_ID\n\ncat &lt;&lt; EOF &gt; echo_dev_$SLURM_JOB_ID\n#!/bin/bash\nprintf -v task \"%02d\" \\$SLURM_PROCID\necho \"Task \\$task or node.local_id \\$SLURM_NODEID.\\$SLURM_LOCALID sees ROCR_VISIBLE_DEVICES=\\$ROCR_VISIBLE_DEVICES\"\nEOF\nchmod +x ./echo_dev_$SLURM_JOB_ID\n\nset -x\nsrun gpu_check -l\nsrun ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID gpu_check -l\nset +x\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID echo_dev_$SLURM_JOB_ID\n</code></pre></p> <p>To run this job successfully, we need 12 GPUs so obviously the tasks will be spread over more than  one node. The <code>echo_dev</code> command in this script only shows us the value of <code>ROCR_VISIBLE_DEVICES</code>  for the task at that point, something that <code>gpu_check</code> in fact also reports as <code>GPU_ID</code>, but this is just in case you don't believe...</p> <p>The output of the first <code>srun</code> command is:</p> <pre><code>+ srun gpu_check -l\nMPI 000 - OMP 000 - HWT 001 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 000 - OMP 001 - HWT 002 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 001 - OMP 000 - HWT 003 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)\nMPI 001 - OMP 001 - HWT 004 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)\nMPI 002 - OMP 000 - HWT 005 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)\nMPI 002 - OMP 001 - HWT 006 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)\nMPI 003 - OMP 000 - HWT 007 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)\nMPI 003 - OMP 001 - HWT 008 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)\nMPI 004 - OMP 000 - HWT 009 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1(GCD4/CCD0)\nMPI 004 - OMP 001 - HWT 010 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1(GCD4/CCD0)\nMPI 005 - OMP 000 - HWT 011 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6(GCD5/CCD1)\nMPI 005 - OMP 001 - HWT 012 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6(GCD5/CCD1)\nMPI 006 - OMP 000 - HWT 013 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9(GCD6/CCD4)\nMPI 006 - OMP 001 - HWT 014 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9(GCD6/CCD4)\nMPI 007 - OMP 000 - HWT 015 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc(GCD7/CCD5)\nMPI 007 - OMP 001 - HWT 016 (CCD2) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc(GCD7/CCD5)\nMPI 008 - OMP 000 - HWT 001 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 008 - OMP 001 - HWT 002 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 009 - OMP 000 - HWT 003 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)\nMPI 009 - OMP 001 - HWT 004 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)\nMPI 010 - OMP 000 - HWT 005 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)\nMPI 010 - OMP 001 - HWT 006 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)\nMPI 011 - OMP 000 - HWT 007 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)\nMPI 011 - OMP 001 - HWT 008 (CCD1) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)\n</code></pre> <p>In other words, we see that we did get cores on two nodes that obviously are not well aligned with the GCDs, and 8 GPUS on the first and 4 on the second node.</p> <p>The output of the second <code>srun</code> is:</p> <pre><code>+ srun ./echo_dev_4359428\n+ sort\nTask 00 or node.local_id 0.0 sees ROCR_VISIBLE_DEVICES=0\nTask 01 or node.local_id 0.1 sees ROCR_VISIBLE_DEVICES=0\nTask 02 or node.local_id 0.2 sees ROCR_VISIBLE_DEVICES=0\nTask 03 or node.local_id 0.3 sees ROCR_VISIBLE_DEVICES=0\nTask 04 or node.local_id 0.4 sees ROCR_VISIBLE_DEVICES=0\nTask 05 or node.local_id 0.5 sees ROCR_VISIBLE_DEVICES=0\nTask 06 or node.local_id 0.6 sees ROCR_VISIBLE_DEVICES=0\nTask 07 or node.local_id 0.7 sees ROCR_VISIBLE_DEVICES=0\nTask 08 or node.local_id 1.0 sees ROCR_VISIBLE_DEVICES=0\nTask 09 or node.local_id 1.1 sees ROCR_VISIBLE_DEVICES=0\nTask 10 or node.local_id 1.2 sees ROCR_VISIBLE_DEVICES=0\nTask 11 or node.local_id 1.3 sees ROCR_VISIBLE_DEVICES=0\n</code></pre> <p>It is normal that each task sees <code>ROCR_VISIBLE_DEVICES=0</code> even though we have seen that they all use a different GPU. This is because each task is locked up in a control group with only one GPU, which then  gets number 0.</p> <p>The output of the third <code>srun</code> command is:</p> <pre><code>+ sort\nTask 00 or node.local_id 0.0 sees ROCR_VISIBLE_DEVICES=\nTask 01 or node.local_id 0.1 sees ROCR_VISIBLE_DEVICES=\nTask 02 or node.local_id 0.2 sees ROCR_VISIBLE_DEVICES=\nTask 03 or node.local_id 0.3 sees ROCR_VISIBLE_DEVICES=\nTask 04 or node.local_id 0.4 sees ROCR_VISIBLE_DEVICES=\nTask 05 or node.local_id 0.5 sees ROCR_VISIBLE_DEVICES=\nTask 06 or node.local_id 0.6 sees ROCR_VISIBLE_DEVICES=\nTask 07 or node.local_id 0.7 sees ROCR_VISIBLE_DEVICES=\nTask 08 or node.local_id 1.0 sees ROCR_VISIBLE_DEVICES=\nTask 09 or node.local_id 1.1 sees ROCR_VISIBLE_DEVICES=\nTask 10 or node.local_id 1.2 sees ROCR_VISIBLE_DEVICES=\nTask 11 or node.local_id 1.3 sees ROCR_VISIBLE_DEVICES=\n</code></pre> <p>Slurm in fact did not set <code>ROCR_VISIBLE_DEVICES</code> because we turned binding off.</p> <p>In the next <code>srun</code> command we set <code>ROCR_VISIBLE_DEVICES</code> based on the local task ID and get:</p> <pre><code>+ srun --gpu-bind=none ./select_gpu_4359428 ./echo_dev_4359428\n+ sort\nTask 00 or node.local_id 0.0 sees ROCR_VISIBLE_DEVICES=0\nTask 01 or node.local_id 0.1 sees ROCR_VISIBLE_DEVICES=1\nTask 02 or node.local_id 0.2 sees ROCR_VISIBLE_DEVICES=2\nTask 03 or node.local_id 0.3 sees ROCR_VISIBLE_DEVICES=3\nTask 04 or node.local_id 0.4 sees ROCR_VISIBLE_DEVICES=4\nTask 05 or node.local_id 0.5 sees ROCR_VISIBLE_DEVICES=5\nTask 06 or node.local_id 0.6 sees ROCR_VISIBLE_DEVICES=6\nTask 07 or node.local_id 0.7 sees ROCR_VISIBLE_DEVICES=7\nTask 08 or node.local_id 1.0 sees ROCR_VISIBLE_DEVICES=0\nTask 09 or node.local_id 1.1 sees ROCR_VISIBLE_DEVICES=1\nTask 10 or node.local_id 1.2 sees ROCR_VISIBLE_DEVICES=2\nTask 11 or node.local_id 1.3 sees ROCR_VISIBLE_DEVICES=3\n</code></pre> <p>Finally, we run <code>gpu_check</code> again and see the same assignment of physical GPUs again as when we started, but now with different logical device numbers passed by <code>ROCR_VISIBLE_DEVICES</code>. The device number for the hip runtime is always 0 though which is normal as <code>ROCR_VISIBLE_DEVICES</code> restricts the access of the hip runtime to one GPU.</p> <pre><code>+ srun --gpu-bind=none ./select_gpu_4359428 gpu_check -l\nMPI 000 - OMP 000 - HWT 001 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 000 - OMP 001 - HWT 002 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 001 - OMP 000 - HWT 003 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 001 - OMP 001 - HWT 004 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 002 - OMP 000 - HWT 005 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 002 - OMP 001 - HWT 006 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 003 - OMP 000 - HWT 007 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\nMPI 003 - OMP 001 - HWT 008 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\nMPI 004 - OMP 000 - HWT 009 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)\nMPI 004 - OMP 001 - HWT 010 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)\nMPI 005 - OMP 000 - HWT 011 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)\nMPI 005 - OMP 001 - HWT 012 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)\nMPI 006 - OMP 000 - HWT 013 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)\nMPI 006 - OMP 001 - HWT 014 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)\nMPI 007 - OMP 000 - HWT 015 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)\nMPI 007 - OMP 001 - HWT 016 (CCD2) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)\nMPI 008 - OMP 000 - HWT 001 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 008 - OMP 001 - HWT 002 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 009 - OMP 000 - HWT 003 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 009 - OMP 001 - HWT 004 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 010 - OMP 000 - HWT 005 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 010 - OMP 001 - HWT 006 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 011 - OMP 000 - HWT 007 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\nMPI 011 - OMP 001 - HWT 008 (CCD1) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\n</code></pre> Example job script when using 2 GPUs per task (click to expand) <p> <pre><code>#! /bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --job-name=map-smallg-2gpt\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=small-g\n#SBATCH --ntasks=6\n#SBATCH --cpus-per-task=2\n#SBATCH --gpus-per-task=2\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/24.03 partition/G lumi-CPEtools/1.2a-cpeCray-24.03\n\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nexport ROCR_VISIBLE_DEVICES=\\$((SLURM_LOCALID*2)),\\$((SLURM_LOCALID*2+1))\nexec \\$*\nEOF\nchmod +x ./select_gpu_$SLURM_JOB_ID\n\ncat &lt;&lt; EOF &gt; echo_dev_$SLURM_JOB_ID\n#!/bin/bash\nprintf -v task \"%02d\" \\$SLURM_PROCID\necho \"Task \\$task or node.local_id \\$SLURM_NODEID.\\$SLURM_LOCALID sees ROCR_VISIBLE_DEVICES=\\$ROCR_VISIBLE_DEVICES\"\nEOF\nchmod +x ./echo_dev_$SLURM_JOB_ID\n\nset -x\nsrun gpu_check -l\nsrun ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID gpu_check -l\nset +x\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID echo_dev_$SLURM_JOB_ID\n</code></pre></p> <p>The changes that were required are only minimal. We now assign 2 GPUs to <code>ROCR_VISIBLE_DEVICES</code> which  is easily done with some bash arithmetic.</p>"},{"location":"intro-evolving/08-Binding/#further-material","title":"Further material","text":"<ul> <li> <p>Another presentation on distribution and binding can be found in the pre-2025     4-day comprehensive courses, e.g., the      \"Advanced Placement\" talk in the course in Amsterdam in October 2024</p> <p>Material of this presentation is available to all LUMI users on the system. Check the course website for the names of the files.</p> </li> <li> <p>Rank reordering in Cray MPICH is discussed is also discussed in more detail in our     comprehensive LUMI courses,     but in the lecture on \"MPI Topics on the HPE Cray EX Supercomputer\"     that discusses more advanced MPI on LUMI, including loads of environment variables that can be used to     improve the performance.</p> </li> </ul>"},{"location":"intro-evolving/09-Lustre/","title":"Lustre","text":"<p>Last update of this page: October 1, 2025</p>"},{"location":"intro-evolving/09-Lustre/#using-lustre","title":"Using Lustre","text":""},{"location":"intro-evolving/09-Lustre/#file-systems-on-lumi","title":"File systems on LUMI","text":"<p>Supercomputing since the second half of the 1980s, has almost always been about  trying to build a very fast system from relatively cheap volume components or technologies (as low as you can go without loosing too much reliability), and very cleverly written software both at the system level (to make the system look like a true single system as much as possible) and at the application level (to deal with the restrictions that inevitably come with such a setup).</p> <p>The Lustre parallel file system that we use on LUMI (and that is its main file system serving user files) fits in that way of thinking. A large file system is built by linking many fairly regular file servers through a fast network to the compute resources to build a single system with a lot of storage capacity and a lot of bandwidth. It has its restrictions also though:  not all types of IOPs (number of I/O operations per second) scale as well or as easily as bandwidth and capacity so this comes with usage restrictions on large clusters that may a lot more severe than you are used to from small systems. And yes, it is completely normal that some file operations are slower than on the SSD of a good PC.</p> <p>Lustre is the main filesystem on many large supercomputers and on all Cray EX systems.</p> <p>HPE Cray EX systems go even one step further than many other systems.  On HPE Cray EX systems, Lustre is the only network file system directly served to the compute nodes. Other network file system come via a piece of software called Data Virtualisation Service (abbreviated DVS) that basically forwards I/O requests to servers in the management section of the cluster where the actual file system runs.  This is part of the measures that Cray takes in Cray Operating System to minimise OS jitter on the compute nodes to improve scalability of applications, and to reduce the memory footprint of the OS on the compute nodes.</p>"},{"location":"intro-evolving/09-Lustre/#lustre-building-blocks","title":"Lustre building blocks","text":"<p>A key element of Lustre - but also of other parallel file systems for large parallel computers such as BeeGFS, Spectrum Scale (formerly GPFS) or PanFS - is the separation of metadata and the actual file data, as the way both are accessed and used is very different.</p> <p>A Lustre system consists of the following blocks:</p> <ol> <li> <p>Metadata servers (MDSes) with one or more metadata targets (MDTs) each store     namespace metadata such as filenames, directories and access permissions, and the     file layout.</p> <p>Each MDT is a filesystem, usually with some level of RAID or similar technologies for increased reliability. Usually there is also more than one MDS and they are put in a high availability setup for increased availability (and this is the case in  LUMI).</p> <p>Metadata is accessed in small bits and it does not need much capacity. However,  metadata accesses are hard to parallelise so it makes sense to go for the fastest storage possible even if that storage is very expensive per terabyte. On LUMI all metadata servers use SSDs.</p> </li> <li> <p>Object storage servers (OSSes) with one or more object storage targets (OSTs) each     store the actual data. Data from a single file can be distributed across multiple     OSTs and even multiple OSSes. As we shall see, this is also the key to getting very     high bandwidth access to the data.</p> <p>Each OST is a filesystem, again usually with some level of RAID or similar technologies to survive disk failures. One OSS typically has between 1 and 8 OSTs, and in a big setup a high availability setup will be used again (as is the case in LUMI).</p> <p>The total capacity of a Lustre file system is the sum of the capacity of all OSTs in the  Lustre file system. Lustre file systems are often many petabytes large.</p> <p>Now you may think differently based upon prices that you see in the PC market for hard drives and SSDs, but SSDs of data centre quality are still up to 10 times as expensive as  hard drives of data centre quality. Building a file system of several tens of petabytes out of SSDs is still extremely expensive and rarely done, certainly in an environment  with a high write pressure on the file system, as that requires the highest quality SSDs. Hence it is not uncommon that supercomputers will use mostly hard drives for their large storage systems.</p> <p>On LUMI there is roughly 80 PB spread across 4 large hard disk based Lustre file systems, and 8.5 PB in an SSD-based Lustre file system. However, all MDSes use SSD storage.</p> </li> <li> <p>Lustre clients access and use the data. They make the whole Lustre file system look like     a single file system.</p> <p>Lustre is transparent in functionality. You can use a Lustre file system just as any  other regular file system, with all your regular applications. However, it is not transparent when it comes to performance: How you use Lustre can have a huge impact on  performance. Lustre is optimised very much for high bandwidth access to large chunks of data at a time from multiple nodes in the application simultaneously, and is not very good  at handling access to a large pool of small files instead. </p> <p>So you have to store your data (but also your applications as the are a kind of data also) in an appropriate way, in fewer but larger files instead of more smaller files.  Some centres with large supercomputers will advise you to containerise software for optimal performance. On LUMI we do advise Python users or users who install software through Conda to do so. </p> </li> <li> <p>All these components are linked together through a high performance interconnect. On HPE Cray EX     systems - but on more an more other systems also - there is no separate network anymore for      storage access and the same high performance interconnect that is also used for internode     communication by applications (through, e.g., MPI) is used for that purpose.</p> </li> <li> <p>There is also a management server which is not mentioned on the slides, but that component     is not essential to understand the behaviour of Lustre for the purpose of this lecture.</p> </li> </ol> <p>Links</p> <p>See also the \"Lustre Components\" in \"Understanding Lustre Internals\" on the Lustre Wiki</p>"},{"location":"intro-evolving/09-Lustre/#striping-large-files-are-spread-across-osts","title":"Striping: Large files are spread across OSTs","text":"<p>On Lustre, large files are typically broken into blocks called stripes or chunks that are then cyclically spread across a number of chunk files called objects in LUSTRE, each on a separate OST.  In the figure in the slide above, the file is spread across the OSTs 0, 2, 4 and 6.</p> <p>This process is completely transparent to the user with respect to correctness. The Lustre client takes care of the process and presents a traditional file to the application program. It is however not transparent with respect to performance. The performance of reading and writing a file depends a lot on how the file is spread across the OSTs of a file system.</p> <p>Basically, there are two parameters that have to be chosen: The size of the stripes (all stripes have the same size in this example, except for the last one which may be smaller) and the number of OSTs  that should be used for a file. Lustre itself takes care of choosing the OSTs in general.</p> <p>There are variants of Lustre where one has multiple layouts per file which can come in handy if one  doesn't know the size of the file in advance. The first part of the file will then typically be written with fewer OSTs and/or smaller chunks, but this is outside the scope of this course. The feature is known as Progressive File Layout.</p> <p>The stripe size and number of OSTs used can be chosen on a file-by-file basis. The default on LUMI is to use only one OST for a file. This is done because that is the most reasonable choice for the many small files that many unsuspecting users have, and as we shall see, it is sometimes even the best choice for users working with large files. But it is not always the best choice. And unfortunately there is no single set of parameters that is good for all users.</p> <p>Objects</p> <p>The term \"object\" nowadays has different meanings, even in the storage world. The Object Storage Servers in Lustre should not be confused with the object storage used in cloud solutions such as Amazon Web Services (AWS) with the S3 storage service or the LUMI-O object storage.  In fact, the Object Storage Servers use a regular file system such as ZFS or ldiskfs to store the \"objects\".</p>"},{"location":"intro-evolving/09-Lustre/#accessing-a-file","title":"Accessing a file","text":"<p>Let's now study how Lustre will access a file for reading or writing. Let's assume that the second client in the above picture wants to write something to the file.</p> <ul> <li> <p>The first step is opening the file. </p> <p>For that, the Lustre client has to talk to the metadata server (MDS) and query some information about the file.</p> <p>The MDS in turn will return information about the file, including the layout of the file: chunksize and the OSSes/OSTs that keep the chunks of the file.</p> </li> <li> <p>From that point on, the client doesn't need to talk to the MDS anymore and can talk directly to      the OSSes to write data to the OSTs or read data from the OSTs.</p> </li> </ul>"},{"location":"intro-evolving/09-Lustre/#parallelism-is-key","title":"Parallelism is key!","text":"<p>The metadata servers can be the bottleneck in a Lustre setup.  It is not easy to spread metadata across multiple MDSes efficiently. Moreover, the amount of metadata for any given file is small, so any metadata operation will translate into small disk accesses on the MDTs and hence not  fully exploit the speed that some RAID setups can give you.</p> <p>However, when reading and writing data, there are up to four levels of parallelism:</p> <ol> <li> <p>The read and write operations can engage multiple OSSes.</p> </li> <li> <p>Since a single modern OSS can handle more bandwidth than a some OSTs can     deliver, OSSes may have multiple OSTs.</p> <p>How many OSTs are engaged is something that a user has control over.</p> </li> <li> <p>An OST will contain many disks or SSDs, typically with some kind of RAID, but hence     each read or write operation to an OST can engage multiple disks.</p> <p>An OST will only be used optimally when doing large enough file accesses. But the  file system client may help you here with caching.</p> </li> <li> <p>Internally, SSDs are also parallel devices. The high bandwidth of modern high-end SSDs     is the result of engaging multiple channels to the actual storage \"chips\" internally.</p> </li> </ol> <p>So to fully benefit from a Lustre file system, it is best to work with relatively few files (to not overload the MDS) but very large disk accesses. Very small I/O operations wouldn't even benefit from the RAID acceleration, and this is especially true for very small files as they cannot even benefit from caching provided by the file system client (otherwise a file system client may read in more data than requested, as file access is often sequential anyway so it would be prepared for the next access). To make efficient use of the OSTs it is important to have a relatively large chunk size and relatively large I/O operations, even more so for hard disk based file systems as if the OST file system manages to organise the data well on disk, it is a good way to reduce the impact on effective bandwidth of the inherent latency of disk access. And to engage multiple OSTs simultaneously (and thus reach a bandwidth which is much  higher than a single OST can provide), even larger disk accesses will be needed so that multiple chunks are read or written simultaneously. Usually you will have to do the I/O in a distributed memory application from multiple nodes simultaneously as otherwise the bandwidth to the interconnect and processing capacity of the client software  of a single node might become the limiting factor.</p> <p></p> <p>Not all codes are using Lustre optimally though, even with the best care of their users.</p> <ul> <li> <p>\ud83d\ude00 Some codes use files in scientific data formats like HDF5 and netCDF, and when written     properly they can have very scalable performance.</p> <p>A good code will write data to large files, from multiple nodes simultaneously, but  will avoid doing I/O from too many ranks simultaneously to avoid bombarding the OSSes/OSTs with I/O requests. But that is a topic for a more advanced course...</p> <p>One user has reported reading data from the hard disk based parallel file systems at about 25% of the maximal bandwidth, which is very good given that other users where also using that file system at the same time and not always in an optimal way.</p> <p>Surprisingly many of these codes may be rather old. But their authors grew up with noisy floppy drives (do you still know what that is) and slow hard drives so learned how to program efficiently.</p> </li> <li> <p>\ud83d\ude2d But some codes open one or more files per MPI rank. Those codes may have difficulties     scaling to a large number of ranks, as they will put a heavy burden on the MDS when those     files are created, but also may bombard each OSS/OST with too many I/O requests.</p> <p>Some of these codes are rather old also, but were never designed to scale to thousands of MPI ranks. However, nowadays some users are trying to solve such big problems that  the computations do scale reasonably well. But the I/O of those codes becomes a problem...</p> </li> <li> <p>\ud83d\ude2d\ud83d\ude2d But some users simply abuse the file system as an unstructured database and simply drop     their data as tens of thousands or even millions of small files with each one data element,     rather than structuring that data in suitable file formats. This is especially common in     science fields that became popular relatively recently - bio-informatics and AI - as those     users typically started their work on modern PCs with fast SSDs. </p> <p>The problem there is that metadata access and small I/O operations don't scale well to large systems. Even copying such a data set to a local SSD would be a problem should a compute node have a local SSD, but local SSDs suitable for supercomputers are also very expensive as they  have to deal with lots of write operations. Your gene data or training data set may be relatively static, but on a supercomputer you cannot keep the same node for weeks so you'd need to rewrite  your data to local disks very often. And there are shared file systems with better small file performance than Lustre, but those that scale to the size of even a fraction of Lustre, are also very expensive. And remember that supercomputing works exactly the opposite way: Try to reduce costs by using relatively cheap hardware but cleverly written software at all levels (system and application) as at a very large scale, this is ultimately cheaper than investing more in hardware and less in software. </p> </li> </ul> <p>Lustre was originally designed to achieve very high bandwidth to/from a small number of files, and that is in fact a good match for well organised scientific data sets and/or checkpoint data, but was not designed to handle large numbers of small files. Nowadays of course optimisations to  deal better with small files are being made, but they may come at a high hardware cost.</p>"},{"location":"intro-evolving/09-Lustre/#how-to-determine-the-striping-values","title":"How to determine the striping values?","text":"<p>If you only access relatively small files (up to a few hundreds of kilobytes) and access them  sequentially, then you are out of luck. \ud83d\ude2d\ud83d\ude2d\ud83d\ude2d There is not much you can do. Engaging multiple OSTs  for a single file is not useful at all in this case, and you will also have no parallelism from accessing multiple files that may be stored on different OSTs. The metadata operations may also be rather expensive compared to the cost of reading the file once opened.</p> <p>As a rule of thumb, if you access a lot of data with a data access pattern that can exploit parallelism, try to use all OSTs of the Lustre file system without unnecessary overloading them:</p> <ul> <li> <p>If the number of files that will be accessed simultaneously is larger than the number of     OSTs, it is best to not spread a single file across OSTs and hence use a stripe count of 1.</p> <p>It will also reduce Lustre contention and OST file locking and as such gain performance for everybody.</p> </li> <li> <p>At the opposite end, if you access only one very large file and use large or parallel disk     accesses, set the stripe count to the number of OSTs (or a smaller number if you notice in     benchmarking that the I/O performance plateaus). On a system the size of LUMI with storage     as powerful as on LUMI, this will only work if you have more than on I/O client. </p> </li> <li> <p>When using multiple similar sized files simultaneously but less files than there are OSTs,     you should probably chose the stripe count such that the product of the number of files     and the stripe count is approximately the number of OSTs. E.g., with 32 OSTs and 8 files,     set the stripe count to 4.</p> </li> </ul> <p>It is better not to force the system to use specific OSTs but to let it chose OSTs at random.</p> <p>The typical stripe size  (size of the chunks) to use can be a bit harder to determine. Typically this will be 1MB or more, and it can be up to 4 GB, but that only makes sense for very large files. The ideal stripe size will also depend on the characteristics of the I/O in the file. If the application never writes more than 1 GB of data in a single  sequential or parallel I/O operation before continuing with more computations, obviously with a stripe size of 1 GB you'd be engaging only a single OST for each write operation.</p>"},{"location":"intro-evolving/09-Lustre/#managing-the-striping-parameters","title":"Managing the striping parameters","text":"<p>The basic Lustre command for regular users to do special operations on Lustre is the <code>lfs</code> command, which has various subcommands.</p> <p>The first interesting subcommand is <code>df</code> which has a similar purpose as the regular Linux <code>df</code> command: Return information about the filesystem. In particular,</p> <pre><code>lfs df -h\n</code></pre> <p>will return information about all available Lustre filesystems. The <code>-h</code> flag tells the command to use \"human-readable\" number formats: return sizes in gigabytes and terabytes rather than blocks. On LUMI, the output starts with:</p> <pre><code>$ lfs df -h\nUUID                       bytes        Used   Available Use% Mounted on\nlustref1-MDT0000_UUID       11.8T       16.8G       11.6T   1% /pfs/lustref1[MDT:0]\nlustref1-MDT0001_UUID       11.8T        4.1G       11.6T   1% /pfs/lustref1[MDT:1]\nlustref1-MDT0002_UUID       11.8T        2.8G       11.7T   1% /pfs/lustref1[MDT:2]\nlustref1-MDT0003_UUID       11.8T        2.7G       11.7T   1% /pfs/lustref1[MDT:3]\nlustref1-OST0000_UUID      121.3T       21.5T       98.5T  18% /pfs/lustref1[OST:0]\nlustref1-OST0001_UUID      121.3T       21.6T       98.4T  18% /pfs/lustref1[OST:1]\nlustref1-OST0002_UUID      121.3T       21.4T       98.6T  18% /pfs/lustref1[OST:2]\nlustref1-OST0003_UUID      121.3T       21.4T       98.6T  18% /pfs/lustref1[OST:3]\n</code></pre> <p>so the command can also be used to see the number of MDTs and OSTs available in each filesystem, with the capacity.</p> <p></p> <p>Striping in Lustre is set at a filesystem level by the sysadmins, but users can adjust the settings at the directory level (which then sets the default for files created in that directory) and file level. Once a file is created, the striping configuration cannot be changed anymore on-the-fly. </p> <p>To inspect the striping configuration, one can use the <code>getstripe</code> subcommand of <code>lfs</code>.</p> <p>Let us first use it at the directory level:</p> <pre><code>$ lfs getstripe -d /appl/lumi/SW\nstripe_count:  1 stripe_size:   1048576 pattern:       0 stripe_offset: -1\n\n$ lfs getstripe -d --raw /appl/lumi/SW\nstripe_count:  0 stripe_size:   0 pattern:       0 stripe_offset: -1\n</code></pre> <p>The <code>-d</code> flag tells that we only want information about the directory itself and not about everything in that directory. The first <code>lfs getstripe</code> command tells us that files  created in this directory will use only a single OST and have a stripe size of 1 MiB.  By adding the <code>--raw</code> we actually see the settings that have been made specifically for this directory. The <code>0</code> for <code>stripe_count</code> and <code>stripe_size</code> means that the default value is being used, and the <code>stripe_offset</code> of <code>-1</code> also indicates the default value.</p> <p>We can also use <code>lfs getstripe</code> for individual files:</p> <pre><code>$ lfs getstripe /appl/lumi/LUMI-SoftwareStack/etc/motd.txt\n/appl/lumi/LUMI-SoftwareStack/etc/motd.txt\nlmm_stripe_count:  1\nlmm_stripe_size:   1048576\nlmm_pattern:       raid0\nlmm_layout_gen:    0\nlmm_stripe_offset: 10\n        obdidx           objid           objid           group\n            10        56614379      0x35fddeb                0\n</code></pre> <p>Now <code>lfs getstripe</code> does not only return the stripe size and number of OSTs used, but it will also show the OSTs that are actually used (in the column <code>obdidx</code> of the output). The <code>lmm_stripe_offset</code> is also the number of the OST with the first object of the file.</p> <p></p> <p></p> <p>The final subcommand that we will discuss is the <code>setstripe</code> subcommand to set the striping policy for a file or directory.</p> <p>Let us first look at setting a striping policy at the directory level:</p> <pre><code>$ module use /appl/local/training/modules/2day-20240502\n$ module load lumi-training-tools\n$ mkdir testdir\n$ lfs setstripe -S 2m -c 4 testdir\n$ cd testdir\n$ mkfile 2g testfile1\n$ lfs getstripe testfile1\ntestfile1\nlmm_stripe_count:  4\nlmm_stripe_size:   2097152\nlmm_pattern:       raid0\nlmm_layout_gen:    0\nlmm_stripe_offset: 28\n        obdidx           objid           objid           group\n            28        66250987      0x3f2e8eb                0\n            30        66282908      0x3f3659c                0\n             1        71789920      0x4476d60                0\n             5        71781120      0x4474b00                0\n</code></pre> <p>The <code>lumi-training-tools</code> module provides the <code>mkfile</code> command that we use in this example.</p> <p>We first create a directory and then set the striping parameters to a stripe size of 2 MiB (the <code>-S</code> flag) and a so-called stripe count, the number of OSTs used for  the file, of 4 (the <code>-c</code> flag).</p> <p>Next we go into the subdirectory and use the <code>mkfile</code> command to generate a file of  2 GiB. </p> <p>When we now check the file layout of the file that we just created with <code>lfs getstripe</code>, we see that the file now indeed uses 4 OSTs with a stripe size of 2 MiB, and has object on in this case OSTs 28, 30, 1 and 5. </p> <p>However, we can even control striping at the level of an individual file. The condition is that the layout of the file is set as soon as it is created. We can do this also with <code>lfs setstripe</code>:</p> <pre><code>$ lfs setstripe -S 16m -c 2 testfile2\n$ ls -lh\ntotal 0\n-rw-rw---- 1 XXXXXXXX project_462000000 2.0G Jan 15 16:17 testfile1\n-rw-rw---- 1 XXXXXXXX project_462000000    0 Jan 15 16:23 testfile2\n$ lfs getstripe testfile2\ntestfile2\nlmm_stripe_count:  2\nlmm_stripe_size:   16777216\nlmm_pattern:       raid0\nlmm_layout_gen:    0\nlmm_stripe_offset: 10\n        obdidx           objid           objid           group\n            10        71752411      0x446dadb                0\n            14        71812909      0x447c72d                0\n</code></pre> <p>In this example, the <code>lfs setstripe</code> command will create an empty file but with the required layout. In this case we have set the stripe size to 16 MiB and use only 2 OSTs, and the <code>lfs getstripe</code> command confirms that information. We can now open the file to write data into it with the regular file operations of the Linux glibc library or your favourite programming language (though of course you need to take into account that the file already exists so you should use routines that do not return an error if the file already exists).</p> <p>Lustre API</p> <p>Lustre also offers a C API to directly set file layout properties, etc., from your package. Few scientific packages seem to support it though.</p>"},{"location":"intro-evolving/09-Lustre/#the-metadata-servers","title":"The metadata servers","text":"<p>Parallelising metadata access is very difficult. Even large Lustre filesystems have very few metadata servers. They are a finite and shared resource, and overloading the metadata server slows down the file system for all users.</p> <p>The metadata servers are involved in many operations. The play a role in creating, opening and also closing files. The provide some of the attributes of a file. And they also play a role in file locking.</p> <p>Yet the metadata servers have a very finite capacity. The Lustre documentation claims that in theory a single metadata server should be capable of up to 200,000 operations per second, depending on the type of request. However, 75,000 operations per second may be more realistic.</p> <p>As a user, many operations that you think are harmless from using your PC, are in fact expensive operations on a supercomputer with a large parallel file system and you will find \"Lustre best practices\" pages on web sites of many large supercomputer centres. Some tips for regular users:</p> <ul> <li> <p>Any command that requests attributes is fairly expensive and should not be used      in large directories. This holds even for something as trivial as <code>ls -l</code>.      But it is even more so for commands as <code>du</code> that run recursively through attributes     of lots of files.</p> </li> <li> <p>Opening a file is also rather expensive as it involves a metadata server and one or more     object servers. It is not a good idea to frequently open and close the same file while      processing data. </p> </li> <li> <p>Therefore access to many small files from many processes is not a good idea. One example of this     is using Python, and even more so if you do distributed memory parallel computing with Python.     This is why on LUMI we ask to do big Python installations in containers. Another alternative is     to run such programs from <code>/tmp</code> (and get them on <code>/tmp</code> from an archive file).</p> <p>For data, it is not a good idea to dump a big dataset as lots of small files on the filesystem. Data should be properly organised, preferably using file formats that support parallel access from many processes simultaneously. Technologies popular in supercomputing are  HDF5, netCDF and ADIOS2. Sometimes libraries that read tar-files or other archive file formats without first  fully uncompressing, may even be enough for read-only data.  Or if your software runs in a container, you may be able to put your read-only dataset into a  SquashFS file and mount into a container.</p> </li> <li> <p>Likewise, shuffling data in a distributed memory program should not be done via the      filesystem (put data on a shared filesystem and then read it again in a different     order) but by direct communication between the processes over the interconnect.</p> </li> <li> <p>It is also obvious that directories with thousands of files should be avoided as even an      <code>ls -l</code> command on that directory generates a high load on the metadata servers. But the same     holds for commands such as <code>du</code> or <code>find</code>.</p> <p>Note that the <code>lfs</code> command also has a subcommand <code>find</code> (see <code>man lfs-find</code>), but it cannot do everything that the regular Linux <code>find</code> command can do. E.g., the <code>--exec</code> functionality is missing. But to simply list files it will put less strain on the filesystem as running the  regular Linux <code>find</code> command.</p> </li> </ul> <p>There are many more tips more specifically for programmers. As good use of the filesystems on a supercomputer is important and wrong use has consequences for all other users, it is an important topic in the 4-day comprehensive LUMI course or \"Performance Analysis and Optimisation Workshop\" that the LUMI User Support Team organises a few times per year, and you'll find many more tips about proper use of Lustre in that lecture (which is only available to actual users on LUMI unfortunately).</p>"},{"location":"intro-evolving/09-Lustre/#lustre-on-lumi","title":"Lustre on LUMI","text":"<p>LUMI has 5 Lustre filesystems:</p> <p>The file storage sometimes denoted as LUMI-P consists of 4 disk based Lustre filesystems, each with a capacity of roughly 18 PB and 240 GB/s aggregated bandwidth in the optimal case (which of course is shared by all users, no single user will ever observe that bandwidth unless they have the machine for themselves). Each of the 4 systems has 2 MDTs, one per MDS (but in a high availability setup), and 32 OSTs spread across 16 OSSes, so 2 OSTs per OSS. All 4 systems are used to serve the home directories, persistent project space and regular scratch space, but also, e.g., most of the software pre-installed on LUMI. Some of that pre-installed software is copied on all 4 systems to  distribute the load.</p> <p>The fifth Lustre filesystem of LUMI is also known as LUMI-F, where the \"F\" stands for flash as it is entirely based on SSDs. It currently has a capacity of approximately 8.5 PB and a total of over 2 TB/s aggregated bandwidth. The system has 4 MDTs spread across 4 MDSes, and 72 OSTs and 72 OSSes, os 1 OST per OSS (as a single OST already offers a lot more bandwidth and hence needs more server capacity than a hard disk based OST).</p>"},{"location":"intro-evolving/09-Lustre/#links","title":"Links","text":"<ul> <li> <p>The <code>lfs</code> command itself is documented through a manual page that can be accessed at the     LUMI command line with <code>man lfs</code>. The various subcommands each come with their own man page,     e.g., <code>lfs-df</code>, <code>lfs-getstripe</code>, <code>lfs-setstripe</code> and <code>lfs-find</code>.</p> </li> <li> <p>Understanding Lustre Internals     on the Lustre Wiki.</p> </li> <li> <p>Lustre Basics and     Lustre Best Practices     in the knowledge base of the NASA supercomputers.</p> </li> <li> <p>Introduction to DVS     in an administration guide</p> </li> </ul>"},{"location":"intro-evolving/09-Lustre/#related-trainings","title":"Related trainings","text":"<ul> <li> <p>Within the VSC, several of the Python trainings already pay attention to working efficiently with     datasets in Python.</p> <ul> <li> <p>Scientific Python. The      current version of the slides     contains a chapter on the use of HDF5. The GitHub repository with various examples, contains     examples and a Jupyter notebook for HDF5 and     examples and a Jupyter notebook for netCDF.</p> </li> <li> <p>Python for HPC. Not everything is mentioned on      the web site, but the current version of the slides     has at the end a chapter on I/O, mostly discussing using HDF5 in Python.     See also these example files and Jupyter notebook on GitHub.</p> </li> <li> <p>Sometimes other data formats may be a better idea, e.g., in AI applications. With Python (and there exist such libraries     for, e.g, C programmers), one can read directly from various archive file formats, e.g., gzip and tar files.     See these examples     from the VSC Python systems programming course</p> </li> </ul> </li> <li> <p>MPI courses will often also discuss MPI I/O which is a building block to implement proper I/O strategies     in parallel applications, and is also used in the implementation of HDF5, netCDF and likely other libraries.</p> </li> <li> <p>J\u00fclich Supercomputer Centre (between Aachen and K\u00f6ln) organises a lot of trainings.     The annual training \"Parallel I/O and Data Formats\" is particularly relevant for this chapter of this tutorial.</p> </li> </ul>"},{"location":"intro-evolving/10-ObjectStorage/","title":"LUMI-O","text":"<p>Last update of this page: May 21, 2025</p>"},{"location":"intro-evolving/10-ObjectStorage/#lumi-o-object-storage","title":"LUMI-O object storage","text":""},{"location":"intro-evolving/10-ObjectStorage/#why-do-i-kneed-to-know-this","title":"Why do I kneed to know this?","text":"<p>Most LUMI users will be unfamiliar with object storage.  It is a popular storage technology in the cloud, with Amazon AWS S3 as the best known example.</p> <p>On LUMI, the LUMI-O object storage system is probably the best option to transfer data to and from LUMI. Tools for object storage access often reach a much higher bandwidth than a single sftp connection can reach.</p> <p>Object storage is also a good technology if you want a backup of some of your data and want to make that backup on LUMI itself. The object storage is completely separate from  the regular storage of LUMI and hence offers additional security, though not as secure  as if you would make a backup at a different compute centre.</p> <p>In some research communities where cloud computing is more common, some datasets may come in cloud-optimised formats, i.e., formats optimised for object storage. Putting such data on the parallel file system without completely changing the  data format is usually not a good idea.</p> <p>In this section of the notes, we will discuss the properties of object storage and show you how to get started. It is by no means meant as a reference manual for all tools that one can use.</p>"},{"location":"intro-evolving/10-ObjectStorage/#in-short-what-is-lumi-o","title":"In short: What is LUMI-O?","text":"<p>LUMI-O is an object storage system (based on Ceph).  Ceph can offer object storage, block storage and file storage, but in LUMI-O, it is  used as an object storage system. Hence it is functioning differently from the Ceph system in use at C\u00c9CI for the common storage system used since 2025. It is similar to a system that CSC, the company operating LUMI, built for users of Finland and is known there as Allas, though LUMI doesn't provide all the functionality of Allas.</p> <p>Object file systems need specific tools to access data. They are usually not mounted as a regular filesystem (though some tools can make them appear as a regular file system) and accessing them needs authentication via temporary authentication keys that are different from your ssh keys and are not only bound to you, but also to the project for which you want to access LUMI-O. So if you want to use LUMI-O for multiple projects simultaneously, you'll need authentication keys for each project.</p> <p>Object storage is not organised in files and directories. A much flatter structure is used with buckets that contain objects:</p> <ul> <li> <p>Projects: LUMI-O works with \"single user tenants/accounts\", where the LUMI project number     is both the tenant/account and LUMI-O project. So the individual users in a LUMI project     are not know on LUMI-O and all users in a LUMI project have the same access to LUMI-O.</p> </li> <li> <p>Buckets: Containers used to store one or more objects. Object storage uses a flat structure with     only one level which means that buckets cannot contain other buckets.</p> </li> <li> <p>Objects: Any type of data. An object is stored in a bucket.</p> <p>This is also a flat namespace. Objects cannot contain other object. However, some tools create a pseudo-folder view with the use of slashes in the name. This does not create some kind of \"directory object\" that would then create a number of \"data objects\" though; it is just  the name of an object in a flat space that contains slashes that are  used to create a folder-like view on the namespace.</p> </li> <li> <p>Metadata: Both buckets and objects have metadata specific to them.      One element of the metadata is the name of the bucket or object. But metadata     can also contains the access rights to the bucket or object.     While traditional file systems have fixed metadata (filename,      creation date, type, etc.), an object storage allows you to add custom metadata     in the form of (key, value) pairs.</p> </li> </ul> <p>Objects can be served on the web also. This is in fact how recordings of some of the LUST courses are served currently. However, LUMI-O is not meant to be used as a data publishing service and is not an alternative to services provided by, e.g., EUDAT or several local academic service providers.</p> <p>The object storage can be easily reached from outside LUMI also with full access  control (and without webbrowsers). In fact, during downtimes, LUMI-O is often still operational as its software stack is managed completely independently from LUMI. It is therefore also very well suited as a mechanism for data transfer to and from LUMI. Moreover, tools for object storage often perform much better on high latency long-distance connections than tools as <code>sftp</code>.</p> <p>LUMI-O is based on the Ceph object file system.  It has a total capacity of 30 PB.  Storage is persistent for the duration of a project. Projects get a quota of 150 TB and can create up to 1K buckets and 500K objects per bucket. These quota are currently fixed and cannot be modified. Storage on LUMI-O is billed at 0.25 TB\u00b7hour per TB per hour, reduced from the rate of 0.5 TB\u00b7hour per TB per hour used before 2025 (which better reflected the true cost) to encourage a broader uptake. It can be a good alternative to store data from your project that still needs to be transferred but is not immediately needed by jobs, or to maintain backups on LUMI yourself.</p>"},{"location":"intro-evolving/10-ObjectStorage/#a-comparison-between-lustre-and-lumi-o","title":"A comparison between Lustre and LUMI-O","text":"<p>Or: What are the differences between a parallel filesystem and object storage?</p> <p></p> <p></p> <p>Integration with the system</p> <p>The Lustre parallel filesystem is very closely integrated in LUMI. In fact, it is the only non-local filesystem that is served directly to the compute nodes. Other remote filesystems on the Cray architecture are served via DVS, Data Virtualisation Service, a service that  forwards requests to the actual filesystem services running on management nodes. HPE Cray does this to minimise the number of OS daemons running in the background. Client software on the compute and login nodes must also fairly closely match the server software on  the actual Lustre servers. As a result of all this, the Lustre software has to be updated with all other system software on LUMI and, e.g., the Lustre filesystems are not available when LUMI is down for maintenance. </p> <p>The LUMI-O object storage system on the other hand,  is a separate system, though located physically fairly close to LUMI. Its software setup is completely independent from the software on the LUMI supercomputer itself. It has  a regular TCP/IP connection to LUMI, but all other software to access data on LUMI-O from LUMI runs entirely in user space. Client software also hardly depends on the version of the Ceph server software as the protocol between both is fairly stable. As a result, upgrades of LUMI do not affect LUMI-O and vice-versa, so LUMI-O is almost always available even when LUMI is down. Moreover, the server software of LUMI-O can often be upgraded on-the-fly, without noticeable downtime. Client and server also communicate through a web-based API.</p> <p>Organisation of data</p> <p>The organisation of data is also very different on Lustre and LUMI-O.  Lustre, although very different from the local filesystem(s) on your laptop or smartphone or more traditional networked filesystems common for networks of PCs or workstations, uses the same organisation of data as those filesystems. Data is organised in files stored in a hierarchical directory structure. In fact, Lustre behaves as much as possible as other classical shared filesystems: Data written on one compute node is almost immediately noticed on other compute nodes also. (In fact, this is also the main cause of performance problems for some operations on Lustre.)</p> <p>These files can be read, written, modified and/or appended, and deleted. So one can change  only a fraction of the data in the file without working on the other data, and different compute nodes can write to different parts of a file simultaneously.</p> <p>The object storage system puts data as objects in buckets, and buckets are on a  per-project basis. So there is basically a 3-level hierarchy: project, buckets and objects, with each level just a flat space. Bucket names have to be unique within a project, and object names have to be unique within a bucket.</p> <p>Buckets cannot contain other buckets (contrary to directories which can contain other directories). Objects are not really arranged in a hierarchy, even though some tools make it appear as if this is the case by showing them in a directory tree-like  structure based on slashes used un the name of the objects,  but this structure is really just created based on those names and you will notice that structuring  the objects like this in a view is actually a rather expensive operation in client software as the software always needs to go through all objects in the bucket and select the appropriate ones.</p> <p>Objects are managed through simple atomic operations. One can put an object in the object storage, get its content, copy an object or delete an object. But contrary to a file  in Lustre, the object cannot be modified: One cannot simply change a part of the content, but the object needs to be replaced with a new object.</p> <p>Some tools (e.g., <code>s3cmd</code>) can do multipart uploads: An large object is uploaded in multiple parts (which can create more parallelism and hence have a higher combined bandwidth), but these parts are actually objects by themselves that are combined in a single object at the end. If the upload would get interrupted, the parts would actually continue to live as  separate objects unless a suitable bucket policy is set (and we have run into issues already with a user depleting their quota on LUMI-O with stale objects from failed multipart uploads). The configuration that the LUMI-O tools generate for <code>s3cmd</code> enables multipart uploads of  big objects.</p> <p>Optimised for?</p> <p>Lustre is optimised first and formost for bandwidth to the compute nodes when doing large file I/O operations from multiple nodes to a file. This optimisation for performance also implies that simpler schemes for redundancy have to be used. Data is protected from a single disk failure and on most systems even from dual disk failure, but performance will be reduced during a disk repair action and even though the server functions are usually done in high availability pairs, we have seen more than one case where a server pair fails making part of the data inaccessible.</p> <p>The LUMI-O object storage is much more optimised for reliability and resilience. It uses a very complicated internal redundancy scheme. On LUMI-O each object is spread over 11 so-called storage nodes, spread over at least 6 racks, with never more than 2 storage nodes in a single rack. As only 8 storage nodes of an object are needed to recover the data of an object, one  can even lose an entire rack of hardware while keeping the object available.</p> <p>Access</p> <p>Lustre, as it is fully integrated in LUMI (or any other supercomputer that uses it), has no separate authentication. You log on to LUMI and you have access to files on the Lustre file system based on permissions: your userid and the linux group(s) that you belong to. You will see your files as on any other POSIX file system. No external access is possible and it depends completely on the authentication system of LUMI running and working properly.</p> <p>The LUMI-O object storage works with a separate credentials-based authentication system.  These credentials have to be created first in one of two ways that we will discus later and they have a very finite lifetime after which they need to be regenerated (when expired) or extended (if you do so before expiration).</p> <p>In fact, in LUMI-O, projects are handled as so-called \"single user tenants/accounts\":  the project numerical ID (e.g., 465000001) is both the tenant/account name and project name. LUMI-O will not further distinguish between the users in a LUMI project.  Hence it is definitely not an option to make a backup from your home directory in a way that it would be secured from access by other users in your LUMI project.</p> <p>LUMI-O clients and servers talk to each other via a web based API. There is a whole range of tools for all popular operating systems that support this API, both command line tools and GUI tools. In this world, external access is natural.  There is really no difference in accessing LUMI-O from LUMI or from any other internet-connected computer. It is even possible to access LUMI-O via a web browser if such access is set up properly. It is documented in the bottom parts of the \"Advanced usage of LUMI-O\" page in the LUMI docs.</p> <p>MDS and ODS</p> <p>Both Lustre and the Ceph object storage system work with metadata servers (MDS) and object data servers (ODS), but this is where the similarities end. The internal organisation of MDS and ODS is  completely different in both, but those differences are not that important at the level of this tutorial.</p> <p>Parallelism is key to performance</p> <p>Both Lustre and LUMI-O require parallel access to get the optimal performance. How that parallelism is best organised, is very different though.</p> <p>Lustre will perform best when large accesses are made to large files by multiple processes preferably on  multiple nodes of the supercomputer. Parallelism through accessing different files from each process will be far less performant and may even lead to an overload on the metadata servers.</p> <p>The object storage is different in that respect: As operations on objects are atomic, they should also be accessed from a single process. But multiple processes or threads should access multiple  objects simultaneously for optimal access. However, even in those cases the bandwidth will be  somewhat limited as the network connection between LUMI-O and LUMI is a lot less as between the Lustre object data servers and the LUMI compute nodes, as both are fully integrated in the same high performance interconnect.</p> <p>Cost</p> <p>The hardware required for Lustre may not be as expensive as for some other fileservers, but a hard disk based Lustre filesystem is still not cheap and if you want a performant flash based filesystem that can endure a high write load also and not only a high read load, it is very expensive. The LUMI-O hardware  is a lot cheaper, though this is also partly because it has a lower bandwidth.</p> <p>The billing units accounted for storage on LUMI originally reflected the purchase cost per petabyte for LUMI-O and  the Lustre filesystems, with the object storage billed at half the price of the hard disk based Lustre filesystems and the flash based Lustre filesystem at 10 times the cost of the hard disk based one. These costs were modified slightly in early 2025 trying to better balance the load over the various storage options.</p> <p></p> <p>The differences between a parallel file system and object storage also translate into differences in \"file formats\" between both (with \"file formats\" between quotes as we cannot speak about files on the object storage). The best way of managing life data on both systems is very different.</p> <p>Take as an example netCDF versus the  zarr data format.  Both are popular in, e.g., earth and climate science. NetCDF is a file format developed specifically with supercomputers with large parallel file systems in mind, while zarr is a cloud-optimised technology.  You cannot work with netCDF files from an object storage system as they would be stored as a single object and only atomic operations are possible. Zarr on the other hand is a format to store large-scale N-dimensional data on an object storage system by breaking it up in a structured set of objects that each can be accessed with atomic  operations. But if you bring that hierarchy of objects as individual files onto a parallel file system, you risk creating problems with the metadata server as you will be dealing with lots of small files.</p> <p>There is no \"best\" here: both are different technologies, developed with a specific purpose in mind, and which one you should use is hence dictated by the technology that you want to use to store your data. </p>"},{"location":"intro-evolving/10-ObjectStorage/#accessing-lumi-o-general-principles","title":"Accessing LUMI-O: General principles","text":"<p>Access to LUMI-O is based on temporary credentials that need to be generated via  one of two web interfaces: Either a dedicated credential management portal, or the Open OnDemand interface to LUMI that we discussed in the  \"Getting access\" session.  We will discuss both options in this chapter of the notes.</p> <p>There are currently three command-line tools pre-installed on LUMI to transfer data back and forth between LUMI and LUMI-O:  rclone (which is the easiest tool if you want both public web-accessible and private data),  s3cmd  and restic. All these tools are made available through the <code>lumio</code> module that can be loaded in any software stack.</p> <p>The <code>boto3</code> Python package  is a good choice if you need programmatic access to the object storage. Note that you need to use fairly recent versions which in turn require a more recent Python than the system Python on LUMI (but any of the <code>cray-python</code> modules would be sufficient). As it is better to containerize Python installations with many packages on LUMI, and as most users also prefer to work with virtual environments, it is not pre-installed on LUMI as it would be impossible to embed it into the Cray Python distributions.</p> <p>But you can also access LUMI-O with similar tools from outside LUMI. Configuring them may be a bit tricky and the LUMI User Support Team cannot help you with each and every client tool on your personal machine. However, the dedicated credential management web interface can also generate code snippets or configuration file snippets for various tools, and that will make configuring them a lot easier.</p> <p>There also exist GUI-based tools for all popular operating systems. Almost every tool suitable for AWS S3 storage should also work for LUMI-O when properly configured.</p> <p>The Open OnDemand web interface also has a tool to show buckets and objects on LUMI-O in a folder-like structure (created based on object \"names\" with slashes) and can be used to upload and download objects, but it is not a substitute for the specific tools for object storage. When you upload or download via the web browser, you don't talk directly to LUMI-O, but you use a web browser that talks to the web server using web APIs, with all performance limitations of those protocols, and the web server then talks to LUMI-O.</p>"},{"location":"intro-evolving/10-ObjectStorage/#the-credential-management-web-interface","title":"The credential management web interface","text":"<p>One way to create the credentials to access LUMI-O, is via the the credential management web interface that can be found at auth.lumidata.eu. This system runs independently from LUMI, just as the object storage  itself, and usually remains available during downtimes of LUMI. So even though you may prefer creating credentials via Open OnDemand, it is good to learn to use this system as it can still give you access to your  data on LUMI-O while LUMI is down for maintenance.</p> <p>Let's walk through the interface:</p> <p>A walk through the credentials management web interface of LUMI-O</p> <p>After entering the URL auth.lumidata.eu, you're presented with a welcome screen on which you have to click the \"Go to login\" button.</p> <p> </p> <p>This will present you with the already familiar (from Open OnDemand) screen to select your authentication provider:</p> <p> </p> <p>Proceed with login in through your relevant authentication provider (not shown here) and you will be presented with a screen that show your active projects:</p> <p> </p> <p>Click the project for which you want to generate access credentials (called \"authentication keys\" or \"access keys\" in the interface), and the column to the right will appear. Chose how long the authentication key should be valid (1 week or 168 hours is the maximum currently, but the life can be extended) and a description for the authentication key. The latter is useful if you generate multiple keys for different use. E.g., for security reasons you may want to use different authentication keys from different machines so that one machine can be disabled quickly if the machine would be compromised or stolen.</p> <p>Key validity</p> <p>At the time of writing, the maximum lifetime for a key was 168 hours which is one week. This will be extended in the future so that key renewal will not be needed anymore. Keys will last for the lifetime of the project.</p> <p>Next click on the \"Generate key\" button, and a new key will appear in the \"Available keys\" section:</p> <p> </p> <p>Now click on the key to get more information about the key: </p> <p> </p> <p>At the top of the screen you see three elements that will be important if you use the LUMI command line tool <code>lumio-conf</code> to generate configuration files for <code>rclone</code> and <code>s3cmd</code>: the project number (but you knew that one), the \"Access key\" and \"Secret key\". The access key and secret key will also be needed if you configure other clients, e.g., a GUI client on your laptop, by hand. The other bit of information that is useful if you configure tools by hand, is the endpoint URL, which is \"https://lumidata.eu/\" and is not shown in the interface.</p> <p>Note: One more configuration parameter</p> <p>LUMI-O requires path-style (https://projectnum.lumidata.eu/bucket) addressing to buckets.  The so-called virtual-hosted style (https://bucket.projectnum.lumidata.eu) is not supported. From a technical point of view, this is because the  <code>*.lumidata.eu</code> wildcard TLS certificate can only represent one layer of subdomains, which is used for the project numbers.</p> <p>Many clients default to path-style addressing, but some will require a configuration option or an environment variable. Commonly used parameters and environment variables are <code>use_path_style</code>, <code>force_path_style</code> or <code>S3_FORCE_PATH_STYLE</code>. Notably <code>aws-sdk</code> defaults to trying virtual-hosted style when reading public buckets.</p> <p>Note that we can only give hints on how to do things, but the LUMI User Support Team cannot support any possible client and certainly not commercial ones to which we cannot get access.</p> <p>Scrolling down a bit more:</p> <p> </p> <p>The \"Extend key\" field can be used to extend the life of the key, to a maximum of 168 hours past the current time. You can extend the life of an access key as often as you can, but it is not possible to create an access key that  remains valid for more than 168 hours. This is done for security reasons as now a compromised key can only be used for a limited amount of time, even if you haven't noticed that the access key has been compromised and hence fail to revoke the access key in the interface.</p> <p>The credential management interface can also be used to generate snippets for configuration files for various tools:</p> <p> </p> <p>The \"Configuration templates\" is the way to generate code snippets or configuration file snippets for various tools (see the list on the slide). After selecting \"rclone\" and clicking the \"Generate\" button, a new screen opens:</p> <p> </p> <p>This screen shows us the snippet for the rclone configuration file (on Linux it is <code>~/.config/rclone/rclone.conf</code>). Notice that it creates two so-called endpoints. In the slide this is <code>lumi-465001102-private</code> and <code>lumi-465001102-public</code>, for storing buckets and objects which are private or public (i.e., also web-accessible). We'll come back to what this means later, when we discuss access rights.</p>"},{"location":"intro-evolving/10-ObjectStorage/#credential-management-and-access-through-open-ondemand","title":"Credential management and access through Open OnDemand","text":"<p>The LUMI Open OnDemand-based web interface can also be used to generate  authentication keys for LUMI-O. It can create configuration files for  <code>rclone</code> and <code>s3cmd</code> but doesn't have all the other features of the dedicated credential management web interface discussed above. The \"Home Directory\" app can also be used to browse, download and upload objects and to create buckets. It is a tool based on <code>rclone</code> and also provides a view similar to many other viewers. The view is based on pseudo-folders which we will discuss below.</p> <p>Note however that this interface is not a replacement for proper tools for access to object storage. Uploading and downloading objects is not done via object storage tools,  but via web protocols. The web server talks to the object storage using proper tools, but to your browser using the regular web API upload and download features. Hence it is only suitable for relatively small objects and you'll quickly run into speed restrictions that native object storage tools will not have.</p> <p>Let us now again walk through the interface.</p> <p>A walk through the Open OnDemand web interface to LUMI-O</p> <p>This demo was made with a project that course participants have no access to, but it gives the idea.</p> <p>After entering the URL www.lumi.csc.fi, you're greeted with the usual login procedure that we discussed already. After logging in, you  get the app overview screen:</p> <p> </p> <p>To create an authentication key and to configure <code>rclone</code> and <code>s3cmd</code>, open the \"Cloud storage configuration\" app. You will be presented with the following screen:</p> <p> </p> <p>At the top of the screen there is an overview of currently configured remotes. These are actually the endpoints for <code>rclone</code>. In this example, this section is still empty as no remotes are  configured for the user. Let's scroll down a bit:</p> <p> </p> <p>Here we see the part of the screen where we can create an authentication key. First select the project for which you want to create an authentication key.</p> <p>Besides the authentication key, by default an <code>rclone</code> endpoint to create buckets and upload objects with  private access only will also be created and this cannot be turned off as the endpoint will also  be used by another app that we discuss later. However, two checkboxes enable the creation of  a configuration file for <code>s3cmd</code> and an <code>rclone</code> endpoint to create buckets and objects with public access ACL can also be created.</p> <p>The <code>rclone</code> endpoints are stored in the <code>rclone</code> configuration file <code>~/.config/rclone/rclone.cfg</code> where they will replace the same endpoints for the project if they are already present. Other endpoints will not be removed from the configuration file.</p> <p>Finally, click the \"Submit\" button. If no suitable key exists, one will be created, and the  requested endpoint and configuration file(s) will be created or updated. When a key is created, it is created with the maximum life span of 7 days (168 hours), but if a suitable one exists, it is currently not extended. (It appears that only a key with the description \"lumi web interface\" is properly recognised as the other key already existed but had its life span extended.)</p> <p>We now notice some changes at the top of that screen:</p> <p> </p> <p>We've created both a private and a public <code>rclone</code> endpoint with the name <code>lumi-465000095-private</code> and <code>lumi-465000095-public</code> respectively. These endpoints can also be used with the command line <code>rclone</code> tool provided by the <code>lumio</code> module.</p> <p>The \"Delete\" button can used to delete the endpoint only, but the authentication key will not be revoked, i.e., the key remains valid. The \"Revoke\" button will invalidate the key and destroy the <code>rclone</code> endpoint(s) as  they don't make sense anymore.</p> <p>Let's check the previously discussed dedicated web credentials portal:</p> <p> </p> <p>A key with the description \"lumi web interface\" is now visible, and this interface can still be used to  extend its life span or create configuration file snippets for other tools. It is just another key.</p> <p>(The screenshot is a bit misleading here. The other key \"Laptop\" actually existed but was not picked up by Open OnDemand, it is just that its life span was extended before making this screenshot which was probably a bad choice.)</p> <p>Let's now also have a look at another Open OnDemand app that enables us to browse buckets and objects for the project for which we have created credentials:</p> <p> </p> <p>We went back to the start screen of Open OnDemand and now open the \"Home Directory\" app.</p> <p> </p> <p>We now see our home directory and project, scratch and flash directory for all our projects, but also the two  endpoints that we just created (assuming we did indeed select the check box to create a public endpoint also). The screenshot is taken after selecting <code>lumi-465000095</code> in the left column and shows the buckets available to that project. Note that they are shown in exactly the same way as directories (folders) would be shown for the regular filesystem, but in fact, they are  buckets with, e.g., all the naming restrictions of buckets. </p> <p>Let's now select the <code>training-materials-web</code> bucket to arrive at:</p> <p> </p> <p>At the top of the right pane we now see the \"path\" <code>lumi-465000095-public:/training-materials-web/</code>. This however is a pseudo-path: The first element is actually the rclone endpoint while the second element is the name of the bucket. The screen also shows the folder <code>intro-evolving</code>. This is neither a bucket nor a true folder. It is an artificial creation and if we click a bit further we will see what's going on:</p> <p> </p> <p>The top bar now shows <code>lumi-465000095-public:/training-materials-web/intro-evolving/files/</code> while we also get a list of elements that look like files. These are the objects, but don't be mistaken: The name of the first object in the  list is not <code>exercises-evolving.tar</code>, but it is the object named  <code>intro-evolving/files/exercises-evolving.tar</code> in the bucket <code>training-materials-web</code> in the project <code>46500095</code>. The slashes in the name are used to create a \"pseudo-folder\" view to bring more structure into the flat space of objects, but the organisation internally in object storage is completely different from a regular filesystem.</p> <p>Note that we would also see the same objects had we used the <code>lumi-465000095-private</code> endpoint in the right column: Both are endpoints for the same project and the browser cannot distinguish between objects created via one or the other endpoint. Those endpoints only matter when creating an object as they determine access rights.</p>"},{"location":"intro-evolving/10-ObjectStorage/#configuring-lumi-o-tools-through-a-command-line-interface","title":"Configuring LUMI-O tools through a command line interface","text":"<p>ONLY VALID FOR THE <code>lumio/2.0.0</code> MODULE AND NEWER.</p> <p>On LUMI, you can use the <code>lumio-conf</code> tool to configure <code>rclone</code>, <code>s3cmd</code>, <code>aws</code> and the <code>boto3</code> Python packages.  To access the tool, you need to load the <code>lumio</code> module first, which is always available. The same module will also load a module that makes <code>rclone</code>, <code>s3cmd</code> and <code>restic</code> available. <code>aws</code> and <code>boto3</code> are not preinstalled though.</p> <p>When starting <code>lumio-conf</code>, it will present you with a couple of questions: The project number associated with the authentication key, the access key and the secret key. We have shown above where in the web interface that information can be found.</p> <p>For <code>rclone</code>, the tool will then add or replace the configuration for the project in the <code>rclone</code> configuration file <code>~/.config/rclone/rclone.conf</code>.  It will create two endpoints:</p> <ul> <li> <p><code>lumi-46YXXXXXX-private</code> is the end point to be used for buckets and objects that should be private, and</p> </li> <li> <p><code>lumi-46YXXXXXX-public</code> is the end point for data that should be publicly accessible.</p> </li> </ul> <p>It will also create an <code>s3cmd</code> configuration file. Since a <code>s3cmd</code> configuration  file can contain information for only one project, the following solution was chosen:</p> <ul> <li> <p>The tool creates the configuration file <code>~/.s3cfg-lumi-46YXXXXXX</code> specific for the project,</p> </li> <li> <p>and overwrites the <code>\u02dc/.s3cfg</code> configuration file with the new configuration.</p> </li> </ul> <p>So if you want to use <code>s3cmd</code> for only one project, or for the last project for which you used <code>lumio-conf</code>, you can simply use it as in all the <code>s3cmd</code> examples, while if you want to use it for more projects, you can always point to the configuration file to use with the <code>-c</code> flag of the <code>s3cmd</code> command, e.g.,</p> <pre><code>s3cmd -c ~/.s3cfg-lumi-46YXXXXXX ls\n</code></pre> <p>to list the buckets in project 46YXXXXXX.</p> <p>Use <code>lumio-conf -h</code> to see how you can also create configuration files for <code>aws</code> and <code>boto3</code>. It also has a bunch of other options.</p> <p>Another way to configure tools for object storage access is simply via the code snippets and configuration files snippets as has already been discussed before. The same snippets  should also work when you run the tools on a different computer. E.g., for rclone you can add the code snippet to the configuration file  <code>~/.config/rclone/rclone.conf</code> (default name for Linux systems) or replace an earlier code snippet for the same project in that file.  These code snippets can also be used to configure tools on  your own laptop or other systems that you have access to. As we have discussed already, there is no difference in accessing LUMI-O from LUMI or from other systems if the same tools are used.</p>"},{"location":"intro-evolving/10-ObjectStorage/#bucket-and-object-names","title":"Bucket and object names","text":"<p>We have already discussed that storage on LUMI-O is organised in three levels, each with a flat namespace:</p> <ol> <li>Projects,</li> <li>buckets and</li> <li>objects.</li> </ol> <p>The project name is simply the 462 or 465 number of the LUMI project.</p> <p>Bucket names have a lot of restrictions. The name should be unique within the project, but different projects can of course use the same bucket names. A bucket name should be between 3 and 63 characters long and can use only lowercase letters, numbers, hyphens and dots, but no uppercase letters or, e.g., underscores. It must also start with a lowercase letter or a number. Dots are considered as separators of labels, but the 63-character limit still applies to the name as a whole and not each individual label. The \"label\" term is a bit  confusing as in the Ceph documentation it is also used in a totally different context.</p> <p>Object names have fewer restrictions. They have to be unique within a bucket. From a technical point of view, an object name can be any UTF-8 encoded string between 1 and 1024 bytes. Note however that all but the standard ASCII characters use more  than one byte in UTF-8 encoding, so this does not imply that any 1024-character string can be used. Also, client software may add more restrictions. It is a common practice to implement a folder-like structure using slashes in the name. Keep in mind though that creating that folder view (which we have seen in the Open OnDemand \"Home Directory\" app) is expensive as the client creating the folder view has to list all objects in a bucket and then sort and select the right entries to create the folder view.</p>"},{"location":"intro-evolving/10-ObjectStorage/#policies-and-acls","title":"Policies and ACLs","text":"<p>Access to buckets and objects is controlled through policies and access control lists (ACLs).</p> <p>Policies are set at the bucket level only, but it is possible to build very complex policies that restrict access to some objects. It is a very powerful mechanism, but also one that is rather hard to use. In Ceph, the object storage system on LUMI, policies are a relatively new addition. Ceph policies are a subset of the policies on Amazon AWS S3 storage. There is some information on policies in the \"Advanced usage of LUMI-O\" section of the LUMI docs and the section on \"Bucket Policies\" in the Ceph Reef documentation is also relevant.</p> <p>ACLs are a less sophisticated mechanism. They can be applied to both buckets and objects. However, they can only add rights and not take rights away. Moreover, they need to be applied to each object in a bucket separately (though commands will usually provide  an option to apply them recursively to all objects in a bucket or maybe even pseudo-folder). They are an easy way though to make an bucket or an individual object public.</p> <p>Note that a private bucket can contain public objects which you can then still see through, e.g., a web browser if you know the name of an object, or a bucket can be public but contain only private objects in which case you can only list the objects.</p> <p>When using <code>rclone</code> on LUMI, specific access control lists will be attached to each object  depending on which end point name you have used.</p>"},{"location":"intro-evolving/10-ObjectStorage/#some-examples","title":"Some examples","text":"<p>The <code>s3cmd</code> command is your friend if you want to inspect or change policies and ACLs on LUMI. It is available via the <code>lumio</code> module.</p> <ol> <li> <p>You can make a bucket and all objects in it public with</p> <pre><code>s3cmd setacl --recursive --acl-public s3://bucket/ \n</code></pre> <p>(replace \"bucket\" with the name of the bucket) or private with</p> <pre><code>s3cmd setacl --recursive --acl-private s3://bucket/\n</code></pre> </li> <li> <p>You can grant or revoke read rights to a bucket with</p> <pre><code>s3cmd setacl --acl-grant=\u2019read:465000000$465000000\u2019 s3://bucket\ns3cmd setacl --acl-revoke=\u2019read:465000000$465000000\u2019 s3://bucket\n</code></pre> <p>Note the use of single quotes as we want to avoid that the Linux shell interpretes <code>$465000000</code> as a (non-existent) environment variable. And of course, replace \"465000000\" with your project number.</p> <p>One can also grant or revoke read rights to an object in a similar way.</p> </li> <li> <p>Checking the current ACLs and more of an object, can also be done with the <code>s3cmd</code> command.</p> <p>Some examples (but you cannot try them yourself as you'd need access credentials for the  462000265 project that contains those buckets and objects):</p> <ul> <li> <p>A bucket used to serve the images on this page:</p> <pre><code>s3cmd info s3://intro-evolving\n</code></pre> <p>produces</p> <pre><code>s3://intro-evolving/ (bucket):\n  Location:  lumi-prod\n  Payer:     BucketOwner\n  Ownership: none\n  Versioning:none\n  Expiration rule: none\n  Block Public Access: none\n  Policy:    none\n  CORS:      none\n  ACL:       *anon*: READ\n  ACL:       LUMI training material: FULL_CONTROL\n  URL:       http://lumidata.eu/intro-evolving/\n</code></pre> <p>Note the ACL \"<code>*anon*: READ</code>\" in the output, showing that this is a public bucket.</p> </li> <li> <p>An example of an object in that bucket:</p> <pre><code>s3cmd info s3://training-materials-web/intro-evolving/img/LUMI-BE-Intro-evolving-10-ObjectStorage/Title.png\n</code></pre> <p>where the output shows again that this is a public object:</p> <pre><code>s3://training-materials-web/intro-evolving/img/LUMI-BE-Intro-evolving-10-ObjectStorage/Title.png (object):\n  File size: 4384337\n  Last mod:  Thu, 21 Nov 2024 14:55:47 GMT\n  MIME type: image/png\n  Storage:   STANDARD\n  MD5 sum:   69e1f1460cff3fca79730530e7fb28d7\n  SSE:       none\n  Policy:    none\n  CORS:      none\n  ACL:       *anon*: READ\n  ACL:       LUMI training material: FULL_CONTROL\n  URL:       http://lumidata.eu/training-materials-web/intro-evolving/img/LUMI-BE-Intro-evolving-10-ObjectStorage/Title.png\n  x-amz-meta-mtime: 1732200870.757744714\n</code></pre> </li> <li> <p>However, if you try</p> <pre><code>s3cmd info s3://4day-20241028/files/LUMI-4day-20241028-1_01_HPE_Cray_EX_Architecture.pdf\n</code></pre> <p>you get</p> <pre><code>s3://4day-20241028/files/LUMI-4day-20241028-1_01_HPE_Cray_EX_Architecture.pdf (object):\n  File size: 1299322\n  Last mod:  Wed, 06 Nov 2024 09:28:35 GMT\n  MIME type: application/pdf\n  Storage:   STANDARD\n  MD5 sum:   9f2ec727731feba562c401fb0cb156c1\n  SSE:       none\n  Policy:    none\n  CORS:      none\n  ACL:       LUMI training material: FULL_CONTROL\n  x-amz-meta-mtime: 1730109719.352306659\n</code></pre> <p>you see that there is no ACL \"<code>*anon*: READ</code>\" as this is a private object, and even though the bucket it is in is public: </p> <pre><code>s3cmd info s3://4day-20241028/\n</code></pre> <p>shows</p> <pre><code>s3://4day-20241028/ (bucket):\n  Location:  lumi-prod\n  Payer:     BucketOwner\n  Ownership: none\n  Versioning:none\n  Expiration rule: none\n  Block Public Access: none\n  Policy:    none\n  CORS:      none\n  ACL:       *anon*: READ\n  ACL:       LUMI training material: FULL_CONTROL\n  URL:       http://lumidata.eu/4day-20241028/\n</code></pre> <p>you'll see that, e.g., accessing https://462000265.lumidata.eu/4day-20241028/files/LUMI-4day-20241028-1_01_HPE_Cray_EX_Architecture.pdf or https://lumidata.eu/462000265:4day-20241028/files/LUMI-4day-20241028-1_01_HPE_Cray_EX_Architecture.pdf produces an \"Access Denied\" error message (in a strange format, but read the text), while https://465000095.lumidata.eu/training-materials-web/intro-evolving/img/LUMI-BE-Intro-evolving-10-ObjectStorage/Title.png or https://lumidata.eu/465000095:training-materials-web/intro-evolving/img/LUMI-BE-Intro-evolving-10-ObjectStorage/Title.png are links to the object used before in this example and work.</p> </li> </ul> </li> </ol>"},{"location":"intro-evolving/10-ObjectStorage/#sharing-data","title":"Sharing data","text":"<p>LUMI-O is also a nice solution to share data between projects and with the outside world.  But note that this does not turn LUMI-O in a data publication and archiving service. There are specific services for that in Europe (e.g., EUDAT offerings). Each solution has its own limitations though:</p> <ol> <li> <p>Public buckets and objects can be read by anyone in the world, even with a simple web browser.     Whether all objects can be listed or not, depends on the read rights of the bucket. But just as     it is possible in Linux/UNIX to make a file readable in an otherwise unreadable directory (so      an <code>ls</code> on the directory would not work), this is also possible on LUMI-O by using a private     bucket with a public object.</p> <p>Using public objects is actually used to serve the slides and presentations in this course.</p> </li> <li> <p>It is also possible to grant specific projects access to buckets and objects of another project.     This is done via access control lists and has already been discussed in the previous examples.</p> <p>The way to access those buckets and objects then differs between <code>s3cmd</code> and <code>rclone</code>:</p> <ul> <li> <p>As <code>s3cmd</code> knows only one set of authentication tokens, it is not needed to indicate which     credentials should be used if the <code>~/.s3cfg</code> file is correct. E.g., listing all objects in      the bucket \"bucket\" of project \"46XXXXXXX\" can be done with:</p> <pre><code>s3cmd ls --recursive s3://46XXXXXXX:bucket/\n</code></pre> <p>so you need to specify - as one could expect as the bucket namespace is per project - the bucket  and the project that contains the bucket.</p> </li> <li> <p>The <code>rclone</code> configuration file can contain multiple endpoints for multiple projects, so here     we will also need to specify the endpoint from which the credentials should be used. Assume     that a user in project 46BAAAAAA has been given read rights to the bucket \"bucket\" in project     \"46YXXXXXX\" and has an endpoint <code>lumi-46BAAAAAA-private</code> configured, then that user can list     the objects in that bucket with:</p> <pre><code>rclone ls lumi-46BAAAAAA-private:\"46YXXXXXX:bucket\"\n</code></pre> </li> </ul> </li> <li> <p>The third technique is using presigned URLs. These are URLs created specifically for access to     a specific bucket or object. Everyone with the URL can access the bucket or object, without further     authentication. It is possible to create a link with a limited validity period (and this is actually     a good idea as URLs can fall in the wrong hands.)</p> <p>Presigned URLs depend on the authentication key that was used to create them. If the authentication key expires or is revoked, the link will no longer be valid, even if this happens within the validity period of the link. It is not possible to create links with a validity period of more than 7 days on LUMI. Also, it is also possible to revoke the link.</p> <p>Presigned URLs can be created and managed through the <code>rclone link</code> command. E.g.,</p> <pre><code>rclone link --expire 2d lumi-46YXXXXXX-private:bucket/object\n</code></pre> <p>will print a URL that can be used to access the object \"object\" in the bucket \"bucket\" of project 46YXXXXXX, and that link will expire automatically 48 hours after creation.</p> </li> <li> <p>The most rudimentary method for data sharing with another user is of course to simply invite that user     to the project. Then data can even be shared in the <code>/project</code>, <code>/scratch</code> and <code>/flash</code> directory of     the project and not only on LUMI-O. </p> <p>This is however not always possible.</p> <ul> <li> <p>Users that entered the system through Puhuri, i.e., a project with a project number starting with     465 cannot always be invited to a CSC project (projects starting with 462). They would first need to get     a CSC userid to have access to myCSC and may end up with a second userID on LUMI.</p> </li> <li> <p>Users who entered LUMI though myCSC, i.e., a 462 project, need to      link their account first to MyAccessID     or they would get a second userID on LUMI when invited by a Puhuri-managed project. </p> </li> </ul> </li> </ol>"},{"location":"intro-evolving/10-ObjectStorage/#tips-tricks","title":"Tips &amp; tricks","text":"<p>A description of the main <code>rclone</code> commands is outside the scope of this tutorial, but some options are discussed in the LUMI documentation, and the same page also contains some documentation for <code>s3cmd</code> and <code>restic</code>. See the links below for even more documentation.</p> <p>Note also that sharing data from project A with project B does not protect the data from being  deleted when project A ends. If the data on LUMI-O is in the space of project A, then that data will be deleted after the 90-day grace period after the end of the project, even if project B is still a valid project. On LUMI-O, data is not deleted while a project is valid, is made read-only after the end of a project for a 90-day grace period, and is then queued for deletion, so just as for the Lustre filesystems, you need to move out the data in time.</p>"},{"location":"intro-evolving/10-ObjectStorage/#further-lumi-o-documentation","title":"Further LUMI-O documentation","text":"<ul> <li>Documentation for the LUMI-O object storage service</li> <li> <p>Software for LUMI-O on LUMI is provided through the     <code>lumio</code> module which     provides the configuration tool on top of the software and the     <code>lumio-ext-tools</code> module     providing rclone, S3cmd and restic and links to the documentation of those tools.</p> <ul> <li>rclone documentation</li> <li>S3cmd tools usage</li> <li>restic documentation</li> </ul> </li> </ul>"},{"location":"intro-evolving/11-Containers/","title":"Containers","text":"<p>Last update of this page: May 22, 2025</p>"},{"location":"intro-evolving/11-Containers/#containers-on-lumi-c-and-lumi-g","title":"Containers on LUMI-C and LUMI-G","text":""},{"location":"intro-evolving/11-Containers/#what-are-we-talking-about-in-this-chapter","title":"What are we talking about in this chapter?","text":"<p>Let's now switch to using containers on LUMI.  This section is about using containers on the login nodes and compute nodes.  Some of you may have heard that there were plans to also have an OpenShift Kubernetes container cloud platform for running microservices but at this point it is not clear if and when this will materialize due to a lack of personpower to get this running and then to support this.</p> <p>In this section, we will </p> <ul> <li> <p>discuss what to expect from containers on LUMI: what can they do and what can't they do,</p> </li> <li> <p>discuss how to get a container on LUMI,</p> </li> <li> <p>discuss how to run a container on LUMI,</p> </li> <li> <p>discuss some enhancements we made to the LUMI environment that are based on containers or help     you use containers,</p> </li> <li> <p>and pay some attention to the use of some of our pre-built AI containers.</p> <p>If you are interested in doing AI on LUMI, we highly recommend that you have a look at the AI course materials developed by the LUNI User Support Team.</p> </li> </ul> <p>Remember though that the compute nodes of LUMI are an HPC infrastructure and not a container cloud! HPC has its own container runtimes specifically for an HPC environment and the typical security constraints of such an environment.</p>"},{"location":"intro-evolving/11-Containers/#what-do-containers-not-provide","title":"What do containers not provide","text":"<p>What is being discussed in this subsection may be a bit surprising. Containers are often marketed as a way to provide reproducible science and as an easy way to transfer software from one machine to another machine. However, containers are neither of those and this becomes  very clear when using containers built on your typical Mellanox/NVIDIA InfiniBand based clusters with Intel processors and NVIDIA GPUs on LUMI. This is only true if you transport software between  sufficiently similar machines (which is why they do work very well in, e.g., the management nodes of a cluster, or a server farm).</p> <p>First, exact reproducibility is a myth. Computational results are almost never 100% reproducible because of the very nature of how computers work. If you use any floating point computation, you can only expect reproducibility of sequential codes  between equal hardware. As soon as you change the CPU type, some floating point computations may produce slightly different results, and as soon as you go parallel this may even be the case between two runs on exactly the same hardware and with exactly the same software.  Besides, by the very nature of  floating point computations, you know that the results are wrong if you really want to work with real numbers. What matters is understanding how wrong the results are and reproduce results that fall within expected error margins for the computation. This is no different from reproducing a lab experiment where, e.g., each measurement instrument introduces errors. The only thing that containers do reproduce very well, is your software stack. But not without problems:</p> <p>Containers are certainly not performance portable unless they have been specifically designed to run optimally on a range of hardware and your hardware falls into that range. E.g., without proper support for the interconnect it may still run but in a much slower mode. But one should also realise that speed gains in the x86 family over the years come to a large extent from adding new instructions to the CPU set, and that two processors with the same instructions set extensions may still benefit from different optimisations by the compilers.  Not using the proper instruction set extensions can have a lot of influence. At my local site we've seen GROMACS  doubling its speed by choosing proper options, and the difference can even be bigger.</p> <p>Many HPC sites try to build software as much as possible from sources to exploit the available hardware as much as  possible. You may not care much about 10% or 20% performance difference on your PC, but 20% on a 160 million EURO investment represents 32 million EURO and a lot of science can be done for that money...</p> <p>But even basic portability is a myth, even if you wouldn't care much about performance (which is already a bad idea on an infrastructure as expensive as LUMI). Containers are really only guaranteed to be portable between similar systems. When well built, they are more portable than just a binary as you may be able to deal with missing or different libraries in the container, but that is where it ends. Containers are usually built for a particular CPU architecture and GPU architecture, two elements where everybody can easily see that if you change this, the container will not run. But  there is in fact more: containers talk to other hardware too, and on an HPC system the first piece of hardware that comes to mind is the interconnect. And they use the kernel of the host and the kernel modules and drivers provided by that kernel. Those can be a problem. A container that is not build to support the SlingShot interconnect, may fall back to TCP sockets in MPI, completely killing scalability. Containers that expect the knem kernel extension for good  intra-node MPI performance may not run as efficiently as LUMI uses xpmem instead.</p>"},{"location":"intro-evolving/11-Containers/#but-what-can-they-then-do-on-lumi","title":"But what can they then do on LUMI?","text":"<p>Containers are in the first place a software management instrument.</p> <ul> <li> <p>A very important reason to use containers on LUMI is reducing the pressure on the file system by software     that accesses many thousands of small files (Python and R users, you know who we are talking about).     That software kills the metadata servers of almost any parallel file system when used at scale.</p> <p>As a container on LUMI is a single file, the metadata servers of the parallel file system have far less  work to do, and all the file caching mechanisms can also work much better.</p> </li> <li> <p>Software installations that would otherwise be impossible.      E.g., some software may not even be suited for installation in     a multi-user HPC system as it uses fixed paths that are not compatible with installation in      module-controlled software stacks.     HPC systems want a lightweight <code>/usr</code> etc. structure as that part of the system     software is often stored in a RAM disk, and to reduce boot times. Moreover, different users may need     different versions of a software library so it cannot be installed in its default location in the system     software region. However, some software is ill-behaved and cannot be relocated to a different directory,     and in these cases containers help you to build a private installation that does not interfere with other     software on the system.</p> <p>They are also of interest if compiling the software takes too much work while any processor-specific optimisation that could be obtained by compiling oneself, isn't really important. E.g., if a full stack of GUI libraries is needed, as they are rarely the speed-limiting factor in an application.</p> </li> <li> <p>As an example, Conda installations are not appreciated on the main Lustre file system.</p> <p>On one hand, Conda installations tend to generate lots of small files (and then even more due to a linking strategy that does not work on Lustre). So they need to be containerised just for storage manageability.</p> <p>They also re-install lots of libraries that may already be on the system in a different version.  The isolation offered by a container environment may be a good idea to ensure that all software picks up the right versions.</p> </li> <li> <p>An example of software that is usually very hard to install is a GUI application, as they tend      to have tons of dependencies and recompiling can be tricky. Yet rather often the binary packages     that you can download cannot be installed wherever you want, so a container can come to the rescue.</p> </li> <li> <p>Another example where containers have proven to be useful on LUMI is to experiment with newer versions     of ROCm or the Cray Programming Environment than we can offer on the system. </p> <p>This often comes with limitations though, as (a) that ROCm version is still limited by the drivers on the  system and (b) we've seen incompatibilities between newer ROCm versions and the Cray MPICH libraries.</p> </li> <li> <p>And a combination of both: LUST with the help of AMD have prepared some containers with popular AI applications.     These containers use some software from Conda, a newer ROCm version installed through RPMs, and some      performance-critical code that is compiled specifically for LUMI.</p> </li> <li> <p>Isolation is often considered as an advantage of containers also. The isolation helps     preventing that software picks up libraries it should not pick up. In a context with      multiple services running on a single server, it limits problems when the security of a container     is compromised to that container. However, it also comes with a big disadvantage in an     HPC context: Debugging and performance profiling also becomes a lot harder.</p> <p>In fact, with the current state of container technology, it is often a pain also when running MPI applications as it would be much better to have only a single container per node, running MPI inside the container at the node level and then between containers on different nodes.</p> </li> </ul> <p>Remember though that whenever you use containers, you are the system administrator and not LUST. We can impossibly support all different software that users want to run in containers, and all possible Linux distributions they may want to run in those containers. We provide some advice on how to build a proper container, but if you chose to neglect it it is up to you to solve the problems that occur.</p>"},{"location":"intro-evolving/11-Containers/#managing-containers","title":"Managing containers","text":"<p>Not all container runtimes are a good match with HPC systems and the security model on  such a system. On LUMI, we currently support only one container runtime.</p> <p>Docker is not available, and will never be on the regular compute nodes as it requires elevated privileges to run the container which cannot be given safely to regular users of the system.</p> <p>Singularity is currently the only supported container runtime and is available on the login nodes and the compute nodes. It is a system command that is installed with the OS, so no module has to be loaded to enable it. We can also offer only a single version of singularity or its close cousin AppTainer  as singularity/AppTainer simply don't really like running multiple versions next to one another,  and currently the version that we offer is determined by what is offered by the OS. Currently we offer  Singularity Community Edition 4.1.3. The reason to chose for Singularity Community Edition rather than Apptainer is that it supports a build model that is compatible with the security restrictions on LUMI and is not offered in Apptainer.</p> <p>To work with containers on LUMI you will either need to pull the container from a container registry, e.g., DockerHub, bring in the container either by creating a tarball from a docker container on the remote system and then converting that to the singularity <code>.sif</code> format on LUMI or by copying the singularity <code>.sif</code> file, or use those container build features of singularity  that can be supported on LUMI within the security constraints (which is why there are no user namespaces on LUMI).</p> <p>Singularity does offer a command to pull in a Docker container and to convert it to singularity format. E.g., to pull a container for the Julia language from DockerHub, you'd use</p> <pre><code>singularity pull docker://julia\n</code></pre> <p>Singularity uses a single flat sif file for storing containers. The <code>singularity pull</code> command does the  conversion from Docker format to the singularity format.</p> <p>Singularity caches files during pull operations and that may leave a mess of files in the <code>.singularity</code> cache directory. This can lead to exhaustion of your disk quota for your home directory. So you may want to use the environment variable <code>SINGULARITY_CACHEDIR</code> to put the cache in, e.g,, your scratch space (but even then you want to clean up after the pull operation so save on your storage billing units).</p> Demo singularity pull <p>Let's try the <code>singularity pull docker://julia</code> command:</p> <p> </p> <p>We do get a lot of warnings but usually this is perfectly normal and usually they can be safely ignored.</p> <p> </p> <p>The process ends with the creation of the file <code>jula_latest.sif</code>. </p> <p>Note however that the process has left a considerable number of files in <code>~/.singularity</code> also:</p> <p> </p> <p></p> <p>There is currently limited support for building containers on LUMI and I do not expect that to change quickly. Container build strategies that require elevated privileges, and even those that require user namespaces, cannot be supported for security reasons (as user namespaces in Linux are riddled with security issues).  Enabling features that are known to have had several serious security vulnerabilities in the recent past, or that themselves are unsecure by design and could allow users to do more on the system than a regular user should be able to do, will never be supported.</p> <p>So you should pull containers from a container repository, or build the container on your own workstation and then transfer it to LUMI.</p> <p>There is some support for building on top of an existing singularity container using what the SingularityCE user guide calls \"unprivileged proot builds\". This requires loading the <code>proot</code> command which is provided by the <code>systools</code> module in CrayEnv or LUMI/23.09 or later or the <code>PRoot</code> module. The SingularityCE user guide mentions several restrictions of this process. The general guideline from the manual is: \"Generally, if your definition file starts from an existing SIF/OCI container image,  and adds software using system package managers, an unprivileged proot build is appropriate.  If your definition file compiles and installs large complex software from source,  you may wish to investigate <code>--remote</code> or <code>--fakeroot</code> builds instead.\" But as we just said, on LUMI we cannot yet provide <code>--fakeroot</code> builds due to security constraints. We have managed to compile software from source in the container, but the installation process through <code>proot</code> does come with a performance penalty. This is only when building  the container though; there is no difference when running the container.</p> <p>We are also working on a number of base images to build upon, where the base images are tested with the OS kernel on LUMI (and some for ROCm are already there).</p>"},{"location":"intro-evolving/11-Containers/#interacting-with-containers","title":"Interacting with containers","text":"<p>There are basically three ways to interact with containers.</p> <p>If you have the sif file already on the system you can enter the container with an interactive shell:</p> <pre><code>singularity shell container.sif\n</code></pre> Demo singularity shell <p> </p> <p>In this screenshot we checked the contents of the <code>/opt</code> directory before and after the <code>singularity shell julia_latest.sif</code> command. This shows that we are clearly in a different environment. Checking the <code>/etc/os-release</code> file only confirms this as LUMI runs SUSE Linux on the login nodes, not a version of Debian.</p> <p>The second way is to execute a command in the container with <code>singularity exec</code>. E.g., assuming the  container has the <code>uname</code> executable installed in it,</p> <pre><code>singularity exec container.sif uname -a\n</code></pre> Demo singularity exec <p> </p> <p>In this screenshot we execute the <code>uname -a</code> command before and with the <code>singularity exec julia_latest.sif</code> command. There are some slight differences in the output though the same kernel version is reported as the container uses the host kernel. Executing</p> <pre><code>singularity exec julia_latest.sif cat /etc/os-release\n</code></pre> <p>confirms though that the commands are executed in the container.</p> <p>The third option is often called running a container, which is done with singularity run:</p> <pre><code>singularity run container.sif\n</code></pre> <p>It does require the container to have a special script that tells singularity what  running a container means. You can check if it is present and what it does with <code>singularity inspect</code>: </p> <pre><code>singularity inspect --runscript container.sif\n</code></pre> Demo singularity run <p> </p> <p>In this screenshot we start the julia interface in the container using <code>singularity run</code>. The second command shows that the container indeed includes a script to tell singularity what <code>singularity run</code> should do.</p> <p>You want your container to be able to interact with the files in your account on the system. Singularity will automatically mount <code>$HOME</code>, <code>/tmp</code>, <code>/proc</code>, <code>/sys</code> and <code>/dev</code> in the container, but this is not enough as your home directory on LUMI is small and only meant to be used for storing program settings, etc., and not as your main work directory. (And it is also not billed and therefore no extension is allowed.) Most of the time you want to be able to access files in your project directories in <code>/project</code>, <code>/scratch</code> or <code>/flash</code>, or maybe even in <code>/appl</code>. To do this you need to tell singularity to also mount these directories in the container, either using the  <code>--bind src1:dest1,src2:dest2</code>  flag (or <code>-B</code>) or via the <code>SINGULARITY_BIND</code> or <code>SINGULARITY_BINDPATH</code> environment variables. E.g.,</p> <pre><code>export SINGULARITY_BIND='/pfs,/scratch,/projappl,/project,/flash'\n</code></pre> <p>will ensure that you have access to the scratch, project and flash directories of your project.</p> <p>For some containers that are provided by the LUMI User Support Team, modules are also available that  set <code>SINGULARITY_BINDPATH</code> so that all necessary system libraries are available in the container and users can access all their files using the same paths as outside the container.</p>"},{"location":"intro-evolving/11-Containers/#running-containers-on-lumi","title":"Running containers on LUMI","text":"<p>Just as for other jobs, you need to use Slurm to run containers on the compute nodes.</p> <p>For MPI containers one should use <code>srun</code> to run the <code>singularity exec</code> command, e.g,,</p> <pre><code>srun singularity exec --bind ${BIND_ARGS} \\\n${CONTAINER_PATH} mp_mpi_binary ${APP_PARAMS}\n</code></pre> <p>(and replace the environment variables above with the proper bind arguments for <code>--bind</code>, container file and parameters for the command that you want to run in the container).</p> <p>On LUMI, the software that you run in the container should be compatible with Cray MPICH, i.e., use the MPICH ABI (currently Cray MPICH is based on MPICH 3.4). It is then possible to tell the container to use Cray MPICH (from outside the container) rather than the MPICH variant installed in the container, so that it can offer optimal performance on the LUMI Slingshot 11 interconnect.</p> <p>Open MPI containers are currently not well supported on LUMI and we do not recommend using them. We only have a partial solution for the CPU nodes that is not tested in all scenarios,  and on the GPU nodes Open MPI is very problematic at the moment. This is due to some design issues in the design of Open MPI and what it expects from a network fabric library, and also to some piece of software to interact with the resource manager that recent versions of Open MPI require but that HPE only started supporting recently on Cray EX systems and that we haven't been able to fully test. Open MPI has a slight preference for the UCX communication library over the OFI libraries, and  until version 5 full GPU support required UCX. Moreover, binaries using Open MPI often use the so-called rpath linking process so that it becomes a lot harder to inject an Open MPI library that is installed elsewhere. The good news though is that the Open MPI developers of course also want Open MPI to work on biggest systems in the USA, and all three currently operating or planned exascale systems use the Slingshot 11 interconnect, so work is going on for better support for OFI in general and  Cray Slingshot in particular and for full GPU support.</p>"},{"location":"intro-evolving/11-Containers/#enhancements-to-the-environment","title":"Enhancements to the environment","text":"<p>To make life easier, LUST with the support of CSC did implement some modules that are either based on containers or help you run software with containers.</p>"},{"location":"intro-evolving/11-Containers/#bindings-for-singularity","title":"Bindings for singularity","text":""},{"location":"intro-evolving/11-Containers/#singularity-bindingssystem","title":"<code>singularity-bindings/system</code>","text":"<p>The <code>singularity-bindings/system</code> module which can be installed via EasyBuild helps to set <code>SINGULARITY_BIND</code> and <code>SINGULARITY_LD_LIBRARY_PATH</code> to use  Cray MPICH. Figuring out those settings is tricky, and sometimes changes to the module are needed for a specific situation because of dependency conflicts between Cray MPICH and other software in the container, which is why we don't provide it in the standard software stacks but instead make it available as an EasyBuild recipe that you can adapt to your situation and install.</p> <p>As it needs to be installed through EasyBuild, it is really meant to be  used in the context of a LUMI software stack (so not in <code>CrayEnv</code>). To find the EasyConfig files, load the <code>EasyBuild-user</code> module and  run</p> <pre><code>eb --search singularity-bindings\n</code></pre> <p>You can also check the  page for the <code>singularity-bindings</code> in the LUMI Software Library.</p> <p>You may need to change the EasyConfig for your specific purpose though. E.g., the singularity command line option <code>--rocm</code> to import the ROCm installation from the system doesn't fully work (and in fact, as we have alternative ROCm versions on the system cannot work in all cases) but that can also be fixed by extending the <code>singularity-bindings</code> module  (or by just manually setting the proper environment variables).</p>"},{"location":"intro-evolving/11-Containers/#singularity-ai-bindings","title":"<code>singularity-AI-bindings</code>","text":"<p>A second module helping with bindings, is the  <code>singularity-AI-bindings</code> module. It is available as an EasyConfig if you want to install it where it works best for you, or you can access it after </p> <pre><code>module use /appl/local/containers/ai-modules\n</code></pre> <p>This module provides bindings to some system libraries and to the regular file systems for some of the AI containers that are provided on LUMI. The module is in no way a generic module that will also work properly for containers that you pull, e.g., from Docker!</p>"},{"location":"intro-evolving/11-Containers/#build-tools-for-conda-and-python","title":"Build tools for Conda and Python","text":""},{"location":"intro-evolving/11-Containers/#cotainr-build-conda-containers-on-lumi","title":"cotainr: Build Conda containers on LUMI","text":"<p>The third tool is <code>cotainr</code>,  a tool developed by DeIC, the Danish partner in the LUMI consortium. It is a tool to pack a Conda installation into a container. It runs entirely in user space and doesn't need any special rights. (For the container specialists: It is based on the container sandbox idea to build containers in user space.)</p> <p>Containers build with <code>cotainr</code> are used just as other containers, so through the <code>singularity</code> commands discussed before.</p> <p>AI course</p> <p>The <code>cotainr</code> tool is also used extensively in the  AI workshop that the LUMI User Support Team  organises from time to time.  It is used in that course to build containers with AI software on top of some  ROCm<sup>TM</sup> containers that we provide. A link to the course material of that training was not yet available at the time of writing.</p>"},{"location":"intro-evolving/11-Containers/#container-wrapper-for-python-packages-and-conda","title":"Container wrapper for Python packages and conda","text":"<p>The fourth tool is a container wrapper tool that users from Finland may also know as Tykky (the name on their national systems).  It is a tool to wrap Python and conda installations in a container and then create wrapper scripts for the commands in the bin subdirectory so that for most practical use cases the commands can be used without directly using singularity commands.  Whereas cotainr fully exposes the container to users and its software is accessed through the regular singularity commands, Tykky tries to hide this complexity with wrapper scripts that take care of all bindings and calling singularity. On LUMI, it is provided by the <code>lumi-container-wrapper</code> module which is available in the <code>CrayEnv</code> environment and in the LUMI software stacks.</p> <p>The tool can work in four modes:</p> <ol> <li> <p>It can create a conda environment based on a Conda environment file and create     wrapper scripts for that installation.</p> </li> <li> <p>It can install a number of Python packages via <code>pip</code> and create wrapper scripts. On LUMI,     this is done on top of one of the <code>cray-python</code> modules that already contain     optimised versions of NumPy, SciPy and pandas. Python packages are specified     in a <code>requirements.txt</code> file used by <code>pip</code>.</p> </li> <li> <p>It can do a combination of both of the above: Install a Conda-based Python      environment and in one go also install a number of additional Python packages     via <code>pip</code>. </p> </li> <li> <p>The fourth option is to use the container wrapper to create wrapper scripts for     commands in an existing container.</p> </li> </ol> <p>For the first three options, the container wrapper will then perform the installation  in a work directory, create some wrapper commands in the <code>bin</code> subdirectory of the directory  where you tell the container wrapper tool to do the installation,  and it will use SquashFS to create as single file that contains the conda or Python installation. So strictly speaking it does not create a  container, but a SquashFS file that is then mounted in a small existing base container.  However, the wrappers created for all commands in the <code>bin</code> subdirectory of the conda or Python installation take care of doing the proper bindings. If you want to use the container through singularity commands however, you'll have to do that mounting by hand, including  mounting the SquashFS file on the right directory in the container.</p> <p>Note that the wrapper scripts may seem transparent, but running a script that contains the wrapper commands outside the container may have different results from running the same script inside the container. After all, the script that runs outside the  container sees a different environment than the same script running inside the container.</p> <p>We do strongly recommend to use cotainr or the container wrapper tool for larger conda and Python installation. We will not raise your file quota if it is to house such installation in your <code>/project</code> directory.</p> Demo lumi-container-wrapper for a Conda installation <p>Create a subdirectory to experiment. In that subdirectory, create a file named <code>env.yml</code> with the content:</p> <pre><code>channels:\n  - conda-forge\ndependencies:\n  - python=3.8.8\n  - scipy\n  - nglview\n</code></pre> <p>and create an empty subdirectory <code>conda-cont-1</code>.</p> <p>Now you can follow the commands on the slides below:</p> <p> </p> <p>On the slide above we prepared the environment.</p> <p>Now lets run the command </p> <pre><code>conda-containerize new --prefix ./conda-cont-1 env.yml\n</code></pre> <p>and look at the output that scrolls over the screen. The screenshots don't show the full output as some parts of the screen get overwritten during the process:</p> <p> </p> <p>The tool will first build the conda installation in a tempororary work directory and also uses a base container for that purpose.</p> <p> </p> <p>The conda installation itself though is stored in a SquashFS file that is then used by the container.</p> <p> </p> <p> </p> <p>In the slide above we see the installation contains both a singularity container and a SquashFS file. They work together to get a working conda installation.</p> <p>The <code>bin</code> directory seems to contain the commands, but these are in fact scripts  that run those commands in the container with the SquashFS file system mounted in it.</p> <p> </p> <p>So as you can see above, we can simply use the <code>python3</code> command without realising what goes on behind the screen...</p> <p>Relevant documentation for <code>lumi-container-wrapper</code></p> <ul> <li>Page in the main LUMI documentation</li> <li><code>lumi-container-wrapper</code> in the LUMI Software Library</li> <li>Tykky page in the CSC documentation </li> </ul>"},{"location":"intro-evolving/11-Containers/#pre-build-containers-vnc-and-ccpe","title":"Pre-build containers: VNC and CCPE","text":""},{"location":"intro-evolving/11-Containers/#vnc","title":"VNC","text":"<p>The fifth tool is a container that we provide with some bash functions to start a VNC server as one way to run GUI programs and as an alternative to  the (currently more sophisticated) VNC-based GUI desktop setup offered in Open OnDemand (see the \"Getting Access to LUMI notes\"). It can be used in <code>CrayEnv</code> or in the LUMI stacks through the <code>lumi-vnc</code> module.  The container also contains a poor men's window manager (and yes, we know that there are sometimes some problems with fonts). It is possible to connect to the VNC server either through a regular VNC client on your PC or a web browser, but in both cases you'll have to create an ssh tunnel to access the server. Try</p> <pre><code>module help lumi-vnc\n</code></pre> <p>for more information on how to use <code>lumi-vnc</code>.</p> <p>For most users, the Open OnDemand web interface and tools offered in that interface will be a better alternative.</p>"},{"location":"intro-evolving/11-Containers/#ccpe","title":"CCPE","text":"<p>We are currently also working with HPE to provide a containerised Cray Programming Environment to be able to test newer versions of the Cray PE than are offered on LUMI.</p> <p>As working in a container requires a very good understanding of the differences between the environment in and out of the container, using those containers is really only for more experienced users who understand how modules and environments work. </p> <p>These containers will be offered with user-installable EasyBuild recipes  (ccpe in the LUMI Software Library) as customisation for the particular purpose of the user will often be necessary. This can often be done by minor changes to the recipes provided by LUST.</p> <p>The functionality of this solution may still be limited though, in particular for  GPU applications. Each version of the Cray PE is developed with one or a few specific versions of ROCm\u2122 in mind, so if that version of ROCm\u2122 is too new for the current driver on the software, running GPU software may fail. Some containers may also expect a newer version of the OS and though they contain the necessary userland libraries, these may expect a newer version of the kernel or libraries that are injected from the system.</p> <p>General availability to users is expected by the summer of 2025.</p>"},{"location":"intro-evolving/11-Containers/#pre-built-ai-containers","title":"Pre-built AI containers","text":"<p>LUST with the help of AMD is also building some containers with popular AI software. These containers contain a ROCm version that is appropriate for the software, use Conda for some components, but have several of the performance critical components built specifically for LUMI for near-optimal performance. Depending on the software they also contain a RCCL library with the appropriate plugin to work well on the Slingshot 11 interconnect, or a horovod compiled to use Cray MPICH. </p> <p>Some of the containers can be provided through a module that is user-installable with EasyBuild. That module sets the <code>SINGULARITY_BIND</code> environment variable to ensure proper bindings (as they need, e.g., the libfabric library from the system and the proper \"CXI provider\" for libfabric to connect to the Slingshot interconnect). The module will also provide an environment variable to refer to the container (name with full path) to make it easy to refer to the container in job scripts. Some of the modules also provide some scripts that may make using the containers easier in some standard scenarios.  Alternatively, the user support team also provides the <code>singularity-AI-bindings</code> module discussed above,  for users who want to run the containers as manually as possible yet want an  easy way to deal with the necessary bindings of user file systems and HPE Cray PE components needed from the system (see also course materials for the AI training/workshop).</p> <p>These containers can be found through the LUMI Software Library and are marked with a container label. At the time of the course, there are containers for</p> <ul> <li>PyTorch, which is the best tested and most developed one,</li> <li>TensorFlow,</li> <li>JAX,</li> <li>AlphaFold,</li> <li>ROCm and</li> <li>mpi4py.</li> </ul>"},{"location":"intro-evolving/11-Containers/#running-the-ai-containers-complicated-way-without-modules","title":"Running the AI containers - complicated way without modules","text":"<p>The containers that we provide have everything they need to use RCCL and/or MPI on LUMI. It is not needed to use the <code>singularity-bindings/system</code> module described earlier as that module tries to bind too much external software to the container.</p> <p>Yet to be able to properly use the containers, users do need to take care of some bindings</p> <ul> <li> <p>Some system directories and libraries have to be bound to the container:</p> <pre><code>-B /var/spool/slurmd,/opt/cray,/usr/lib64/libcxi.so.1\n</code></pre> <p>The first one is needed to work together with Slurm. The second one contains the MPI and libfabric library. The third one is the actual component that binds libfabric to the Slingshot network adapter and is called  the CXI provider.</p> </li> <li> <p>By default your home directory will be available in the container, but as your home directory is not your     main workspace, you may want to bind your subdirectory in <code>/project</code>, <code>/scratch</code> and/or <code>/flash</code> also, using, e.g.,</p> <pre><code>-B /pfs,/scratch,/projappl,/project,/flash\n</code></pre> </li> </ul> <p>There are also a number of components that may need further initialisation:</p> <ul> <li> <p>The MIOpen library (which is the equivalent of the CUDA cuDNN library)     has problems with file/record locking on Lustre so some environment variables     are needed to move some work directories to <code>/tmp</code>.</p> </li> <li> <p>RCCL (the ROCm\u2122 equivalent of the NVIDIA NCCL communication library)     needs to be told the right network interfaces to use as otherwise it tends to take the interface     to the management network of the cluster instead and gets stuck.</p> </li> <li> <p>GPU-aware MPI also needs to be set up (see earlier in the course)</p> </li> <li> <p>Your AI package may need some environment variables too (e.g.,      <code>MASTER_ADDR</code> and <code>MASTER_PORT</code> for distributed learning with PyTorch)</p> </li> </ul> <p>Moreover, most (if not all at the moment) containers that we provide with Python packages, are built using Conda to install Python. When entering those containers, conda needs to be activated. The containers are built in such a way that the environment variable <code>WITH_CONDA</code> provides the  necessary command, so in most cases you only need to run </p> <pre><code>$WITH_CONDA\n</code></pre> <p>as a command in the script that is executed in the container or on the command line.</p>"},{"location":"intro-evolving/11-Containers/#running-the-containers-through-easybuild-generated-modules","title":"Running the containers through EasyBuild-generated modules","text":"<p>Doing all those initialisations, is a burden. Therefore we provide EasyBuild recipes to \"install\" the containers and to provide a module that helps setting environment variables in the initialisation.</p> <p>For packages for which we know generic usage patterns, we provide some scripts that do most settings. We may have to drop that though, as sometimes there are simply too many scenarios and promoting a particular one too much may mislead users and encourage them to try to map their problem on an approach which may be less efficient than theirs. When using the module, those scripts will be available in the <code>/runscripts</code> directory in the container, but are also in a subdirectory on the Lustre file system. So in principle you can even edit them or add your own scripts, though they would be erased if you reinstall the module with EasyBuild.</p> <p>Some of the newer PyTorch containers (from PyTorch 2.6.0 on) also provide wrapper scripts similar to the wrapper scripts provided by the CSC <code>pytorch</code> modules, so many of the examples in their documentation should also work with minimal changes (such as the module name).</p> <p>The modules also define a number of environment variables that make life easier. E.g., the <code>SINGULARITY_BINDPATH</code>  environment variable is already set to bind the necessary files and directories from the system and to make sure that your project, scratch and flash spaces are available at the same location as on LUMI so that even symbolic links in those directories should still work.</p> <p>We recently started adding a pre-configured virtual environment to the containers to add your own packages. The virtual environment can be found in the container in a subdirectory of <code>/user-software/venv</code>. To install packages from within the container, this directory needs to be writeable which is done by binding <code>/user-software</code> to the <code>$CONTAINERROOT/user-software</code> subdirectory outside the container. If you add a lot of packages that way, you re-create the filesystem issues that the container is supposed to solve, but we have a solution for that also. These containers provide the <code>make-squashfs</code> command to generate a SquashFS file from the installation that will be used by the container instead next time the module for  the container is reloaded. And in case you prefer to fully delete the <code>user-software</code> subdirectory afterwards from <code>$CONTAINERROOT</code>, it can be re-created using <code>unmake-squashfs</code> so that you can add further packages. You can also use <code>/user-software</code> to install software in other ways from within the container and can basically create whatever subdirectory you want into it.  This is basically automating the procedure described in the \"Extending containers with virtual environments for faster testing\" lecture of the AI training provided by the LUMI User Support Team.</p> <p>These containers with pre-configured virtual environment offer another advantage also: The module injects a number of environment variables into the container so that it is no longer needed to activate the conda environment and Python virtual environment by sourcing scripts.</p> <p>In fact, someone with EasyBuild experience may even help you to further extend the recipe that we provide to already install extra packages, and we provide an example of how to do that with  our PyTorch containers.</p> <p></p> <p>Installing the EasyBuild recipes for those containers is also done via the <code>EasyBuild-user</code> module, but it is best to use a special trick. There is a special partition called <code>partition/container</code> that is only used to install those containers and when using that partition for the installation, the container will be available in all versions of the LUMI stack and in the CrayEnv stack.</p> <p>Installation is as simple as, e.g., </p> <pre><code>module load LUMI partition/container EasyBuild-user\neb PyTorch-2.6.0-rocm-6.2.4-python-3.12-singularity-20250404.eb\n</code></pre> <p>Before running it is best to clean up (<code>module purge</code>) or take a new shell to avoid conflicts with  environment variables provided by other modules.</p> <p>The installation with EasyBuild will make a copy from the <code>.sif</code> Singularity container image file that we provide somewhere in <code>/appl/local/containers</code> to the software installation subdirectory of your <code>$EBU_USER_PREFIX</code> EasyBuild installation directory. These files are big and you may wish to delete that file which is easily done: After loading the container module, the environment variable <code>SIF</code> contains the name with full path of the container file.  After removing the container file from your personal software directory, you need to reload the container module and from then on, <code>SIF</code> will point to the corresponding container in  <code>/appl/local/containers/easybuild-sif-images</code>. So:</p> <pre><code>module load PyTorch/2.6.0-rocm-6.2.4-python-3.12-singularity-20250404\nrm \u2013f $SIF\nmodule load PyTorch/2.6.0-rocm-6.2.4-python-3.12-singularity-20250404\n</code></pre> <p>We don't really recommend removing the container image though and certainly not if you are interested in reproducibility. We may remove the image in <code>/appl/local/containers/easybuild-sif-images</code> without prior notice if we notice that the container has too many problems, e.g., after a system update. But that same container that doesn't work well for others, may work well enough for you that you don't want to rebuild whatever environment you built with the container.</p>"},{"location":"intro-evolving/11-Containers/#example-distributed-learning-without-using-easybuild","title":"Example: Distributed learning without using EasyBuild","text":"<p>To really run this example, some additional program files and data files are needed that are not explained in this text. You can find more information on the  PyTorch page in the LUMI Software Library.</p> <p></p> <p>We'll need to create a number of scripts before we can even run the container. The job script alone is not enough as there are also per-task initialisations needed that cannot be done directly in the job script. In this example, we run the script that does the per-task initialisations in the container, but it is also possible to do this outside the container which would give access to the Slurm commands and may even simplify a bit. </p> <p>If you want to know more about running AI loads on LUMI, we strongly recommend to take a look at the course materials of the AI course. Basically, running AI on AMD GPUs is not that different from NIVIDA GPUs, but there are some initialisations that are different. The main difference may be the difference between cloud environments, clusters with easy access to the compute nodes and clusters like LUMI that require you to always go through the resource manager if you want access to a compute node. We'll need to create a number of scripts before we can even run the container.</p> <p>The first script is a Python program to extract the name of the master node from a Slurm environment variable. This will be needed to set up the communication in PyTorch. Store it in <code>get-master.py</code>:</p> <pre><code>import argparse\ndef get_parser():\n    parser = argparse.ArgumentParser(description=\"Extract master node name from Slurm node list\",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"nodelist\", help=\"Slurm nodelist\")\n    return parser\n\n\nif __name__ == '__main__':\n    parser = get_parser()\n    args = parser.parse_args()\n\n    first_nodelist = args.nodelist.split(',')[0]\n\n    if '[' in first_nodelist:\n        a = first_nodelist.split('[')\n        first_node = a[0] + a[1].split('-')[0]\n\n    else:\n        first_node = first_nodelist\n\n    print(first_node)\n</code></pre> <p></p> <p>Second, we need a script that we will run in the container. Store the script as <code>run-pytorch.sh</code>:</p> <pre><code>#!/bin/bash -e\n\n# Make sure GPUs are up\nif [ $SLURM_LOCALID -eq 0 ] ; then\n    rocm-smi\nfi\nsleep 2\n\n# !Remove this if using an image extended with cotainr or a container from elsewhere.!\n# Start conda environment inside the container\n$WITH_CONDA\n\n# MIOPEN needs some initialisation for the cache as the default location\n# does not work on LUMI as Lustre does not provide the necessary features.\nexport MIOPEN_USER_DB_PATH=\"/tmp/$(whoami)-miopen-cache-$SLURM_NODEID\"\nexport MIOPEN_CUSTOM_CACHE_DIR=$MIOPEN_USER_DB_PATH\n\nif [ $SLURM_LOCALID -eq 0 ] ; then\n    rm -rf $MIOPEN_USER_DB_PATH\n    mkdir -p $MIOPEN_USER_DB_PATH\nfi\nsleep 2\n\n# Optional! Set NCCL debug output to check correct use of aws-ofi-rccl (these are very verbose)\nexport NCCL_DEBUG=INFO\nexport NCCL_DEBUG_SUBSYS=INIT,COLL\n\n# Set interfaces to be used by RCCL.\n# This is needed as otherwise RCCL tries to use a network interface it has\n# no access to on LUMI.\nexport NCCL_SOCKET_IFNAME=hsn0,hsn1,hsn2,hsn3\n# Next line not needed anymore in ROCm 6.2. You may see PHB also instead of 3, which is equivalent \nexport NCCL_NET_GDR_LEVEL=3\n\n# Set ROCR_VISIBLE_DEVICES so that each task uses the proper GPU\nexport ROCR_VISIBLE_DEVICES=$SLURM_LOCALID\n\n# Report affinity to check\necho \"Rank $SLURM_PROCID --&gt; $(taskset -p $$); GPU $ROCR_VISIBLE_DEVICES\"\n\n# The usual PyTorch initialisations (also needed on NVIDIA)\n# Note that since we fix the port ID it is not possible to run, e.g., two\n# instances via this script using half a node each.\nexport MASTER_ADDR=$(python get-master.py \"$SLURM_NODELIST\")\nexport MASTER_PORT=29500\nexport WORLD_SIZE=$SLURM_NPROCS\nexport RANK=$SLURM_PROCID\n\n# Run app\ncd /workdir/mnist\npython -u mnist_DDP.py --gpu --modelpath model\n</code></pre> <p>The script needs to be executable.</p> <p>The script sets a number of environment variables. Some are fairly standard when using PyTorch on an HPC cluster while others are specific for the LUMI interconnect and architecture or the  AMD ROCm environment. We notice a number of things:</p> <ul> <li> <p>At the start we just print some information about the GPU. We do this only ones on each node     on the process which is why we test on <code>$SLURM_LOCALID</code>, which is a numbering starting from 0     on each node of the job:</p> <pre><code>if [ $SLURM_LOCALID -eq 0 ] ; then\n    rocm-smi\nfi\nsleep 2\n</code></pre> </li> <li> <p>The container uses a Conda environment internally. So to make the right version of Python     and its packages availabe, we need to activate the environment. The precise command to     activate the environment is stored in <code>$WITH_CONDA</code> and we can just call it by specifying     the variable as a bash command.</p> </li> <li> <p>The <code>MIOPEN_</code> environment variables are needed to make      MIOpen create its caches on <code>/tmp</code>     as doing this on Lustre fails because of file locking issues:</p> <pre><code>export MIOPEN_USER_DB_PATH=\"/tmp/$(whoami)-miopen-cache-$SLURM_NODEID\"\nexport MIOPEN_CUSTOM_CACHE_DIR=$MIOPEN_USER_DB_PATH\n\nif [ $SLURM_LOCALID -eq 0 ] ; then\n    rm -rf $MIOPEN_USER_DB_PATH\n    mkdir -p $MIOPEN_USER_DB_PATH\nfi\n</code></pre> <p>These caches are used to store compiled kernels.</p> </li> <li> <p>It is also essential to tell RCCL, the communication library, which network adapters to use.      These environment variables start with <code>NCCL_</code> because ROCm tries to keep things as similar as     possible to NCCL in the NVIDIA ecosystem:</p> <pre><code>export NCCL_SOCKET_IFNAME=hsn0,hsn1,hsn2,hsn3\nexport NCCL_NET_GDR_LEVEL=3\n</code></pre> <p>Without this RCCL may try to use a network adapter meant for system management rather than inter-node communications!</p> </li> <li> <p>We also set <code>ROCR_VISIBLE_DEVICES</code> to ensure that each task uses the proper GPU.     This is again based on the local task ID of each Slurm task.</p> </li> <li> <p>Furthermore some environment variables are needed by PyTorch itself that are also needed on     NVIDIA systems.</p> <p>PyTorch needs to find the master for communication which is done through the <code>get-master.py</code> script that we created before:</p> <pre><code>export MASTER_ADDR=$(python get-master.py \"$SLURM_NODELIST\")\nexport MASTER_PORT=29500\n</code></pre> <p>As we fix the port number here, the <code>conda-python-distributed</code> script that we provide,  has to run on exclusive nodes. Running, e.g., 2 4-GPU jobs on the same node with this command will not work as there will be a conflict for the TCP port for communication on the master as <code>MASTER_PORT</code> is hard-coded in  this version of the script.</p> </li> </ul> <p></p> <p>And finally you need a job script that you can then submit with <code>sbatch</code>. Lets call it <code>my-job.sh</code>:</p> <pre><code>#!/bin/bash -e\n#SBATCH --nodes=4\n#SBATCH --gpus-per-node=8\n#SBATCH --tasks-per-node=8\n#SBATCH --output=\"output_%x_%j.txt\"\n#SBATCH --partition=standard-g\n#SBATCH --mem=480G\n#SBATCH --time=00:10:00\n#SBATCH --account=project_&lt;your_project_id&gt;\n\nCONTAINER=your-container-image.sif\n\nc=fe\nMYMASKS=\"0x${c}000000000000,0x${c}00000000000000,0x${c}0000,0x${c}000000,0x${c},0x${c}00,0x${c}00000000,0x${c}0000000000\"\n\nsrun --cpu-bind=mask_cpu:$MYMASKS \\\n  singularity exec \\\n    -B /var/spool/slurmd \\\n    -B /opt/cray \\\n    -B /usr/lib64/libcxi.so.1 \\\n    -B /usr/lib64/libjansson.so.4 \\\n    -B $PWD:/workdir \\\n    $CONTAINER /workdir/run-pytorch.sh\n</code></pre> <p>The important parts here are:</p> <ul> <li> <p>We start PyTorch via <code>srun</code> and this is recommended. The <code>torchrun</code> command has to be used with care     as not all its start mechanisms are compatible with LUMI.</p> </li> <li> <p>We also use a      particular CPU mapping so that each rank can use the corresponding GPU number (which is taken care of in the      <code>run-pytorch.sh</code> script).      We use the     \"Linear assignment of GCD, then match the cores\" strategy.</p> </li> <li> <p>Note the bindings. In this case we do not even bind the full <code>/project</code>, <code>/scratch</code> and <code>/flash</code> subdirectories,     but simply make the current subdirectory that we are using outside the container available as <code>/workdir</code> in      the container. This also implies that any non-relative symbolic link or any relative symbolic link that takes     you out of the current directory and its subdirectories, will not work, which is awkward as you may want     several libraries to run from to have simultaneous jobs, but, e.g., don't want to copy your dataset to     each of those directories.</p> </li> </ul>"},{"location":"intro-evolving/11-Containers/#example-distributed-learning-with-the-easybuild-generated-module","title":"Example: Distributed learning with the EasyBuild-generated module","text":"<p>To really run this example, some additional program files and data files are needed that are not explained in this text. You can find more information on the  PyTorch page in the LUMI Software Library.</p> <p></p> <p>It turns out that the first two above scripts in the example above, are fairly generic. Therefore the module provides a slight variant of the second script, now called <code>conda-python-distributed</code>, that at the end calls python, passing it all arguments it got and hence can be used to start other Python code also. It is in <code>$CONTAINERROOT/runscripts</code> or in the container as <code>/runscripts</code>.</p> <p>As the module also takes care of bindings, the job script is simplified to</p> <pre><code>#!/bin/bash -e\n#SBATCH --nodes=4\n#SBATCH --gpus-per-node=8\n#SBATCH --tasks-per-node=8\n#SBATCH --output=\"output_%x_%j.txt\"\n#SBATCH --partition=standard-g\n#SBATCH --mem=480G\n#SBATCH --time=00:10:00\n#SBATCH --account=project_&lt;your_project_id&gt;\n\nmodule load LUMI  # Which version doesn't matter, it is only to get the container.\nmodule load PyTorch/2.6.0-rocm-6.2.4-python-3.12-singularity-20250404\n\nc=fe\nMYMASKS=\"0x${c}000000000000,0x${c}00000000000000,0x${c}0000,0x${c}000000,0x${c},0x${c}00,0x${c}00000000,0x${c}0000000000\"\n\ncd mnist\nsrun --cpu-bind=mask_cpu:$MYMASKS \\\n  singularity exec $SIFPYTORCH \\\n    conda-python-distributed -u mnist_DDP.py --gpu --modelpath model\n</code></pre> <p>So basically you only need to take care of the proper CPU bindings where we again use the \"Linear ssignment of GCD, then match the cores\" strategy.</p>"},{"location":"intro-evolving/11-Containers/#extending-the-containers","title":"Extending the containers","text":"<p>We can never provide all software that is needed for every user in our containers.  But there are several mechanisms that can be used to extend the containers that we provide:</p>"},{"location":"intro-evolving/11-Containers/#extending-the-container-with-cotainr","title":"Extending the container with <code>cotainr</code>","text":"<p>The LUMI Software Library offers some container images for ROCm\u2122). Though these images can be used simply to experiment with different versions of ROCm, an important use of those images is as base images for the cotainr tool that supports Conda to install software in the container.</p> <p>Some care is needed though when you want to build your own AI containers. You need to ensure that binaries for AMD GPUs are used, as by default you may get served NVIDIA-only binaries. MPI can also be a problem, as the base image does not yet provide, e.g., a properly configures <code>mpi4py</code> (which would likely be installed in a way that conflicts with <code>cotainr</code>).</p> <p>The container images that we provide can be found in the following directories on LUMI:</p> <ul> <li> <p><code>/appl/local/containers/sif-images</code>: Symbolic link to the latest version of the container for each ROCm version provided.      Those links can change without notice!</p> </li> <li> <p><code>/appl/local/containers/tested-containers</code>: Tested containers provided as a Singularity .sif file and a docker-generated tarball.      Containers in this directory are removed quickly when a new version becomes available.</p> </li> <li> <p><code>/appl/local/containers/easybuild-sif-images</code> : Singularity .sif images used with the EasyConfigs that we provide.      They tend to be available for a longer time than in the other two subdirectories.</p> </li> </ul> <p>First you need to create a yaml file to tell Conda which is called by <code>cotainr</code> which packages need to be installed. This is also discussed in  \"Using the images as base image for cotainr\" section of the LUMI Software Library rocm page.</p> <p>For example, to create a PyTorch installation for ROCm 6.0.3, one can first create the  YAML file <code>py312_rocm603_pytorch.yml</code> with content</p> <pre><code>name: py312_rocm603_pytorch.yml\nchannels:\n  - conda-forge\ndependencies:\n  - filelock=3.15.4\n  - fsspec=2024.9.0\n  - jinja2=3.1.4\n  - markupsafe=2.1.5\n  - mpmath=1.3.0\n  - networkx=3.3\n  - numpy=2.1.1\n  - pillow=10.4.0\n  - pip=24.0\n  - python=3.12.3\n  - sympy=1.13.2\n  - typing-extensions=4.12.2\n  - pip:\n    - --extra-index-url https://download.pytorch.org/whl/rocm6.0/\n    - pytorch-triton-rocm==3.0.0\n    - torch==2.4.1+rocm6.0\n    - torchaudio==2.4.1+rocm6.0\n    - torchvision==0.19.1+rocm6.0\n</code></pre> <p>Next we need to run <code>cotainr</code> with the right base image to generate the container:</p> <pre><code>module load LUMI/24.03 cotainr\ncotainr build my-new-image.sif \\\n    --base-image=/appl/local/containers/sif-images/lumi-rocm-rocm-6.0.3.sif \\\n    --conda-env=py312_rocm603_pytorch.yml\n</code></pre> <p>or, as for the current version of <code>cotainr</code> in <code>LUMI/24.03</code> this image is actually the base image for the <code>lumi-g</code> preset:</p> <pre><code>module load LUMI/24.03 cotainr\ncotainr build my-new-image.sif \\\n    --system=lumi-g \\\n    --conda-env=py312_rocm603_pytorch.yml\n</code></pre> <p>The <code>cotainr</code> command takes three arguments in this example:</p> <ul> <li> <p><code>my-new-image.sif</code> is the name of the container image that it will generate.</p> </li> <li> <p><code>--base-image=/appl/local/containers/sif-images/lumi-rocm-rocm-6.0.3.sif</code> points to the base image that we will use,     in this case the latest version of the ROCm 6.0.3 container provided on LUMI.</p> <p>This version was chosen for this case as ROCm 6.0.3 version corresponding to the driver on LUMI at the time of writing (early FEbruary 2025), but with that driver we could also have chosen PyTorch versions that require ROCm 6.1 or 6.2.</p> </li> <li> <p><code>--conda-env=py312_rocm603_pytorch.yml</code>: The YAML file with the environment definition.</p> </li> </ul> <p>The result is a container for which you will still need to provide the proper bindings to some libraries on the system (to interface properly with Slurm and so that RCCL with the OFI plugin can work) and to your spaces in the file system that you want to use. Use, e.g., <code>singularity-AI-bindings</code> which should work for many cases. Or you can adapt an EasyBuild-generated module for the ROCm container that you used to use your container instead (which will require the EasyBuild <code>eb</code> command flag <code>--sourcepath</code> to specify where it can find the container that you generated, and you cannot delete it from the installation afterwards). </p> <p>Note that <code>cotainr</code> can build upon the ROCm\u2122 containers that are provided on LUMI, but not upon containers that already contain a Conda installation. It cannot extend an existing Conda installation in a container.</p>"},{"location":"intro-evolving/11-Containers/#extending-the-container-with-the-singularity-unprivileged-proot-build","title":"Extending the container with the singularity unprivileged <code>proot</code> build","text":"<p>Singularity specialists can also build upon an existing container using <code>singularity build</code>.  The options for build processes are limited though because we have no support for user namespaces or the fakeroot feature. The \"Unprivileged <code>proot</code> builds\" feature from recent SingularityCE versions is supported though.</p> <p>To use this feature, you first need to write a singularity-compatible container definition file, e.g.,</p> <pre><code>Bootstrap: localimage\n\nFrom: /appl/local/containers/easybuild-sif-images/lumi-pytorch-rocm-6.0.3-python-3.12-pytorch-v2.3.1-dockerhash-2c1c14cafd28.sif\n\n%post\n\nzypper -n install -y Mesa libglvnd libgthread-2_0-0 hostname\n</code></pre> <p>which is a definition file that will use the SUSE <code>zypper</code> software installation tool to add a number of packages to one of the LUMI PyTorch containers to provide support for software OpenGL rendering (the CDNA GPUs do not support OpenGL acceleration) and the <code>hostname</code> command.</p> <p>To use the <code>singularity build</code> command, we first need to make the <code>proot</code> command available. This is currently not installed in the LUMI system image, but is provided by the <code>systools/24.03</code> and later modules that can be found in the corresponding LUMI stack and in the CrayEnv environment or by the <code>PRoot</code> module in all LUMI stacks and the CrayEnv stack.</p> <p>To update the container, run:</p> <pre><code>module load LUMI/24.03 systools\nsingularity build my-new-container.sif my-container-definition.def\n</code></pre> <p>Note:</p> <ul> <li> <p>In this example, as we use the <code>LUMI/24.03</code> module, there is no need to specify the version of <code>systools</code> as there     is only one in this stack. An alternative would have been to use</p> <pre><code>module load CrayEnv systools/24.03\n</code></pre> </li> <li> <p>The <code>singularity build</code> command takes two options: The first one is the name of the new container image that it     generates and the second one is the container definition file.</p> </li> </ul> <p>When starting from a base image installed with one of our EasyBuild recipes, it is possible to overwrite the image file and in fact, the module that was generated with EasyBuild might just work...</p>"},{"location":"intro-evolving/11-Containers/#extending-the-container-through-a-python-virtual-environment","title":"Extending the container through a Python virtual environment","text":"<p>Some newer containers installed with EasyBuild already include a pre-initialised virtual environment  (created with <code>venv</code>). The location in the filesystem of that virtual environment is:</p> <ul> <li> <p><code>/user-software/venv/MyVEnv</code> in the container, where <code>MyVEnv</code> is actually different in different containers.     We used the same name as for the Conda environment.</p> </li> <li> <p><code>$CONTAINERROOT/user-software/venv/MyVEnv</code> outside the container (unless that directory structure is replaced     with the <code>$CONTAINERROOT/user-software.squashfs</code> file).</p> </li> </ul> <p>That directory struture was chosen to (a) make it possible to install a second virtual environment in <code>/user-software/venv</code> while  (b) also leaving space to install software by hand in <code>/user-software</code> and hence create a <code>bin</code> and <code>lib</code> subdirectory in those (though they currently are not automatically added to the search paths for executables and shared libraries in the container).</p> <p>The whole process is very simple with those containers that already have a pre-initialised virtual environment as  the module already intialises several environment variables in the container that have the combined effect of activating both the Conda installation and then on top of it, the default Python virtual environment.</p> <p>Outside the container, we need to load the container module, and then we can easily go into the container using the <code>SIF</code> environment variable to point to its name:</p> <pre><code>module load LUMI\nmodule load PyTorch/2.6.0-rocm-6.2.4-python-3.12-singularity-20250404\nsingularity shell $SIF\n</code></pre> <p>and in the container, at the <code>Singularity&gt;</code> prompt, we can use <code>pip install</code> without extra options, e.g.,</p> <pre><code>pip install pytorch-lightning\n</code></pre> <p></p> <p>As already discussed before in this session of the tutorial, such a Python virtual environment has the potential to create a lot of small files in the Lustre <code>$CONTAINERROOT/user-software</code> subdirectory, which can wipe out all benefits we got from using a container for the Python installation. But our modules with virtual environment support offer a solution for this also: the <code>make-squashfs</code> command (which should be run outside the container) will convert the <code>user-software</code> subdirectory in <code>$CONTAINERROOT</code> into the SquashFS file <code>user-software.squashfs</code> which, after reloading the module, will be used to provide the <code>/user-software</code> subdirectory in the container. The downside is that now <code>/user-software</code> is read-only as it comes from the SquashFS file. To install further packages, you'd have to remove the <code>user-software.squashfs</code> file again and reload the container module.</p> <p>Currently the <code>make-squashfs</code> file will not remove the <code>$CONTAINERROOT/user-software</code> subdirectory, but once you have verified that the SquashFS file is OK and useable in the container, you can safely delete it yourself. We also provide the <code>unmake-squasfs</code> script to re-generate the <code>$CONTAINERROOT/user-software</code> subdirectory (though attribues such as file time, etc., will not be the same as before).</p> <p>It is of course possible to use this technique with all Python containers, but you may have to do a lot more steps by hand, such as adding the binding for a directory for the virtual environment, creating and activating the environment, and replacing the directory with a SquashFS file to improve file system performance.</p>"},{"location":"intro-evolving/11-Containers/#conclusion-container-limitations-on-lumi","title":"Conclusion: Container limitations on LUMI","text":"<p>The idea of \"bring your own userland and run it on a system-optimised kernel\" idea that proponents of containers promote, has two major flaws</p> <ol> <li> <p>Every set of userland libraries comes with certain expectations for kernel versions, kernel drivers and their      versions, hardware, etc. If these expectations are not met, the container may not work at all or may work inefficiently.</p> <p>This is particulary true for ROCm(TM) support as each version of the ROCm\u2122 libraries only works with a limited range of GPU driver versions. If the ROCm\u2122 libraries in the container are too old or too new for the driver on the system, the container will not work. As the ROCm\u2122 ecosystem is maturing, the range of driver versions that are compatible with each ROC\u2122 version is growing, but this issue will likely never be completely solved.</p> </li> <li> <p>Support for specific hardware is not done in the kernel alone. Most of the optimisations for hardware     on an HPC system are actually in userland. </p> <ul> <li> <p>As most of the time of an application is spent in userland, this is where you need to     optimise for a specific CPU and GPU. If a binary is not compiled to benefit from the      additional speed of new instructions in a newer architecture, no container runtime      can inject that support.</p> </li> <li> <p>Support for the SlingShot network, is in the libfabric library and its CXI     provider, which are userland elements (and the same holds for other network technologies). </p> <p>Container promoters will tell you that is not a problem and that you can inject those libraries in the container, but the reality is that that strategy does not  always work, as the library you have on the system may not be the right version for the  container, or may need other libraries that conflict with the versions in the container.</p> <p>Likewise, for containers for distributed AI, one may need to inject an appropriate RCCL plugin to fully use the Slingshot 11 interconnect.</p> </li> </ul> </li> </ol> <p>The support for building containers on LUMI is currently limited due to security concerns. Any build process that requires elevated privileges, fakeroot or user namespaces will not work.</p>"},{"location":"intro-evolving/A01-SlurmIssues/","title":"Slurm issues on LUMI","text":"<p>Note: Use <code>sbatch --version</code> to check the version of Slurm.</p>"},{"location":"intro-evolving/A01-SlurmIssues/#wrong-allocations-on-small-g-when-requesting-1-cpu-per-task","title":"Wrong allocations on small-g when requesting 1 CPU per task","text":"<p>Observed on Slurm 22.05.8.</p> <p>When requesting a GPU allocation requesting only 1 CPU per task with <code>--cpus-per-task=1</code> and requesting GPUs with <code>--gpus-per-task</code>, we get invalid allocations at least when  the job has to span multiple nodes. The problems disappear as soon as a value larger than 1 is used for <code>--cpus-per-task</code>.</p> Sample job script showing the bug <p> <pre><code>#! /bin/bash\n#SBATCH --job-name=map-smallg-1gpt-error\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=small-g\n#SBATCH --ntasks=12\n#SBATCH --cpus-per-task=1\n#SBATCH --gpus-per-task=1\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\necho \"Requested resources as reported through SLURM_ variables:\"\necho \"- SLURM_NTASKS: $SLURM_NTASKS\"\necho \"- SLURM_CPUS_PER_TASK: $SLURM_CPUS_PER_TASK\"\necho \"- SLURM_GPUS_PER_TASK: $SLURM_GPUS_PER_TASK\"\necho \"Distribution based on SLURM_ variables:\"\necho \"- SLURM_JOB_NUM_NODES: $SLURM_JOB_NUM_NODES\"\necho \"- SLURM_JOB_NODELIST: $SLURM_JOB_NODELIST\"\necho \"- SLURM_TASKS_PER_NODE: $SLURM_TASKS_PER_NODE\"\necho \"- SLURM_JOB_CPUS_PER_NODE: $SLURM_JOB_CPUS_PER_NODE\"\necho\necho \"Control: All SLURM_ and SRUN_ variables:\"\nenv | egrep ^SLURM_\nenv | egrep ^SRUN_\necho\necho -e \"Control: Job script\\n\\n======================================================\"\ncat $0\necho -e \"======================================================\\n\"\n\nset -x\nsrun -n $SLURM_NTASKS -c $SLURM_CPUS_PER_TASK --gpus-per-task=$SLURM_GPUS_PER_TASK gpu_check -l\nset +x\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID echo_dev_$SLURM_JOB_ID\n</code></pre></p> <p>Workaround: Use a value larger than 1 for <code>--cpus-per-task</code>. A pure MPI application will not use the additional CPU cores, but since in an ideal case a CCD should not be used by more than 1 task unless the GPU is also shared by multiple tasks.</p>"},{"location":"intro-evolving/A02-Documentation/","title":"Documentation links","text":"<p>Note that documentation, and especially web based documentation, is very fluid. Links change rapidly and were correct when this page was developed right after the course. However, there is no guarantee that they are still correct when you read this and are only guaranteed to be updated at a course event.</p> <p>This documentation page is far from complete but bundles a lot of links mentioned during the presentations, and some more.</p>"},{"location":"intro-evolving/A02-Documentation/#web-documentation","title":"Web documentation","text":"<ul> <li> <p>Slurm version 23.02.7, on the system after the August-September 2024 update</p> </li> <li> <p>HPE Cray Programming Environment web documentation      contains a lot of HTML-processed man pages in an easier-to-browse     format than the man pages on the system.</p> <p>The presentations on debugging and profiling tools referred a lot to pages that can be found on this web site. The manual pages mentioned in those presentations are also in the web documentation and are the easiest way to access that documentation.</p> <ul> <li> <p>Specific documentation for CPE 24.03</p> </li> <li> <p>Version for the most recent HPE release (may not be on LUMI)</p> </li> </ul> </li> <li> <p>Cray PE Github account with whitepapers and some documentation.</p> </li> <li> <p>Cray DSMML - Distributed Symmetric Memory Management Library</p> </li> <li> <p>Cray Library previously provides as TPSL build instructions</p> </li> <li> <p>Clang latest version documentation (Usually for the latest version)</p> <ul> <li> <p>Clang 13.0.0 version      (basis for aocc/3.2.0)</p> </li> <li> <p>Clang 15.0.0 version      (cce/15.0.0 and cce/15.0.1 in 22.12/23.03)</p> </li> <li> <p>Clang 16.0.0 version      (cce/16.0.0 in 23.09 and aocc/4.1.0 in 23.12/24.03)</p> </li> <li> <p>Clang 17.0.1 version      (cce/17.0.0 in 23.12 and cce/17.0.1 in 24.03)</p> </li> </ul> </li> <li> <p>AMD Developer Information</p> <ul> <li> <p>AOCC 4.0 Compiler Options Quick Reference Guide      (Version 4.0 compilers will come when the 23.05 or later CPE release gets installed on LUMI)</p> </li> <li> <p>AOCC 4.0 User Guide</p> </li> </ul> </li> <li> <p>ROCm<sup>TM</sup> documentation overview</p> <ul> <li> <p>rocminfo application for reporting system info.</p> </li> <li> <p>rocm-smi</p> </li> <li> <p>HIP porting guide</p> </li> <li> <p>ROCm Software Platform GitHub repository</p> </li> <li> <p>Libraries:</p> <ul> <li> <p>BLAS: rocBLAS and hipBLAS</p> </li> <li> <p>FFTs: rocFFT and hipFFT</p> </li> <li> <p>Random number generation: rocRAND</p> </li> <li> <p>Sparse linear algebra: rocSPARSE and hipSPARSE</p> </li> <li> <p>Iterative solvers: rocALUTION</p> </li> <li> <p>Parallel primitives: rocPRIM and hipCUB</p> </li> <li> <p>Machine Learning Libraries: MIOpen (similar to cuDNN),      Tensile (GEMM Autotuner),     RCCL (ROCm analogue of NCCL) and      Horovod (Distributed ML)</p> </li> <li> <p>Machine Learning Frameworks: Tensorflow,     Pytorch and     Caffe</p> </li> <li> <p>Machine Learning Benchmarks:     DeepBench and      MLPerf</p> </li> </ul> </li> <li> <p>Development tools:</p> <ul> <li> <p>rocgdb resources:</p> <ul> <li> <p>AMD documentation</p> </li> <li> <p>2021 presentation by Justin Chang</p> </li> <li> <p>2021 Linux Plumbers Conference presentation     with youTube video with a part of the presentation</p> </li> </ul> </li> <li> <p>rocprof profiler</p> </li> <li> <p>OmniTrace</p> </li> <li> <p>Omniperf</p> </li> </ul> </li> </ul> </li> <li> <p>HDF5 generic documentation</p> </li> <li> <p>SingularityCE 4.1 User Guide</p> </li> </ul>"},{"location":"intro-evolving/A02-Documentation/#man-pages","title":"Man pages","text":"<p>A selection of man pages explicitly mentioned during the course:</p> <ul> <li> <p>Compilers</p> PrgEnv C C++ Fortran PrgEnv-cray <code>man craycc</code> <code>man crayCC</code> <code>man crayftn</code> PrgEnv-gnu <code>man gcc</code> <code>man g++</code> <code>man gfortran</code> PrgEnv-aocc/PrgEnv-amd - - - Compiler wrappers <code>man cc</code> <code>man CC</code> <code>man ftn</code> </li> <li> <p>Web-based versions of the compiler wrapper manual pages (the version on the system is currently hijacked by     the GNU manual pages):</p> <ul> <li> <p><code>man cc</code>     (or latest version)</p> </li> <li> <p><code>man CC</code>     (or latest version)</p> </li> <li> <p><code>man ftn</code>      (or latest version)</p> </li> </ul> </li> <li> <p>OpenMP in CCE</p> <ul> <li><code>man intro_openmp</code>     (or latest version)</li> </ul> </li> <li> <p>OpenACC in CCE</p> <ul> <li><code>man intro_openacc</code>     (or latest version)</li> </ul> </li> <li> <p>MPI:</p> <ul> <li> <p>MPI itself: <code>man intro_mpi</code> or <code>man mpi</code>     (or latest version)</p> </li> <li> <p>libfabric: <code>man fabric</code></p> </li> <li> <p>CXI: `man fi_cxi'</p> </li> </ul> </li> <li> <p>LibSci</p> <ul> <li> <p><code>man intro_libsci</code> and <code>man intro_libsci_acc</code></p> </li> <li> <p><code>man intro_blas1</code>,     <code>man intro_blas2</code>,     <code>man intro_blas3</code>,     <code>man intro_cblas</code></p> </li> <li> <p><code>man intro_lapack</code></p> </li> <li> <p><code>man intro_scalapack</code> and <code>man intro_blacs</code></p> </li> <li> <p><code>man intro_irt</code></p> </li> <li> <p><code>man intro_fftw3</code></p> </li> </ul> </li> <li> <p>DSMML - Distributed Symmetric Memory Management Library </p> <ul> <li><code>man intro_dsmml</code></li> </ul> </li> <li> <p>Slurm manual pages are also all on the web      and are easily found by Google, but are usually those for the latest version.     The links on this page are for the version on LUMI at the time of the course.</p> <ul> <li> <p><code>man sbatch</code></p> </li> <li> <p><code>man srun</code></p> </li> <li> <p><code>man salloc</code></p> </li> <li> <p><code>man squeue</code></p> </li> <li> <p><code>man scancel</code></p> </li> <li> <p><code>man sinfo</code></p> </li> <li> <p><code>man sstat</code></p> </li> <li> <p><code>man sacct</code></p> </li> <li> <p><code>man scontrol</code></p> </li> </ul> </li> </ul>"},{"location":"intro-evolving/A02-Documentation/#via-the-module-system","title":"Via the module system","text":"<p>Most HPE Cray PE modules contain links to further documentation. Try <code>module help cce</code> etc.</p>"},{"location":"intro-evolving/A02-Documentation/#from-the-commands-themselves","title":"From the commands themselves","text":"PrgEnv C C++ Fortran PrgEnv-cray <code>craycc --help</code><code>craycc --craype-help</code> <code>crayCC --help</code><code>crayCC --craype-help</code> <code>crayftn --help</code><code>crayftn --craype-help</code> PrgEnv-gnu <code>gcc --help</code> <code>g++ --help</code> <code>gfortran --help</code> PrgEnv-aocc <code>clang --help</code> <code>clang++ --help</code> <code>flang --help</code> PrgEnv-amd <code>amdclang --help</code> <code>amdclang++ --help</code> <code>amdflang --help</code> Compiler wrappers <code>cc --craype-help</code><code>cc --help</code> <code>CC --craype-help</code><code>CC --help</code> <code>ftn --craype-help</code><code>ftn --help</code> <p>For the PrgEnv-gnu compiler, the <code>--help</code> option only shows a little bit of help information, but mentions further options to get help about specific topics.</p> <p>Further commands that provide extensive help on the command line:</p> <ul> <li><code>rocm-smi --help</code>, even on the login nodes.</li> </ul>"},{"location":"intro-evolving/A02-Documentation/#documentation-of-other-cray-ex-systems","title":"Documentation of other Cray EX systems","text":"<p>Note that these systems may be configured differently, and this especially applies to the scheduler. So not all documentations of those systems applies to LUMI. Yet these web sites do contain a lot of useful information.</p> <ul> <li> <p>Archer2 documentation.      Archer2 is the national supercomputer of the UK, operated by EPCC. It is an AMD CPU-only cluster.     Two important differences with LUMI are that (a) the cluster uses AMD Rome CPUs with groups of 4 instead     of 8 cores sharing L3 cache and (b) the cluster uses Slingshot 10 instead of Slinshot 11 which has its     own bugs and workarounds.</p> <p>It includes a page on cray-python referred to during the course.</p> </li> <li> <p>ORNL Frontier User Guide and      ORNL Crusher Qucik-Start Guide.     Frontier is the first USA exascale cluster and is built up of nodes that are very similar to the     LUMI-G nodes (same CPA and GPUs but a different storage configuration) while Crusher is the     192-node early access system for Frontier. One important difference is the configuration of     the scheduler which has 1 core reserved in each CCD to have a more regular structure than LUMI.</p> </li> <li> <p>KTH Dardel documentation. Dardel is the Swedish \"baby-LUMI\" system.     Its CPU nodes use the AMD Rome CPU instead of AMD Milan, but its GPU nodes are the same as in LUMI.</p> </li> <li> <p>Setonix User Guide.     Setonix is a Cray EX system at Pawsey Supercomputing Centre in Australia. The CPU and GPU compute     nodes are the same as on LUMI.</p> </li> </ul>"},{"location":"intro-evolving/Demo1/","title":"Container demo 1","text":"<p>Last update of this page: February 27, 2025</p>"},{"location":"intro-evolving/Demo1/#container-demo-1-fooocus","title":"Container demo 1: Fooocus","text":"<p>Fooocus is an AI-based image generating package that is available under the GNU General Public License V3.</p> <p>The version on which we first prepared this demo, insists on writing in the directories with some of the Fooocus files, so we cannot put Fooocus in a container at the moment.</p> <p>It is based on PyTorch. However, we cannot use the containers provided on LUMI as-is as additional system level libraries are needed for the graphics.</p> <p>This demo shows:</p> <ul> <li> <p>Installing one of the containers provided on LUMI with EasyBuild,</p> </li> <li> <p>Installing additional software in the container with the     SingularityCE \"unprivileged proot builds\" process     and the SUSE Linux <code>zypper</code> install tool,</p> </li> <li> <p>Further adding packages in a virtual environment and putting them in      a SquashFS file for better file system performance, and</p> </li> <li> <p>Using that setup with Fooocus.</p> </li> </ul>"},{"location":"intro-evolving/Demo1/#video-of-the-demo","title":"Video of the demo","text":"<p>This is a video for a previous version of the demo though.</p>"},{"location":"intro-evolving/Demo1/#installing-fooocus","title":"Installing Fooocus","text":""},{"location":"intro-evolving/Demo1/#step-1-checking-fooocus","title":"Step 1: Checking Fooocus","text":"<p>Let's create an installation directory for the demo. Set the environment variable <code>installdir</code> to a proper value for the directories on LUMI that you have access to.</p> <pre><code>installdir=/scratch/project_465001726/$USER/DEMO1\nmkdir -p \"$installdir\" ; cd \"$installdir\"\n</code></pre> <p>We are now in the installation directory of which we also ensured its existence first. Let's now download and unpack Fooocus release 2.5.5 (the one we tested for this demo)</p> <pre><code>fooocusversion=2.5.5\nwget https://github.com/lllyasviel/Fooocus/archive/refs/tags/v$fooocusversion.zip\nunzip v$fooocusversion.zip\nrm -f v$fooocusversion.zip\n</code></pre> <p>If we check what's in the Fooocus directory:</p> <pre><code>ls Fooocus-$fooocusversion\n</code></pre> <p>we see a rather messy bunch of mostly Python files missing the traditional setup scripts that you  expect with a Python package. So installing this could become a messy thing...</p> <p>It also contains a <code>Dockerfile</code> (to build a base Docker container), a <code>requirements_docker.txt</code> and a <code>requirements_versions.txt</code> file that give hints about what exactly is needed.  The <code>Dockerfile</code> suggests close to the top that some OpenGL libraries will be needed. And the fact that it can be fully installed in a docker container also indicates that there must in fact be ways to run it in readonly directories, but in this demo we'll put Fooocus in a place were it can write. The <code>requirements_docker.txt</code> file also suggests to use Pytorch 2.1.0, but we'll take some risks though and use a newer version of PyTorch than suggested as for AMD GPUs it is often important to use recently enough versions (and because that version has a more sophisticated module better suited for what we want to demonstrate). </p>"},{"location":"intro-evolving/Demo1/#step-2-install-the-pytorch-container","title":"Step 2: Install the PyTorch container","text":"<p>We can find an overview of the available PyTorch containers on the PyTorch page in the LUMI Software Library. We'll use a version that already has support for Python virtual environments built in as that  will make it a lot easier to install extra Python packages. Moreover, as we have also seen that we will need to change the container, we'll follow a somewhat atypical build process.</p> <p>Rather than installing directly from the available EasyBuild recipes, we'll edit an EasyConfig to change the name to reflect that we have made changes and installed Fooocus with it. First we must prepare a temporary directory to do this work and also set up EasyBuild:</p> <pre><code>mkdir -p \"$installdir/tmp\" ; cd \"$installdir/tmp\"\nmodule purge\nmodule load LUMI/24.03 partition/container EasyBuild-user\n</code></pre> <p>We'll now use a function of EasyBuild to copy an existing EasyConfig file to a new location, and rename it in one move to reflect the module version that we want:</p> <pre><code>eb --copy-ec PyTorch-2.3.1-rocm-6.0.3-python-3.12-singularity-20240923.eb PyTorch-2.3.1-rocm-6.0.3-python-3.12-Fooocus-singularity-20240923.eb\n</code></pre> <p>This is not enough to generate a module <code>PyTorch/2.3.1-rocm-6.0.3-python-3.12-Fooocus-singularity-20240923</code>,  we also need to edit the <code>versionsuffix</code> line in the EasyBuild recipe. Of course you can do this easily with your favourite editor, but to avoid errors we'll use a command for the demo that you only need to copy:</p> <pre><code>sed -e \"s|^\\(versionsuffix.*\\)-singularity-\\(.*\\)|\\1-Fooocus-singularity-\\2|\" -i PyTorch-2.3.1-rocm-6.0.3-python-3.12-Fooocus-singularity-20240923.eb\n</code></pre> <p>Let's check:</p> <pre><code>grep versionsuffix PyTorch-2.3.1-rocm-6.0.3-python-3.12-Fooocus-singularity-20240923.eb\n</code></pre> <p>which returns</p> <pre><code>versionsuffix = f'-rocm-{local_c_rocm_version}-python-{local_c_python_mm}-Fooocus-singularity-{local_c_date}'\n</code></pre> <p>so we see that the <code>versionsuffix</code> line looks rather strange but we do see that the <code>-Fooocus-</code> part is injected in the name so we assume everything is OK.</p> <p>We're now ready to install the container with EasyBuild:</p> <pre><code>eb PyTorch-2.3.1-rocm-6.0.3-python-3.12-Fooocus-singularity-20240923.eb\n</code></pre> <p>The documentation in the PyTorch page in the LUMI Software Library suggests that we can now delete the container file in the installation directory, but this is a bad idea in this case as we want to build our own container and hence will not use one of the containers provided on the system while running.</p> <p>We're now finished with EasyBuild so don't need the modules related to EasyBuild anymore. So lets's clean the environment an load the PyTorch container module that we just built with EasyBuild:</p> <pre><code>module purge\nmodule load LUMI/24.03\nmodule load PyTorch/2.3.1-rocm-6.0.3-python-3.12-Fooocus-singularity-20240923\n</code></pre> <p>Notice that we don't need to load <code>partition/container</code> anymore. Any partition would do, and in fact, we can  even use <code>CrayEnv</code> instead of <code>LUMI/24.03</code>.</p> <p>Notice that the container module provides the environment variables <code>SIF</code> and <code>SIFPYTORCH</code>, both of which  point to the <code>.sif</code> file of the container:</p> <pre><code>echo $SIF\necho $SIFPYTORCH\n</code></pre> <p>We'll make use of that when we add SUSE packages to the container.</p>"},{"location":"intro-evolving/Demo1/#step-3-adding-some-suse-packages","title":"Step 3: Adding some SUSE packages","text":"<p>To update the singularity container, we need three things.</p> <p>First, the <code>PyTorch</code> module cannot be loaded as it sets a number of singularity-related  environment variables. Yet we want to use the value of <code>SIF</code>, so we will simply save it in a different environment variable before unloading the module:</p> <pre><code>export CONTAINERFILE=\"$SIF\"\nmodule unload PyTorch/2.3.1-rocm-6.0.3-python-3.12-Fooocus-singularity-20240923\n</code></pre> <p>Second, the <code>proot</code> command is not available by default on LUMI, but it can be enabled by loading the <code>systools</code> module in <code>LUMI/23.09</code> or newer stacks, or <code>systools/23.09</code> or newer in <code>CrayEnv</code>:</p> <pre><code>module load systools\n</code></pre> <p>Third, we need a file defining the build process for singularity. This is a bit technical and outside the scope of this tutorial to explain what goes into this file. It can be created with the following shell command:</p> <pre><code>cat &gt; lumi-pytorch-rocm-6.0.3-python-3.10-pytorch-v2.3.1-Fooocus.def &lt;&lt;EOF\n\nBootstrap: localimage\n\nFrom: $CONTAINERFILE\n\n%post\n\nzypper -n install -y Mesa libglvnd libgthread-2_0-0 hostname\n\nEOF\n</code></pre> <p>You can check the file with</p> <pre><code>cat lumi-pytorch-rocm-6.0.3-python-3.10-pytorch-v2.3.1-Fooocus.def\n</code></pre> <p>We basically install an OpenGL library that emulates on the CPU and some missing tools. Note that the AMD MI250X GPUs are not rendering GPUs, so we cannot run hardware accelerated rendering on them.</p> <p>An annoying element of the singularity build procedure is that it is not very friendly for a Lustre filesystem. We'll do the build process on a login node, where we have access to a personal RAM disk area that will also be cleaned automatically when we log out, which is always useful for a demo. Therefore we need to set two environment variables for Singularity, and create two directories, which is done with the following commands:</p> <pre><code>export SINGULARITY_CACHEDIR=/tmp/singularity/cache\nexport SINGULARITY_TMPDIR=/tmp/singularity/tmp\n\nmkdir -p $SINGULARITY_CACHEDIR\nmkdir -p $SINGULARITY_TMPDIR\n</code></pre> <p>Note that <code>$SINGULARITY_TMPDIR</code> can contain a lot of files during the <code>singularity build</code> process as the container will be unpacked in this area.</p> <p>Now we're ready to do the actual magic and rebuild the container with additional packages installed in it:</p> <pre><code>singularity build $CONTAINERFILE lumi-pytorch-rocm-6.0.3-python-3.10-pytorch-v2.3.1-Fooocus.def\n</code></pre> <p>The build process will ask you if you want to continue as it will overwrite the container file, so confirm with <code>y</code>. The whole build process may take a couple of minutes.</p> <p>We'll be kind to our fellow LUMI users and already clean up the directories that we just created:</p> <pre><code>rm -rf /tmp/singularity\n</code></pre> <p>Let's reload the container:</p> <pre><code>module load PyTorch/2.3.1-rocm-6.0.3-python-3.12-Fooocus-singularity-20240923\n</code></pre> <p>and do some checks:</p> <pre><code>singularity shell $SIF\n</code></pre> <p>brings us into the container (note that the command prompt has changed).</p> <p>The command</p> <pre><code>which python\n</code></pre> <p>returns</p> <pre><code>/user-software/venv/pytorch/bin/python\n</code></pre> <p>which shows that the virtual environment pre-installed in the container is indeed active.</p> <p>We do have the hostname command in the container (one of the packages mentioned in the container <code>.def</code> file that we created) as is easily tested:</p> <pre><code>hostname\n</code></pre> <p>and</p> <pre><code>ls /usr/lib64/*mesa*\n</code></pre> <p>shows that indeed a number of MESA libraries are installed (the OpenGL installation that we did).</p> <pre><code>pip list\n</code></pre> <p>shows that there are already a lot of Python packages in the container.</p> <p>We can now leave the container with the </p> <pre><code>exit\n</code></pre> <p>command (or CTRL-D key combination).</p> <p>So it looks we are ready to start installing Python packages...</p>"},{"location":"intro-evolving/Demo1/#step-4-adding-python-packages","title":"Step 4: Adding Python packages","text":"<p>To install the packages, we'll base ourselves on the <code>requirements_versions.txt</code> file which we found in the Fooocus directories. We'll omit packages that have close enough versions and are already in the container. The installation has to happen from within the container though. So let's got to the Fooocus directory and go into the container again:</p> <p>First we copy the <code>requirements_versions.txt</code> file from the Fooocus directory and  edit it a bit:</p> <pre><code>cd $installdir\ncp Fooocus-$fooocusversion/requirements_versions.txt requirements.txt\n</code></pre> <p>and edit it to</p> <pre><code>torchsde==0.2.6\ntransformers==4.42.4\nomegaconf==2.3.0\ngradio==3.41.2\npygit2==1.15.1\nopencv-contrib-python-headless==4.10.0.84\nhttpx==0.27.0\nonnxruntime==1.18.1\ntimm==1.0.7\nrembg==2.0.57\ngroundingdino-py==0.4.0\nsegment_anything==1.0\n</code></pre> <p>(i.e., delete the lines for <code>einops</code>,  <code>safetensors</code>, <code>accelerate</code>, <code>pyyamml</code>, <code>pillow</code>, <code>scipy</code>, <code>tqdm</code>, <code>psutil</code>, <code>pytorch_lightning</code>, <code>numpy</code>, <code>tokenizers</code> and <code>packaging</code>). Even though there is already a version of <code>transformers</code> in the package, it is too old and Fooocus is rather picky so we leave that line in.</p> <p>Alternatively, you can simply create the file by copying the following command:</p> <pre><code>cat &gt; requirements.txt &lt;&lt;EOF\ntorchsde==0.2.6\ntransformers==4.42.4\nomegaconf==2.3.0\ngradio==3.41.2\npygit2==1.15.1\nopencv-contrib-python-headless==4.10.0.84\nhttpx==0.27.0\nonnxruntime==1.18.1\ntimm==1.0.7\nrembg==2.0.57\ngroundingdino-py==0.4.0\nsegment_anything==1.0\nEOF\n</code></pre> <p>Now go back into the container:</p> <pre><code>singularity shell $SIF\n</code></pre> <p>We'll install the extra packages simply with the <code>pip</code> tool:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>This process may again take a few minutes.</p> <p>An error message suggests that there are some problems with the container that we used which are solved with</p> <pre><code>pip install lightning-utilities torchmetrics\n</code></pre> <p>After finishing,</p> <pre><code>ls /user-software/venv/pytorch/lib/python3.12/site-packages/\n</code></pre> <p>shows that indeed a lot of packages have been installed. Though accessible from the container, they are not in the container <code>.sif</code> file as that file cannot be written.</p> <p>Let's leave the container again:</p> <pre><code>exit\n</code></pre> <p>Now try:</p> <pre><code>ls $CONTAINERROOT/user-software/venv/pytorch/lib/python3.12/site-packages/\n</code></pre> <p>and notice that we see the same long list of packages. In fact, a trick to see the number of files  and directories is</p> <pre><code>lfs find $CONTAINERROOT/user-software/venv/pytorch/lib/python3.12/site-packages | wc -l\n</code></pre> <p>which prints the name of all files and directories and then counts the number of lines, and we see that  this is a considerable number. Lustre isn't really that fond of it. However, the module also provides an easy solution: We can convert the <code>$EBROOTPYTORCH/user-software</code> subdirectory into a SquashFS file that can be mounted as a filesystem in the container, and the module provides all the tools to make this easy to do. All we need to do is to run</p> <pre><code>make-squashfs\n</code></pre> <p>This will also take some time as the script limits the resources the <code>make-squashfs</code> can use to keep the  load on the login nodes low. Now we can then safely remove the <code>user-software</code> subdirectory:</p> <pre><code>rm -rf $CONTAINERROOT/user-software\n</code></pre> <p>Before continuing, we do need to reload the module so that the bindings between the container and files and directories on LUMI are reset:</p> <pre><code>module load PyTorch/2.3.1-rocm-6.0.3-python-3.12-Fooocus-singularity-20240923\n</code></pre> <p>Just check</p> <pre><code>singularity exec $SIF ls /user-software/venv/pytorch/lib/python3.12/site-packages\n</code></pre> <p>and see that our package installation is still there!</p> <p>However, we can no longer write in that directory. E.g., try</p> <pre><code>singularity exec $SIF touch /user-software/test\n</code></pre> <p>to create an empty file <code>test</code> in <code>/user-software</code> and note that we get an error message.</p> <p>So now we are ready-to-run.</p>"},{"location":"intro-evolving/Demo1/#the-reward-running-fooocus","title":"The reward: Running Fooocus","text":"<p>First go into the directory containing the Fooocus package :</p> <pre><code>cd \"$installdir/Fooocus-$fooocusversion\"\n</code></pre> <p>We'll start an interactive job with a single GPU:</p> <pre><code>srun -psmall-g -n1 -c7 --time=30:00 --gpus=1 --mem=60G -A project_465001726 --pty bash\n</code></pre> <p>The necessary modules will still be available, but if you are running from a new shell, you  can load them again:</p> <pre><code>module load LUMI/24.03\nmodule load PyTorch/2.3.1-rocm-6.0.3-python-3.12-Fooocus-singularity-20240923\n</code></pre> <p>Also check the hostname if it is not part of your prompt as you will need it later:</p> <pre><code>hostname\n</code></pre> <p>We can now go into the container:</p> <pre><code>singularity shell $SIF\n</code></pre> <p>and launch Fooocus:</p> <pre><code>python launch.py --listen --disable-xformers\n</code></pre> <p>Fooocus provides a web interface. If you're the only one on the node using Fooocus, it should run on port 7865. To access it from our laptop, we need to create an SSH tunnel to LUMI. The precise statement needed for this will depend on your ssh implementation. With an OpenSSH-like client, you can use:</p> <pre><code>ssh -N -L 7865:nid00XXXX:7865 &lt;myusername&gt;@lumi.csc.fi\n</code></pre> <p>replacing with the node name that we got from the <code>hostname</code> command<code>,</code>` with your username on LUMI and adding command line options to point to your key if needed. <p>Next, simply open a web browser on your laptop and point to </p> <pre><code>http://localhost:7865\n</code></pre> <p>Note though that the initialisation of Fooocus can take a while and you cannot connect to it as long as a line similar to</p> <pre><code>App started successful. Use the app with http://localhost:7865/ or 0.0.0.0:7865\n</code></pre> <p>is shown.</p>"},{"location":"intro-evolving/Demo1/#alternative-way-of-running","title":"Alternative way of running","text":"<p>We can also launch Fooocus directly from the <code>srun</code> command, e.g., from the directory containing the  Fooocus code,</p> <pre><code>module load LUMI/24.03\nmodule load PyTorch/2.3.1-rocm-6.0.3-python-3.12-Fooocus-singularity-20240923\nsrun -psmall-g -n1 -c7 --time=30:00 --gpus=1 --mem=60G -A project_465001726 --pty \\\n   bash -c 'echo -e \"Running on $(hostname)\\n\" ; singularity exec $SIF python launch.py --listen --disable-xformers'\n</code></pre> <p>It will also print the host name on which the Fooocus is running, so you can connect to Fooocus using the same procedure as above.</p>"},{"location":"intro-evolving/Demo1/#further-discovery","title":"Further discovery","text":"<ul> <li>YouTube channel \"Jump Into AI\"     has a Fooocus playlist</li> </ul>"},{"location":"intro-evolving/Demo2/","title":"Container demo 2","text":"<p>Last update of this page: February 27, 2025</p>"},{"location":"intro-evolving/Demo2/#container-demo-2-a-short-walk-through-for-distributed-learning","title":"Container demo 2: A short walk-through for distributed learning","text":"<p>In this demo, we will install one of the PyTorch containers provided on LUMI and run a simple distributed learning example that the LUMI User Support Team also uses for internal testing.</p> <p>The demo follows largely the instructions for distributed learning from the PyTorch page in the LUMI Software Library.</p> <p>This demo shows:</p> <ul> <li> <p>How to install one of the containers for which we provide EasyBuild recipes</p> </li> <li> <p>How to use our more recent PyTorch containers for distributed learning</p> </li> </ul>"},{"location":"intro-evolving/Demo2/#video-of-the-demo","title":"Video of the demo","text":"<p>This is a video for a previous version of the demo though.</p>"},{"location":"intro-evolving/Demo2/#step-1-getting-some-files-that-we-will-use","title":"Step 1: Getting some files that we will use","text":"<p>Let's create an installation directory for the demo. Set the environment variable <code>installdir</code> to a proper value for the directories on LUMI that you have access to.</p> <pre><code>installdir=/scratch/project_465001726/$USER/DEMO2\nmkdir -p \"$installdir\" ; cd \"$installdir\"\n</code></pre> <p>We are now in the installation directory of which we also ensured its existence first. Let's now download some files that we will use:</p> <pre><code>mkdir mnist ; pushd mnist\nwget https://raw.githubusercontent.com/Lumi-supercomputer/lumi-reframe-tests/98327968ff300ed0181d5d14b5dd49cdf1d7b743/checks/containers/ML_containers/src/pytorch/mnist/mnist_DDP.py\nsed -i -e 's|download=True|download=False|' mnist_DDP.py\nmkdir -p model ; cd model\nwget https://github.com/Lumi-supercomputer/lumi-reframe-tests/raw/98327968ff300ed0181d5d14b5dd49cdf1d7b743/checks/containers/ML_containers/src/pytorch/mnist/model/model_gpu.dat\ncd ..\n</code></pre> <p>The first two files are actually files that were developed for testing some PyTorch containers  on LUMI after system upgrades.</p> <p>The demo also uses a popular dataset (one of the MNIST datasets) from  Yann LeCun, a data scientist at Meta. The pointers to the dataset are actually included in the <code>torchvision</code> package which is why it is not easy to track where the data comes from.  The script that we use will download the data if it is not present, but does so on each process, leading to a high load on the web server providing the data and throttling after a few tries, so we will prepare the data instead in the <code>$installdir</code> subdirectory:</p> <pre><code>mkdir -p data/MNIST/raw\npushd data/MNIST/raw\nwget https://github.com/golbin/TensorFlow-MNIST/raw/refs/heads/master/mnist/data/train-images-idx3-ubyte.gz\nwget https://github.com/golbin/TensorFlow-MNIST/raw/refs/heads/master/mnist/data/train-labels-idx1-ubyte.gz\nwget https://github.com/golbin/TensorFlow-MNIST/raw/refs/heads/master/mnist/data/t10k-images-idx3-ubyte.gz\nwget https://github.com/golbin/TensorFlow-MNIST/raw/refs/heads/master/mnist/data/t10k-labels-idx1-ubyte.gz    \ngunzip -k *.gz\npopd\nfor i in $(seq 0 31); do ln -s data \"data$i\"; done\n</code></pre>"},{"location":"intro-evolving/Demo2/#step-2-installing-the-container","title":"Step 2: Installing the container","text":"<p>We can find an overview of the available PyTorch containers on the PyTorch page in the LUMI Software Library. We'll use a version that already has support for Python virtual environments built in as that  will make it a lot easier to install extra Python packages. </p> <p>First we need to load and configure EasyBuild and make sure that EasyBuild can run in a clean environment:</p> <pre><code>module purge\nmodule load LUMI/24.03 partition/container EasyBuild-user\n</code></pre> <p>The <code>partition/container</code> is a \"special\" partition whose main purpose is to tell EasyBuild-user (and other modules that we use to install software on the system) to configure EasyBuild to install container modules. Afterwards, these containers are available in any partition of the <code>LUMI</code> stacks and in the <code>CrayEnv</code> stack.  The <code>EasyBuild-user</code> module here is responsible of configuring EasyBuild and also ensures that a proper version of EasyBuild is loaded.</p> <p>After loading <code>EasyBuild-user</code>, installing the container from the EasyBuild recipe is very easy:</p> <pre><code>eb PyTorch-2.3.1-rocm-6.0.3-python-3.12-singularity-20240923.eb\n</code></pre> <p>We're now finished with EasyBuild so don't need the modules related to EasyBuild anymore. So lets's clean the environment an load the PyTorch container module that we just built with EasyBuild:</p> <pre><code>module purge\nmodule load LUMI/24.03\nmodule load PyTorch/2.3.1-rocm-6.0.3-python-3.12-singularity-20240923\n</code></pre> <p>Note that the module defines two environment variables that point to the <code>.sif</code> file of the container:</p> <pre><code>echo $SIF\necho $SIFPYTORCH\n</code></pre> <p>All our container modules provide the <code>SIF</code> environment variable, but the name of the second one depends on the name of the package, and it may be safer to use should you load multiple container modules of different packages to quickly switch between them.</p> <p>If you're really concerned about disk space...</p> <p>... you may chose to delete the version of the container that we have installed. To continue, you then need to reload the <code>PyTorch</code> module:</p> <pre><code>rm -f $SIF\nmodule load PyTorch/2.3.1-rocm-6.0.3-python-3.12-singularity-20240923\n</code></pre> <p>Now check again the <code>SIF</code> and <code>SIFPYTORCH</code> environment variables and note that they now point to files in <code>/appl/local/containers</code>:</p> <pre><code>echo $SIF\necho $SIFPYTORCH\n</code></pre> <p>We do not recommend you remove the container file as your module will stop working if the image is removed from <code>/appl/local/containers</code> which we do when we deem the file not useful anymore as it causes trouble for too many users. But it may still work fine for what you do with it...</p> <p>All containers with module files also define the environment variable <code>CONTAINERROOT</code>, pointing to the  directory in which EasyBuild installs the <code>.sif</code> file (and not pointing to <code>/appl/local/containers</code> if you've removed the container <code>.sif</code> file). The standard EasyBuild variable <code>EBROOTPYTORCH</code> is also defined and serves the same purpose, but of course has a different name for other packages.</p> <p>Let's do some checks:</p> <pre><code>singularity shell $SIF\n</code></pre> <p>brings us into the container (note that the command prompt has changed).</p> <p>The command</p> <pre><code>which python\n</code></pre> <p>returns</p> <pre><code>/user-software/venv/pytorch/bin/python\n</code></pre> <p>which shows that the virtual environment pre-installed in the container is indeed active.</p> <p>Let's leave the container again:</p> <pre><code>exit\n</code></pre> <p>and check the <code>$CONTAINERROOT</code> directory:</p> <pre><code>module load systools\ntree $CONTAINERROOT\n</code></pre> <p>There is a lot of stuff in there. If we scroll up enough, we see:</p> <ul> <li> <p>A subdirectory <code>easybuild</code> which among other things turns out to contain copies     of the EasyBuild recipe that we used. This directory basically contains all important     files to reproduce the installation, except for the container it used itself.</p> </li> <li> <p>The <code>user-software</code> subdirectory contains all the files that can     be found in the container also in <code>/user-software</code>. (It is simply bound to      that directory in the container through an environmet variable that the module sets.) </p> </li> <li> <p>There is a <code>bin</code> subdirectory with some scripts. The <code>start-shell</code> script is only there     for historical reasons and compatibility with some other containers, but the      <code>make-squashfs</code> and <code>unmake-squashfs</code> files are useful and can be used to make the Python     virtual environment more filesystem-friendly by converting the <code>user-software</code> subdirectory     into a SquashFS file which is then mounted in the container.</p> </li> <li> <p>The <code>runscripts</code> subdirectory contains some scripts that we will use to simplify     running the container. The scripts by no means cover all use cases, but they are nice     examples about how scripts for your specific tasks could be written.     This directory is also mounted in the container as <code>/runscripts</code> so that it is     easy to access.</p> </li> </ul>"},{"location":"intro-evolving/Demo2/#step-3-running-a-distributed-learning-example","title":"Step 3: Running a distributed learning example.","text":"<p>The <code>conda-python-distributed</code> script is written to ease distributed learning with PyTorch. Distributed learning requires some initialisation of environment variables that are used by PyTorch or by libraries from the ROCm<sup>TM</sup> stack. It passes its arguments to the Python command. It is mostly meant to be used on full nodes with one task per GPU, as in  other cases not all initialisations make sense or are even valid.</p> <p>Let's check the script:</p> <pre><code>cat $CONTAINERROOT/runscripts/conda-python-distributed\n</code></pre> <p>The first block,</p> <pre><code>if [ $SLURM_LOCALID -eq 0 ] ; then\n    rocm-smi\nfi\nsleep 2\n</code></pre> <p>has mostly a debugging purpose. One task per node will run <code>rocm-smi</code> on that node and its output can be used to check if all GPUs are available as expected. The <code>sleep</code> command is there because we have experienced that sometimes there is still stuff going on in the background that may prevent later commands to fail.</p> <p>The next block does some very needed initialisations for the MIOpen cache, an important library for neural networks, as the default location causes problems on LUMI as Lustre locking is not  compatible with MIOpen:</p> <pre><code>export MIOPEN_USER_DB_PATH=\"/tmp/$(whoami)-miopen-cache-$SLURM_NODEID\"\nexport MIOPEN_CUSTOM_CACHE_DIR=$MIOPEN_USER_DB_PATH\n\n# Set MIOpen cache to a temporary folder.\nif [ $SLURM_LOCALID -eq 0 ] ; then\n    rm -rf $MIOPEN_USER_DB_PATH\n    mkdir -p $MIOPEN_USER_DB_PATH\nfi\nsleep 2\n</code></pre> <p>These commands basically move the cache to a subdirectory of <code>/tmp</code>.</p> <p>Next we need to tell RCCL, the communication library, which interfaces it should use as otherwise it may try to communicate over the management network of LUMI which does not work. This is done through some <code>NCCL_*</code> environment variables which may be counterintuitive, but RCCL is basically the equivalent of NVIDIA NCCL.</p> <pre><code>export NCCL_SOCKET_IFNAME=hsn0,hsn1,hsn2,hsn3\nexport NCCL_NET_GDR_LEVEL=3\n</code></pre> <p>Note that in the future, from ROCm 6.2 on, the second environment variable isn't strictly needed anymore as this value will be the default. And it turns out that in current versions or ROCm, the first line can also be simplified to </p> <pre><code>export NCCL_SOCKET_IFNAME=hsn\n</code></pre> <p>Fourth, we need to ensure that each task uses the proper GPU. This is one point where we  assume that one GPU (GCD) per task is used. The script also assumes that the \"Linear assignment of GCD, then match the cores\" idea is used, so we will need some more complicated CPU mapping in the job script.</p> <p>PyTorch also needs some initialisation that are basically the same on NVIDIA and AMD hardware. This includes setting a master for the communication (the first node of  a job) and a port for the communication. That port is hard-coded, so a second instance of the script on the same node would fail. So we basically assume that we use full nodes. To determine that master, another script from the <code>runscripts</code> subdirectory is used.</p> <pre><code>export MASTER_ADDR=$(/runscripts/get-master \"$SLURM_NODELIST\")\nexport MASTER_PORT=29500\nexport WORLD_SIZE=$SLURM_NPROCS\nexport RANK=$SLURM_PROCID\n</code></pre> <p>Now we can turn our attention to the job script. Create a script <code>mnist.slurm</code> in the demo directory <code>$installdir</code> by copying the code below:</p> <pre><code>#!/bin/bash -e\n#SBATCH --nodes=4\n#SBATCH --gpus-per-node=8\n#SBATCH --output=\"output_%x_%j.txt\"\n#SBATCH --partition=standard-g\n#SBATCH --mem=480G\n#SBATCH --time=5:00\n\nmodule load LUMI/24.03\nmodule load PyTorch/2.3.1-rocm-6.0.3-python-3.12-singularity-20240923\n\nc=fe\nMYMASKS=\"0x${c}000000000000,0x${c}00000000000000,0x${c}0000,0x${c}000000,0x${c},0x${c}00,0x${c}00000000,0x${c}0000000000\"\n\ncd mnist\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=mask_cpu:$MYMASKS \\\n  singularity exec $SIFPYTORCH \\\n    conda-python-distributed -u mnist_DDP.py --gpu --modelpath model\n</code></pre> <p>Launch the script by setting some environment variables to use the course account and reservation:</p> <pre><code>export SBATCH_ACCOUNT=project_465001726\nexport SBATCH_RESERVATION=TODO\n</code></pre> <p>Modify the <code>SBATCH_ACCOUNT</code> if the account project is no longer available, and omit <code>SBATCH_RESERVATION</code> outside the course days, when there is no reservation.</p> <p>Next launch the job script:</p> <pre><code>sbatch mnist.slurm\n</code></pre> <p>When the job script ends (which is usually fast once it gets the resources to run), the output can be found in <code>output_mnist.slurm_1234567.txt</code> where you need to replace <code>1234567</code> with the actual job id.</p>"},{"location":"intro-evolving/E02-CPE/","title":"Exercises: HPE Cray Programming Environment","text":"<p>See the instructions to set up for the exercises. For these exercises, you'll need the files in the <code>CPE</code> subdirectory.</p> <p>These exercises are optional during the session, but useful if you expect  to be compiling software yourself. The source files mentioned can be found in the subdirectory CPE of the download.</p>"},{"location":"intro-evolving/E02-CPE/#compilation-of-a-program-1-a-simple-hello-world-program","title":"Compilation of a program 1: A simple \"Hello, world\" program","text":"<p>Four different implementations of a simple \"Hello, World!\" program are provided in the <code>CPE</code> subdirectory:</p> <ul> <li><code>hello_world.c</code> is an implementation in C,</li> <li><code>hello_world.cc</code> is an implementation in C++,</li> <li><code>hello_world.f</code> is an implementation in Fortran using the fixed format source form,</li> <li><code>hello_world.f90</code> is an implementation in Fortran using the more modern free format source form.</li> </ul> <p>Try to compile these programs using the programming environment of your choice.</p> Click to see the solution. <p>We'll use the default version of the programming environment (24.03 at the moment of the last update of this page in November 2024), but in case you want to use a particular version, e.g., the 23.12 version, and want to be very sure that all modules are loaded correctly from the start you could consider using</p> <pre><code>module load cpe/23.12\nmodule load cpe/23.12\n</code></pre> <p>(but don't try this now or undo again by loading <code>cpe/24.03</code> twice or logging in again).</p> <p>So note that we do twice the same command as the first iteration does not always succeed to reload all modules in the correct version. Do not combine both lines into a single <code>module load</code> statement as that would again trigger the bug that prevents all modules to be reloaded in the first iteration.</p> <p>The sample programs that we asked you to compile do not use the GPU. So there are three programming environments that we can use: <code>PrgEnv-gnu</code>, <code>PrgEnv-cray</code> and <code>PrgEnv-aocc</code>. All three will work, and they work almost the same.</p> <p>Let's start with an easy case, compiling the C version of the program with the GNU C compiler. For this all we need to do is</p> <pre><code>module load PrgEnv-gnu\ncc hello_world.c\n</code></pre> <p>which will generate an executable named <code>a.out</code>. </p> <p>Of course it is better to give the executable a proper name which can be done with the <code>-o</code> compiler option:</p> <pre><code>module load PrgEnv-gnu\ncc hello_world.c -o hello_world.x\n</code></pre> <p>Try running this program:</p> <pre><code>./hello_world.x\n</code></pre> <p>to see that it indeed works. We did forget another important compiler option, but we'll discover that in the next exercise.</p> <p>The other programs are equally easy to compile using the compiler wrappers:</p> <pre><code>CC hello_world.cc -o hello_world.x\nftn hello_world.f -o hello_world.x\nftn hello_world.f90 -o hello_world.x\n</code></pre>"},{"location":"intro-evolving/E02-CPE/#compilation-of-a-program-2-a-program-with-blas","title":"Compilation of a program 2: A program with BLAS","text":"<p>In the <code>CPE</code> subdirectory you'll find the C program <code>matrix_mult_C.c</code> and the Fortran program <code>matrix_mult_F.f90</code>. Both do the same thing: a matrix-matrix multiplication using the 6 different orders of the three nested loops involved in doing a matrix-matrix multiplication, and a call to the BLAS routine DGEMM that does the same for comparison.</p> <p>Compile either of these programs using the Cray LibSci library for the BLAS routine. Do not use OpenMP shared memory parallelisation. The code does not use MPI.</p> <p>The resulting executable takes one command line argument, the size of the square matrix. Run the script using <code>1000</code> for the matrix size and see what happens.</p> <p>Note that the time results may be very unreliable as we are currently doing this on the login nodes. In the session of Slurm you'll learn how to request compute nodes and it might be interesting to redo this on a compute node with a larger matrix size as the with a matrix size of 1000 all data may stay in the third level cache and you will not notice the differences that you should note. Also, because these nodes are shared with a lot of people, any benchmarking is completely unreliable.</p> <p>If you're doing things right, the time reported for the <code>ijk</code>-variant should be well under 3 seconds for both the C and Fortran versions... It is not a shame at all if you can't find the solution without looking into the solution. Only people with experience with compilers will likely be able to make this exercise only relying on the course materials and the compiler documentation, so don't search too long.</p> Click to see the solution. <p>Just as in the previous exercise, this is a pure CPU program so we can chose between the same three programming environments.</p> <p>The one additional \"difficulty\" is that we need to link with the BLAS library. This is very easy however in  the HPE Cray PE if you use the compiler wrappers rather than calling the compilers yourself: you only need to make sure that the <code>cray-libsci</code> module is loaded and the wrappers will take care of the rest. And on most systems (including LUMI) this module will be loaded automatically when you load the <code>PrgEnv-*</code> module.</p> <p>To compile with the GNU C compiler, all you need to do is</p> <pre><code>module load PrgEnv-gnu\ncc -O3 matrix_mult_C.c -o matrix_mult_C_gnu.x\n</code></pre> <p>will generate the executable <code>matrix_mult_C_gnu.x</code>.</p> <p>Note that we add the <code>-O3</code> option and it is very important to add either <code>-O2</code> or <code>-O3</code> as by default the GNU compiler will generate code without any optimization for debugging purposes, and that code is in this case easily four times or more slower. So if you got much longer run times than indicated this is likely the mistake that you made.</p> <p>To use the Cray C compiler instead only one small change is needed: Loading a different programming  environment module:</p> <pre><code>module load PrgEnv-cray\ncc -O3 matrix_mult_C.c -o matrix_mult_C_cray.x\n</code></pre> <p>will generate the executable <code>matrix_mult_C_cray.x</code>.</p> <p>Likewise for the AMD AOCC compiler we can try with loading yet another <code>PrgEnv-*</code> module:</p> <pre><code>module load PrgEnv-aocc\ncc -O3 matrix_mult_C.c -o matrix_mult_C_aocc.x\n</code></pre> <p>but it turns out that this fails with linker error messages about not being able to find the <code>sin</code> and <code>cos</code> functions. When using the AOCC compiler the <code>libm</code> library with basic math functions is not linked automatically, but this is easily done by adding the <code>-lm</code> flag:</p> <pre><code>module load PrgEnv-aocc\ncc -O3 matrix_mult_C.c -lm -o matrix_mult_C_aocc.x\n</code></pre> <p>For the Fortran version of the program we have to use the <code>ftn</code> compiler wrapper instead, and the issue with the math libraries in the AOCC compiler does not occur. So we get</p> <pre><code>module load PrgEnv-gnu\nftn -O3 matrix_mult_F.f90 -o matrix_mult_F_gnu.x\n</code></pre> <p>for the GNU Fortran compiler,</p> <pre><code>module load PrgEnv-cray\nftn -O3 matrix_mult_F.f90 -o matrix_mult_F_cray.x\n</code></pre> <p>for the Cray Fortran compiler and</p> <pre><code>module load PrgEnv-aocc\nftn -O3 matrix_mult_F.f90 -o matrix_mult_F_aocc.x\n</code></pre> <p>for the AMD Fortran compiler.</p> <p>When running the program you will see that even though the 6 different loop orderings  produce the same result, the time needed to compute the matrix-matrix product is very different and those differences would be even more pronounced with bigger matrices (which you can do after the session on using Slurm).</p> <p>The exercise also shows that not all codes are equal even if they produce a result of the same quality. The six different loop orderings run at very different speed, and none of our simple implementations can beat a good library, in this case the BLAS library included in LibSci. The optimal variants are also different for C and Fortran as the matrix elements are stored in a different order in memory.</p> <p>The results with the Cray Fortran compiler are particularly interesting. The result for the BLAS library is slower which we do not yet understand, but it also turns out that  for four of the six loop orderings we get the same result as with the BLAS library DGEMM routine. It looks like the compiler simply recognized that this was code for a matrix-matrix multiplication and replaced it with a call to the BLAS library. The Fortran 90 matrix multiplication is also replaced by a call of the DGEMM routine. To confirm all this, unload the <code>cray-libsci</code> module and try to compile again and you will see five error messages about not being able to find DGEMM. If you don't want the Cray Fortran compiler to recognise patterns that it can replace with library routines, add the <code>-h nopattern</code> flag to the command line:</p> <pre><code>module load PrgEnv-cray\nftn -O3 -h nopattern matrix_mult_F.f90 -o matrix_mult_F_cray.x\n</code></pre>"},{"location":"intro-evolving/E02-CPE/#compilation-of-a-program-3-a-hybrid-mpiopenmp-program","title":"Compilation of a program 3: A hybrid MPI/OpenMP program","text":"<p>The file <code>mpi_omp_hello.c</code> is a hybrid MPI and OpenMP C program that sends a message from each thread in each MPI rank. It is basically a simplified version of the programs found in the <code>lumi-CPEtools</code> modules that can be used to quickly check  the core assignment in a hybrid MPI and OpenMP job (see later in this tutorial). It is again just a CPU-based program.</p> <p>Compile the program with your favourite C compiler on LUMI.</p> <p>We have not yet seen how to start an MPI program. However, you can run the executable on the login nodes and it will then contain just a single MPI rank. </p> Click to see the solution. <p>In the HPE Cray PE environment, you don't use <code>mpicc</code> to compile a C MPI program, but you just use the <code>cc</code> wrapper as for any other C program. To enable MPI you  have to make sure that the <code>cray-mpich</code> module is loaded. This module will usually be loaded by loading one of the <code>PrgEnv-*</code> modules, but only if the right network target module, which is <code>craype-network-ofi</code>, is also already loaded. </p> <p>Compiling the program is very simple:</p> <pre><code>module load PrgEnv-gnu\ncc -O3 -fopenmp mpi_omp_hello.c -o mpi_omp_hello_gnu.x\n</code></pre> <p>to compile with the GNU C compiler, </p> <pre><code>module load PrgEnv-cray\ncc -O3 -fopenmp mpi_omp_hello.c -o mpi_omp_hello_cray.x\n</code></pre> <p>to compile with the Cray C compiler, and</p> <pre><code>module load PrgEnv-aocc\ncc -O3 -fopenmp mpi_omp_hello.c -o mpi_omp_hello_aocc.x\n</code></pre> <p>to compile with the AMD AOCC compiler.</p> <p>To run the executables it is not even needed to have the respective <code>PrgEnv-*</code> module loaded since the binaries will use a copy of the libraries stored in a default directory, though there have been bugs in the past preventing this to work with <code>PrgEnv-aocc</code>.</p> What about the <code>mpicc</code>, etc., compiler wrappers? <p>The usual MPI compiler wrappers like <code>mpicc</code>, <code>mpicxx</code> or <code>mpifort</code> are provided also. Though they will take care of linking with the MPI libraries properly, they do not take care of linking with the BLAS libraries and some other HPE CPE libraries that are linked in automatically when using the CPE wrappers.</p> <p>Moreover, when using the wrappers in the <code>PrgEnv-gnu</code> version of Cray MPICH,  they don't understand the right names for the GCC compiler executables based on which GCC module is loaded. As a result, when loading one of the <code>gcc-native</code> modules which are now the standard GCC modules in the HPE Cray PE, they will run the system  compiler instead, and you may get errors when trying to link or run with other libraries that are properly compiled with the much newer GCC versions in the <code>gcc-native</code> modules.</p> <p>Just try:</p> <pre><code>module load PrgEnv-gnu\nmpicc --version\n</code></pre> <p>to see this issue when using a recent version of the programming environment (such as  the 24.03 default version).</p> <p>A workaround is to point the environment variable <code>MPICH_CC</code> to the correct GCC C compiler executable. If you're making this exercise in the 24.03 version of the CPE (the default  version when this exercise was last revised), you can use:</p> <pre><code>module load PrgEnv-gnu\nexport MPICH_CC=gcc-13\nmpicc --version\n</code></pre> <p>Similarly, for C++ and Fortran, you'd have to point <code>MPICH_CXX</code> and <code>MPICH_FC</code> to the correct versions of <code>g++</code> and <code>gfortran</code>.</p>"},{"location":"intro-evolving/E03-Access/","title":"Exercises: Accessing LUMI","text":"<p>See the instructions to set up for the exercises.</p> <ol> <li> <p>Log on to an arbitrary login node of LUMI.</p> <p>Can you find how to check your quota and status of your allocation?</p> Click to see the solution. <p>How to check your quota and status of your allocation, is explained in the message-of-the-day at the bottom of the \"Announcements\" section: you can use the <code>lumi-workspaces</code> command.</p> <p>The <code>lumi-workspaces</code> command uses Lustre commands to check the quota in real-time. It will however get stuck if one of the Lustre filesystems used by your project or user account, is unavailable. In that case we still have the <code>lumi-ldap-userinfo</code> and <code>lumi-ldap-projectinfo</code> commands. Don't try to understand all output of those commands. They are meant to be used by the  LUMI User Support Team in the first place. However, it does show the last stored quota. But it can take a while after, e.g., deleting files before that shows up in this data (typically up to one hour, but longer in case of technical problems on the system which you may not always notice).</p> </li> <li> <p>How can you log on to a specific login node of LUMI, e.g., the login node \"uan01\"?</p> Click to see the solution. <p>To log in to the login node \"uan01\", use the hostname <code>lumi-uan01.csc.fi</code> instead of <code>lumi.csc.fi</code>.</p> <p>This may be useful if you use software on your desktop that tries to connect repeatedly to LUMI and then tries to find, e.g., a running server that it  created before.</p> </li> <li> <p>Create a shell on a login node using the Open OnDemand web interface.</p> Click to see the solution. <ul> <li>Point your web browser to      <code>https://www.lumi.csc.fi</code>.      With some browsers     it is sufficient to type <code>lumi.csc.fi</code> in the address bar while others     require <code>www.lumi.csc.fi</code>.</li> <li>Click the \"Go to login\" button. What you need to do here, depends on how     you got your account. For the course you will have to proceed with the      \"MyAccessID\" option \"Login Puhuri\" in most cases.</li> <li> <p>Once you're in the web interface, click on \"Login node shell\" (likely the third     choice on the first line). It will open a new tab in the browser with a login shell     on LUMI. Note that Open OnDemand uses a different set of login nodes.</p> <p>The \"Login node shell\" app does not require billing units, but has the same restrictions on use as the regular login nodes. It should not be used for heavy computations. The \"Compute node shell\" app will give you an interactive shell on a specific set of compute nodes set aside for interactive use, but will be charged to your billing units. It will give you exclusive access to a number of cores and associated memory.  The \"Desktop\" app gives you a richer environment, but will also require billing units. Depending on the type of node that you use for this, this will require either CPU or GPU billing units, but more about  that in the session on Slurm on day 2 of this course.</p> </li> </ul> </li> <li> <p>Try to transfer a file from your desktop/laptop to your home directory via the Open OnDemand web interface.</p> Click to see the solution. <ul> <li>Go back into Open OnDemand if you have left it after the previous exercise.</li> <li>On the main screen of the web interface, choose \"Home directory\".</li> <li>Depending on the browser and your system you may be able to just drag-and-drop files      into the frame that shows your files, or you can click the blue \"Upload\" button towards     the top of the screen.</li> </ul> </li> </ol>"},{"location":"intro-evolving/E04-Modules/","title":"Exercises: Modules on LUMI","text":"<p>See the instructions to set up for the exercises.</p> <ol> <li> <p>The Cray CPE comes with a number of differently configured HDF5 libraries.</p> <p>a.  Which ones can you find?</p> <p>b.  Can you find more documentation about those libraries?</p> Click to see the solution <pre><code>module spider HDF5\n</code></pre> <p>or </p> <pre><code>module spider hdf5\n</code></pre> <p>can produce a lot of output on the system. It will show you three modules though  (but this might be under \"Other possible matches\" in the second case) that have <code>cray-</code> in their name: <code>cray-hdf5</code>, <code>cray-hdf5-parallel</code> and <code>cray-netcdf-hdf5parallel</code>. The first two of these really provide HDF5 configured in two different ways. The third one is another library using HDF5 as a backend. The other <code>hdf5</code> modules that you might see are modules generated by Spack (see a little bit in the next session).</p> <p>If you want more information about the <code>cray-hdf5</code> module, you can try</p> <pre><code>module spider cray-hdf5\n</code></pre> <p>and then for a specific version</p> <pre><code>module spider cray-hdf5/1.12.2.11\n</code></pre> <p>and see that there is not much information. Even worse, the help of this particular version refers to the release info but mentions the wrong filename. The path is correct, but the file where the info is, is</p> <pre><code>/opt/cray/pe/hdf5/1.12.2.11/release_info.md\n</code></pre> <p>(and the same holds true for <code>cray-hdf5-parallel</code>)</p> </li> <li> <p>The <code>Bison</code> program installed in the OS image is pretty old (version 3.0.4) and     we want to use a newer one. Is there one available on LUMI?</p> Click to see the solution. <pre><code>module spider Bison\n</code></pre> <p>tells us that there are indeed newer versions available on the system. </p> <p>The versions that have a compiler name (usually <code>gcc</code>) in their name followed by some seemingly random characters are installed with Spack and not in the CrayEnv or LUMI environments.</p> <p>To get more information about <code>Bison/3.8.2</code> if you didn't get it already with the previous command: </p> <pre><code>module spider Bison/3.8.2\n</code></pre> <p>tells us that Bison 3.8.2 is provided by a couple of <code>buildtools</code> modules and available in all partitions in several versions of the <code>LUMI</code> software stack and in <code>CrayEnv</code>.</p> <p>Alternatively, in this case</p> <pre><code>module keyword Bison\n</code></pre> <p>would also have shown that Bison is part of several versions of the <code>buildtools</code> module.</p> <p>The <code>module spider</code> command is often the better command if you use names that with a high  likelihood could be the name of a package, while <code>module keyword</code> is often the better choice for words that are more a keyword. But if one does not return the solution it is a good idea  to try the other one also.</p> <p>A problem with too much different versions of software on the system...</p> <p>If you tried </p> <pre><code>module spider bison\n</code></pre> <p>to look for Bison, you may not have found the version in <code>buildtools</code> which is the main  version of Bison on LUMI in the main supported software stack (see the next presentation), but only versions that are currently on the system and installed through Spack.</p> </li> <li> <p>The <code>htop</code> command is a nice alternative for the <code>top</code> command with a more powerful user interface.     However, typing <code>htop</code> on the command line produces an error message. Can you find and run <code>htop</code>?</p> Click to see the solution. <p>We can use either <code>module spider htop</code> or <code>module keyword htop</code> to find out that <code>htop</code> is indeed available on the system. With <code>module keyword htop</code> we'll find out immediately that it is in the  <code>systools</code> modules and some of those seem to be numbered after editions of the LUMI stack suggesting that they may be linked to a stack, with <code>module spider</code> you'll first see that it is an extension of a module and see the versions. You may again see some versions installed with Spack.</p> <p>Let's check further for <code>htop/3.3.0</code> that should exist according to <code>module spider htop</code>:</p> <pre><code>module spider htop/3.3.0\n</code></pre> <p>tells us that this version of <code>htop</code> is available in all partitions of <code>LUMI/24.03</code> and in <code>CrayEnv</code>. Let us just run it in the <code>CrayEnv</code> environment:</p> <pre><code>module load CrayEnv\nmodule load systools/24.03\nhtop\n</code></pre> <p>(You can quit <code>htop</code> by pressing <code>q</code> on the keyboard.)</p> </li> <li> <p>LUMI now offers Open OnDemand as a browser-based interface to LUMI that enables     running some graphical programs through a VNC server.      But for users who do not want to use Open OnDemand apps, there is currently another     way to start a VNC server (and that was the way to use graphical programs before      the Open OnDemand interface was ready and may still be relevant if Open OnDemand     would fail after a system update).     Can you find the tool on LUMI, and if so, how can we use it?</p> Click to see the solution. <p><code>module spider VNC</code> and <code>module keyword VNC</code> can again both be used to check if there is software available to use VNC. Both will show that there is a module <code>lumi-vnc</code> in several versions. If you  try loading the older ones of these (the version number points at the date of some scripts) you will notice that some produce a warning as they are deprecated. However, when installing a new version we  cannot remove older ones in one sweep, and users may have hardcoded full module names in scripts they use to set their environment, so we chose to not immediate delete these older versions.</p> <p>One thing you can always try to get more information about how to run a program, is to ask for the help information of the module. For this to work the module must first be available, or you have to use  <code>module spider</code> with the full name of the module. We see that version <code>20230110</code> is the newest version of the module, so let's try that one:</p> <pre><code>module spider lumi-vnc/20230110\n</code></pre> <p>The output may look a little strange as it mentions <code>init-lumi</code> as one of the modules that you can load. That is because this tool is available even outside <code>CrayEnv</code> or the LUMI stacks. But this command also shows a long help test telling you how to use this module (though it does assume some familiarity with how X11 graphics work on Linux).</p> <p>Note that if there is only a single version on the system, as has been the case recently, the <code>module spider VNC</code> command without specific version or correct module name will already display the help information.</p> </li> </ol>"},{"location":"intro-evolving/E05-SoftwareStacks/","title":"Exercises: LUMI Software Stacks","text":"<p>See the instructions to set up for the exercises. For these exercises, you'll need the files in the <code>EasyBuild</code> subdirectory.</p>"},{"location":"intro-evolving/E05-SoftwareStacks/#information-in-the-lumi-software-library","title":"Information in the LUMI Software Library","text":"<p>Explore the LUMI Software Library.</p> <ol> <li> <p>Search for information for the package GROMACS and quickly read through the page</p> Click to see the solution. <p>Link to the GROMACS documentation</p> <p>It is an example of a package for which we have both user-level and some technical information. The page will first show some license information, then the actual user information which in this case also contains a link to training materials used by a course at CSC.</p> <p>Some versions are available for multiple compilers to let you experiment which one works best, but this is not the case for all versions as that is outside the scope of what the main LUMI User Support Team can do given its small size and the amount of domain expertise that is needed to select a set of relevant benchmarks for GROMACS.</p> </li> </ol>"},{"location":"intro-evolving/E05-SoftwareStacks/#using-modules-in-the-lumi-software-stack","title":"Using modules in the LUMI software stack","text":"<ol> <li> <p>Search for the <code>brotli</code> tool and make      sure that you can use software compiled with the Cray compilers in the LUMI stacks in the same session.</p> Click to see the solution. <pre><code>module spider brotli\n</code></pre> <p>shows that there are versions of <code>brotli</code> for several of the <code>cpe*</code> toolchains and in several versions of the LUMI software stack.</p> <p>Of course we prefer to use a recent software stack, the <code>23.12</code> or <code>24.03</code> (and preferably the <code>24.03</code> as  that is the best supported on at this time, early December 2024).  Since we want to use other software compiled with the Cray compilers also, we really want a <code>cpeCray</code> version to avoid conflicts between  different toolchains. So the module we want to load is <code>Brotli/1.1.0-cpeCray-24.03</code>.</p> <p>To figure out how to load it, use</p> <pre><code>module spider Brotli/1.1.0-cpeCray-24.03\n</code></pre> <p>and see that (as expected from the name) we need to load <code>LUMI/24.03</code> and can then use it in any of the partitions.</p> <p>Instead of using the <code>module spider</code> command, you could also have searched for <code>brotli</code> in the  LUMI Software Guide and you would end up on the Brotli page which shows that the package is pre-installed.  In the \"Pre-installed modules (and EasyConfigs)\" section of that page, you can also see which modules are available. That list is slightly less reliable than using  <code>module spider</code> as there may still be references to versions that used to be on the system but are no longer as the programming environment cannot be properly supported.</p> </li> </ol>"},{"location":"intro-evolving/E05-SoftwareStacks/#installing-software-with-easybuild","title":"Installing software with EasyBuild","text":"<p>These exercises are based on material from the EasyBuild tutorials (and we have a special version for LUMI also).</p> <p>Note: If you want to be able to uninstall all software installed through the exercises easily, we suggest you make a separate EasyBuild installation for the course, e.g., in <code>/scratch/project_465001102/$USER/eb-course</code> if you make the exercises during the course:</p> <ul> <li>Start from a clean login shell with only the standard modules loaded.</li> <li> <p>Set <code>EBU_USER_PREFIX</code>: </p> <pre><code>export EBU_USER_PREFIX=/scratch/project_465001102/$USER/eb-course\n</code></pre> <p>You'll need to do that in every shell session where you want to install or use that software.</p> </li> <li> <p>From now on you can again safely load the necessary <code>LUMI</code> and <code>partition</code> modules for the exercise.</p> </li> <li> <p>At the end, when you don't need the software installation anymore, you can simply remove the directory     that you just created.</p> <pre><code>rm -rf /scratch/project_465001102/$USER/eb-course\n</code></pre> </li> </ul>"},{"location":"intro-evolving/E05-SoftwareStacks/#installing-a-simple-program-without-dependencies-with-easybuild","title":"Installing a simple program without dependencies with EasyBuild","text":"<p>The LUMI Software Library contains the package <code>eb-tutorial</code>. Install the version of the package for the <code>cpeCray</code> toolchain in the 24.03 version of the software stack. Install the software for the LUMI-C compute nodes.</p> Click to see the solution. <ul> <li> <p>We can check the      eb-tutorial page     in the      LUMI Software Library     if we want to see more information about the package.</p> <p>You'll notice that there are versions of the EasyConfigs for <code>cpeGNU</code>, <code>cpeCray</code> and <code>cpeAOCC</code>. As we want to install software with the <code>cpeCray</code> toolchain for <code>LUMI/24.03</code>, we'll need the <code>cpeCray-24.03</code> version which is the EasyConfig <code>eb-tutorial-1.0.1-cpeCray-24.03.eb</code>.</p> </li> <li> <p>Obviously we need to load the <code>LUMI/24.03</code> module. If we would like to install software     for the CPU compute nodes, you need to also load <code>partition/C</code>.     To be able to use EasyBuild, we also need the <code>EasyBuild-user</code> module.</p> <pre><code>module load LUMI/24.03 partition/C\nmodule load EasyBuild-user\n</code></pre> </li> <li> <p>Now all we need to do is run the <code>eb</code> command from EasyBuild to install the software.</p> <p>Let's however take the slow approach and first check if what dependencies the package needs:</p> <pre><code>eb eb-tutorial-1.0.1-cpeCray-24.03.eb -D\n</code></pre> <p>We can do this from any directory as the EasyConfig file is already in the LUMI Software Library and will be located automatically by EasyBuild. You'll see that all dependencies are already on  the system so we can proceed with the installation:</p> <pre><code>eb eb-tutorial-1.0.1-cpeCray-24.03.eb \n</code></pre> </li> <li> <p>After this you should have a module <code>eb-tutorial/1.0.1-cpeCray-24.03</code>. Try</p> <pre><code>module av eb-tutorial/1.0.1-cpeCray-24.03\n</code></pre> <p>This may take a while as EasyBuild has erased the Lmod cache to ensure that the new module can be found.</p> </li> <li> <p>Now that we have the module, we can check what it actually does:</p> <pre><code>module help eb-tutorial/1.0.1-cpeCray-24.03\n</code></pre> <p>and we see that it provides the <code>eb-tutorial</code> command.</p> </li> <li> <p>So let's now try to run this command:</p> <pre><code>module load eb-tutorial/1.0.1-cpeCray-24.03\neb-tutorial\n</code></pre> <p>Note that if you now want to install one of the other versions of this module, EasyBuild will complain that some modules are loaded that it doesn't like to see, including the <code>eb-tutorial</code> module and the <code>cpeCray</code> modules so it is better to unload those first:</p> <pre><code>module unload cpeCray eb-tutorial\n</code></pre> </li> </ul>"},{"location":"intro-evolving/E05-SoftwareStacks/#installing-an-easyconfig-given-to-you-by-lumi-user-support","title":"Installing an EasyConfig given to you by LUMI User Support","text":"<p>Sometimes we have no solution ready in the LUMI Software Library, but we prepare one or more custom EasyBuild recipes for you. Let's mimic this case. In practice we would likely send  those as attachments to a mail from the ticketing system and you would be asked to put them in a separate directory (basically since putting them at the root of your home directory would in some cases let EasyBuild search your whole home directory for dependencies which would be a very slow process).</p> <p>You've been given two EasyConfig files to install a tool called <code>py-eb-tutorial</code> which is in fact a Python package that uses the <code>eb-tutorial</code> package installed in the previous exercise. These EasyConfig files are in the <code>EasyBuild</code> subdirectory of the exercises for this course. In the first exercise you are asked to install the version of <code>py-eb-tutorial</code> for the <code>cpeCray/24.03</code> toolchain.</p> Click to see the solution. <ul> <li> <p>Go to the <code>EasyBuild</code> subdirectory of the exercises and check that it indeed contains the     <code>py-eb-tutorial-1.0.0-cpeCray-24.03-cray-python-3.11.7.eb</code> and     <code>py-eb-tutorial-1.0.0-cpeGNU-24.03-cray-python-3.11.7.eb</code> files.     It is the first one that we need for this exercise.</p> <p>You can see that we have used a very long name as we are also using a version suffix to make clear which version of Python we'll be using.</p> </li> <li> <p>Let's first check for the dependencies (out of curiosity):</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeCray-24.03-cray-python-3.11.7.eb -D\n</code></pre> <p>and you'll see that all dependencies are found (at least if you made the previous exercise  successfully). You may find it strange that it shows no Python module but that is because we are using the <code>cray-python</code> module which is not installed through EasyBuild and only known to EasyBuild as an external module.</p> </li> <li> <p>And now we can install the package:</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeCray-24.03-cray-python-3.11.7.eb\n</code></pre> </li> <li> <p>To use the package all we need to do is to load the module and to run the command that it     defines:</p> <pre><code>module load py-eb-tutorial/1.0.0-cpeCray-24.03-cray-python-3.11.7\npy-eb-tutorial\n</code></pre> <p>with the same remark as in the previous exercise if Lmod fails to find the module.</p> <p>You may want to do this step in a separate terminal session set up the same way, or you will get an error message in the next exercise with EasyBuild complaining that there are some modules loaded that should not be loaded.</p> </li> </ul>"},{"location":"intro-evolving/E05-SoftwareStacks/#installing-software-with-uninstalled-dependencies","title":"Installing software with uninstalled dependencies","text":"<p>Now you're asked to also install the version of <code>py-eb-tutorial</code> for the <code>cpeGNU</code> toolchain in <code>LUMI/24.03</code> (and the solution given below assumes you haven'ty accidentally installed the wrong EasyBuild recipe in one of the previous two exercises).</p> Click to see the solution. <ul> <li> <p>We again work in the same environment as in the previous two exercises. Nothing has changed here.     Hence if not done yet we need</p> <pre><code>module load LUMI/24.03 partition/C\nmodule load EasyBuild-user\n</code></pre> </li> <li> <p>Now go to the <code>EasyBuild</code> subdirectory of the exercises (if not there yet from the previous     exercise) and check what the <code>py-eb-tutorial-1.0.0-cpeGNU-24.03-cray-python-3.11.7.eb</code> needs:</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeGNU-24.03-cray-python-3.11.7.eb -D\n</code></pre> <p>We'll now see that there are two missing modules. Not only is the  <code>py-eb-tutorial/1.0.0-cpeGNU-24.03-cray-python-3.11.7</code> that we try to install missing, but also the <code>eb-tutorial/1.0.1-cpeGNU-24.03</code>. EasyBuild does however manage to find a recipe from which this module can be built in the pre-installed build recipes.</p> </li> <li> <p>We can install both packages separately, but it is perfectly possible to install both packages in a single     <code>eb</code> command by using the <code>-r</code> option to tell EasyBuild to also install all dependencies.</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeGNU-24.03-cray-python-3.11.7.eb -r\n</code></pre> </li> <li> <p>At the end you'll now notice (with <code>module avail</code>) that both the module      <code>eb-tutorial/1.0.1-cpeGNU-24.03</code> and <code>py-eb-tutorial/1.0.0-cpeGNU-24.03-cray-python-3.11.7</code>     are now present.</p> <p>To run you can use</p> <pre><code>module load py-eb-tutorial/1.0.0-cpeGNU-24.03-cray-python-3.11.7\npy-eb-tutorial\n</code></pre> </li> </ul>"},{"location":"intro-evolving/E07-Slurm/","title":"Exercises: Slurm on LUMI","text":""},{"location":"intro-evolving/E07-Slurm/#basic-exercises","title":"Basic exercises","text":"<ol> <li> <p>In this exercise we check how cores would be assigned to a shared memory program.      Run a single task on the CPU partition with <code>srun</code> using 16 cpu cores.      Inspect the default task allocation with the <code>taskset</code> command      (<code>taskset -cp $$</code> will show you the cpu numbers allocated to the current process). </p> Click to see the solution. <pre><code>srun --partition=small --nodes=1 --tasks=1 --cpus-per-task=16 --time=5 --account=&lt;project_id&gt; bash -c 'taskset -cp $$' \n</code></pre> <p>Note that you need to replace <code>&lt;project_id&gt;</code> with the actual project account ID of the  form  <code>project_</code> plus a 9 digits number (and this argument can be omitted if you use the <code>exercises/small</code> module during the course).</p> <p>The command runs a single process (<code>bash</code> shell with the native Linux <code>taskset</code> tool showing  process's CPU affinity) on a compute node.  You can use the <code>man taskset</code> command to see how the tool works.</p> </li> <li> <p>Next we'll try a hybrid MPI/OpenMP program.     For this we will use the <code>hybrid_check</code> tool from the <code>lumi-CPEtools</code> module of the LUMI Software Library.      This module is preinstalled on the system and has versions for all versions of the <code>LUMI</code> software stack     and all toolchains and partitions in those stacks.</p> <p>Use the simple job script below to run a parallel program with multiple tasks (MPI ranks) and threads (OpenMP).  Submit with <code>sbatch</code> on the CPU partition and check task and thread affinity.</p> <pre><code>#!/bin/bash -l\n#SBATCH --partition=small           # Partition (queue) name\n#SBATCH --nodes=1                   # Total number of nodes\n#SBATCH --ntasks-per-node=8         # 8 MPI ranks per node\n#SBATCH --cpus-per-task=16          # 16 threads per task\n#SBATCH --time=5                    # Run time (minutes)\n#SBATCH --account=&lt;project_id&gt;      # Project for billing\n\nmodule load LUMI/24.03\nmodule load lumi-CPEtools/1.2-cpeGNU-24.03\n\nsrun --cpus-per-task=$SLURM_CPUS_PER_TASK hybrid_check -n -r\n</code></pre> <p>Be careful with copy/paste of the script body as copy problems with special characters or a double dash may  occur, depending on the editor you use.</p> Click to see the solution. <p>Save the script contents into the file  <code>job.sh</code> (you can use the <code>nano</code> console text editor for instance).  Remember to use valid project account name (or omit the line if you are using the <code>exercises/small</code> module).</p> <p>Submit the job script using the <code>sbatch</code> command:</p> <pre><code>sbatch job.sh\n</code></pre> <p>The job output is saved in the <code>slurm-&lt;job_id&gt;.out</code> file.  You can view its content with either the <code>less</code> or <code>more</code> shell commands.</p> <p>The actual task/threads affinity may depend on the specific OpenMP runtime  (if you literally use this job script it will be the GNU OpenMP runtime).</p> </li> </ol>"},{"location":"intro-evolving/E07-Slurm/#advanced-exercises","title":"Advanced exercises","text":"<p>These exercises combine material from several chapters of the tutorial. This particular exercise makes most sense if you will be building software on LUMI (but remember that this will be more of you than you may expect)!</p> <ol> <li> <p>Build the <code>hello_jobstep</code> program tool using interactive shell on a GPU node     (small-g<code>partition, 1 GPU, 7 cores and 60 GB of memory should be more     than enough).      You can pull the source code for the program from git repository      [</code>https://code.ornl.gov/olcf/hello_jobstep.git<code>](https://code.ornl.gov/olcf/hello_jobstep).      It uses a</code>Makefile<code>for building and requires Clang and HIP. The code also     contains a</code>README.md<code>file with instructions, but they will need some changes.     The</code>hello_jobstep<code>program is actually the main source of inspiration for the</code>gpu_check<code>program in the</code>lumi-CPEtools<code>modules for</code>partition/G`.     Try to run the program interactively. </p> <p>The <code>Makefile</code> contains a conditional section to set proper arguments for the compiler. LUMI is very similar to Frontier, so when calling the <code>make</code> program to build the code from the Makefile, don't use simply <code>make</code> as suggested in the <code>README.md</code>, but use</p> <pre><code>make LMOD_SYSTEM_NAME=\"frontier\"\n</code></pre> <p>Note: Given that the reservation is on <code>standard-g</code> where you can only get whole nodes, which is rather stupid for this example, it is better to try to get a single GPU with 7 cores and 60GB of memory on the <code>small-g</code> partition.</p> Click to see the solution. <p>Clone the code using <code>git</code> command:</p> <pre><code>git clone https://code.ornl.gov/olcf/hello_jobstep.git\n</code></pre> <p>It will create <code>hello_jobstep</code> directory consisting source code and <code>Makefile</code>.</p> <p>Allocate resources for a single task with a single GPU with <code>salloc</code>:</p> <pre><code>salloc --partition=small-g --tasks=1 --cpus-per-task=7 --gpus=1 --mem=60G --time=10 --account=&lt;project_id&gt;\n</code></pre> <p>Note that, after allocation is granted, you receive new shell but are still on the compute node.  You need to use the <code>srun</code> command to run on the allocated node. </p> <p>Start interactive session on a GPU node:</p> <pre><code>srun --pty bash -i\n</code></pre> <p>Note now you are on the compute node. <code>--pty</code> option for <code>srun</code> is required to interact with the remote shell.</p> <p>Enter the <code>hello_jobstep</code> directory. There is a Makefile to build the code using the <code>make</code> command, but first we need to make sure that there is a proper programming environment loaded. </p> <p>As an example we will built with the system default programming environment, <code>PrgEnv-cray</code> in <code>CrayEnv</code>.  Just to be sure we'll load even the programming environment module explicitly.</p> <p>The build will fail if the <code>rocm/6.0.3</code> module is not loaded when using <code>PrgEnv-cray</code>. Whereas the instructions suggest to simply use the <code>rocm</code>  module, we're specifying a version as at the time of the course, there  was a newer but not fully supported <code>rocm</code> module on the system.</p> <pre><code>module load CrayEnv\nmodule load PrgEnv-cray\nmodule load rocm/6.0.3\n</code></pre> <p>Alternatively, you can build in the <code>LUMI/24.03</code> stack using the EasyBuild toolchain instead of <code>PrgEnv-cray</code>:</p> <pre><code>module load LUMI/24.03 partition/G cpeCray\n</code></pre> <p>In this case, you do not need to load the ROCm module as it is loaded automatically by <code>cpeCray/24.03</code> when working in <code>partition/G</code>.</p> <p>To build the code, use</p> <pre><code>make LMOD_SYSTEM_NAME=\"frontier\"\n</code></pre> <p>You need to add <code>LMOD_SYSTEM_NAME=\"frontier\"</code> variable for make as the code originates from the Frontier system and doesn't know LUMI.</p> <p>(As an exercise you can try to fix the <code>Makefile</code> and enable it for LUMI :))</p> <p>Finally you can just execute <code>./hello_jobstep</code> binary program to see how it behaves:</p> <pre><code>./hello_jobstep\n</code></pre> <p>Note that executing the program with <code>srun</code> in the srun interactive session will result in a hang. You need to work with <code>--overlap</code> option for srun to mitigate this.</p> <p>Remember to terminate your interactive session with <code>exit</code> command.</p> <pre><code>exit\n</code></pre> <p>and then do the same for the shell created by <code>salloc</code> also.</p> </li> </ol>"},{"location":"intro-evolving/E08-Binding/","title":"Exercises: Process and Thread Distribution and Binding","text":""},{"location":"intro-evolving/E08-Binding/#basic-exercises","title":"Basic exercises","text":"<ol> <li> <p>We return to the hybrid MPI/OpenMP example from the     Slurm exercises. </p> <pre><code>#!/bin/bash -l\n#SBATCH --partition=small           # Partition (queue) name\n#SBATCH --nodes=1                   # Total number of nodes\n#SBATCH --ntasks-per-node=8         # 8 MPI ranks per node\n#SBATCH --cpus-per-task=16          # 16 threads per task\n#SBATCH --time=5                    # Run time (minutes)\n#SBATCH --account=&lt;project_id&gt;      # Project for billing\n\nmodule load LUMI/24.03\nmodule load lumi-CPEtools/1.2-cpeGNU-24.03\n\nsrun --cpus-per-task=$SLURM_CPUS_PER_TASK hybrid_check -n -r\n</code></pre> <p>Improve the thread affinity with OpenMP runtime variables.  Alter the above script and ensure that each thread is bound to a specific core. </p> Click to see the solution. <p>Add the following OpenMP environment variables definition to your script:</p> <pre><code>export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=close\n</code></pre> <p>To avoid having to use the <code>--cpus-per-task</code> flag, you can also set the environment variable <code>SRUN_CPUS_PER_TASK</code> instead: </p> <pre><code>export SRUN_CPUS_PER_TASK=16 \n</code></pre> <p>On LUMI this is not strictly necessary as the Slurm <code>sbatch</code> processing has been modified to set this environment variable, but that was a clunky patch to reconstruct some old behaviour of Slurm and we have already seen cases where the patch did not work (but that were more complex cases that required different environment variables for a similar function).</p> <p>The list of environment variables that the <code>srun</code> command can use as input, is actually confusing, as some start with <code>SLURM_</code> but a few start with <code>SRUN_</code> while the <code>SLURM_</code> equivalent is ignored.</p> <p>So we end up with the following script:</p> <pre><code>#!/bin/bash -l\n#SBATCH --partition=small           # Partition (queue) name\n#SBATCH --nodes=1                   # Total number of nodes\n#SBATCH --ntasks-per-node=8         # 8 MPI ranks per node\n#SBATCH --cpus-per-task=16          # 16 threads per task\n#SBATCH --time=5                    # Run time (minutes)\n#SBATCH --account=&lt;project_id&gt;      # Project for billing\n\nmodule load LUMI/24.03\nmodule load lumi-CPEtools/1.2-cpeGNU-24.03\n\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=cores\n\nsrun hybrid_check -n -r\n</code></pre> Using <code>MPICH_CPUMASK_DISPLAY</code> <p>Cray MPICH can also return information about the CPU mask per process in binary form (a long string of zeros and ones) where the last number is for core 0. For this, you need to set</p> <pre><code>export MPICH_CPUMASK_DISPLAY=1\n</code></pre> <p>You'll see that with the OpenMP environment variables set as in the example, it will look like only one core can be used by each MPI task, but that is because it only shows the mask for the main process which becomes OpenMP thread 0. Remove the OpenMP environment variables and you'll see that each task now gets 16 possible cores to run on, and the same is true for each OpenMP thread (at least when using the GNU compilers, the Cray compilers have different default behaviour for OpenMP which actually makes more sense for most scientific computing codes).</p> <p>The format in which the mask is reported, is different from that used by <code>hybrid_check</code>, where the first digit corresponds to core 0. The <code>MPICH_CPUMASK_DISPLAY</code> style of reporting corresponds to the way Slurm CPU masks for <code>--cpu-bind</code> are constructed.</p> </li> <li> <p>Binding on GPU nodes:     Allocate one GPU node with one task per GPU and bind tasks to each CCD (8-core group sharing L3 cache)      leaving the first (#0) and last (#7) cores unused.      Run a program with 6 threads per task and inspect the actual task/threads affinity     using the   <code>gpu_check</code> command from the <code>lumi-CPEtools</code> module.</p> Click to see the solution. <p>We can chose between different approaches. In the example below, we follow the  \"GPU binding: Linear GCD, match cores\"  slides and we only need to adapt the CPU mask:</p> <pre><code>#!/bin/bash -l\n#SBATCH --partition=standard-g      # Partition (queue) name\n#SBATCH --nodes=1                   # Total number of nodes\n#SBATCH --ntasks-per-node=8         # 8 MPI ranks per node\n#SBATCH --gpus-per-node=8           # Allocate one gpu per MPI rank\n#SBATCH --time=5                    # Run time (minutes)\n#SBATCH --account=&lt;project_id&gt;      # Project for billing\n#SBATCH --hint=nomultithread\n\nmodule load LUMI/24.03 partition/G\nmodule load lumi-CPEtools/1.2-cpeGNU-24.03\n\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nexport ROCR_VISIBLE_DEVICES=\\$SLURM_LOCALID\nexec \\$*\nEOF\nchmod +x ./select_gpu_$SLURM_JOB_ID\n\nCPU_BIND=\"mask_cpu:0x7e000000000000,0x7e00000000000000,\"\nCPU_BIND=\"${CPU_BIND}0x7e0000,0x7e000000,\"\nCPU_BIND=\"${CPU_BIND}0x7e,0x7e00,\"\nCPU_BIND=\"${CPU_BIND}0x7e00000000,0x7e0000000000\"\n\nexport OMP_NUM_THREADS=6\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=cores\n\nsrun --cpu-bind=${CPU_BIND} ./select_gpu_$SLURM_JOB_ID gpu_check -l\n</code></pre> <p>The base mask we need for this exercise, with each first and last core of a chiplet disabled, is <code>01111110</code> which is <code>0x7e</code> in hexadecimal notation (though using <code>0xfe</code> as the building block would also have worked as we already limit the number of threads to 6 through <code>OMP_NUM_THREADS</code> and use other binding variables that will bind all threads as close as possible to the core with relative number 1 of each chiplet).</p> <p>Save the job script as <code>job_step.sh</code> then simply submit it with sbatch. Inspect the job output.</p> <p>Note that in fact, if you had used the <code>cpeCray</code> version of the <code>lumi-CPEtools</code> module,  you don't even need to use the <code>OMP_*</code> environment variables above as the threads are  automatically pinned to a single core and as the correct number of threads is derived from the affinity mask for each task.</p> </li> </ol>"},{"location":"intro-evolving/E10-ObjectStorage/","title":"Exercises: LUMI-O object storage","text":"<p>These exercises need to be made in the indicated order, as some exercises prepare materials for the following exercises.</p> <p>You do need a project on LUMI to make these exercises. Wherever we use <code>46YXXXXXX</code>, replace with the number of your project (likely a number starting with 465 for a project obtained via LUMI-BE).</p> <ol> <li> <p>This exercise is the first step in preparing the environment needed for exercise 3     and later.</p> <p>Go into Open OnDemand and use the  \"Cloud storage configuration\" app to create an access key for your project <code>project_46YXXXXXX</code>. Also create a configuration for <code>s3cmd</code> and a public rclone endpoint.</p> <p>Check the result in the \"Home directory\" app.</p> Click here to see the solution. <p>Solving the exercise requires several steps.</p> <ol> <li> <p>Log on to Open OnDemand: Go to www.lumi.csc.fi as      discussed in the \"Getting Access\" lecture in day one of this course. </p> </li> <li> <p>You need to create an authentication key to access LUMI-O.</p> <ol> <li>Open the \"Cloud storage configuration\" app.</li> <li>Scroll towards the bottom.</li> <li>Select project 46YXXXXXX</li> <li>Also click the checkboxes next to \"Generate s3cmd configuration\" and     \"Configure public remote\".</li> <li>Click on the \"Submit\" button.</li> </ol> </li> <li> <p>Now we'll check the result.</p> <ol> <li>Leave the \"Cloud storage configuration\" app by navigating back in the browser or clicking     the \"LUMI\" logo at the top left of the screen.</li> <li>Open the \"Home Directory\" app.</li> <li> <p>Towards the bottom in the left column, you should now see \"lumi-46YXXXXXX-private\" and      \"lumi-46YXXXXXX-public\".</p> <p>Notice once more that these are just endpoints. Uploading to them will set a different ACL (Access Control List) for the objects and buckets, but when you browse in both you see both private and public objects with no way to distinguish between them.\\</p> </li> </ol> </li> </ol> </li> <li> <p>This is not really an exercise, but instructions to set up the environment for the next      exercises.</p> <p>In the directory with LUST exercises that were downloaded following the instructions to set up for the exercises, go into the <code>ObjectStorage</code> directory. In that directory, run the <code>create_buckets.sh</code> script with your project number <code>46YXXXXXX</code> as the argument:</p> <pre><code>./create_buckets.sh 46YXXXXXX\n</code></pre> <p>This will create two buckets and populate them.</p> <p>The script will only work on LUMI, and after successfully making the first exercise, including the creation of the <code>s3cmd</code> configuration file. The script does rely on the file <code>~/.s3cfg-lumi-46YXXXXXX</code>.</p> </li> <li> <p>Now go in to the web credentials management system auth.lumidata.eu, find back the     authentication key that we created in the first exercise, and generate an s3cmd config file for it in the browser     (no need to also install it on the system as we've done that already, but what would be the right filename?)</p> <p>Do not close your browser window after this exercise as it will prove useful for other exercises.</p> Click here to see the solution. <p>Solving this exercise requires again several steps.</p> <ol> <li> <p>Go into the web credentials management system auth.lumidata.eu.     You can log in in the same way you did in Open OnDemand in the previous exercise.     After logging in, you should see a screen \"Your projects\" with at least a line for the      project 46YXXXXXX.</p> </li> <li> <p>To find back the authentication key, simply click on the line for the project 46YXXXXXX.     A new pane will appear at the right with first a section to generate a new authentication     key pair and then a section \"Available keys\" which will list a key with the key description     \"lumi web interface\".</p> </li> <li> <p>To generate a configuration file for <code>s3cmd</code> for that key, simply click on the key and a new right     pane appears. At the top, you find the \"Access key details\", then a section to extend the key      duration and then a section \"Configuration templates\".</p> <p>Use the \"Select format\" box to select <code>s3cmd</code> and click on \"Generate\". A new tab will appear in the browser with text that looks like</p> <pre><code># s3cmd configuration template for project \n# Generated for UUUUUUUU\n# Valid until 2024-12-07T11:39:17+02:00\n# DO NOT SHARE!\n\n# Default location is ${HOME}/.s3cfg\n\n[lumi-46YXXXXXX]\naccess_key   = XXXXXXXXXXXXXXXXXXXX\nsecret_key   = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nhost_base    = https://lumidata.eu\nhost_bucket  = https://lumidata.eu\nhuman_readable_sizes = True\nenable_multipart = True\nsignature_v2 = True\nuse_https = True\n</code></pre> <p>This text can be copied to the <code>s3cmd</code> configuration file which on Linux-like systems is <code>~/.s3cfg</code>, and if you want to have an <code>s3cmd</code> configuration for multiple systems, the suggested name is <code>~/.s3cfg-lumi-46YXXXXXX</code>.</p> </li> </ol> </li> <li> <p>Use the <code>lumio-conf</code> command line tool to generate a configuration for <code>s3cmd</code> and for <code>rclone</code>     (which will overwrite the configuration files we made in the first exercise).</p> Click here to see the solution. <ol> <li> <p>If you type <code>lumio-conf</code> in a shell command line, you're likely get an error. That is because     you first need to load a module to make the command available. In this exercise, we'll use     the latest version of this tool, and all we need to do to make the command available, is</p> <pre><code>module load lumio\n</code></pre> <p>You will see a warning which is meant for users who have been using this module before as at the end of November 2024 a new version was installed that creates different configurations that are more equivalent to those created by Open OnDemand.</p> </li> <li> <p>Type <code>lumio-conf</code> to start the <code>lumio-conf</code> tool in default mode, where it will create configuration     files for <code>rclone</code> and <code>s3cmd</code>.</p> <p>The first question it will ask you, is the project number. Fill in <code>46YXXXXXX</code>.</p> <p>Next it will ask you for the \"Access key\". We found that information in the previous exercise: It was at the top of the right column after selecting the project in the web credentials management system auth.lumidata.eu and then selecting the key. You can copy the access key information from their. Many terminal emulators support copy and paste, so you can copy it from the web browser and paste it into your terminal. Copying is easy with the rectangular icon next to the value of the Access key. Note that when you paste the data or type the access key, it will  not be shown so you have no feedback. Press the enter key.</p> <p>Next the program will ask for the \"Secret key\" which again you can find in the web credentials management system, on the next line. Again copy and paste into your terminal window, and again the key  will not be shown on the screen.</p> <p><code>lumio-conf</code> will now create the configuration files. It will print information about its  <code>rclone</code> configuration which is stored in the file <code>~/.config/rclone/rclone.conf</code> and creates two endpoints for <code>rclone</code>: <code>lumi-46YXXXXXX-private</code> and <code>lumi-46YXXXXXX-public</code>, and for <code>s3cmd</code> it will actually create two files: A configuration file <code>~/.s3cfg-lumi-46YXXXXXX</code>, and it will then also create or overwrite <code>~/.s3cfg</code> with that configuration. </p> </li> <li> <p>Feel free to inspect those files.</p> <p>In <code>~/.config/rclone/rclone.conf</code>, you'll see a section similar to</p> <pre><code>[lumi-46YXXXXXX-private]\ntype              = s3\nacl               = private\nenv_auth          = false\nprovider          = Ceph\nendpoint          = https://lumidata.eu\naccess_key_id     = XXXXXXXXXXXXXXXXXXXX\nsecret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nproject_id        = 46YXXXXXX\n\n[lumi-46YXXXXXX-public]\ntype              = s3\nacl               = public\nenv_auth          = false\nprovider          = Ceph\nendpoint          = https://lumidata.eu\naccess_key_id     = XXXXXXXXXXXXXXXXXXXX\nsecret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nproject_id        = 46YXXXXXX\n</code></pre> <p>while in <code>~/.s3cfg</code> and <code>~/.s3cfg-lumi-46YXXXXXX</code>, you'll see something similar to</p> <pre><code>use_https            = True\nsecret_key           = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nhost_base            = https://lumidata.eu\nproject_id           = 46YXXXXXX\nchunk_size           = 15\nhuman_readable_sizes = True\nenable_multipart     = True\nsignature_v2         = True\nsignurl_use_https    = True\naccess_key           = XXXXXXXXXXXXXXXXXXXX\nhost_bucket          = https://lumidata.eu\n</code></pre> </li> </ol> Using Open OnDemand instead <p>It is also possible to generate the exactly same files the Open OnDemand \"Cloud storage tool\" by checking both the \"Generate s3cmd configuration\" and \"Configure public remote\" checkboxes. In fact, this version of the <code>lumio-conf</code> tool was developed to also be used under the hood of Open OnDemand. The command line tool can do more though as it can also generate configurations for the AWS tools and the <code>boto3</code> Python package.</p> Why don't we have all the same tools from the web credentials management system auth.lumidata.eu in Open OnDemand? <p>The web credential management system was actually there already before Open OnDemand was deployed. Writing a more powerful app for Open OnDemand with the same functionality, takes time and as a small team, we have to make choices. It is important to have a separate platform also for the credentials management. Open OnDemand is down during LUMI maintenance as it is part of the main LUMI installation. However, the web based credential management system is more closely integrated with the object storage itself and has a different maintenance cycle. Hence it can remain available when LUMI is down, so that users can still access their data on LUMI-O from their home institution or personal computer.</p> </li> <li> <p>Which buckets and objects are there in the training project <code>project_46YXXXXXX</code>? Check with the command line     tools for which you prepared the configuration file in the previous exercise.     And maybe try Open Ondemand als as we haven't done that yet.</p> <p>Hint: Many commands have a <code>--help</code> option to get you on the way.</p> Click here to see the solution. <ul> <li> <p>With <code>s3cmd</code>: <code>s3cmd ls</code> will show you the buckets. It will return something like</p> <pre><code>2025-02-11 17:23  s3://training.private\n2025-02-11 17:23  s3://training.public\n</code></pre> <p>There may be more lines if some course participants have already created additional buckets.</p> <p>And we can then use <code>s3cmd ls s3://training.private</code> to see the objects in that bucket. It will hopefully (if nobody messed with the bucket) return:</p> <pre><code>                    DIR  s3://training.private/HTML/\n2025-02-11 17:23    59   s3://training.private/private-in-private.txt\n2025-02-11 17:23    58   s3://training.private/public-in-private.txt\n</code></pre> <p>This is not the complete object list as it shows a pseudo-folder view. The first line starts with <code>DIR</code> which  indicates a pseudo-directory, but you can now use </p> <pre><code>s3cmd ls s3://training.private/HTML/\n</code></pre> <p>where the slash at the end is actually important to see</p> <pre><code>2025-02-11 17:23   235   s3://training.private/HTML/private.html\n</code></pre> <p>Now if we use instead</p> <pre><code>s3cmd ls --recursive s3://training.private\n</code></pre> <p>we do get all objects in the bucket:</p> <pre><code>2025-02-11 17:23   235   s3://training.private/HTML/private.html\n2025-02-11 17:23    59   s3://training.private/private-in-private.txt\n2025-02-11 17:23    58   s3://training.private/public-in-private.txt\n</code></pre> </li> <li> <p>With <code>rclone</code>: Now we need to specify the endpoint as <code>rclone</code> supports multiple projects in a      single configuration.</p> <p>The command to use is now: <code>rclone ls lumi-46YXXXXXX-private:</code> which returns something similar to</p> <pre><code>    235 training.private/HTML/private.html\n    59 training.private/private-in-private.txt\n    58 training.private/public-in-private.txt\n    231 training.public/HTML/public.html\n    58 training.public/private-in-public.txt\n    57 training.public/public-in-public.txt\n</code></pre> <p>Now if you'd try <code>rclone ls lumi-46YXXXXXX-public:</code> instead, you'd see exactly the same because these are two endpoints for the same project. Their behaviour is different though when uploading objects.</p> <p>In these case we also already see all three objects in both the <code>training.private</code> and <code>training.public</code> buckets.</p> </li> <li> <p>With Open OnDemand:</p> <ol> <li>Log in to Open OnDemand if you're no longer logged     in after exercise 1.</li> <li>Open the \"Home Directory\" app.</li> <li> <p>Towards the bottom in the left column, you should now see \"lumi-46YXXXXXX-private\", and if you     created a public access point, also \"lumi-46YXXXXXX-public\".</p> <p>Notice once more that these are just endpoints. Uploading to them will set a different ACL (Access Control List) for the objects and buckets, but when you browse in both you see both private and public objects with no way to distinguish between them.\\</p> </li> <li> <p>If you know click on \"lumi-46YXXXXXX-private\", you should see a number of buckets in the right pane,     and from there you can browse further into these buckets. There are two buckets that we created     for this training: <code>training.public</code> and <code>training.private</code>. Both contain 3 objects, and in both     one of the objects contained a slash in the name, so you get to see a directory first with one \"file\".     E.g., the objects in the bucket <code>training.public</code> are <code>private-in-public.txt</code>, <code>public-in-public.txt</code>     and <code>HTML/public.html</code>.</p> </li> </ol> </li> </ul> </li> <li> <p>Check the ACLs of the <code>training.public</code> and <code>training.private</code> buckets and the objects in those buckets.     Which objects are publicly available and which are not?</p> Click here to see the solution. <p>Your friend for this is the <code>s3cmd info</code> command.  E.g., to check the bucket <code>training.public</code>, use <code>s3cmd info s3://training.public</code>. The crucial lines in the output are:</p> <pre><code>ACL:       *anon*: READ\nACL:       LUST Training / 2024-12-10-11 Supercomputing with LUMI - Online: FULL_CONTROL\n</code></pre> <p>The last line will always be present, with the name of the project and then <code>FULL_CONTROL</code> as whoever has the credentials of the project can do everything with the bucket. The first line says that everybody has read rights to this bucket and tells that this is a public bucket. When you use <code>s3cmd info s3://training.private</code>, only the second line will be present in the output, telling that this is a private object.</p> <p>To check the credentials of the <code>public-in-private.txt</code> object in the <code>training.private</code> bucket, use</p> <pre><code>s3cmd info s3://training.private/public-in-private.txt\n</code></pre> <p>The output will contain</p> <pre><code>ACL:       *anon*: READ\nACL:       LUST Training / 2024-12-10-11 Supercomputing with LUMI - Online: FULL_CONTROL\n</code></pre> <p>which shows that this object is actually public. So a private bucket can contain a public object, and in fact, you can access it with, e.g., a web browser without authenticating anywhere.</p> <p>You can do this for all objects in both commands:</p> <pre><code>s3cmd info s3://training.public/public-in-public.txt\ns3cmd info s3://training.public/private-in-public.txt\ns3cmd info s3://training.private/public-in-private.txt\ns3cmd info s3://training.private/private-in-private.txt\ns3cmd info s3://training.public/HTML/public.html\ns3cmd info s3://training.private/HTML/private.html\n</code></pre> <p>The name of each object suggests the answer.</p> </li> <li> <p>Use command line tools to download the file <code>private-in-private.txt</code> from the <code>training.private</code> bucket in     the <code>project_46YXXXXXX</code> training project of this training.</p> Click here to see the solution. <ul> <li> <p>With <code>s3cmd</code>: </p> <pre><code>s3cmd get s3://training.private/private-in-private.txt\n</code></pre> </li> <li> <p>With <code>rclone</code>:</p> <pre><code>rclone copy lumi-46YXXXXXX-private:training.private/private-in-private.txt .\n</code></pre> </li> </ul> </li> <li> <p>What would be the web-URL to access the public object <code>public-in-public.txt</code> in the <code>training.public</code> bucket?     Next try the same strategy to access <code>private-in-public.txt</code> in the  <code>training.public</code> bucket and both     <code>public-in-private.txt</code> and <code>private-in-private.txt</code> in the <code>training.private</code> bucket. What works and what doesn't?</p> Click here to see the solution. <ul> <li> <p><code>public-in-public.txt</code> in the <code>training.public</code> bucket:     https://46YXXXXXX.lumidata.eu/training.public/public-in-public.txt     or     https://lumidata.eu/46YXXXXXX:training.public/public-in-public.txt     both work. So we can access a public object in a public bucket.</p> </li> <li> <p><code>private-in-public.txt</code> in the <code>training.public</code> bucket:     Neither     https://46YXXXXXX.lumidata.eu/training.public/private-in-public.txt     nor     https://lumidata.eu/46YXXXXXX:training.public/private-in-public.txt     work.</p> </li> <li> <p><code>public-in-private.txt</code> in the <code>training.private</code> bucket:     https://46YXXXXXX.lumidata.eu/training.private/public-in-private.txt     or     https://lumidata.eu/46YXXXXXX:training.private/public-in-private.txt     both work. So we can access a public object in a public bucket.</p> </li> <li> <p><code>private-in-private.txt</code> in the <code>training.private</code> bucket:     Neither     https://46YXXXXXX.lumidata.eu/training.private/private-in-private.txt     nor     https://lumidata.eu/46YXXXXXX:training.private/private-in-private.txt     work.</p> </li> </ul> Check this remark only after the solution. <p>So if we can access public objects in both public and private buckets, what is then the difference between both? Well, in a public bucket you can list the objects without using credentials while you cannot in a private bucket.</p> <p>Try either  https://46YXXXXXX.lumidata.eu/training.private or https://lumidata.eu/46YXXXXXX:training.private and notice that you get a cryptic error message.</p> <p>However, try either https://46YXXXXXX.lumidata.eu/training.public or https://lumidata.eu/46YXXXXXX:training.public and you get a much longer answer though again rather cryptic for ordinary people. It is an XML file and if you read through it, you'll find the names of the objects that we know are in the bucket.</p> </li> <li> <p>Create a web link (presigned URL) to share the private object <code>HTML/private.html</code> in the <code>training.private</code> bucket. Next     open a private/incognito browser window and check that the link indeed works (we use a private browser window / incognito mode to     be sure that it doesn't pick up any credentials anywhere just to be sure).</p> Click here to see the solution. <p>For this, we can use the <code>rclone link</code> tool: </p> <pre><code>rclone link lumi-46YXXXXXX-private:training.private/HTML/private.html\n</code></pre> <p>will produce output that will look like this:</p> <pre><code>2024/12/04 21:43:29 NOTICE: S3 bucket training.private path HTML: Public Link: Reducing expiry to 1w as off is greater than the max time allowed\nhttps://lumidata.eu/training.private/HTML/private.html?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=KEII85V27JOJTCGM6XQQ%2F20241204%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20241204T194329Z&amp;X-Amz-Expires=604800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=35a3bc04d997d50471ecd1a6637cd063f9dfb2a40e173f758030d6ced48926bf\n</code></pre> <p>Note that the validity is automatically restricted to 7 days (604800 seconds) which is a limit imposed by LUMI, but the link would actually fail sligthly earlier as the key expires if the lifetime of the key used to create the link, is not extended.</p> <p>One can also set a shorter link lifetime, e.g.,</p> <pre><code>rclone link --expire 2d lumi-46YXXXXXX-private:training.private/HTML/private.html\n</code></pre> <p>which will produce output that will look like this:</p> <pre><code>https://lumidata.eu/training.private/HTML/private.html?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=KEII85V27JOJTCGM6XQQ%2F20241204%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20241204T194645Z&amp;X-Amz-Expires=172800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=055f936dc0c23576cf2ba6211c4465f6f2488501c27a7096d0c9af0925d8f884\n</code></pre> <p>so just the URL without further warning. But if you analyse the URL carefully, you see the field  <code>X-Amz-Expires=172800</code> which indicates that the link expires after 2 days or 172800 seconds.</p> <p>Now just copy the URL and check in a browser...</p> </li> <li> <p>Data sharing example On LUMI, <code>project_462000265</code> is used to store      materials from previous trainings given by LUST, HPE and AMD, and make some of those materials     available on the web. However, you are not part of that project so cannot request an authentication key for that     project. But, as some files are public, you are able to access some buckets and objects in this project with     some tools. We've created two buckets, <code>intro-training.public</code> and <code>intro-training.private</code> with the same contents     and ACLs as the <code>training.public</code> and <code>training.private</code> buckets in the <code>project_46YXXXXXX</code> training project.     Let's see if we can access them with command line tools.</p> <p>List the objects in both buckets.</p> Click here to see the solution. <ul> <li> <p>With <code>s3cmd</code>: </p> <pre><code>s3cmd ls --recursive s3://462000265:intro-training.public/\n</code></pre> <p>returns something along the lines of</p> <pre><code>2025-02-12 14:05   231   s3://462000265:intro-training.public/HTML/public.html\n2025-02-12 14:05   343   s3://462000265:intro-training.public/HTML/shared.html\n2025-02-12 14:05    58   s3://462000265:intro-training.public/private-in-public.txt\n2025-02-12 14:05    57   s3://462000265:intro-training.public/public-in-public.txt\n</code></pre> <p>while</p> <pre><code>s3cmd ls --recursive s3://462000265:intro-training.private/\n</code></pre> <p>returns</p> <pre><code>ERROR: Access to bucket '462000265:intro-training.private' was denied\nERROR: S3 error: 403 (AccessDenied)\n</code></pre> <p>This should not surprise you, as you are not a member of the <code>462000265</code> project and are not using access credentials for that project in this exercise, but for <code>46YXXXXXX</code> training project.</p> <p>Note that in the first command we did list an object whose name suggests that it is a private object.</p> </li> <li> <p>With <code>rclone</code>: </p> <pre><code>rclone ls lumi-46YXXXXXX-private:\"462000265:intro-training.public\"\n</code></pre> <p>returns something along the lines of</p> <pre><code>    231 HTML/public.html\n    343 HTML/shared.html\n    58 private-in-public.txt\n    57 public-in-public.txt\n</code></pre> <p>while</p> <pre><code>rclone ls lumi-46YXXXXXX-private:\"462000265:intro-training.private\"\n</code></pre> <p>returns something similar to</p> <pre><code>2024/12/07 19:39:03 Failed to ls: AccessDenied:\n    status code: 403, request id: tx0000092793a87e000e519-0067548837-61b0c46-lumi-prod, host id:\n</code></pre> <p>so an error (as we would expect, see the comments for the solution with <code>s3cmd</code>)</p> </li> </ul> </li> <li> <p>We continue on the data sharing example.      Can we check the ACLs of the objects in the <code>intro-training.public</code> bucket?</p> Click here to see the solution. <p>For this exercise, <code>s3cmd</code> is our friend.</p> <p>Let's try for <code>public-in-public.txt</code>: The output of</p> <pre><code>s3cmd info s3://462000265:intro-training.public/public-in-public.txt\n</code></pre> <p>actually produces output with an error message. The precise output:</p> <pre><code>File size: 57\nLast mod:  Sat, 07 Dec 2024 17:31:04 GMT\nMIME type: text/plain\nStorage:   STANDARD\nMD5 sum:   db24072368ff20ad202395aa7dd66487\nSSE:       none\nPolicy:    Not available: GetPolicy permission is needed to read the policy\nERROR: Access to bucket '462000265:intro-training.public' was denied\nERROR: S3 error: 403 (AccessDenied)\n</code></pre> <p>The reason is that listing permissions does require more rights than the ones we have in the bucket because even though the bucket itself is actually public to the world, this is not enough to also check permissions.</p> </li> <li> <p>We continue on the data sharing example.      Download the <code>HTML/public.html</code> from the <code>intro-training.public</code> bucket in <code>project_465000265</code>.     We couldn't check in the previous exercise, but this is actually a public object in a public     bucket.     Can you do so with a web browser also (or the <code>wget</code> or <code>curl</code> commands if you are familiar     with them)?</p> Click here to see the solution. <ul> <li> <p>With <code>s3cmd</code>:</p> <pre><code>s3cmd get s3://462000265:intro-training.public/HTML/public.html\n</code></pre> </li> <li> <p>With <code>rclone</code>:</p> <pre><code>rclone copy lumi-46YXXXXXX-private:\"462000265:intro-training.public/HTML/public.html\" .\n</code></pre> </li> <li> <p>With a web browser: both the URL     https://462000265.lumidata.eu/intro-training.public/HTML/public.html     and      https://lumidata.eu/462000265:intro-training.public/HTML/public.html     work.</p> </li> <li> <p>With the <code>wget</code> command: both</p> <pre><code>wget https://462000265.lumidata.eu/intro-training.public/HTML/public.html\n</code></pre> <p>and</p> <pre><code>wget https://lumidata.eu/462000265:intro-training.public/HTML/public.html\n</code></pre> <p>work.</p> </li> <li> <p>With the <code>curl</code> command: Both</p> <pre><code>curl https://462000265.lumidata.eu/intro-training.public/HTML/public.html\n</code></pre> <p>and</p> <pre><code>curl https://lumidata.eu/462000265:intro-training.public/HTML/public.html\n</code></pre> <p>will print the content of the file on the terminal.</p> <pre><code>curl -o public.html https://462000265.lumidata.eu/intro-training.public/HTML/public.html\n</code></pre> <p>would store the result in the file <code>public.html</code> in the current directory.</p> </li> </ul> </li> <li> <p>Should you want to clean up your space on LUMI-O and remove the buckets used in the      exercise, there is a script for that.</p> <p>In the directory with LUST exercises that were downloaded following the instructions to set up for the exercises, go into the <code>ObjectStorage</code> directory. In that directory, run the <code>delete_buckets.sh</code> script with your project number <code>46YXXXXXX</code> as the argument:</p> <pre><code>./delete_buckets.sh 46YXXXXXX\n</code></pre> <p>The buckets should now be removed.</p> </li> </ol>"},{"location":"intro-evolving/M01-Architecture/","title":"LUMI Architecture","text":"<p>Some insight in the hardware of LUMI is necessary to understand what LUMI can do and what it cannot do, and to understand how an application can be mapped upon the machine for optimal performance.</p>"},{"location":"intro-evolving/M01-Architecture/#materials","title":"Materials","text":"<ul> <li>Slides</li> <li>Notes</li> </ul>"},{"location":"intro-evolving/M01-Architecture/#similar-presentations","title":"Similar presentations","text":"<ul> <li>\"LUMI Architecture\" talk in the 2-day LUST introductory training in Amsterdam, May 2024</li> </ul>"},{"location":"intro-evolving/M02-CPE/","title":"HPE Cray Programming Environment","text":"<p>As Linux itself is not a complete supercomputer operating system, many components that are essential for the proper functioning of a supercomputer are separate packages (such as the Slurm scheduler discussed later in this course)  or part of programming environments.  It is important to understand the consequences of this, even if all you want is to simply run a program.</p>"},{"location":"intro-evolving/M02-CPE/#materials","title":"Materials","text":"<ul> <li>Slides</li> <li>Notes</li> </ul>"},{"location":"intro-evolving/M02-CPE/#similar-presentations","title":"Similar presentations","text":"<ul> <li>\"HPE Cray Programming Environment\" talk in the 2-day LUST introductory training in Amsterdam, May 2024</li> </ul>"},{"location":"intro-evolving/M03-Access/","title":"Getting Access to LUMI","text":"<p>We discuss the options to log on to LUMI and to transfer data.</p>"},{"location":"intro-evolving/M03-Access/#materials","title":"Materials","text":"<ul> <li>Slides</li> <li>Notes</li> <li>Exercises</li> </ul>"},{"location":"intro-evolving/M03-Access/#similar-presentations","title":"Similar presentations","text":"<ul> <li>\"Getting Access to LUMI\" talk in the 2-day LUST introductory training in Amsterdam, May 2024</li> </ul>"},{"location":"intro-evolving/M04-Modules/","title":"Modules on LUMI","text":"<p>LUMI uses Lmod, but as Lmod can be configured in different ways, even an experienced Lmod user can learn from this presentation how we use modules on LUMI and how modules can be found.</p>"},{"location":"intro-evolving/M04-Modules/#materials","title":"Materials","text":"<ul> <li>Slides</li> <li>Notes</li> <li>Exercises</li> </ul>"},{"location":"intro-evolving/M04-Modules/#similar-presentations","title":"Similar presentations","text":"<ul> <li>\"Modules on LUMI\" talk in the 2-day LUST introductory training in Amsterdam, May 2024</li> </ul>"},{"location":"intro-evolving/M05-SoftwareStacks/","title":"LUMI Software Stacks","text":"<p>In this presentation we discuss how application software is made available to users of LUMI. For users of smaller Tier-2 clusters with large support teams compared to the user base of the machine, the approach taken on LUMI may be a bit unusual...</p>"},{"location":"intro-evolving/M05-SoftwareStacks/#materials","title":"Materials","text":"<ul> <li>Slides</li> <li>Notes</li> <li>Exercises</li> </ul>"},{"location":"intro-evolving/M05-SoftwareStacks/#similar-presentations","title":"Similar presentations","text":"<ul> <li>\"LUMI Software Stacks\" talk in the 2-day LUST introductory training in Amsterdam, May 2024</li> </ul>"},{"location":"intro-evolving/M06-Support/","title":"LUMI Support and Documentation","text":"<p>Where can I find documentation or get training, and which support services are  available for what problems? And how can I formulate a support ticket so that I can get a quick answer without much back-and-forth mailing?</p>"},{"location":"intro-evolving/M06-Support/#materials","title":"Materials","text":"<ul> <li>Slides</li> <li>Notes</li> </ul>"},{"location":"intro-evolving/M06-Support/#similar-presentations","title":"Similar presentations","text":"<ul> <li> <p>\"LUMI Support and Documentation\" talk in the 2-day LUST introductory training in Amsterdam, May 2024</p> <p>This is a generic presentation however that misses all Belgium-specific information.</p> </li> </ul>"},{"location":"intro-evolving/M07-Slurm/","title":"Slurm on LUMI","text":"<p>Slurm is the batch job scheduler used on LUMI. As no two Slurm configurations are identical, even an experienced Slurm user should have a quick look at the notes of this talk to understand the particular configuration on LUMI.</p>"},{"location":"intro-evolving/M07-Slurm/#materials","title":"Materials","text":"<ul> <li>Slides</li> <li>Notes</li> <li>Exercises</li> </ul>"},{"location":"intro-evolving/M07-Slurm/#similar-presentations","title":"Similar presentations","text":"<ul> <li>\"Slurm on LUMI\" talk in the 2-day LUST introductory training in Amsterdam, May 2024</li> </ul>"},{"location":"intro-evolving/M08-Binding/","title":"Process and Thread Distribution and Binding","text":"<p>To get good performance on hardware with a strong hierarchy as AMD EPYC processors and GPUs, it is important to map processes and threads properly on the hardware. This talk discusses the various mechanisms available on LUMI for this.</p>"},{"location":"intro-evolving/M08-Binding/#materials","title":"Materials","text":"<ul> <li>Slides</li> <li>Notes</li> <li>Exercises</li> </ul>"},{"location":"intro-evolving/M08-Binding/#similar-presentations","title":"Similar presentations","text":"<ul> <li>\"Process and Thread Distribution and Binding\" talk in the 2-day LUST introductory training in Amsterdam, May 2024</li> </ul>"},{"location":"intro-evolving/M09-Lustre/","title":"Using Lustre","text":"<p>Lustre is a parallel file system and the main file system on LUMI. It is important to realise what the strengths and weaknesses of Lustre at the scale of a machine as LUMI are and how to use it properly and not disturb the work of other users.</p>"},{"location":"intro-evolving/M09-Lustre/#materials","title":"Materials","text":"<ul> <li>Slides</li> <li>Notes</li> </ul>"},{"location":"intro-evolving/M09-Lustre/#similar-presentations","title":"Similar presentations","text":"<ul> <li> <p>\"I/O and File Systems on LUMI\" talk in the 2-day LUST introductory training in Amsterdam, May 2024</p> <p>Some of the material is covered in the \"Getting Access to LUMI\" section of this tutorial</p> </li> </ul>"},{"location":"intro-evolving/M10-ObjectStorage/","title":"LUMI-O Object Storage","text":"<p>LUMI also has an object storage system. It is useful as a staging location to transfer data to LUMI, but some programs may also benefit from accessing the  object storage directly. We highlight the differences between a parallel filesystem such as Lustre and object storage, and discuss how LuMI-O can be accessed.</p>"},{"location":"intro-evolving/M10-ObjectStorage/#materials","title":"Materials","text":"<ul> <li>Slides</li> <li>Notes</li> </ul>"},{"location":"intro-evolving/M11-Containers/","title":"Containers on LUMI-C and LUMI-G","text":"<p>Containers are a way on LUMI to deal with the too-many-small-files software installations on LUMI, e.g., large Python or Conda installations. They are also a  way to install software that is hard to compile, e.g., because no source code is available or because there are simply too many dependencies.</p>"},{"location":"intro-evolving/M11-Containers/#materials","title":"Materials","text":"<ul> <li>Slides</li> <li>Notes</li> </ul>"},{"location":"intro-evolving/M11-Containers/#similar-presentations","title":"Similar presentations","text":"<ul> <li>\"Containers on LUMI-C and LUMI-G\" talk in the 2-day LUST introductory training in Amsterdam, May 2024</li> </ul>"},{"location":"intro-evolving/schedule/","title":"Example schedule","text":"<ul> <li>Day 1 <li>Day 2 DAY 1 - ADD DATE              09:15 CEST          Welcome and Introduction              09:30 CEST          LUMI Architecture Some insight in the hardware of LUMI is necessary to understand what         LUMI can do and what it cannot do, and to understand how an application can         be mapped upon the machine for optimal performance.                       10:15 CEST          HPE Cray Programming Environment As Linux itself is not a complete supercomputer operating system, many components         that are essential for the proper functioning of a supercomputer are separate packages         (such as the Slurm scheduler discussed on day 2) or part of programming environments.          It is important to understand the consequences of this, even if all you want is to simply         run a program.                       11:15 CEST          Break and networking (30 minutes)              11:45 CEST          Getting Access to LUMI We discuss the options to log on to LUMI and to transfer data.                       12:30 CEST          Lunch break (60 minutes)              13:30 CEST          Exercises (session #1)              14:00 CEST          Modules on LUMI          LUMI uses Lmod, but as Lmod can be configured in different ways, even an experienced         Lmod user can learn from this presentation how we use modules on LUMI and how         modules can be found.                       14:40 CEST          Exercises (session #2)              15:00 CEST          Break and networking (20 minutes)              15:20 CEST          LUMI Software Stacks In this presentation we discuss how application software is made available to         users of LUMI. For users of smaller Tier-2 clusters with large support teams compared         to the user base of the machine, the approach taken on LUMI may be a bit unusual...                       16:20 CEST          Exercises (session #3)              16:50 CEST          Wrap-up of the day              17:00 CEST          Free Q&amp;A              17:30 CEST          End of day 1 DAY 2 - ADD DATE              09:15 CEST          Short welcome, recap and plan for the day              09:30 CEST          Slurm on LUMI Slurm is the batch job scheduler used on LUMI. As no two Slurm configurations are         identical, even an experienced Slurm user should have a quick look at the notes of this         talk to understand the particular configuration on LUMI.                       11:00 CEST          Break and networking (30 minutes)              11:30 CEST          Process and Thread Distribution and Binding To get good performance on hardware with a strong hierarchy as AMD EPYC processors and         GPUs, it is important to map processes and threads properly on the hardware. This talk discusses         the various mechanisms available on LUMI for this.                       12:30 CEST          Lunch break (60 minutes)              13:30 CEST          Exercises (session #4)              14:00 CEST          Using Lustre Lustre is a parallel file system and the main file system on LUMI.         It is important to realise what the strengths and weaknesses of Lustre at the         scale of a machine as LUMI are and how to use it properly and not disturb the         work of other users.                       14:30 CEST          Containers on LUMI-C and LUMI-G Containers are a way on LUMI to deal with the too-many-small-files software         installations on LUMI, e.g., large Python or Conda installations. They are also a          way to install software that is hard to compile, e.g., because no source code is         available or because there are simply too many dependencies.                       15:30 CEST          Break and networking (30 minutes)              16:00 CEST          LUMI Support and Documentation Where can I find documentation or get training, and which support services are          available for what problems? And how can I formulate a support ticket so that I can         get a quick answer without much back-and-forth mailing?                       16:30 CEST          SPACE FOR A LOCAL TALK              16:45 CEST          What Else?         A brief discussion about what else LUST offers, what is not covered in this course,         and how you can learn about it.              17:00 CEST          Free Q&amp;A              17:30 CEST          End of day 2"}]}