{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LUMI-BE training and event materials","text":"<p>Welcome to the future site of training materials of LUMI-BE,  the Belgian participation to LUMI.</p> <ul> <li>Continuously evolving version of the 1-day intro training materials</li> </ul>"},{"location":"intro-evolving/","title":"LUMI 1-day training evolving version","text":"<p>This document includes the base materials for the 1-day training that we provide to Belgian LUMI users. We try to evolve it as soon as possible when it becomes outdated due to changes on LUMI, and it is certainly more up-to-date than any other version you may find on this site in the future.</p> <p>Short URL: https://klust.github.io/intro-evolving.</p>"},{"location":"intro-evolving/#setting-up-for-the-exercises","title":"Setting up for the exercises","text":"<p>If you have an active project on LUMI, you should be able to make the exercises in that project. You will only need an very minimum of CPU and GPU billing units for this.</p> <ul> <li> <p>Create a directory in the scratch of your project, or if you want to     keep the exercises around for a while, in a subdirectory of your project directory      or in your home directory (though we don't recommend the latter).     Then go into that directory.</p> <p>E.g., in the scratch directory of your project:</p> <pre><code>mkdir -p /scratch/project_465XXXXXX/$USER/exercises\ncd /scratch/project_465XXXXXX/$USER/exercises\n</code></pre> <p>where you have to replace <code>project_465XXXXXX</code> using the number of your own project.</p> </li> <li> <p>Now download the exercises and un-tar:</p> <pre><code>wget https://465000095.lumidata.eu/training-materials-web/intro-evolving/files/exercises-evolving.tar.gz\ntar -xf exercises-evolving.tar.gz\n</code></pre> <p>Link to the tar-file with the exercises</p> </li> <li> <p>You're all set to go!</p> </li> </ul>"},{"location":"intro-evolving/#downloads","title":"Downloads","text":"<p>As the course is still under development, some links are still dead.</p> Presentation Slides Notes recording Introduction / notes / LUMI Architecture slides notes / HPE Cray Programming Environment slides notes / Getting access to LUMI slides notes / Modules on LUMI slides notes / LUMI Software Stacks slides notes / Exercises 1 / notes / Slurm on LUMI slides notes / Binding resources slides notes / Exercises 2 / notes / Using Lustre slides notes / Containers on LUMI slides notes / LUMI User Support slides notes / A1 Slurm issues / notes / A2 Additional documentation / notes /"},{"location":"intro-evolving/#web-links","title":"Web links","text":"<ul> <li> <p>Links to additional HPE Cray documentation</p> </li> <li> <p>LUMI documentation</p> <ul> <li> <p>Main documentation</p> </li> <li> <p>Shortcut to the LUMI Software Library</p> </li> </ul> </li> <li> <p>Clusters in Belgium</p> <ul> <li> <p>VSC documentation</p> </li> <li> <p>C\u00c9CI documentation</p> </li> <li> <p>Lucia@Cenaero documentation</p> </li> </ul> </li> <li> <p>LUMI training resources</p> <ul> <li> <p>LUMI training announcements</p> </li> <li> <p>LUMI training materials archive</p> </li> </ul> </li> <li> <p>Local (BE) trainings</p> <ul> <li> <p>VSC training page</p> <ul> <li> <p>Local materials UAntwerpen</p> </li> <li> <p>Local materials VUB</p> </li> <li> <p>Local materials UGent</p> </li> </ul> </li> <li> <p>C\u00c9CI HPC training page</p> <p>Training materials of previous sessions are available via the  CISM indico event management system.  For each training, go to the \"Contribution List\" and then to the presentation itself, and training materials are available at the bottom of the page.</p> <p>Recordings of several presentations are available on the  C\u00c9CI and CISM HPC YouTube Channel</p> </li> <li> <p>EuroCC Belgium training page</p> </li> </ul> </li> <li> <p>Other course materials</p> <ul> <li>Archive of PRACE training materials (no https unfortunately)</li> </ul> </li> </ul>"},{"location":"intro-evolving/00_Introduction/","title":"Introduction to the notes","text":"<p>These notes and training materials were prepared in the framework of the  VSC Tier-0 support activities.</p> <p>The notes, and all other materials used for the course, borrow a lot from the course material made available by the LUMI User Support Team. Their course materials are avialable on the LUMI training materias web site.</p> <p>Various training materials and the documentation from the Walloon HPC project C\u00c9CI were also a great source of inspiration.</p> <p>This course is offered to the LUMI-BE organisation.</p>"},{"location":"intro-evolving/00_Introduction/#about-the-structure-of-the-notes","title":"About the structure of the notes","text":"<p>Colour coding and boxes in the material:</p> <p>Remark</p> <p>This is a remark: Some additional information that may be nice to read. or some additional information that you may want to have a look at.</p> <p>Note</p> <p>Just a quick note on the side, but do have a look at it.</p> <p>Audience</p> <p>A box telling you who this part of the notes is written for, or why it would be good to read it even if you think you don't need it.</p> <p>Lumi-be</p> <p>This is used to point out differences with local infrastructures in Belgium and is less useful if you are not familiar with any of the HPC clusters in Belgium.</p> <p>Example</p> <p>An example to make the material clearer or to try something out.</p> <p>Exercise</p> <p>An exercise</p> <p>Solution</p> <p>The solution to the exercise. You will have to click on the box to see the solution.</p> <p>Bug</p> <p>This is a discussion about a bug.</p> <p>Nice-to-know</p> <p>This is a little fact which is nice-to-know but not necessary to understand the material.</p> <p>Intermediate</p> <p>Information that may not be useful to every LUMI user, but if you are the kind of person who likes to explore manuals and try out things that we did not discuss in the course, you may want to read this...</p> <p>Advanced</p> <p>Similar to the intermediate material, but it requires even more technical skills to understand this stuff.</p> <p>Technical</p> <p>Material specific to readers with very technical interests.</p>"},{"location":"intro-evolving/01_Architecture/","title":"The LUMI Architecture","text":"<p>In this presentation, we will build up LUMI part by part, stressing those aspects that are important to know to run on LUMI efficiently and define jobs that can scale.</p>"},{"location":"intro-evolving/01_Architecture/#why-do-i-kneed-to-know-this","title":"Why do I kneed to know this?","text":"<p>You may wonder why you need to know about system architecture if all you want to do is to run  some programs.</p> <p>A supercomputer is not simply a scaled-up smartphone or PC that will offer good performance automatically.  It is a shared infrastructure and you don't get the whole machine to yourself. Instead you have to request a suitable fraction of the computer for the work you want to do. But it is also a very expensive infrastructure, with an investment of 160M EURO for LUMI and an estimated total cost (including operations) of 250M EURO. So it is important to use the computer efficiently. </p> <p>And that efficiency comes not for free. Instead in most cases it is important to properly map an  application on the available resources to run efficiently.  The way an application is developed is important for this, but it is not the only factor. Every application needs some user help  to run in the most efficient way, and that requires an understanding of</p> <ol> <li> <p>The hardware architecture of the supercomputer, which is something that we discuss in this     section.</p> </li> <li> <p>The middleware: the layers of software that sit between the application on one hand and the     hardware and operating system on the other hand. LUMI runs a sligthly modified version of Linux.     But Linux is not a supercomputer operating system. Missing functionality in Linux is offered     by other software layers instead that on supercomputers often come as part of the programming     environment.     This is a topic of discussion in several sessions of this course.</p> </li> <li> <p>The application. This is very domain-specific and application-specific and hence cannot be the     topic of a general course like this one. In fact, there are so many different applications and     often considerable domain knowledge is required so that a small support team like the one of      LUMI cannot provide that information. </p> </li> <li> <p>Moreover, the way an application should be used may even depend on the particular problem that you     are trying to solve. Bigger problems, bigger computers, and different settings may be needed in     the application.</p> <p>It is up to scientific communities to organise trainings that teach you individual applications and how to use them for different problem types, and then up to users to combine the knowledge of an application obtained from such a course with the knowledge about the computer you want to use and its middleware obtained from courses such as this one or our 4-day more advanced course.</p> </li> </ol> <p>Some users expect that a support team can give answers to all those questions, even to the third and fourth bullet of the above list. If a support team could do that, it would basically imply that they could simply do all the research that users do and much faster as they are assumed to have the answer ready in hours...</p>"},{"location":"intro-evolving/01_Architecture/#lumi-is","title":"LUMI is ...","text":"<p>LUMI is a pre-exascale supercomputer, and not a superfast PC nor a compute cloud architecture.</p> <p>Each of these architectures have their own strengths and weaknesses and offer different  compromises and it is key to chose the right infrastructure for the job and use the right  tools for each infrastructure.</p> <p>Just some examples of using the wrong tools or infrastructure:</p> <ul> <li> <p>The single thread performance of the CPU is lower than on a high-end PC.      We've had users who were disappointed about the speed of a single core and were expecting     that this would be much faster than their PCs. Supercomputers however are optimised for      performance per Watt and get their performance from using lots of cores through well-designed     software. If you want the fastest core possible, you'll need a gaming PC.</p> <p>E.g., the AMD 5800X is a popular CPU for high end gaming PCs using the same core architecture  as the CPUs in LUMI. It runs at a base clock of 3.8 GHz and a boost clock of 4.7 GHz if only one core is used and the system has proper cooling. The 7763 used in the compute nodes of LUMI-C runs at a base clock of 2.45 GHz and a boost clock of 3.5 GHz. If you have only one single core job to run on your PC, you'll be able to reach that boost clock while on LUMI you'd probably need to have a large part of the node for yourself, and even then the performance for jobs that are not memory bandwidth limited will be lower than that of the gaming PC.</p> </li> <li> <p>For some data formats the GPU performance may be slower also than on a high end gaming PC.     This is even more so because     an MI250x should be treated as two GPUs for most practical purposes. The better double precision     floating point operations and matrix operations, also at full precision, require transistors also      that on some other GPUs are used for rendering hardware or for single precision compute units.</p> <p>E.g., a single GPU die of the MI250X (half a GPU) has a peak FP32 performance at the boost clock of almost 24 TFlops or 48 TFlops in the packed format which is actually hard for a compiler to exploit, while the high-end AMD graphics GPU RX 7900 XTX claims 61 TFlops at the boost clock. But the FP64 performance of one MI250X  die is also close to 24 TFlops in vector math, while the RX 7900 XTX does less than 2 TFlops in that data format which is important for a lot of scientific computing applications.</p> </li> <li> <p>Compute GPUs and rendering GPUs are different beasts these days.     We had a user who wanted to use the ray tracing units to do rendering. The MI250X does not     have texture units or ray tracing units though. It is not a real graphics processor anymore.</p> </li> <li> <p>The environment is different also. It is not that because it runs some Linux it handles are your     Linux software.     A user complained that they did not succeed in getting their nice remote development environment to     work on LUMI. The original author of these notes took a test license and downloaded a trial version.     It was a very nice environment but really made for local development and remote development in a      cloud environment with virtual machines individually protected by personal firewalls and was      not only hard to get working on a supercomputer but also insecure.</p> </li> <li> <p>And supercomputer need proper software that exploits the strengths and works around the weaknesses     of their architecture.     CERN came telling on a EuroHPC Summit Week before the COVID pandemic that they would start using more     HPC and less cloud and that they expected a 40% cost reduction that way. A few years later they     published a paper with their experiences and it was mostly disappointment. The HPC infrastructure     didn't fit their model for software distribution and performance was poor. Basically their solution     was designed around the strengths of a typical cloud infrastructure and relied precisely on those things     that did make their cloud infrastructure more expensive than the HPC infrastructure they tested. It relied     on fast local disks that require a proper management layer in the software, (ab)using the file system as     a database for unstructured data, a software distribution mechanism that requires an additional daemon     running permanently on the compute nodes (and local storage on those nodes), ...</p> </li> </ul> <p>True supercomputers, and LUMI in particular, are built for scalable parallel applications and features that are found on smaller clusters or on workstations that pose a threat to scalability are removed from the system. It is also a shared infrastructure but with a much more lightweight management layer than a cloud infrastructure and far less isolation between users, meaning that abuse by one user can have more of a negative impact on  other users than in a cloud infrastructure. Supercomputers since the mid to late '80s are also built according to the principle of trying to reduce the hardware cost by using cleverly designed software both at the system and application level. They perform best when streaming data through the machine at all levels of the  memory hierarchy and are not built at all for random access to small bits of data (where the definition of \"small\" depends on the level in the memory hierarchy).</p> <p>At several points in this course you will see how this impacts what you can do with a supercomputer and how you work with a supercomputer.</p>"},{"location":"intro-evolving/01_Architecture/#lumi-spec-sheet-a-modular-system","title":"LUMI spec sheet: A modular system","text":"<p>So we've already seen that LUMI is in the first place a EuroHPC pre-exascale machine. LUMI is built to prepare for the exascale era and to fit in the EuroHPC ecosystem.  But it does not even mean that it has to cater to all pre-exascale compute needs. The EuroHPC JU tries to build systems that have some flexibility, but also does not try to cover  all needs with a single machine. They are building 3 pre-exascale systems with different architecture to explore multiple architectures and to cater to a more diverse audience. LUMI is an AMD GPU-based supercomputer,  Leonardo uses NVIDIA H100 GPUS and also has a CPU section with nodes with some  high-bandwidth memory, and MareNostrum5 has a very large CPU section besides an NVIDIA GPU section.</p> <p>LUMI is also a very modular machine designed according to the principles explored in a series of European projects, and in particular DEEP and its successors) that explored the cluster-booster concept. E.g., in a complicated multiphysics simulation  you could be using regular CPU nodes for the physics that cannot be GPU-accelerated communicating with compute GPU nodes for the physics that can be GPU-accelerated, then add a number of CPU nodes to do the I/O and a specialised render GPU node for in-situ visualisation.</p> <p>LUMI is in the first place a huge GPGPU supercomputer. The GPU partition of LUMI, called LUMI-G, contains 2978 nodes with a single 64-core AMD EPYC 7A53 CPU and 4 AMD MI250x GPUs. Each node has 512 GB of RAM attached to the CPU (the maximum the CPU can handle without compromising bandwidth) and 128 GB of HBM2e memory per GPU. Each GPU node has a theoretical peak performance of nearly 200 TFlops in single (FP32) or double (FP64) precision vector arithmetic (and twice that with the packed FP32 format, but that  is not well supported so this number is not often quoted). The matrix units are capable of about 400 TFlops in FP32 or FP64. However, compared to the NVIDIA GPUs, the performance for lower precision formats used in some AI applications is not that stellar.</p> <p>LUMI also has a large CPU-only partition, called LUMI-C, for jobs that do not run well on GPUs, but also integrated enough with the GPU partition that it is possible to have applications that combine both node types. LUMI-C consists of 2048 nodes with 2 64-core AMD EPYC 7763 CPUs. 32 of those nodes have 1TB of RAM (with some of these nodes actually reserved for special purposes such as connecting to a Quantum computer), 128 have 512 GB and 1888 have 256 GB of RAM.</p> <p>LUMI also has two smaller groups of nodes for interactive data analytics.  8 of those nodes have two  64-core Zen2/Rome CPUs with 4 TB of RAM per node, while 8 others have dual 64-core Zen2/Rome CPUs and 8 NVIDIA A40 GPUs for visualisation.  There is also an Open OnDemand based service (web interface) to make some fo those facilities available. Note though that these nodes are meant for a very specific use, so it is not that we will also be offering, e.g., GPU compute facilities on NVIDIA hardware, and that these are shared resources that should not be monopolised by a single user (so no hope to run an MPI job on 8 4TB nodes).</p> <p>LUMI also has a 8 PB flash based file system running the Lustre parallel file system. This system is often denoted as LUMI-F. The bandwidth of that system is over 2 TB/s.  Note however that this is still a remote file system with a parallel file system on it, so do not expect that it will behave as the local SSD in your laptop.  But that is  also the topic of another session in this course.</p> <p>The main work storage is provided by 4 20 PB hard disk based Lustre file systems with a bandwidth of 240 GB/s each. That section of the machine is often denoted  as LUMI-P. </p> <p>Big parallel file systems need to be used in the proper way to be able to offer the performance that one would expect from their specifications. This is important enough that  we have a separate session about that in this course.</p> <p>An object based file system similar to the Allas service of CSC that some of the Finnish users may be familiar with is also being worked on. At the  moment the interface to that system is still rather primitive.</p> <p>Currently LUMI has 4 login nodes for ssh access, called user access nodes in the HPE Cray world. They each have 2 64-core AMD EPYC 7742 processors and 1 TB of RAM. Note that  whereas the GPU and CPU compute nodes have the Zen3 architecture code-named \"Milan\", the processors on the login nodes are Zen2 processors, code-named \"Rome\". Zen3 adds some new instructions so if a compiler generates them, that code would not run on the login nodes. These instructions are basically used in cryptography though. However, many instructions have very different latency, so a compiler that optimises specifically for Zen3 may chose another ordering of instructions then when optimising for Zen2 so it may still make sense to compile specifically for the compute nodes on LUMI.</p> <p>There are also some additional login nodes for access via the web-based Open OnDemand interface.</p> <p>All compute nodes, login nodes and storage are linked together through a  high-performance interconnect. LUMI uses the Slingshot 11 interconnect which is developed by HPE Cray, so not the Mellanox/NVIDIA InfiniBand that you may be familiar with from many smaller clusters, and as we shall discuss later this also influences how you work on LUMI.</p> <p>Early on a small partition for containerised micro-services managed with Kubernetes was also planned, but that may never materialize due to lack of  people to set it up and manage it.</p> <p>In this section of the course we will now build up LUMI step by step.</p>"},{"location":"intro-evolving/01_Architecture/#building-lumi-the-cpu-amd-7xx3-milanzen3-cpu","title":"Building LUMI: The CPU AMD 7xx3 (Milan/Zen3) CPU","text":"<p>The LUMI-C and LUMI-G compute nodes use third generation AMD EPYC CPUs. Whereas Intel CPUs launched in the same period were built out of a single large monolithic piece of silicon (that only changed recently with some variants of the Sapphire Rapids CPU launched in early 2023), AMD CPUs are made up of multiple so-called chiplets. </p> <p>The basic building block of Zen3 CPUs is the Core Complex Die (CCD). Each CCD contains 8 cores, and each core has 32 kB of L1 instruction  and 32 kB of L1 data cache, and 512 kB of L2 cache. The L3 cache is shared across all cores on a chiplet and has a total size of 32 MB on LUMI (there are some variants of the processor where this is 96MB). At the user level, the instruction set is basically equivalent to that of the Intel Broadwell generation. AVX2 vector instructions and the FMA instruction are fully supported, but there is no support for any of the AVX-512 versions that can be found on Intel Skylake server processors and later generations. Hence the number of floating point operations that a core can in theory do each clock cycle is 16 (in  double precision) rather than the 32 some Intel processors are capable of. </p> <p></p> <p>The full processor package for the AMD EPYC processors used in LUMI have 8 such Core Complex Dies for a total of 64 cores. The caches are not shared between different CCDs, so it also implies that the processor has 8 so-called L3 cache regions. (Some cheaper variants have only 4 CCDs, and some have CCDs with only 6 or fewer cores enabled but the same 32 MB of L3 cache per CCD).</p> <p>Each CCD connects to the memory/IO die through an Infinity Fabric link.  The memory/IO die contains the memory controllers, connections to connect two CPU packages together, PCIe lanes to connect to external hardware, and some additional hardware, e.g., for managing the processor.  The memory/IO die supports 4 dual channel DDR4 memory controllers providing  a total of 8 64-bit wide memory.  From a logical point of view the memory/IO-die is split in 4 quadrants, with each quadrant having a dual channel memory controller and 2 CCDs. They basically act as 4 NUMA domains. For a core it is slightly faster to access memory in its own quadrant than memory attached to another quadrant, though for the 4 quadrants within the same socket the difference is small. (In fact, the BIOS can be set to show only two or one NUMA domain which is advantageous in some cases, like the typical load pattern of login nodes where it is impossible to nicely spread processes and their memory across the 4 NUMA domains).</p> <p>The theoretical memory bandwidth of a complete package is around 200 GB/s. However, that bandwidth is not available to a single core but can only be used if enough  cores spread over all CCDs are used.</p> <p>Clusters in Belgium</p> <p>The CPUs used in the LUMI-C compute nodes are identical to those used in  the Cenaero/C\u00c9CI cluster lucia or the Milan partition of the VSC cluster  hortense. The UGent VSC cluster gallade uses a very similar processor also but with more cache per CCD. Some other cluster, e.g., accelgor and doduo+  at UGent or the Milan partition of vaughan in at UAntwerpen, also use CPUs of this generation but with only 4 CCDs per processor and/or 6 active cores  per CCD. But many topics that we cover in this course also applies to those clusters.</p> <p>Some other clusters, e.g., the older Rome partition of the VSC cluster hortense, the C\u00c9CI cluster NIC5 at ULi\u00e8ge or the older main partition of the VSC cluster vaughan at UAntwerpen, use the older Zen2/Rome CPUs which are also used on the login nodes of LUMI. These have two groups of 4 cores each with their own separated L3 cache per CCD and 4 or 8 CCDs per  socket. </p>"},{"location":"intro-evolving/01_Architecture/#building-lumi-a-lumi-c-node","title":"Building LUMI: a LUMI-C node","text":"<p>A compute node is then built out of two such processor packages, connected  through 4 16-bit wide Infinity Fabric connections with a total theoretical bandwidth of 144 GB/s in each direction. So note that the bandwidth in each direction is less than the memory bandwidth of a socket. Again, it is not really possible to use the full memory bandwidth of a node using just cores on a single socket. Only one of the two sockets has a direct connection to the high performance Slingshot interconnect though.</p>"},{"location":"intro-evolving/01_Architecture/#a-strong-hierarchy-in-the-node","title":"A strong hierarchy in the node","text":"<p>As can be seen from the node architecture in the previous slide, the CPU compute nodes have a very hierarchical architecture. When mapping an application onto  one or more compute nodes, it is key for performance to take that hierarchy into account. This is also the reason why we will pay so much attention to thread and process pinning in this tutorial course.</p> <p>At the coarsest level, each core supports two hardware threads (what Intel calls hyperthreads). Those hardware threads share all the resources of a core, including the  L1 data and instruction caches and the L2 cache, execution units and space for register renaming.  At the next level, a Core Complex Die contains (up to) 8 cores. These cores share the L3 cache and the link to the memory/IO die.  Next, as configured on the LUMI compute nodes, there are 2 Core Complex Dies in a NUMA node. These two CCDs share the DRAM channels of that NUMA node. At the fourth level in our hierarchy 4 NUMA nodes are grouped in a socket. Those 4  nodes share an inter-socket link. At the fifth and last level in our shared memory hierarchy there are two sockets in a node. On LUMI, they share a single Slingshot inter-node link.</p> <p>The finer the level (the lower the number), the shorter the distance and hence the data delay is between threads that need to communicate with each other through the memory hierarchy, and the higher the bandwidth.</p> <p>This table tells us a lot about how one should map jobs, processes and threads onto a node. E.g., if a process has fewer then 8 processing threads running concurrently, these should be mapped to cores on a single CCD so that they can share  the L3 cache, unless they are sufficiently independent of one another, but even in the latter case the additional cores on those CCDs should not be used by other processes as they may push your data out of the cache or saturate the link to the memory/IO die and hence slow down some threads of your process. Similarly, on a 256 GB compute node each NUMA node has 32 GB of RAM (or actually a bit less as the OS also needs memory, etc.), so if you have a job that uses 50 GB of memory but only, say, 12 threads, you should really have two NUMA nodes reserved for that job as otherwise other threads or processes running on cores in those NUMA nodes could saturate some resources needed by your job. It might also be preferential to spread those 12 threads over the 4  CCDs in those 2 NUMA domains unless communication through the L3 threads would be the bottleneck in your application.</p>"},{"location":"intro-evolving/01_Architecture/#hierarchy-delays-in-numbers","title":"Hierarchy: delays in numbers","text":"<p>This slide shows the ACPI System Locality distance Information Table (SLIT) as returned by, e.g., <code>numactl -H</code> which gives relative distances to memory from a core. E.g., a value of 32 means that access takes 3.2x times the  time it would take to access memory attached to the same NUMA node.  We can see from this table that the penalty for accessing memory in  another NUMA domain in the same socket is still relatively minor (20%  extra time), but accessing memory attached to the other socket is a lot  more expensive. If a process running on one socket would only access memory attached to the other socket, it would run a lot slower which is why Linux has mechanisms to try to avoid that, but this cannot be done in all scenarios which is why on some clusters you will be allocated cores in proportion to the amount of memory you require, even if that is more cores than you really need (and you will be billed for them).</p>"},{"location":"intro-evolving/01_Architecture/#building-lumi-concept-lumi-g-node","title":"Building LUMI: Concept LUMI-G node","text":"<p>This slide shows a conceptual view of a LUMI-G compute node. This node is unlike any Intel-architecture-CPU-with-NVIDIA-GPU compute node you may have  seen before, and rather mimics the architecture of the USA pre-exascale machines Summit and Sierra which have IBM POWER9 CPUs paired with  NVIDIA V100 GPUs.</p> <p>Each GPU node consists of one 64-core AMD EPYC CPU and 4 AMD MI250x GPUs.  So far nothing special. However, two elements make this compute node very special. First, the GPUs are not connected to the CPU though a PCIe bus. Instead they are connected through the same links that AMD uses to link the GPUs together, or to link the two sockets in the LUMI-C compute nodes, known as xGMI or Infinity Fabric. This enables unified memory across CPU and GPUS and  provides partial cache coherency across the system. The CPUs coherently cache the CPU DDR and GPU HBM memory, but each GPU only coherently caches  its own local memory. The second remarkable element is that the Slingshot interface cards connect directly to the GPUs (through a PCIe interface on the GPU) rather than two the CPU. The CPUs have a shorter path to the communication  network than the CPU in this design. </p> <p>This makes the LUMI-G compute node really a \"GPU first\" system. The architecture looks more like a GPU system with a CPU as the accelerator for tasks that a GPU is not good at such as some scalar processing or running an OS, rather than a CPU node with GPU accelerator.</p> <p>It is also a good fit with the cluster-booster design explored in the DEEP project series. In that design, parts of your application that cannot be properly accelerated would run on CPU nodes, while booster GPU nodes would be used for those parts  that can (at least if those two could execute concurrently with each other). Different node types are mixed and matched as needed for each specific application,  rather than building clusters with massive and expensive nodes that few applications can fully exploit. As the cost per transistor does not decrease anymore, one has to look for ways to use each transistor as efficiently as possible...</p> <p>It is also important to realise that even though we call the partition \"LUMI-G\", the MI250x is not a GPU in the true sense of the word. It is not a rendering GPU, which for AMD is  currently the RDNA architecture with version 3 just out, but a compute accelerator with an architecture that evolved from a GPU architecture, in this case the VEGA architecture from AMD. The architecture of the MI200 series is also known as CDNA2, with the MI100 series being just CDNA, the first version. Much of the hardware that does not serve compute purposes has been removed from the design to have more transistors available for compute.  Rendering is possible, but it will be software-based  rendering with some GPU acceleration for certain parts of the pipeline, but not full hardware rendering. </p> <p>This is not an evolution at AMD only. The same is happening with NVIDIA GPUs and there is a reason why the latest generation is called \"Hopper\" for compute and \"Ada Lovelace\" for rendering GPUs.  Several of the functional blocks in the Ada Lovelace architecture are missing in the Hopper  architecture to make room for more compute power and double precision compute units. E.g., Hopper does not contain the ray tracing units of Ada Lovelace. The Intel Data Center GPU Max code named \"Ponte Vecchio\" is the only current GPU for  HPC that still offers full hardware rendering support (and even ray tracing).</p> <p>Graphics on one hand and HPC and AI on the other hand are becoming separate workloads for which manufacturers make different, specialised cards, and if you have applications that need both, you'll have to rework them to work in two phases, or to use two types of nodes and communicate between them over the interconnect, and look for supercomputers that support both workloads.</p> <p>But so far for the sales presentation, let's get back to reality...</p>"},{"location":"intro-evolving/01_Architecture/#building-lumi-what-a-lumi-g-node-really-looks-like","title":"Building LUMI: What a LUMI-G node really looks like","text":"<p>Or the full picture with the bandwidths added to it:</p> <p>The LUMI-G node uses the 64-core AMD 7A53 EPYC processor, known under the code name \"Trento\". This is basically a Zen3 processor but with a customised memory/IO die, designed specifically  for HPE Cray (and in fact Cray itself, before the merger) for the USA Coral-project to build the Frontier supercomputer, the fastest system in the world at the end of 2022 according to at least the Top500 list. Just as the CPUs in the LUMI-C nodes, it is a design with 8 CCDs and a memory/IO die.</p> <p>The MI250x GPU is also not a single massive die, but contains two compute dies besides the 8 stacks of HBM2e memory, 4 stacks or 64 GB per compute die. The two compute dies in a package are linked together  through 4 16-bit Infinity Fabric links. These links run at a higher speed than the links between two CPU sockets in a LUMI-C node, but per link the bandwidth is still only 50 GB/s per direction, creating a total bandwidth of 200 GB/s per direction between the two compute dies in an MI250x GPU. That amount of bandwidth is very low compared to even the memory bandwidth, which is roughly 1.6 TB/s peak per die, let alone compared to whatever bandwidth caches on the compute dies would have or the bandwidth of the internal structures that  connect all compute engines on the compute die. Hence the two dies in a single package cannot function efficiently as as single GPU which is one reason why each MI250x GPU on LUMI is actually seen as two GPUs. </p> <p>Each compute die uses a further 2 or 3 of those Infinity Fabric (or xGNI) links to connect to some compute dies in other MI250x packages. In total, each MI250x package is connected through 5 such links to other MI250x packages. These links run at the same 25 GT/s speed as the  links between two compute dies in a package, but even then the bandwidth is only a meager  250 GB/s per direction, less than an NVIDIA A100 GPU which offers 300 GB/s per direction or the NVIDIA H100 GPU which offers 450 GB/s per direction. Each Infinity Fabric link may be twice as fast as each NVLINK 3 or 4 link (NVIDIA Ampere and Hopper respectively), offering 50 GB/s per direction rather than 25 GB/s per direction for NVLINK,  but each Ampere GPU has 12 such links and each Hopper GPU 18 (and in fact a further 18 similar ones to link to a Grace CPU), while each MI250x package has only 5 such links available to link to other GPUs (and the three that we still need to discuss).</p> <p>Note also that even though the connection between MI250x packages is all-to-all, the connection between GPU dies is all but all-to-all. as each GPU die connects to only 3 other GPU dies. There are basically two bidirectional rings that don't need to share links in the topology, and then some extra connections. The rings are:</p> <ul> <li>Green ring: 1 - 0 - 6 - 7 - 5 - 4 - 2 - 3 - 1</li> <li>Red ring: 1 - 0 - 2 - 3 - 7 - 6 - 4 - 5 - 1</li> </ul> <p>These rings play a role in the inter-GPU communication in AI applications using RCCL.</p> <p>Each compute die is also connected to one CPU Core Complex Die (or as documentation of the node sometimes says, L3 cache region). This connection only runs at the same speed as the links between CPUs on the LUMI-C CPU nodes, i.e., 36 GB/s per direction (which is still enough for  all 8 GPU compute dies together to saturate the memory bandwidth of the CPU).  This implies that each of the 8 GPU dies has a preferred CPU die to work with, and this should definitely be taken into account when mapping processes and threads on a LUMI-G node. </p> <p>The figure also shows another problem with the LUMI-G node: The mapping between CPU cores/dies and GPU dies is all but logical:</p> GPU die CCD hardware threads NUMA node 0 6 48-55, 112-119 3 1 7 56-63, 120-127 3 2 2 16-23, 80-87 1 3 3 24-31, 88-95 1 4 0 0-7, 64-71 0 5 1 8-15, 72-79 0 6 4 32-39, 96-103 2 7 5 40-47, 104, 11 2 <p>and as we shall see later in the course, exploiting this is a bit tricky at the moment.</p>"},{"location":"intro-evolving/01_Architecture/#what-the-future-looks-like","title":"What the future looks like...","text":"<p>Some users may be annoyed by the \"small\" amount of memory on each node. Others may be annoyed by the limited CPU capacity on a node compared to some systems  with NVIDIA GPUs. It is however very much in line with the cluster-booster philosophy already mentioned a few times, and it does seem to be the future according to AMD (with Intel also working into that direction).  In fact, it looks like with respect to memory  capacity things may even get worse.</p> <p>We saw the first little steps of bringing GPU and CPU closer together and  integrating both memory spaces in the USA pre-exascale systems Summit and Sierra. The LUMI-G node which was really designed for one of the first USA exascale systems continues on this philosophy, albeit with a CPU and GPU from a different manufacturer. Given that manufacturing large dies becomes prohibitively expensive in newer semiconductor processes and that the transistor density on a die is also not increasing at the same rate anymore with process shrinks, manufacturers are starting to look at other ways of increasing the number of transistors per \"chip\" or should we say package. So multi-die designs are here to stay, and as is already the case in the AMD CPUs, different dies may be manufactured with different processes for economical reasons.</p> <p>Moreover, a closer integration of CPU and GPU would not only make programming easier as memory management becomes easier, it would also enable some codes to run on GPU  accelerators that are currently bottlenecked by memory transfers between GPU and CPU.</p> <p>Such a chip is exactly what AMD launched in December 2023 with the MI300A version of  the MI300 series.  It employs 13 chiplets in two layers, linked to (still only) 8  memory stacks (albeit of a much faster type than on the MI250x).  The 4 chiplets on the bottom layer are the memory controllers and inter-GPU links (an they can be at the bottom as they produce less heat). Furthermore each package features 6 GPU dies (now called XCD or Accelerated Compute Die as they really can't do graphics) and 3 Zen4 \"Genoa\" CPU dies. In the MI300A the memory is still limited to 8 16 GB stacks,  providing a total of 128 GB of RAM. The MI300X,  which is the regular version  without built-in CPU, already uses 24 GB stacks for a total of 192 GB of memory, but presumably those were not yet available when the design of MI300A was tested for the launch customer, the El Capitan supercomputer.  HLRS is building the Hunter cluster based on AMD MI300A  as a transitional system to their first exascale-class system Herder that will become operational by 2027.</p> <p>Intel at some point has shown only very conceptual drawings of its Falcon Shores chip  which it calls an XPU, but those drawings suggest that that chip will also support some low-bandwidth but higher capacity external memory, similar to the approach taken in some Sapphire  Rapids Xeon processors that combine HBM memory on-package with DDR5 memory outside  the package. Falcon Shores will be the next generation of Intel GPUs for HPC, after  Ponte Vecchio which will be used in the Aurora supercomputer. It is currently very likely though that Intel will revert to a traditional design for Falcon Shores and push out the integrated CPU+GPU model to a later generation.</p> <p>However, a CPU closely integrated with accelerators is nothing new as Apple Silicon is  rumoured to do exactly that in its latest generations, including the M-family chips.</p>"},{"location":"intro-evolving/01_Architecture/#building-lumi-the-slingshot-interconnect","title":"Building LUMI: The Slingshot interconnect","text":"<p>All nodes of LUMI, including the login, management and storage nodes, are linked together using the Slingshot interconnect (and almost all use Slingshot 11, the full implementation with 200 Gb/s bandwidth per direction).</p> <p>Slingshot is an interconnect developed by HPE Cray and based on Ethernet, but with proprietary extensions for better HPC performance. It adapts to the regular Ethernet protocols when talking to a node that only supports Ethernet, so one of the attractive features is that regular servers with Ethernet can be directly connected to the  Slingshot network switches. HPE Cray has a tradition of developing their own interconnect for very large systems. As in previous generations, a lot of attention went to adaptive routing and congestion control. There are basically two versions of it. The early version was named Slingshot 10, ran at 100 Gb/s per direction and did not yet have all features. It was used on the initial deployment of LUMI-C compute nodes but has since been upgraded to the full version. The full version with all features is called Slingshot 11. It supports a bandwidth of 200 Gb/s per direction, comparable to HDR InfiniBand with 4x links. </p> <p>Slingshot is a different interconnect from your typical Mellanox/NVIDIA InfiniBand implementation and hence also has a different software stack. This implies that there are no UCX libraries on the system as the Slingshot 11 adapters do not support that. Instead, the software stack is  based on libfabric (as is the stack for many other Ethernet-derived solutions and even Omni-Path has switched to libfabric under its new owner).</p> <p>LUMI uses the dragonfly topology. This topology is designed to scale to a very large number of  connections while still minimizing the amount of long cables that have to be used. However, with its complicated set of connections it does rely heavily on adaptive routing and congestion control for optimal performance more than the fat tree topology used in many smaller clusters. It also needs so-called high-radix switches. The Slingshot switch, code-named Rosetta, has 64 ports. 16 of those ports connect directly to compute nodes (and the next slide will show you how). Switches are then combined in groups. Within a group there is an all-to-all connection between  switches: Each switch is connected to each other switch. So traffic between two nodes of a  group passes only via two switches if it takes the shortest route. However, as there is typically only one 200 Gb/s direct connection between two switches in a group, if all 16 nodes on two  switches in a group would be communicating heavily with each other, it is clear that some traffic will have to take a different route. In fact, it may be statistically better if the 32 involved nodes would be spread  more evenly over the group, so topology based scheduling of jobs and getting the processes of a job on as few switches as possible may not be that important on a dragonfly Slingshot network.  The groups in a slingshot network are then also connected in an all-to-all fashion, but the number of direct links between two groups is again limited so traffic again may not always want to take  the shortest path. The shortest path between two nodes in a dragonfly topology never involves  more than 3 hops between switches (so 4 switches): One from the switch the node is connected to  the switch in its group that connects to the other group, a second hop to the other group, and then a third hop in the destination group to the switch the destination node is attached to.</p>"},{"location":"intro-evolving/01_Architecture/#assembling-lumi","title":"Assembling LUMI","text":"<p>Let's now have a look at how everything connects together to the supercomputer LUMI. It does show that LUMI is not your standard cluster build out of standard servers.</p> <p>LUMI is built very compactly to minimise physical distance between nodes and to reduce the cabling mess typical for many clusters. LUMI does use a custom rack design for the compute nodes that is also fully water cooled. It is build out of units that can contain up to 4 custom cabinets, and a cooling distribution unit (CDU). The size of the complex as depicted in the slide is approximately 12 m<sup>2</sup>. Each cabinet contains 8 compute chassis in 2 columns of 4 rows. In between the two columns is all the power circuitry. Each compute chassis can contain 8 compute blades that are mounted vertically. Each compute blade can contain multiple nodes, depending on the type of compute blades. HPE Cray have multiple types of compute nodes, also with  different types of GPUs. In fact, the Aurora supercomputer which uses Intel CPUs and GPUs and El Capitan, which uses the MI300A APUs (integrated CPU and GPU) will use the same design with a different compute blade. Each LUMI-C compute blade contains 4 compute nodes and two network interface cards, with each network interface card implementing two Slingshot interfaces and connecting to two nodes. A LUMI-G compute blade contains two nodes and 4 network interface cards, where each interface card now connects to two GPUs in the same  node. All connections for power, management network and high performance interconnect of the compute node are at the back of the compute blade. At the front of the compute blades one can find the connections to the cooling manifolds that distribute cooling water to the blades. One compute blade of LUMI-G can consume up to 5kW, so the power density of this setup is incredible, with 40 kW for a single compute chassis.</p> <p>The back of each cabinet is equally genius. At the back each cabinet has 8 switch chassis, each matching the position of a compute chassis. The switch chassis contains the connection to the power delivery system and a switch for the management network and has 8 positions for  switch blades. These are mounted horizontally and connect directly to the compute blades. Each slingshot switch has 8x2 ports on the inner side for that purpose, two for each compute blade. Hence for LUMI-C two switch blades are needed in each switch chassis as each blade has 4 network interfaces, and for LUMI-G 4 switch blades are needed for each compute chassis as those nodes have 8 network interfaces. Note that this also implies that the nodes on the same  compute blade of LUMI-C will be on two different switches even though in the node numbering they are numbered consecutively. For LUMI-G both nodes on a blade will be on a different pair of switches  and each node is connected to two switches. So when you get a few sequentially numbered nodes, they will not be on a single switch (LUMI-C) or switch pair (LUMI-G). The switch blades are also water cooled (each one can  consume up to 250W). No currently possible configuration of the Cray EX system needs  all switch positions in the switch chassis.</p> <p>This does not mean that the extra positions cannot be useful in the future. If not for an interconnect, one could, e.g., export PCIe ports to the back and attach, e.g., PCIe-based storage via blades as the  switch blade environment is certainly less hostile to such storage than the very dense and very hot compute blades.</p>"},{"location":"intro-evolving/01_Architecture/#lumi-assembled","title":"LUMI assembled","text":"<p>This slide shows LUMI fully assembled (as least as it was at the end of 2022).</p> <p>At the front there are 5 rows of cabinets similar to the ones in the exploded Cray EX picture  on the previous slide. Each row has 2 CDUs and 6 cabinets with compute nodes.  The first row, the one with the wolf, contains all nodes of LUMI-C, while the other four  rows, with the letters of LUMI, contain the GPU accelerator nodes. At the back of the room there are more  regular server racks that house the storage, management nodes, some special compute nodes , etc. The total size is roughly the size of a tennis court. </p> <p>Remark</p> <p>The water temperature that a system like the Cray EX can handle is so high that in fact the water can be cooled again with so-called \"free cooling\", by just radiating the heat to the environment rather  than using systems with compressors similar to air conditioning systems, especially in regions with a colder climate. The LUMI supercomputer is housed in Kajaani in Finland, with moderate temperature almost  year round, and the heat produced by the supercomputer is fed into the central heating system of the city, making it one of the greenest supercomputers in the world as it is also fed with renewable energy.</p>"},{"location":"intro-evolving/01_Architecture/#local-trainings","title":"Local trainings","text":"<p>The following trainings provide useful preliminary material for this section, and in one case even more details.</p> <ul> <li> <p>VSC:</p> <ul> <li> <p>VSC@UAntwerpen: Supercompters for Starters course,     organised twice each year.</p> <p>Almost complete course notes are available</p> <p>This course given in 2 4 hour sessions goes into more detail about CPU architecutre, memory, storage, accelerators, and the software that binds all hardware together to build a cluster.</p> </li> <li> <p>The other introductory courses have a high-level overview of a cluster</p> <ul> <li> <p>VSC@VUB HPC Introduction</p> </li> <li> <p>VSC@UGent Introduction to HPC</p> </li> <li> <p>VSC@KULeuven HPC-intro</p> </li> </ul> </li> </ul> </li> <li> <p>C\u00c9CI: The annual introductory course \"Learning how to use HPC infrastructure\" covers the basics      of cluster architecture.</p> <ul> <li>2022 edition: presentation \"Introduction to high-performance computing</li> </ul> </li> </ul>"},{"location":"intro-evolving/02_CPE/","title":"The HPE Cray Programming Environment","text":"<p>In this session we discuss some of the basics of the operating system and programming environment on LUMI. Whether you like it or not, every user of a supercomputer like LUMI gets confronted with these elements at some point.</p>"},{"location":"intro-evolving/02_CPE/#why-do-i-need-to-know-this","title":"Why do I need to know this?","text":"<p>The typical reaction of someone who only wants to run software on an HPC system when confronted with a talk about development tools is \"I only want to run some programs, why do I need to know about programming environments?\"</p> <p>The answer is that development environments are an intrinsic part of an HPC system.  No HPC system is as polished as a personal computer and the software users want to use is typically very unpolished. And some of the essential middleware that turns the hardware with some variant of Linux into a parallel supercomputers is part of the programming  environment. The binary interfaces to those libraries are also not as standardised  as for the more common Linux system libraries.</p> <p>Programs on an HPC cluster are preferably installed from sources to generate binaries optimised for the system. CPUs have gotten new instructions over time that can sometimes speed-up execution of a program a lot, and compiler optimisations that take specific strengths and weaknesses of particular CPUs into account can also gain some performance. Even just a 10% performance gain on an investment of 160 million EURO such as LUMI means a lot of money. When running, the build environment on most systems needs to be at least partially recreated. This is somewhat less relevant on Cray systems as we will see at the end of this part of the course, but if you want reproducibility it becomes important again.</p> <p>Compiling on the system is also the easiest way to guarantee compatibility of the binaries with the system. </p> <p>Even when installing software from prebuilt binaries some modules might still be needed. Prebuilt binaries will typically include the essential runtime libraries for the parallel technologies they use, but these may not be compatible with LUMI.  In some cases this can be solved by injecting a library from LUMI, e.g., you may want to inject an optimised MPI library as we shall see in the container section of this course. But sometimes a binary is simply incompatible with LUMI and there is no other solution than to build the software from sources.</p> <p>Clusters in Belgium</p> <p>There are differences in the setup of the operating system on LUMI compared to the VSC, C\u00c9CI and Cenaero clusters in Belgium.</p> <p>Lucia, the Cenaero cluster and Walloon tier-1 system, has a programming environment which is similar to LUMI. But all other Belgian systems have  a programming environment that is more inspired on the GNU and other  Open Source software approach. (I don't want to say \"a more traditional\"  programming environment as in fact the HPE Cray PE goes back in its approach to the environment on UNIX workstations and supercomputers in the '90s, so is in fact the more traditional one. It's just that we have forgotten those  traditions...)</p>"},{"location":"intro-evolving/02_CPE/#the-operating-system-on-lumi","title":"The operating system on LUMI","text":"<p>The login nodes of LUMI run a regular SUSE Linux Enterprise Server 15 SP4 distribution. The compute nodes however run Cray OS, a restricted version of the SUSE Linux that runs on the login nodes. Some daemons are inactive or configured differently and Cray also  does not support all regular file systems. The goal of this is to minimize OS jitter, interrupts that the OS handles and slow down random cores at random moments, that can  limit scalability of programs. Yet on the GPU nodes there was still the need to reserve one core for the OS and driver processes. This in turn led to an asymmetry in the setup so now 8 cores are reserved, one per CCD, so that all CCDs are equal again.</p> <p>This also implies that some software that works perfectly fine on the login nodes may not work on the compute nodes. E.g., there is no <code>/run/user/$UID</code> directory and we have experienced that D-Bus (which stands for Desktop-Bus) also does not work as one should expect.</p> <p>Large HPC clusters also have a small system image, so don't expect all the bells-and-whistles  from a Linux workstation to be present on a large supercomputer (and certainly not in the same way as they would be on a workstation). Since LUMI compute nodes are diskless, the system image actually occupies RAM which is another reason to keep it small.</p> Some missing pieces <p>Compute nodes don't run a per-user dbus daemon, so some if not all DBUS functionality is missing. And D-Bus may sometimes show up in places where you don't expect it... It may come from freedesktop.org but is is not only used for desktop software.</p> <p>Compute nodes on a Cray system have Lustre as the main file system. They do not import any networked file system like NFS, GPFS or CernVM-FS (the latter used by, e.g., Cern for  distributing software for the Large Haedron Collider and the EESSI project). Instead these file systems are mounted on external servers in the admin section of the cluster and the Cray Data Virtualisation Service (DVS) is then used to access those file systems from the compute nodes over the high-speed interconnect.</p>"},{"location":"intro-evolving/02_CPE/#low-noise-mode","title":"Low-noise mode","text":"<p>Low-noise mode has meant different things throughout the history of Cray systems.  Sometimes the mode described above, using only a selection of the regular Linux daemons on the compute nodes, was already called low-noise mode while some Cray systems provided another mode in which those daemons were activated. Depending on the cluster this was then called \"emulation mode\" or \"Cluster Compatibility Mode\". The latter is not implemented on LUMI, and even if it would, compatibility would still be limited by the special requirements to use the Slingshot interconnect and to have GPU-aware communication over Slingshot.</p> <p>However, it turned out that even the noise reduction described above was not yet sufficient to pass some large-scale scalability tests, and therefore another form of \"low-noise\" mode is implemented on the GPU nodes of LUMI where OS processes are restricted to a reserved core, actually core 0. This leaves us with an asymmetric structure of the node, where the first CCD has 7 available cores while the other ones have 8, and as we shall see in the  Process and thread distribution and binding\" chapter this will create some headaches when trying to get maximal efficiency for GPU applications. (For this reason some other clusters based on the same architecture reserve on core on each CCD.)</p> <p>This is actually an idea Cray has been experimenting with in the past already, ever since we've had nodes with 20 or more cores.</p>"},{"location":"intro-evolving/02_CPE/#programming-models","title":"Programming models","text":"<p>On LUMI we have several C/C++ and Fortran compilers. These will be discussed more in this session.</p> <p>There is also support for MPI and SHMEM for distributed applications. And we also support RCCL, the ROCm-equivalent of the  CUDA NCCL library that is popular in machine learning packages.</p> <p>All compilers have some level of OpenMP support,  and two compilers support OpenMP offload to  the AMD GPUs, but again more about that later.</p> <p>OpenACC, the other directive-based model for GPU offloading,  is only supported in the Cray Fortran compiler. There is no commitment of neither HPE Cray or AMD to extend that support to C/C++ or other compilers, even though there is work going on in the LLVM community and several compilers on the system are based on LLVM.</p> <p>The other important programming model for AMD GPUs is HIP,  which is their alternative for the proprietary CUDA model. It does not support all CUDA features though (basically it is more CUDA 7 or 8 level)  and there is also no equivalent to CUDA Fortran.</p> <p>The commitment to OpenCL is very unclear, and this actually holds for other GPU vendors also.</p> <p>We also try to provide SYCL as it is a programming language/model that works on all three GPU families currently used in HPC. </p> <p>Python is of course pre-installed on the system but we do ask to use big Python installations in a special way as Python puts a tremendous load on the file system. More about that later in this course.</p> <p>Some users also report some success in running Julia. We don't have full support though and have to depend on binaries as provided by julialang.org. The AMD GPUs are not yet fully supported by Julia.</p> <p>It is important to realise that there is no CUDA on AMD GPUs and there will never be as this is a  proprietary technology that other vendors cannot implement. The visualisation nodes in LUMI have NVIDIA rendering GPUs but these nodes are meant for visualisation and not for compute.</p>"},{"location":"intro-evolving/02_CPE/#the-development-environment-on-lumi","title":"The development environment on LUMI","text":"<p>Long ago, Cray designed its own processors and hence had to develop their own compilers. They kept doing so, also when they moved to using more standard components, and had a lot of expertise in that field, especially when it comes to the needs of  scientific codes, programming models that are almost only used in scientific computing or stem from such projects. As they develop their own interconnects, it does make sense to also develop an MPI implementation that can use the interconnect in an optimal way. They also have a long tradition in developing performance measurement and analysis tools  and debugging tools that work in the context of HPC.</p> <p>The first important component of the HPE Cray Programming Environment is the compilers. Cray still builds its own compilers for C/C++ and Fortran, called the Cray Compiling Environment (CCE). Furthermore, the GNU compilers are also supported on every Cray system, though at the moment AMD GPU support is not enabled. Depending on the hardware of the  system other compilers will also be provided and integrated in the environment. On LUMI two other compilers are available: the AMD AOCC compiler for CPU-only code and the  AMD ROCm compilers for GPU programming. Both contain a C/C++ compiler based on Clang and LLVM and a Fortran compiler which is currently based on the former PGI frontend with LLVM backend. The ROCm compilers also contain the support for HIP, AMD's CUDA clone.</p> <p>The second component is the Cray Scientific and Math libraries, containing the usual suspects as BLAS, LAPACK and ScaLAPACK, and FFTW, but also some data libraries and Cray-only libraries.</p> <p>The third component is the Cray Message Passing Toolkit. It provides an MPI implementation optimized for Cray systems, but also the Cray SHMEM libraries, an implementation of OpenSHMEM 1.5.</p> <p>The fourth component is some Cray-unique sauce to integrate all these components, and  support for hugepages to make memory access more efficient for some programs that  allocate huge chunks of memory at once.</p> <p>Other components include the Cray Performance Measurement and Analysis Tools and the  Cray Debugging Support Tools that will not be discussed in this one-day course, and Python and R modules that both also provide some packages compiled with support for the Cray Scientific Libraries.</p> <p>Besides the tools provided by HPE Cray, several of the development tools from the ROCm stack are also available on the system while some others can be user-installed (and one of those, Omniperf, is not available due to security concerns). Furthermore there are some third party tools available on LUMI, including Linaro Forge (previously ARM Forge) and Vampir and some open source profiling tools.</p> <p>Specifically not on LUMI are the Intel and NVIDIA programming environments, nor is the regular Intel oneAPI HPC Toolkit. The classic Intel compilers pose problems on AMD CPUs as <code>-xHost</code> cannot be relied on, but it appears that the new compilers that are based on Clang and an LLVM backend behave better. Various MKL versions are also troublesome, with different workarounds for different versions, though here also it seems that Intel now has  code that works well on AMD for many MKL routines. We have experienced problems with Intel  MPI when testing it on LUMI though in principle it should be possible to use Cray MPICH as they are derived from the same version of MPICH.  The NVIDIA programming environment doesn't make sense on an AMD GPU system,  but it could be useful for some visualisation software on the  visualisation nodes so it is currently installed on those nodes.</p> <p>We will now discuss some of these components in a little bit more detail, but refer to the 4-day trainings that we organise several times a year with HPE for more material.</p>"},{"location":"intro-evolving/02_CPE/#the-cray-compiling-environment","title":"The Cray Compiling Environment","text":"<p>The Cray Compiling Environment are the default compilers on many Cray systems and on LUMI. These compilers are designed specifically for scientific software in an HPC environment. The current versions are LLVM-based with extensions by HPE Cray for automatic vectorization and shared memory parallelization, technology that they have experience with since the late '70s or '80s.</p> <p>The compiler offers extensive standards support. The C and C++ compiler is essentially their own build of Clang with LLVM with some of their optimisation plugins and OpenMP run-time. The version numbering of the CCE currently follows the major versions of the Clang compiler used. The support for C and C++ language standards corresponds to that of Clang. The Fortran compiler uses a frontend and optimiser developed by HPE Cray, but an LLVM-based code generator.  The compiler supports most of Fortran 2018 (ISO/IEC 1539:2018). The CCE Fortran compiler is known to be very strict with language standards. Programs that use GNU or Intel extensions will usually fail to compile, and unfortunately since many developers only test with these compilers, much Fortran code is not fully standards compliant and will fail.</p> <p>All CCE compilers support OpenMP, with offload for AMD and NVIDIA GPUs. They claim full OpenMP 4.5 support with partial (and growing) support for OpenMP 5.0 and 5.1. More  information about the OpenMP support is found by checking a manual page: <pre><code>man intro_openmp\n</code></pre> which does require that the <code>cce</code> module is loaded. The Fortran compiler also supports OpenACC for AMD and NVIDIA GPUs. That implementation claims to be fully OpenACC 2.0 compliant, and offers partial support for OpenACC 2.x/3.x.  Information is available via <pre><code>man intro_openacc\n</code></pre> AMD and HPE Cray still recommend moving to OpenMP which is a much broader supported standard. There are no plans to also support OpenACC in the Cray C/C++ compiler, nor are there any  plans for support by AMD in the ROCm stack.</p> <p>The CCE compilers also offer support for some PGAS (Partitioned Global Address Space) languages. UPC 1.2 is supported, as is Fortran 2008 coarray support. These implementations do not require a preprocessor that first translates the code to regular C or Fortran. There is also support for debugging with Linaro Forge.</p> <p>Lastly, there are also bindings for MPI.</p>"},{"location":"intro-evolving/02_CPE/#scientific-and-math-libraries","title":"Scientific and math libraries","text":"<p>Some mathematical libraries have become so popular that they basically define an API for which several implementations exist, and CPU manufacturers and some open source groups spend a significant amount of resources to make optimal implementations for each CPU architecture.</p> <p>The most notorious library of that type is BLAS, a set of basic linear algebra subroutines for vector-vector, matrix-vector and matrix-matrix implementations. It is the basis for many other libraries that need those linear algebra operations, including Lapack, a library with solvers for linear systems and eigenvalue problems.</p> <p>The HPE Cray LibSci library contains BLAS and its C-interface CBLAS, and LAPACK and its C interface LAPACKE. It also adds ScaLAPACK, a distributed memory version of LAPACK, and BLACS, the  Basic Linear Algebra Communication Subprograms, which is the communication layer used by ScaLAPACK. The BLAS library combines implementations from different sources, to try to offer the most optimal one for several architectures and a range of matrix and vector sizes.</p> <p>LibSci also contains one component which is HPE Cray-only: IRT, the Iterative Refinement Toolkit,  which allows to do mixed precision computations for LAPACK operations that can speed up the generation of a double precision result with nearly a factor of two for those problems that are suited for iterative refinement. If you are familiar with numerical analysis, you probably know that the matrix should not be too ill-conditioned for that.</p> <p>There is also a GPU-optimized version of LibSci, called LibSci_ACC, which contains a subset of the routines of LibSci. We or the LUMI USer Support Team don't have much experience with this library though. It can be compared with what Intel is doing with oneAPI MKL which also offers GPU versions of some of the traditional MKL routines.</p> <p>Another separate component of the scientific and mathematical libraries is FFTW3,  Fastest Fourier Transforms in the West, which comes with optimized versions for all CPU architectures supported by recent HPE Cray machines.</p> <p>Finally, the scientific and math libraries also contain HDF5 and netCDF libraries in sequential and parallel versions.  These are included because it is essential that  they interface properly with MPI parallel I/O and the Lustre file system to offer the best bandwidth to and from storage. </p> <p>Cray used to offer more pre-installed third party libraries for which the only added value was that they compiled the binaries. Instead they now offer build scripts in a GitHub repository.</p>"},{"location":"intro-evolving/02_CPE/#cray-mpi","title":"Cray MPI","text":"<p>HPE Cray build their own MPI library with optimisations for their own interconnects. The Cray MPI library is derived from the ANL MPICH 3.4 code base and fully supports the  ABI (Application Binary Interface) of that application which implies that in principle it should be possible to swap the MPI library of applications build with that ABI with the Cray MPICH library. Or in other words, if you can only get a binary distribution of an application and that application was build against an MPI library compatible with  the MPICH 3.4 ABI (which includes Intel MPI) it should be possible to exchange that library for the Cray one to have optimised communication on the Cray Slingshot interconnect.</p> <p>Cray MPI contains many tweaks specifically for Cray systems. HPE Cray claim improved algorithms for many collectives, an asynchronous progress engine to improve overlap of communications and computations,  customizable collective buffering when using MPI-IO, and optimized remote memory access (MPI one-sided communication) which also supports passive remote memory access.</p> <p>When used in the correct way (some attention is needed when linking applications) it is also fully GPU aware with currently support for AMD and NVIDIA GPUs.</p> <p>The MPI library also supports bindings for Fortran 2008.</p> <p>MPI 3.1 is almost completely supported, with two exceptions. Dynamic process management is not supported (and a problem anyway on systems with batch schedulers), and when using CCE MPI_LONG_DOUBLE and MPI_C_LONG_DOUBLE_COMPLEX are also not supported.</p> <p>The Cray MPI library does not support the <code>mpirun</code> or <code>mpiexec</code> commands, which is in fact allowed by the standard which only requires a process starter and suggest <code>mpirun</code> or <code>mpiexec</code>  depending on the version of the standard. Instead the Slurm <code>srun</code> command is used as the  process starter. This actually makes a lot of sense as the MPI application should be mapped correctly on the allocated resources, and the resource manager is better suited to do so.</p> <p>Cray MPI on LUMI is layered on top of libfabric, which in turn uses the so-called Cassini provider to interface with the hardware. UCX is not supported on LUMI (but Cray MPI can support it when used on InfiniBand clusters). It also uses a GPU Transfer Library (GTL) for GPU-aware MPI.</p>"},{"location":"intro-evolving/02_CPE/#gpu-aware-mpi","title":"GPU-aware MPI","text":"<p>Cray MPICH does support GPU-aware MPI, so it is possible to directly use GPU-attached communication buffers using device pointers. The implementation supports (a) GPU-NIC RDMA for efficient inter-node MPI transfers and (b) GPU Peer2PEer IPC for efficient intra-node transfers. The latter mechanism comes with some restrictions though that we will discuss in the chapter \"Process and thread distribution and binding\".</p> <p>GPU-aware MPI needs to be enabled explicitly, which you can do by setting an environment variable:</p> <pre><code>export MPICH_GPU_SUPPORT_ENABLED=1\n</code></pre> <p>In addition to this, if the GPU code does use MPI operations that access GPU-attached memory regions it is best to also set</p> <pre><code>export MPICH_OFI_NIC_POLICY=GPU\n</code></pre> <p>If only CPU communication buffers are used, then it may be better to set</p> <pre><code>export MPICH_OFI_NIC_POLICY=NUMA\n</code></pre> <p>Depending on how Slurm is used, Peer2Peer IPC may not work and in those cases you may want to turn it off using <pre><code>export MPICH_GPU_IPC_ENABLED=0\n</code></pre> or alternatively <pre><code>export MPICH_SMP_SINGLE_COPY_MODE=NONE\n</code></pre> Both options entail a serious loss of performance. The underlying problem is that the way in which Slurm does GPU binding using control groups makes other GPUS from other tasks in the node invisible to a task.</p>"},{"location":"intro-evolving/02_CPE/#lmod","title":"Lmod","text":"<p>Virtually all clusters use modules to enable the users to configure the environment and select the versions of software they want. There are three different module systems around. One is an old implementation that is hardly evolving anymore but that can still be found on a number of clusters. HPE Cray still offers it as an option. Modulefiles are written in TCL, but the tool itself is in C. The more popular tool at the moment is probably Lmod. It is largely compatible with modulefiles for the old tool, but prefers modulefiles written in LUA. It is also supported by the HPE Cray PE and is our choice on LUMI. The final implementation is a full TCL implementation developed  in France and also in use on some large systems in Europe.</p> <p>Fortunately the basic commands are largely similar in those implementations, but what differs is the way to search for modules. We will now only discuss the basic commands, the more advanced ones will be discussed in the next session of this tutorial course.</p> <p>Modules also play an important role in configuring the HPE Cray PE, but before touching that  topic we present the basic commands:</p> <ul> <li><code>module avail</code>: Lists all modules that can currently be loaded. </li> <li><code>module list</code>: Lists all modules that are currently loaded</li> <li><code>module load</code>: Command used to load a module. Add the name and version of the module.</li> <li><code>module unload</code>: Unload a module. Using the name is enough as there can only one version be      loaded of a module.</li> <li><code>module swap</code>:  Unload the first module given and then load the second one. In Lmod this is      really equivalent to a <code>module unload</code> followed by a <code>module load</code>.</li> </ul> <p>Lmod supports a hierarchical module system. Such a module setup distinguishes between installed modules and available modules. The installed modules are all modules that can be loaded in one way or another by the module systems, but loading some of those may require loading other modules first. The available modules are the modules that can be loaded directly without loading any other module. The list of available modules changes all the time based on modules that are already loaded, and if you unload a module that makes other loaded modules unavailable, those will also be deactivated by Lmod. The advantage of a hierarchical module system is that one can support multiple configurations of a module while all configurations can have the same name and version. This is not fully exploited on LUMI, but it  is used a lot in the HPE Cray PE. E.g., the MPI libraries for the various compilers on the system all have the same name and version yet make different binaries available depending on the compiler that is being used.</p> <p>Different configurations on some Belgian clusters</p> <p>Depending on the configuration Lmod can behave rather differently on different systems. Some clusters in Belgium have Lmod configured to mimic the original Tcl module system better rather than exposing the full power of Lmod.</p> <p>E.g., on LUMI, <code>module swap</code> isn't really needed as the auto-swap feature of Lmod is  enabled. Automatically unloading a module if another module with the same name is  loaded, is also enabled. Both features make with a hierarchical scheme much more powerful and using the HPE Cray PE with these features disabled would be very difficult.</p>"},{"location":"intro-evolving/02_CPE/#compiler-wrappers","title":"Compiler wrappers","text":"<p>The HPE Cray PE compilers are usually used through compiler wrappers. The wrapper for C is <code>cc</code>, the one for C++ is <code>CC</code> and the one for Fortran is <code>ftn</code>. The wrapper then calls the selected compiler. Which compiler will be called is determined by which compiler module is loaded. As shown on the slide  \"Development environment on LUMI\", on LUMI the Cray Compiling Environment (module <code>cce</code>), GNU Compiler Collection (module <code>gcc</code>),  the AMD Optimizing Compiler for CPUs (module <code>aocc</code>) and the ROCm LLVM-based compilers (module <code>amd</code>) are available. On the visualisation nodes, the NVIDIA HPC compiler is currently also installed (module <code>nvhpc</code>). On other HPE Cray systems, you may also find the Intel compilers.</p> <p>The target architectures for CPU and GPU are also selected through modules, so it is better to not use compiler options such as <code>-march=native</code>. This makes cross compiling also easier.</p> <p>The wrappers will also automatically link in certain libraries, and make the include files available, depending on which other modules are loaded. In some cases it tries to do so cleverly, like selecting an MPI, OpenMP, hybrid or sequential option depending on whether the MPI module is loaded and/or OpenMP compiler flag is used. This is the case for:</p> <ul> <li>The MPI libraries. There is no <code>mpicc</code>, <code>mpiCC</code>, <code>mpif90</code>, etc. on LUMI (well, there is nowadays, but their     use is discouraged). The regular compiler     wrappers do the job as soon as the <code>cray-mpich</code> module is loaded.</li> <li>LibSci and FFTW are linked automatically if the corresponding modules are loaded. So no need     to look, e.g., for the BLAS or LAPACK libraries: They will be offered to the linker if the     <code>cray-libsci</code> module is loaded (and it is an example of where the wrappers try to take the     right version based not only on compiler, but also on whether MPI is loaded or not and the     OpenMP compiler flag).</li> <li>netCDF and HDF5</li> </ul> <p>It is possible to see which compiler and linker flags the wrappers add through the <code>-craype-verbose</code> flag.</p> <p>The wrappers do have some flags of their own, but also accept all flags of the selected compiler and  simply pass those to those compilers.</p> <p>The compiler wrappers are provided by the <code>craype</code> module (but you don't have to load that module by hand).</p> <p>The new online documentation is now also complete enough that it makes sense trying the search box on that page instead.</p>"},{"location":"intro-evolving/02_CPE/#selecting-the-version-of-the-cpe","title":"Selecting the version of the CPE","text":"<p>The version numbers of the HPE Cray PE are of the form <code>yy.dd</code>, e.g., <code>23.09</code> for the version released in September 2023. There are several releases each year (at least 4), but not all of them are offered on LUMI. </p> <p>There is always a default version assigned by the sysadmins when installing the programming environment. It is possible to change the default version for loading further modules by loading one of the versions of the <code>cpe</code> module. E.g., assuming the 23.09 version would be present on the system, it can be loaded through <pre><code>module load cpe/23.09\n</code></pre> Loading this module will also try to switch the already loaded PE modules to the versions from that release. This does not always work correctly, due to some bugs in most versions of this module and a limitation of Lmod. Executing the <code>module load</code> twice will fix this: <pre><code>module load cpe/23.09\nmodule load cpe/23.09\n</code></pre> The module will also produce a warning when it is unloaded (which is also the case when you do a <code>module load</code> of <code>cpe</code> when one is already loaded, as it then first unloads the already loaded <code>cpe</code> module). The warning can be ignored, but keep in mind that what it says is true, it cannot restore the environment you found on LUMI at login.</p> <p>The <code>cpe</code> module is also not needed when using the LUMI software stacks, but more about that later.</p>"},{"location":"intro-evolving/02_CPE/#the-target-modules","title":"The target modules","text":"<p>The target modules are used to select the CPU and GPU optimization targets and to  select the network communication layer. </p> <p>On LUMI there are three CPU target modules that are relevant:</p> <ul> <li><code>craype-x86-rome</code> selects the Zen2 CPU family code named Rome. These CPUs are     used on the login nodes and the nodes of the data analytics and visualisation      partition of LUMI. However, as Zen3 is a superset of Zen2, software compiled     to this target should run everywhere, but may not exploit the full potential     of the LUMI-C and LUMI-G nodes (though the performance loss is likely minor).</li> <li><code>craype-x86-milan</code> is the target module for the Zen3 CPUs code named Milan that     are used on the CPU-only compute nodes of LUMI (the LUMI-C partition).</li> <li><code>craype-x86-trento</code> is the target module for the Zen3 CPUs code named Trento that     are used on the GPU compute nodes of LUMI (the LUMI-G partition).</li> </ul> <p>Two GPU target modules are relevant for LUMI:</p> <ul> <li><code>craype-accel-host</code>: Will tell some compilers to compile offload code for the host     instead.</li> <li><code>craype-accel-gfx90a</code>: Compile offload code for the MI200 series GPUs that are used on LUMI-G.</li> </ul> <p>Two network target modules are relevant for LUMI:</p> <ul> <li><code>craype-network-ofi</code> selects the libfabric communication layer which is needed for     Slingshot 11.</li> <li><code>craype-network-none</code> omits all network specific libraries.</li> </ul> <p>The compiler wrappers also have corresponding compiler flags that can be used to overwrite these settings: <code>-target-cpu</code>, <code>-target-accel</code> and <code>-target-network</code>.</p>"},{"location":"intro-evolving/02_CPE/#prgenv-and-compiler-modules","title":"PrgEnv and compiler modules","text":"<p>In the HPE Cray PE, the <code>PrgEnv-*</code> modules are usually used to load a specific variant of the programming environment. These modules will load the compiler wrapper (<code>craype</code>), compiler, MPI and LibSci module and may load some other modules also.</p> <p>The following table gives an overview of the available <code>PrgEnv-*</code> modules and the compilers they activate:</p> PrgEnv Description Compiler module Compilers PrgEnv-cray Cray Compiling Environment <code>cce</code> <code>craycc</code>, <code>crayCC</code>, <code>crayftn</code> PrgEnv-gnu GNU Compiler Collection <code>gcc</code> <code>gcc</code>, <code>g++</code>, <code>gfortran</code> PrgEnv-aocc AMD Optimizing Compilers(CPU only) <code>aocc</code> <code>clang</code>, <code>clang++</code>, <code>flang</code> PrgEnv-amd AMD ROCm LLVM compilers (GPU support) <code>amd</code> <code>amdclang</code>, <code>amdclang++</code>, <code>amdflang</code> <p>There is also a second module that offers the AMD ROCm environment, <code>rocm</code>. That module has to be used with <code>PrgEnv-cray</code> and <code>PrgEnv-gnu</code> to enable MPI-aware GPU, hipcc with the GNU compilers or GPU support with the Cray compilers.</p> Changes to the GNU compilers in 23.12 <p>The HPE Cray PE will change the way it offers the GNU compilers in releases starting from 23.12. Rather than packaging the GNU compilers, HPE Cray will use the default development compiler version of SUSE Linux, which for SP4 is currently GCC 12.3 (not to be confused with the system default which is still 7.5, the compiler that was offered with the initial release of SUSE Enterprise Linux 15).</p> <p>In releases up to the 23.09 which we currently have on Linux, the GNU compilers are offered through the <code>gcc</code> compiler module. When loaded, it adds newer versions of the <code>gcc</code>, <code>g++</code> and <code>gfortran</code> compilers to the path, calling the version indicated by the version of the <code>gcc</code> module.</p> <p>In releases from 23.12 on, that compiler module is now called <code>gcc-native</code>, and the compilers are - at least in the version for SUSE 15 SP4 - called <code>gcc-12</code>, <code>g++-12</code> and <code>gfortran-12</code>, while <code>gcc</code>, <code>g++</code> and <code>gfortran</code> will compile with version 7.5, the default version for SUSE 15.</p>"},{"location":"intro-evolving/02_CPE/#getting-help","title":"Getting help","text":"<p>Help on the HPE Cray Programming Environment is offered mostly through manual pages and compiler flags. Online help is limited and difficult to locate.</p> <p>For the compilers, the following man pages are relevant:</p> PrgEnv C C++ Fortran PrgEnv-cray <code>man craycc</code> <code>man crayCC</code> <code>man crayftn</code> PrgEnv-gnu <code>man gcc</code> <code>man g++</code> <code>man gfortran</code> PrgEnv-aocc/PrgEnv-amd - - - <p>There used to be manual pages for the wrappers also but they are currently hijacked by the GNU manual pages.</p> <p>Recently, HPE Cray have also created  a web version of some of the CPE documentation.</p> <p>Some compilers also support the <code>--help</code> flag, e.g., <code>amdclang --help</code>. For the wrappers, the switch <code>-help</code> should be used instead as the double dash version is passed to the  compiler.</p> <p>The wrappers have a number of options specific to them. Information about them can be obtained by using the <code>--craype-help</code> flag with the wrappers. The wrappers also support the <code>-dumpversion</code> flag to show the version of the underlying compiler. Many other commands, including the actual compilers, use <code>--version</code> to show the version.</p> <p>For Cray Fortran compiler error messages, the <code>explain</code> command is also helpful. E.g.,</p> <pre><code>$ ftn\nftn-2107 ftn: ERROR in command line\n  No valid filenames are specified on the command line.\n$ explain ftn-2107\n\nError : No valid filenames are specified on the command line.\n\nAt least one file name must appear on the command line, with any command-line\noptions.  Verify that a file name was specified, and also check for an\nerroneous command-line option which may cause the file name to appear to be\nan argument to that option.\n</code></pre> <p>On older Cray systems this used to be a very useful command with more compilers but as  HPE Cray is using more and more open source components instead there are fewer commands that give additional documentation via the <code>explain</code> command.</p> <p>Lastly, there is also a lot of information in the \"Developing\" section of the LUMI documentation.</p>"},{"location":"intro-evolving/02_CPE/#google-chatgpt-and-lumi","title":"Google, ChatGPT and LUMI","text":"<p>When looking for information on the HPE Cray Programming Environment using search engines such as Google, you'll be disappointed how few results show up. HPE doesn't put much information on the  internet, and the environment so far was mostly used on Cray systems of which there are not that many. </p> <p>The same holds for ChatGPT. In fact, much of the training of the current version of ChatGPT was done with data of two or so years ago and there is not that much suitable training data available on the internet either.</p> <p>The HPE Cray environment has a command line alternative to search engines though: the <code>man -K</code> command that searches for a term in the manual pages. It is often useful to better understand some error messages. E.g., sometimes Cray MPICH will suggest you to set some environment variable to work around some problem. You may remember that <code>man intro_mpi</code> gives a lot of information about Cray MPICH, but if you don't and, e.g., the error message suggests you to set <code>FI_CXI_RX_MATCH_MODE</code> to either <code>software</code> or <code>hybrid</code>, one way to find out where you can get more information about this environment variable is</p> <pre><code>man -K FI_CXI_RX_MATCH_MODE\n</code></pre>"},{"location":"intro-evolving/02_CPE/#other-modules","title":"Other modules","text":"<p>Other modules that are relevant even to users who do not do development:</p> <ul> <li>MPI: <code>cray-mpich</code>. </li> <li>LibSci: <code>cray-libsci</code></li> <li>Cray FFTW3 library: <code>cray-fftw</code></li> <li>HDF5:<ul> <li><code>cray-hdf5</code>: Serial HDF5 I/O library</li> <li><code>cray-hdf5-parallel</code>: Parallel HDF5 I/O library</li> </ul> </li> <li>NetCDF:<ul> <li><code>cray-netcdf</code></li> <li><code>cray-netcdf-hdf5parallel</code></li> <li><code>cray-parallel-netcdf</code></li> </ul> </li> <li>Python: <code>cray-python</code>, already contains a selection of packages that interface with     other libraries of the HPE Cray PE, including mpi4py, NumPy, SciPy and pandas.</li> <li>R: <code>cray-R</code></li> </ul> <p>The HPE Cray PE also offers other modules for debugging, profiling, performance analysis, etc. that are not covered in this short version of the LUMI course. Many more are covered in the 4-day courses for developers that we organise several times per year with the help of HPE and AMD.</p>"},{"location":"intro-evolving/02_CPE/#warning-1-you-do-not-always-get-what-you-expect","title":"Warning 1: You do not always get what you expect...","text":"<p>The HPE Cray PE packs a surprise in terms of the libraries it uses, certainly for users who come from an environment where the software is managed through EasyBuild, but also for most other users.</p> <p>The PE does not use the versions of many libraries determined by the loaded modules at runtime but instead uses default versions of libraries (which are actually in <code>/opt/cray/pe/lib64</code> on the system) which correspond to the version of the programming environment that is set as the system default when installed. This is very much the behaviour of Linux applications also that pick standard libraries in a few standard directories and it enables many programs build with the HPE Cray PE to run without reconstructing the environment and in some cases to mix programs compiled with different compilers with ease (with the emphasis on some as there may still be library conflicts between other libraries when not using the  so-called rpath linking). This does have an annoying side effect though: If the default PE on the system  changes, all applications will use different libraries and hence the behaviour of your application may  change. </p> <p>Luckily there are some solutions to this problem.</p> <p>By default the Cray PE uses dynamic linking, and does not use rpath linking, which is a form of dynamic linking where the search path for the libraries is stored in each executable separately. On Linux, the search path for libraries is set through the environment variable <code>LD_LIBRARY_PATH</code>. Those Cray PE modules that have their libraries also in the default location, add the directories that contain the actual version of the libraries corresponding to the version of the module to the PATH-style environment variable <code>CRAY_LD_LIBRARY_PATH</code>. Hence all one needs to do is to ensure that those directories are put in <code>LD_LIBRARY_PATH</code> which is searched before the default location: <pre><code>export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH\n</code></pre></p> Small demo of adapting <code>LD_LIBRARY_PATH</code>: <p>An example that can only be fully understood after the section on the LUMI software stacks: <pre><code>$ module load LUMI/22.08\n$ module load lumi-CPEtools/1.0-cpeGNU-22.08\n$ ldd $EBROOTLUMIMINCPETOOLS/bin/mpi_check\n      linux-vdso.so.1 (0x00007f420cd55000)\n      libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f420c929000)\n      libmpi_gnu_91.so.12 =&gt; /opt/cray/pe/lib64/libmpi_gnu_91.so.12 (0x00007f4209da4000)\n      ...\n$ export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH\n$ ldd $EBROOTLUMIMINCPETOOLS/bin/mpi_check\n        linux-vdso.so.1 (0x00007fb38c1e0000)\n      libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007fb38bdb4000)\n      libmpi_gnu_91.so.12 =&gt; /opt/cray/pe/mpich/8.1.18/ofi/gnu/9.1/lib/libmpi_gnu_91.so.12 (0x00007fb389198000)\n      ...\n</code></pre> The <code>ldd</code> command shows which libraries are used by an executable. Only a part of the very long output is shown in the above example. But we can already see that in the first case, the library <code>libmpi_gnu_91.so.12</code> is taken from <code>opt/cray/pe/lib64</code> which is the directory with the default versions, while in the second case it is taken from <code>/opt/cray/pe/mpich/8.1.18/ofi/gnu/9.1/lib/</code> which clearly is for a specific version of <code>cray-mpich</code>.</p> <p>We do provide an experimental module <code>lumi-CrayPath</code>  that tries to fix <code>LD_LIBRARY_PATH</code> in a way that unloading the module fixes <code>LD_LIBRARY_PATH</code> again to the state before adding <code>CRAY_LD_LIBRARY_PATH</code> and that reloading the module adapts <code>LD_LIBRARY_PATH</code> to the current value of <code>CRAY_LD_LIBRARY_PATH</code>. Loading that module after loading all other modules should fix this issue for most if not all software.</p> <p>The second solution would be to use rpath-linking for the Cray PE libraries, which can be done by setting the <code>CRAY_ADD_RPATH</code>environment variable: <pre><code>export CRAY_ADD_RPATH=yes\n</code></pre></p> <p>However, there is also a good side to the standard Cray PE behaviour. Updates of the underlying operating system or network software stack may break older versions of the MPI library. By letting the applications use the default libraries and updating the defaults to a newer version, most applications will still run while they would fail if any of the two tricks to force the use of the intended library version are used. This has actually happened after a big LUMI update in March 2023, when all software that used rpath-linking had to be rebuild as the MPICH library that was present before the update did not longer work.</p>"},{"location":"intro-evolving/02_CPE/#warning-2-order-matters","title":"Warning 2: Order matters","text":"<p>Lmod is a hierarchical module scheme and this is exploited by the HPE Cray PE. Not all modules are available right away and some only become available after loading other modules. E.g.,</p> <ul> <li><code>cray-fftw</code> only becomes available when a processor target module is loaded</li> <li><code>cray-mpich</code> requires both the network target module <code>craype-network-ofi</code> and a compiler module to be loaded</li> <li><code>cray-hdf5</code> requires a compiler module to be loaded and <code>cray-netcdf</code> in turn requires <code>cray-hdf5</code></li> </ul> <p>but there are many more examples in the programming environment.</p> <p>In the next section of the course we will see how unavailable modules can still be found with <code>module spider</code>. That command can also tell which other modules should be loaded  before a module can be loaded, but unfortunately due to the sometimes non-standard way  the HPE Cray PE uses Lmod that information is not always complete for the PE, which is also why we didn't demonstrate it here.</p>"},{"location":"intro-evolving/02_CPE/#note-compiling-without-the-hpe-cray-pe-wrappers","title":"Note: Compiling without the HPE Cray PE wrappers","text":"<p>It is now possible to work without the HPE Cray PE compiler wrappers  and to use the compilers in a way you may be more familiar with from other  HPC systems. </p> <p>In that case, you would likely want to load a compiler module without loading the  <code>PrgEnv-*</code> module and <code>craype</code> module (which would be loaded automatically by the <code>PrgEnv-*</code> module). The compiler module and compiler driver names are then given by the following table:</p> Description Compiler module Compilers Cray Compiling Environment <code>cce</code> <code>craycc</code>, <code>crayCC</code>, <code>crayftn</code> GNU Compiler Collection <code>gcc</code><code>gcc-native</code> <code>gcc</code>, <code>g++</code>, <code>gfortran</code><code>gcc-12</code>, <code>g++-12</code>, <code>gfortran-12</code> AMD Optimizing Compilers(CPU only) <code>aocc</code> <code>clang</code>, <code>clang++</code>, <code>flang</code> AMD ROCm LLVM compilers (GPU support) <code>amd</code> <code>amdclang</code>, <code>amdclang++</code>, <code>amdflang</code> <p>Recent versions of the <code>cray-mpich</code> module now also provide the traditional MPI compiler wrappers such as <code>mpicc</code>, <code>mpicxx</code> or <code>mpifort</code>. Note that you will still need to ensure that the network target module <code>craype-network-ofi</code> is loaded to be able to load the <code>cray-mpich</code> module! The <code>cray-mpich</code> module also defines the environment variable <code>MPICH_DIR</code> that points to the MPI installation for the selected compiler.</p> <p>To manually use the BLAS and LAPACK libraries, you'll still have to load the <code>cray-libsci</code> module. This module defines the <code>CRAY_LIBSCI_PREFIX_DIR</code> environment variable that points to the directory with the library and include file subdirectories for the selected compiler. (This environment variable will be renamed to <code>CRAY_PE_LIBSCI_PREFIX_DIR</code> in release 23.12 of the programming environment.)  See the <code>intro_libsci</code> manual page for information about the different libraries.</p> <p>To be able to use the <code>cray-fftw</code> FFTW libraries, you still need to load the right CPU target module, even though you need to specify the target architecture yourself now when calling the compilers.  This is because the HPE Cray PE does not come with a multi-cpu version of the FFTW libraries, but  specific versions for each CPU (or sometimes group of similar CPUs). Here again some environment variables may be useful to point the compiler and linker to the installation: <code>FFTW_ROOT</code> for the  root of the installation for the specific CPU (the library is otherwise compiler-independent), <code>FFTW_INC</code> for the subdirectory with the include files and <code>FFTW_DIR</code> for the directory with the libraries.</p> <p>Other modules that you may want to use also typically define some useful environment variables.</p>"},{"location":"intro-evolving/03_LUMI_access/","title":"Getting Access to LUMI","text":""},{"location":"intro-evolving/03_LUMI_access/#who-pays-the-bills","title":"Who pays the bills?","text":"<p>LUMI is one of the larger EuroHPC supercomuters. EuroHPC currently funds  supercomputers in three different clases:</p> <ol> <li> <p>There are a number of so-called petascale supercomputers. The first ones of those are     Meluxina (in Luxembourg), VEGA (in Slovenia), Karolina (in the Czech Republic) and      Discoverr (ikn Bulgaria), with Deucalion (in Portugal) under construction.</p> </li> <li> <p>A number of pre-exascale supercomputers, LUMI being one of them. The other two are Leonardo (in Italy)     and MareNostrum 5 (in Spain and still under construction)</p> </li> <li> <p>A decision has already been taken on two exascale supercomputers: Jupiter (in Germany) and     Jules Verne (in France).</p> </li> </ol> <p>Depending on the machine, EuroHPC pays one third up to half of the bill, while the remainder of the budget comes from the hosting country, usually with the help of a consortium of countries.</p> <p>LUMI is hosted in Finland but operated by a consortium of 10 countries, with Belgium being the third largest contributor to LUMI and the second largest in the consortium of countries. The Belgian contribution is brought together by 4 entities:</p> <ol> <li>BELSPO, the science agency of the Federal government, invested 5M EURO in the project.</li> <li>The SPW \u00c9conomie, Empoli, Recherche from Wallonia also invested 5M EURO in the project.</li> <li>The Depertment of Economy, Science and Innovation (EWI) of the Flemish government invested 3.5M EURO in the project.</li> <li>Innoviris (Brussels) invested 2M EURO.</li> </ol> <p>The resources of LUMI are allocated proportional to the investments. As a result EuroHPC can allocate half of the resources. The Belgian share is approximately 7.4%.</p> <p>Each LUMI consortium country can set its own policies for a national access program, within the limits of what the supercomputer can technically sustain. In Belgium, the 4 entities that invested in LUMI do so together. The access conditions for projects in the Belgian share are advertised via the  EuroCC Belgium National Competence Centre.</p> <p>Web links:</p> <ul> <li>EuroHPC JU supercomputers</li> <li>EuroCC Belgium National Competence Centre     with the specifics of LUMI access via the Belgian share.</li> </ul>"},{"location":"intro-evolving/03_LUMI_access/#users-and-projects","title":"Users and projects","text":"<p>LUMI works like most European large supercomputers: Users are members of projects.</p> <p>A project corresponds to a coherent amount of work that is being done by a single person or a collaboration of a group of people. It typically corresponds to a research project, though there are other project types also, e.g., to give people access in the context of a course, or for organisational issues, e.g., a project for VSC support. Most projects are short-lived, with a typical duration of 4 to 6 months for benchmarking projects or one year for a  regular project.</p> <p>Projects are also the basis for most research allocations on LUMI. In LUMI there are three types of resource allocations, and each project needs at least two of them:</p> <ol> <li>A compute budget for the CPU nodes of LUMI (LUMI-C), expressed in core-hours.</li> <li>A compute budget for the GPU nodes of LUMI (LUMI-G), expressed in GPU-hours. As the mechanism was     already fixed before it became publically known that for all practical purposes one AMD MI250X GPU     should really be treated as 2 GPUs, one GPU-hour is one hour on a full MI250X, so computing for one     hour on a full LUMI-G GPU node costs 4 GPU-hours.</li> <li> <p>A storage budget which is expressed in TB-hours. Only storage that is actually being used is charged     on LUMI, to encourage users to clean up temporary storage. The rate at which storage is charged depends     on the file system:</p> <ol> <li>Storing one TB for one hour on the disk based Lustre file systems costs 1 TB-hour.</li> <li>Storing one TB for one hour on the flash based Lustre file system costs 10 TB-hour, also reflecting     the purchase cost difference of the systems.</li> <li>Storing one TB for one hour on the object based file system costs 0.5 TB-hour.</li> </ol> </li> </ol> <p>These budgets are assigned and managed by the resource allocators, not by the LUMI User Support Team. For Belgium the VSC and C\u00c9CI both have the role of resource allocator, but both use a common help desk.</p> <p>LUMI projects will typically have multiple project numbers which may be a bit confusing:</p> <ol> <li>Each RA may have its own numbering system, often based on the numbering used for the project     requests. Note that the LUMI User Support Team is not aware of that numbering as it is purely     internal to the RA.</li> <li> <p>Each project on LUMI also gets a LUMI project ID which also corresponds to a Linux group to manage     access to the project resources. These project IDs are of the form <code>project_465XXXXXX</code> for most     projects but <code>project_462XXXXXX</code> for projects that are managed by the internal system of      CSC Finland. </p> <p>This is also the project number that you should mention when contacting the  central LUMI User Support.</p> </li> </ol> <p></p> <p>Besides projects there are also user accounts.  Each user account on LUMI corresponds to a physical person, and user accounts should not be shared. Some physical persons have more than one user account but this is an unfortunate consequence of decisions made very early in the LUMI project about how projects on LUMI would be managed. Users themselves cannot do a lot without a projects as all a user has on LUMI is a small personal disk space which is simply a Linux requirement.  To do anything useful on LUMI users need to be member of a project. There is some discussion about special \"robot accounts\" for special purposes  that would not correspond to a physical person but have a specific goal  (like organising data ingestion from an external source) but those do not yet exist.</p> <p>There ia a many-to-may mapping between projects and user accounts. Projects can of course have multiple users who collaborate in the project, but a user account can also be part of multiple projects. The latter is more common than you may think, as. e.g., you may become member of a training project when you take a LUMI training.</p> <p>Most resources are attached to projects. The one resource that is attached to a user account is a small home directory to store user-specific configuration files. That home directory is not billed but can also not be extended. For some purposes you may have to store things that would usually be automatically be placed in the home directory in a separate directory, e.g., in the project scratch space, and link to it. This may be the case when you try to convert big docker containers into singularity containers as the singularity cache can eat a lot of disk space.</p>"},{"location":"intro-evolving/03_LUMI_access/#project-management","title":"Project management","text":"<p>A large system like LUMI with many entities giving independent access to the system to users  needs an automated system to manage those projects and users. There are two such systems for LUMI. CSC, the hosting institution from Finland, uses its own internal system to manage projects allocated on the Finnish national share. This system manages the \"642\"-projects. The other system is called Puhuri and is developed in a collaboration between the Nordic countries to manage more than just LUMI projects. It can be used to manage multiple supercomputers but also to manage access to other resources such as experimental equipment.  Puhuri projects can span multiple resources (e.g., multiple supercomputers so that you can create a workflow involving Tier-2, Tier-1 and Tier-0 resources).</p> <p>In Belgium two entities manage projects for the Belgian LUMI organisation: VSC and C\u00c9CI. These entities are called the resource allocators.</p> <p>All projects allocated by Belgium ara managed through the Puhuri system, and VSC and C\u00c9CI  both have their own zone in that system. For Belgium it is only used to manage access to LUMI, not to any of the VSC, C\u00c9CI or Ceanero systems or other infrastructure. Belgian users log in to the Puhuri portal via MyAccessID, which is a G\u00c9ANT service. G\u00c9ANT is  the international organisation that manages the research network in Europe.  MyAccessID then in turn connects to your institute identity provider and a number of alternatives. It is important that you always use the same credentials to log in via MyAccessID, otherwise you create another user in MyAccessID that is unknown to Puhuri and get all kinds of strange error messages.</p> <p>The URL to the Puhuri portal is: puhuri-portal.neic.no.</p> <p>Puhuri can be used to check your remaining project resources, but once your user account  on LUMI is created it is very easy to do this on the command line with the <code>lumi-workspaces</code> command.</p> <p>Web links</p> <ul> <li>Puhuri documentation, look for the \"User Guides\".     Resource allocations done by VSC or C\u00c9CI are managed via a     shared puhuri portal,     though we recommend having a look at the      \"New interface\" documentation.</li> <li>The <code>lumi-workspaces</code> command is provided through the [`lumi-tools module]](https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/l/lumi-tools/#lumi-workspaces)     which is loaded by default. The command will usually give the output you need when used     without any argument.</li> </ul>"},{"location":"intro-evolving/03_LUMI_access/#file-spaces","title":"File spaces","text":"<p>LUMI has file spaces that are linked to a user account and file spaces that are linked to projects.</p> <p>The only permanent file space linked to a user account is the home directory which is of the form <code>/users/&lt;my_uid&gt;</code>. It is limited in both size and number of files it can contain, and neither limit can be expanded. It should only be used for things that are not project-related and first and foremost for those things that Linux and software automatically stores in a home directory like user-specific software configuration files. It is not billed as users can exist temporarily without an active project but therefore is also very limited in size.</p> <p>Each project also has 4 permanent or semi-permanent file spaces that are all billed against the storage budget of the project.</p> <ol> <li> <p>Permanent (for the duration of the project) storage on a hard disk based Lustre filesystem     accessed via <code>/project/project_46yXXXXXX</code>. This is the place to perform the software installation     for the project (as it is assumed that a project is a coherent amount of work it is only      natural to assume that everybody in the project needs the same software), or to store input data     etc. that will be needed for the duration of the project.</p> </li> <li> <p>Semi-permanent scratch storage on a hard disk based Lustre filesystem accessed via     <code>/scratch/project_46YXXXXXX</code>. Files in this storage space can in principle be erased      automatically after 90 days. This is not happening yet on LUMI, but will be activated if     the storage space starts to fill up.</p> </li> <li> <p>Semi-permanent scratch storage on an SSD based Lustre filesystem accessed via     <code>/flash/project_46YXXXXXX</code>. Files in this storage space can in principle be erased     automatically after 30 days. This is not happening yet on LUMI, but will be activated if     the scratch storage space starts to fill up.</p> </li> <li> <p>Permanent (for the duration of the project) storage on the hard disk based     object filesystem.</p> </li> </ol> <p></p> <p>The use of space in each file space is limited by block and file quota. Block quota limit the capacity you can use, while file quota limit the number of so-called inodes you can use. Each file, each subdirectory and each link use an inode. As we shall see later in this course or as you may have seen in other HPC courses already (e.g., the VSC \"Supercomputers for Starters\" course organised by UAntwerpen), parallel file systems are not build to deal with hundreds of thousands of small files and are very inefficient at that. Therefore block quota on LUMI tend to be rather flexible (except for the home directory) but file quota are rather strict and will not easily get extended. Software installations that require tens of thousands of small files should be done in  containers (e.g., conda installations or any big Python installation) while data should also be organised in proper file formats rather than being dumped on the file system abusing the file system as a database. Quota extensions are currently handled by the central LUMI User Support Team.</p> <p>So storage billing units come from the RA, quota come from the LUMI User Support Team!</p> <p>LUMI has four disk based Lustre file systems that house <code>/users</code>, <code>/project</code> and <code>/scratch</code>. The <code>/project</code> and <code>/scratch</code> directories of your project will always be on the same parallel file system, but your home directory may be on a different one. Both are assigned automatically suring project and account creation and these assignements cannot be changed by the LUMI User Support Team. As htere is a many-to-may mapping between user accounts and projects it is not possible to ensure that user accounts are on the same file system as their main project. In fact, many users enter LUMI for the first time through a course project and not through one of their main compute projects...</p> <p>It is important to note that even though <code>/flash</code> is SSD based storage, it is still a parallel file  system and will not behave the way an SSD in your PC does. The cost of opening and closing a file is still very high due to it being both a networked and a parallel file system rather than a local drive. In fact, the cost for metadata operations is similar as on the hard disk based parallel file systems as both use SSDs to store the metadata. Once a file is opened and with a proper data access pattern (big accesses, properly striped files which we will discuss later in this course) the flash file system can give a lot more bandwidth than the disk based ones.</p> <p>It is important to note that LUMI is not a data archiving service. \"Permanent\" in the above discussion only means \"for the duration of the project\". There is no backup, not even of the home directory. And three months after the end of the project all data from the project is irrevocably deleted from the system. User accounts without project will also be closed, as will user accounts that remain inactive for several months, even if an active project is still attached to them.</p> <p>If you run out of storage billing units, access to the job queues or even to the storage  can be blocked and you should contact your resource allocator for extra billing units. Our experience within Belgium is that projects tend to heavily under-request storage billing units. It is important that you clean up after a run as LUMI is not meant for long-term data archiving. But at the same time it is completely normal that you cannot do  so right after a run, so data from a run has to stay on the system for a few days or weeks, and you need to budget for that in your project request.</p> <p>Web links:</p> <ul> <li>Overview of storage systems on LUMI</li> <li>Billing policies (includes those for storage)</li> </ul>"},{"location":"intro-evolving/03_LUMI_access/#access","title":"Access","text":"<p>LUMI currently has 4 login nodes through which users can enter the system via key-based ssh. The generic name of those login nodes is <code>lumi.csc.fi</code>. Using the generic names will put you onto one of the available nodes more or less at random and will avoid contacting a login node that is down for maintenance. However, in some cases one needs to enter a specific login node.  E.g., tools for remote editing or remote file synchronisation such as Visual Studio Code or Eclipse  usually don't like it if they get a different node every time they try to connect, e.g., because  they may start a remote server and try to create multiple connections to that server. In that case you have to use a specific login node, which you can do through the names <code>lumi-uan01.csc.fi</code> up to <code>lumi-uan04.csc.fi</code>.  (UAN is the abbreviation for User Access Node, the term Cray uses for login nodes.)</p> <p>Key management is for most users done via MyAccessID: mms.myaccessid.org. This is the case for all user accounts who got their first project on LUMI via Puhuri, which is the case for almost all Belgian users. User accounts that were created via the My CSC service have to use the my.csc.fi portal to manage their keys. It recently became possible to link your account in My CSC to MyAccessID so that you do not get a second account on LUMI ones you join a  Puhuri-managed project, and in this case your keys are still managed through the My CSC service. But this procedure is only important for those LUMI-BE users who may have gotten their first access  to LUMI via a project managed by CSC.</p> <p>There is currently not much support for GUI applications on LUMI.  Running X11 over ssh (via <code>ssh -X</code>) is unbearibly slow for users located in Belgium.  The alternative is some support offered for VNC, though the window manager and fonts used by the server do look a little dated. Access is possible via a browser or VNC client.  On the system, check for the  <code>lumi-vnc</code> module.</p> <p>In the future we plan to provide a web interface via Open OnDemand, but there is no date set yet for availability. The setup of Open OnDemand is not as easy as many would like you to believe as  the product does suffer from a number of security issues and needs to be linked to multiple  authentication services.</p> <p>Web links:</p> <ul> <li>LUMI documentation on logging in to LUMI and creating suitable SSH keys</li> <li>CSC documentation on linking My CSC to MyAccessID</li> </ul>"},{"location":"intro-evolving/03_LUMI_access/#data-transfer","title":"Data transfer","text":"<p>There are currently two main options to transfer data to and from LUMI.</p> <p>The first one is to use sftp to the login nodes, authenticating via your ssh key.  There is a lot of software available for all major operating systems, both command line based and GUI based. The sftp protocol can be very slow over high latency connections. This is because it is a protocol that opens only a single stream for communication with the remote host, and the bandwidth one can reach via a single stream in the  TCP network protocol used for such connections, is limited not only by the bandwidth of all links involved but also by the latency. After sending a certain amount of data, the sender will wait for a confirmation that the data has arrived, and if the latency is hight, that confirmation takes more time to reach the sender, limiting the effective bandwidth that can be reached over the connection. LUMI is not to blame for that; the whole path from the system from which you initiate the connection to LUMI is responsible and every step adds to the latency. We've seen many cases where the biggest contributor to the latency was actually the campus network of the user.</p> <p>The second important option is to transfer data via the object storage system LUMI-O. To transfer data to LUMI, you'd first push the data to LUMI-O and then on LUMI pull it  from LUMI-O. When transferring data to your home institute, you'd first push it onto LUMI-O from LUMI and then pull the data from LUMI-O to your work machine.  LUMI offers some support for various tools, including  rclone and S3cmd. There also exist many GUI clients to access object storage.  Even though in principle any tool that can connect via the S3 protocol can work, the LUMI User Support Team nor the local support in Belgium can give you instructions for every possible tool.  Those tools for accessing object storage tend to set up multiple data streams and hence will offer a much higher effective bandwidth, even on high latency connections. These tools work with keys which are different from SSH keys and temporary in nature. They can be obtained from auth.lumidata.eu where you need to log in either via MyAccessID (if your project is managed through Puhuri) or your \"My CSC\" account (if your project is managed via that platform).</p> <p>Alternatively, you can also chose to access external servers from LUMI if you have client software that runs on LUMI (or if that software is already installed on LUMI, e.g., rclone and S3cmd),</p> <p>Unfortunately there is no support yet for Globus or other forms of gridFTP. </p> <p>Web links:</p> <ul> <li>Documentation for the LUMI-O object storage service</li> <li> <p>Software for LUMI-O on LUMI is provided throug the     <code>lumio</code> module which     provides the configuration tool on top of the software and the     <code>lumio-ext-tools</code> module     providing rclone, S3cmd and restic and links to the documentation of those tools.</p> <ul> <li>rclone documentation</li> <li>S3cmd tools usage</li> <li>restic documentation</li> </ul> </li> </ul>"},{"location":"intro-evolving/03_LUMI_access/#local-trainings","title":"Local trainings","text":"<p>Any HPC introductory training in Belgium covers logging in via ssh and transferring files. Such a course is a prerequisite for this section.</p>"},{"location":"intro-evolving/04_Modules/","title":"Modules on LUMI","text":"<p>Intended audience</p> <p>As this course is designed for people already familiar with HPC systems and as virtually any cluster nowadays uses some form of module environment, this section assumes that the reader is already familiar with a module environment but not necessarily the one used on LUMI.</p> <p>However, even if you are very familiar with Lmod it makes sense to go through these notes as not every Lmod configuration is the same.</p>"},{"location":"intro-evolving/04_Modules/#module-environments","title":"Module environments","text":"<p>An HPC cluster is a multi-user machine. Different users may need different  versions of the same application, and each user has their own preferences for the environment. Hence there is no \"one size fits all\" for HPC and mechanisms are needed to support the diverse requirements of multiple users on a single machine. This is where modules play an important role. They are commonly used on HPC systems to enable users to create  custom environments and select between multiple versions of applications. Note that this also implies that applications on HPC systems are often not installed in the regular directories one would expect from the documentation of some packages, as that location may not even always support proper multi-version installations and as system administrators prefer to have a software stack which is as isolated as possible from the system installation to keep the image that has to be loaded on the compute nodes small.</p> <p>Another use of modules not mentioned on the slide is to configure the programs that are being activated. E.g., some packages expect certain additional environment variables to be set and modules can often take care of that also.</p> <p>There are 3 systems in use for module management. The oldest is a C implementation of the commands using module files written in Tcl. The development of that system stopped around 2012, with version 3.2.10.  This system is supported by the HPE Cray Programming Environment. A second system builds upon the C implementation but now uses Tcl also for the module command and not only for the module files. It is developed in France at the C\u00c9A compute centre. The version numbering was continued from the C implementation, starting with version 4.0.0.  The third system and currently probably the most popular one is Lmod, a version written in Lua with module files also written in Lua. Lmod also supports most Tcl module files. It is also supported by HPE Cray, though they tend to be a bit slow in following versions. The original developer of Lmod, Robert McLay, retired  at the end of August 2023, but TACC, the centre where he worked, is committed to at least maintain Lmod though it may not see much new development anymore.</p> <p>On LUMI we have chosen to use Lmod. As it is very popular, many users may already be familiar with it, though it does make sense to revisit some of the commands that are specific for Lmod and differ from those in the two other implementations.</p> <p>It is important to realise that each module that you see in the overview corresponds to a module file that contains the actual instructions that should be executed when loading  or unloading a module, but also other information such as some properties of the module, information for search and help information.</p> Links <ul> <li>Old-style environment modules on SourceForge</li> <li>TCL Environment Modules home page on SourceForge and the     development on GitHub</li> <li>Lmod documentation and      Lmod development on GitHub</li> </ul> <p>I know Lmod, should I continue?</p> <p>Lmod is a very flexible tool. Not all sites using Lmod use all features, and Lmod can be configured in different ways to the extent that it may even look like a very different module system for people coming from another cluster. So yes, it makes sense to continue reading as Lmod on LUMI may have some tricks that are not available on your home cluster. E.g., several of the features that  we rely upon on LUMI are disabled on the UGhent clusters to make them more similar to clusters running the C/Tcl module implementation which was used in the early days of the VSC.</p> <p>Standard OS software</p> <p>Most large HPC systems use enterprise-level Linux distributions: derivatives of the stable Red Hat or SUSE distributions. Those distributions typically have a life span of 5 years or even more during which they receive security updates and ports of some newer features, but some of the core elements of such a distribution stay at the same version to break as little as possible between minor version updates. Python and the system compiler are typical examples of those. Red Hat 8 and SUSE Enterprise Linux 15 both came with Python 3.6 in their first version, and  keep using this version as the base version of Python even though official support from the Python Software Foundation has long ended. Similarly, the default GNU compiler version offered on those system also remains the same. The compiler may not even fully support some of the newer CPUs the code is running on. E.g., the system compiler of SUSE Enterprise Linux 15, GCC 7.5, does not support the zen2 \"Rome\" or zen3 \"Milan\" CPUs on LUMI. </p> <p>HPC systems will usually offer newer versions of those system packages through modules and users should always use those. The OS-included tools are really only for system management and system related tasks and serve a different purpose which actually requires a version that remains stable across a number of updates to not break things at the core of the OS. Users however will typically have a choice between several newer versions through modules, which also enables them to track the evolution and transition to a new version at the best suited moment.</p>"},{"location":"intro-evolving/04_Modules/#exploring-modules-with-lmod","title":"Exploring modules with Lmod","text":"<p>Contrary to some other module systems, or even some other Lmod installations, not all modules are immediately available for loading. So don't be disappointed by the few modules you will see with <code>module available</code> right after login. Lmod has a so-called hierarchical setup that tries to protect you from being confronted with all modules at the same time, even those that may conflict with  each other, and we use that to some extent on LUMI. Lmod distinguishes between installed modules and available modules. Installed modules are all modules on the system that can be loaded one way or another, sometimes through loading other modules first. Available modules are all those modules that can be loaded at a given point in time without first loading other modules.</p> <p>The HPE Cray Programming Environment also uses a hierarchy though it is not fully implemented in the way the Lmod developer intended so that some features do not function as they should.</p> <ul> <li>For example, the <code>cray-mpich</code> module can only be loaded if both a network target module and a     compiler module are loaded (and that is already the example that is implemented differently from     what the Lmod developer had in mind). </li> <li>Another example is the performance monitoring tools. Many of those     tools only become available after loading the <code>perftools-base</code> module. </li> <li>Another example is the     <code>cray-fftw</code> module which requires a processor target module to be loaded first.</li> </ul> <p>Lmod has several tools to search for modules. </p> <ul> <li>The <code>module avail</code> command is one that is also     present in the various Environment Modules implementations and is the command to search in the     available modules. </li> <li>But Lmod also has other commands, <code>module spider</code> and <code>module keyword</code>, to      search in the list of installed modules.</li> </ul>"},{"location":"intro-evolving/04_Modules/#benefits-of-a-hierarchy","title":"Benefits of a hierarchy","text":"<p>When the hierarchy is well designed, you get some protection from loading modules that do not work together well. E.g., in the HPE Cray PE it is not possible to load the MPI library built for another compiler than your current main compiler. This is currently not exploited as much as we could on LUMI, mainly because we realised at the start that too many users are not familiar enough with hierarchies and would get confused more than the hierarchy helps them.</p> <p>Another benefit is that when \"swapping\" a module that makes other modules available with a different one, Lmod will try to look for equivalent modules in the list of modules made available by the newly loaded module.</p> <p>An easy example (though a tricky one as there are other mechanisms at play also) it to load a different programming environment in the default login environment right after login:</p> <pre><code>$ module load PrgEnv-aocc\n</code></pre> <p>which results in the next slide:</p> <p></p> <p>The first two lines of output are due to to other mechanisms that are at work here,  and the order of the lines may seem strange but that has to do with the way Lmod works internally. Each of the PrgEnv modules hard loads a compiler module which is why Lmod tells you that it is loading <code>aocc/3.2.0</code>. However, there is also another mechanism at work that causes <code>cce/16.0.0</code> and <code>PrgEnv-cray/8.4.0</code> to be unloaded, but more about that in the next subsection (next slide).</p> <p>The important line for the hierarchy in the output are the lines starting with  \"Due to MODULEPATH changes...\". Remember that we said that each module has a corresponding module file. Just as binaries on a system, these are organised in a directory structure, and there is a path, in this case MODULEPATH, that determines where Lmod will look for module files. The hierarchy is implemented with a directory structure and the environment variable MODULEPATH, and when the <code>cce/16.0.0</code> module was unloaded and <code>aocc/3.2.0</code> module was loaded, that  MODULEPATH was changed. As a result, the version of the cray-mpich module for the  <code>cce/16.0.0</code> compiler became unavailable, but one with the same module name for the <code>aocc/3.2.0</code> compiler became available and hence Lmod unloaded the version for the <code>cce/16.0.0</code> compiler as it is no longer available but loaded the matching one for the <code>aocc/3.2.0</code> compiler. </p>"},{"location":"intro-evolving/04_Modules/#about-module-names-and-families","title":"About module names and families","text":"<p>In Lmod you cannot have two modules with the same name loaded at the same time. On LUMI, when you load a module with the same name as an already loaded module, that other module will be unloaded automatically before loading the new one. There is  even no need to use the <code>module swap</code> command for that (which in Lmod corresponds to a <code>module unload</code> of the first module and a <code>module load</code> of the second). This gives you an automatic protection against some conflicts if the names of the modules are properly chosen. </p> <p>Note</p> <p>Some clusters do not allow the automatic unloading of a module with the same name as the one you're trying to load, but on LUMI we felt that this is a  necessary feature to fully exploit a hierarchy.</p> <p>Lmod goes further also. It also has a family concept: A module can belong to a family (and at most 1) and no two modules of the same family can be loaded together.  The family property is something that is defined in the module file. It is commonly  used on systems with multiple compilers and multiple MPI implementations to ensure  that each compiler and each MPI implementation can have a logical name without  encoding that name in the version string (like needing to have <code>compiler/gcc-11.2.0</code> or <code>compiler/gcc/11.2.0</code> rather than <code>gcc/11.2.0</code>), while still having an easy way to avoid having two  compilers or MPI implementations loaded at the same time.  On LUMI, the conflicting module of the same family will be unloaded automatically when loading another module of that particular family.</p> <p>This is shown in the example in the previous subsection (the <code>module load PrgEnv-gnu</code> in  a fresh long shell) in two places. It is the mechanism that unloaded <code>PrgEnv-cray</code> when loading <code>PrgEnv-gnu</code> and that then unloaded <code>cce/16.0.1</code> when the  <code>PrgEnv-gnu</code> module loaded the <code>gcc/11.2.0</code> module.</p> <p>Note</p> <p>Some clusters do not allow the automatic unloading of a module of the same family as the one you're trying to load and produce an error message instead. On LUMI, we felt that this is a necessary feature to fully exploit the  hierarchy and the HPE Cray Programming Environment also relies very much on this feature being enabled to make live easier for users.</p>"},{"location":"intro-evolving/04_Modules/#extensions","title":"Extensions","text":"<p>It would not make sense to have a separate module for each of the hundreds of R packages or tens of Python packages that a software stack may contain. In fact, as the software for each module is installed in a separate directory it would also create a performance problem due to excess directory accesses simply to find out where a command is located, and very long search path environment variables such as PATH or the various variables packages such as Python, R or Julia use to find extension packages. On LUMI related packages are often bundled in a single module. </p> <p>Now you may wonder: If a module cannot be simply named after the package it contains as it contains several ones, how can I then find the appropriate module to load? Lmod has a solution for that through the so-called extension mechanism. An Lmod module can define extensions, and some of the search commands for modules will also search in the extensions of a module. Unfortunately, the HPE Cray PE cray-python and cray-R modules do not provide that  information at the moment as they too contain several packages that may benefit from linking to optimised math libraries.</p>"},{"location":"intro-evolving/04_Modules/#searching-for-modules-the-module-spider-command","title":"Searching for modules: the module spider command","text":"<p>There are three ways to use <code>module spider</code>, discovering software in more and more detail.</p> <ol> <li> <p><code>module spider</code> by itself will show a list of all installed software with a short description.     Software is bundled by name of the module, and it shows the description taken from the default     version. <code>module spider</code> will also look for \"extensions\" defined in a module and show those also     and mark them with an \"E\". Extensions are a useful Lmod feature to make clear that a module offers     features that one would not expect from its name. E.g., in a Python module the extensions could be     a list of major Python packages installed in the module which would allow you to find <code>NumPy</code> if     it were hidden in a module with a different name. This is also a very useful feature to make     tools that are bundled in one module to reduce the module clutter findable.</p> </li> <li> <p><code>module spider</code> with the name of a package will show all versions of that package installed on     the system. This is also case-insensitive.      The spider command will not only search in module names for the package, but also in extensions     of the modules and so will be able to tell you that a package is delivered by another module. See      Example 4 below where we will search for the CMake tools.</p> </li> <li> <p>The third use of <code>module spider</code> is with the full name of a module.      This shows two kinds of information. First it shows which combinations of other modules one     might have to load to get access to the package. That works for both modules and extensions     of modules. In the latter case it will show both the module, and other modules that you might     have to load first to make the module available.     Second it will also show help information for the module if the module file provides      such information. </p> </li> </ol>"},{"location":"intro-evolving/04_Modules/#example-1-running-module-spider-on-lumi","title":"Example 1: Running <code>module spider</code> on LUMI","text":"<p>Let's first run the <code>module spider</code> command. The output varies over time, but at the time of writing, and leaving out a lot of the output, one would have gotten:</p> <p></p> <p></p> <p></p> <p>On the second screen we see, e.g., the ARMForge module which was available in just a single version at that time, and then Autoconf where the version is in blue and followed by <code>(E)</code>. This denotes that the Autoconf package is actually provided as an extension of another module, and one of the next examples will tell us how to figure out which one.</p> <p>The third screen shows the last few lines of the output, which actually also shows some help information for the command.</p>"},{"location":"intro-evolving/04_Modules/#example-2-searching-for-the-fftw-module-which-happens-to-be-provided-by-the-pe","title":"Example 2: Searching for the FFTW module which happens to be provided by the PE","text":"<p>Next let us search for the popular FFTW library on LUMI:</p> <pre><code>$ module spider FFTW\n</code></pre> <p>produces</p> <p></p> <p>This shows that the FFTW library is actually provided by the <code>cray-fftw</code> module and was at the time that this was tested available in 3 versions.  Note that (a) it is not case sensitive as FFTW is not in capitals in the module name and (b) it also finds modules where the argument of module spider is only part of the name.</p> <p>The output also suggests us to dig a bit deeper and  check for a specific version, so let's run</p> <pre><code>$ module spider cray-fftw/3.3.10.3\n</code></pre> <p>This produces:</p> <p></p> <p></p> <p>We now get a long list of possible combinations of modules that would enable us to load this module. What these modules are will be explained in the next session of this course. However, it does show a weakness when module spider is used with the HPE Cray PE. In some cases, not all possible combinations are shown (and this is the case here as the module is actually available directly after login and also via some other combinations of modules that are not shown). This is because the HPE Cray Programming Environment is system-installed and sits next to the application software stacks that are managed differently, but in some cases also because the HPE Cray PE sometimes fails to give the complete combination of modules that is needed. The command does work well with the software managed by the LUMI User Support Team as the next two examples will show.</p>"},{"location":"intro-evolving/04_Modules/#example-3-searching-for-gnuplot","title":"Example 3: Searching for GNUplot","text":"<p>To see if GNUplot is available, we'd first search for the name of the package:</p> <pre><code>$ module spider GNUplot\n</code></pre> <p>This produces:</p> <p></p> <p></p> <p>The output again shows that the search is not case sensitive which is fortunate as uppercase and lowercase letters are not always used in the same way on different clusters. Some management tools for scientific software stacks will only use lowercase letters, while the package we use for the LUMI software stacks often uses both.</p> <p>We see that there are a lot of versions installed on the system and that the version actually contains more  information (e.g., <code>-cpeGNU-23.09</code>) that we will explain in the next part of this course. But you might of course guess that it has to do with the compilers that were used. It may look strange to you to have the same software built with different compilers. However, mixing compilers is sometimes risky as a library compiled with one compiler may not work in an executable compiled with another one, so to enable workflows that use multiple tools we try  to offer many tools compiled with multiple compilers (as for most software we don't use rpath linking which could help to solve that problem). So you want to chose the appropriate line in terms of the other software that you will be using.</p> <p>The output again suggests to dig a bit further for more information, so let's try</p> <pre><code>$ module spider gnuplot/5.4.8-cpeGNU-23.09\n</code></pre> <p>This produces:</p> <p></p> <p></p> <p>In this case, this module is provided by 3 different combinations of modules that also will be explained in the next part of this course. Furthermore, the output of the command now also shows some help information about the module, with some links to further documentation available on the system or on the web. The format of the output is generated automatically by the software installation tool that we use and we sometimes have to do some effort to fit all information in there.</p> <p>For some packages we also have additional information in our LUMI Software Library web site so it is often worth looking there also.</p>"},{"location":"intro-evolving/04_Modules/#example-4-searching-for-an-extension-of-a-module-cmake","title":"Example 4: Searching for an extension of a module: CMake.","text":"<p>The <code>cmake</code> command on LUMI is available in the operating system image, but as is often the case with such tools distributed with the OS, it is a rather old version and you may want to use a newer one.</p> <p>If you would just look through the list of available modules, even after loading some other modules to activate a larger software stack, you will not find any module called <code>CMake</code> though. But let's use the powers of <code>module spider</code> and try</p> <pre><code>$ module spider CMake\n</code></pre> <p>which produces</p> <p></p> <p>The output above shows us that there are actually four other versions of CMake on the system, but their version is followed by <code>(E)</code> which says that they are extensions of other modules. There is no module called <code>CMake</code> on the system.  But Lmod already tells us how to find out which module actually provides the CMake tools. So let's try</p> <pre><code>$ module spider CMake/3.27.7\n</code></pre> <p>which produces</p> <p></p> <p>This shows us that the version is provided by a number of <code>buildtools</code> modules, and for each of those modules also shows us which other modules should be loaded to get access to the commands. E.g., the first line tells us that there is a module <code>buildtools/23.09</code> that provides that version of CMake, but that we first need to load some other modules, with <code>LUMI/23.09</code> and <code>partition/L</code> (in that order)  one such combination.</p> <p>So in this case, after</p> <pre><code>$ module load LUMI/23.09 partition/L buildtools/23.09\n</code></pre> <p>the <code>cmake</code> command would be available.</p> <p>And you could of course also use</p> <pre><code>$ module spider buildtools/23.09\n</code></pre> <p>to get even more information about the buildtools module, including any help included in the module.</p>"},{"location":"intro-evolving/04_Modules/#alternative-search-the-module-keyword-command","title":"Alternative search: the module keyword command","text":"<p>Lmod has a second way of searching for modules: <code>module keyword</code>. It searches  in some of the information included in module files for the given keyword, and shows in which modules the keyword was found. We do an effort to put enough information in the modules to make this a suitable additional way to discover software that is installed on the system.</p> <p>Let us look for packages that allow us to download software via the <code>https</code> protocol. One could try</p> <pre><code>$ module keyword https\n</code></pre> <p>which produces the following output:</p> <p></p> <p></p> <p><code>cURL</code> and <code>wget</code> are indeed  two tools that can be used to fetch files from the internet.</p> <p>LUMI Software Library</p> <p>The LUMI Software Library also has a search box in the upper right. We will see in the next section of this course that much of the software of LUMI is managed through a tool called EasyBuild, and each module file corresponds to an EasyBuild recipe which is a file with the <code>.eb</code> extension. Hence the keywords can also be found in the EasyBuild recipes which are included in this web site, and from a page with an EasyBuild recipe (which may not mean much for you) it is easy to go back to the software package page itself for more information. Hence you can use the search box to search for packages that may not be installed on the system.</p> <p>The example given above though, searching for <code>https</code>, would not work via that box as most EasyBuild recipes include https web links to refer to, e.g., documentation and would be  shown in the result.</p> <p>The LUMI Software Library site includes both software installed in our central software stack and software for which we make customisable build recipes available for user installation, but more about that in the tutorial section on LUMI software stacks.</p>"},{"location":"intro-evolving/04_Modules/#sticky-modules-and-the-module-purge-command","title":"Sticky modules and the module purge command","text":"<p>On some systems you will be taught to avoid <code>module purge</code> as many HPC systems do their default user configuration also through modules. This advice is often given on Cray systems as it is a common practice to preload a suitable set of target modules and a programming environment. On LUMI both are used. A default programming environment and set of target modules suitable for the login nodes is preloaded when you log in to the system, and next the <code>init-lumi</code> module is loaded which in turn makes the LUMI software stacks available that we will discuss in the next session.</p> <p>Lmod however has a trick that helps to avoid removing necessary modules and it is called sticky modules. When issuing the <code>module purge</code> command these modules are automatically reloaded. It is very important to realise that those modules will not just be kept \"as is\" but are in fact unloaded and loaded again as we shall see later that this may have consequences. It is still possible to force unload all these modules using <code>module --force purge</code> or selectively unload those using <code>module --force unload</code>.</p> <p>The sticky property is something that is defined in the module file and not used by the module files ot the HPE Cray Programming Environment, but we shall see that there is a partial workaround for this in some of the LUMI software stacks. The <code>init-lumi</code> module mentioned above though is a sticky module, as are the modules that activate a software stack so that you don't have to start from scratch if you have already chosen a software stack but want to clean up your environment.</p> <p>Let us look at the output of the <code>module avail</code> command, taken just after login on the system at the time of writing of these notes (the exact list of modules shown is a bit fluid):</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Next to the names of modules you sometimes see one or more letters. The <code>(D)</code> means that that is currently the default version of the module, the one that will be loaded if you do not specify a version. Note that the default version may depend on other modules that are already loaded as we have seen in the discussion of the programming environment.</p> <p>The <code>(L)</code> means that a module is currently loaded.</p> <p>The <code>(S)</code> means that the module is a sticky module.</p> <p>Next to the <code>rocm</code> module (on the fourth screen) you see <code>(D:5.0.2:5.2.0)</code>.  The <code>D</code> means that this version of the module, <code>5.2.3</code>, is currently the default on the system. The two version numbers next to this module show that the module can also  be loaded as <code>rocm/5.0.2</code> and <code>rocm/5.2.0</code>. These are two modules that were removed from the system during the last update of the system, but version 5.2.3 can be loaded as a replacement of these modules so that software that used the removed modules may still work without recompiling.</p> <p>At the end of the overview the extensions are also shown. If this would be fully implemented on LUMI, the list might become very long. However, as we shall see next, there is an easy way to hide those from view.</p>"},{"location":"intro-evolving/04_Modules/#changing-how-the-module-list-is-displayed","title":"Changing how the module list is displayed","text":"<p>You may have noticed in the above example that we don't show directories of module files in the overview (as is the case on most clusters) but descriptive texts about the module group. This is just one view on the module tree though, and it can be changed easily by loading a  version of the <code>ModuleLabel</code> module.</p> <ul> <li><code>ModuleLabel/label</code> produces the default view of the previous example</li> <li><code>ModuleLabel/PEhierarchy</code> still uses descriptive texts but will show the whole      module hierarchy of the HPE Cray Programming Environment.</li> <li><code>ModuleLabel/system</code> does not use the descriptive texts but shows module directories instead.</li> </ul> <p>When using any kind of descriptive labels, Lmod can actually bundle module files from different  directories in a single category and this is used heavily when <code>ModuleLabel/label</code> is loaded  and to some extent also when <code>ModuleLabel/PEhierarchy</code> is loaded.</p> <p>It is rather hard to provide multiple colour schemes in Lmod, and as we do not know how your  terminal is configured it is also impossible to find a colour scheme that works for all users. Hence we made it possible to turn on and off the use of colours by Lmod through the <code>ModuleColour/on</code> and <code>ModuleColour/off</code> modules.</p> <p>As the module extensions list in the output of <code>module avail</code> could potentially become very long over time (certainly if there would be Python or R modules installed with EasyBuild that show all included Python or R packages in that list) you may want to hide those. You can do this by loading the <code>ModuleExtensions/hide</code> module and undo this again by loading <code>ModuleExtensions/show</code>.</p> <p>We also hide some modules from regular users because we think they are not useful at all for regular users or not useful in the context you're in at the moment.  You can still load them if you know they exist and specify the full version but  you cannot see them with <code>module available</code>. It is possible though to still show most if not all of  them by loading <code>ModulePowerUser/LUMI</code>. Use this at your own risk however, we will not help you to make things work if you use modules that are hidden in the context you're in or if you try to use any module that was designed for us to maintain the system and is therefore hidden  from regular users.</p> <p>Example</p> <p>An example that will only become clear in the next session: When working with the software stack called <code>LUMI/23.09</code>, which is built upon the HPE Cray Programming Environment version 23.09, all (well, most) of the modules corresponding to other version of the Cray PE are hidden.</p>"},{"location":"intro-evolving/04_Modules/#getting-help-with-the-module-help-command","title":"Getting help with the module help command","text":"<p>Lmod has the <code>module help</code> command to get help on modules</p> <pre><code>$ module help\n</code></pre> <p>without further arguments will show some help on the <code>module</code> command. </p> <p>With the name of a module specified, it will show the help information for the default version of that module, and with a full name and version specified it will show this information specifically for that version of the module. But note that <code>module help</code> can only show help for currently available modules.</p> <p>Try, e.g., the following commands:</p> <pre><code>$ module help cray-mpich\n$ module help cray-python/3.10.10\n$ module help buildtools/23.09\n</code></pre> <p>Lmod also has another command that produces more limited information (and is currently not fully exploited on LUMI): <code>module whatis</code>. It is more a way to tag a module with different kinds of information, some of  which has a special meaning for Lmod and is used at some places, e.g., in the output of <code>module spider</code> without arguments.</p> <p>Try, e.g.,:</p> <pre><code>$ module whatis Subversion\n$ module whatis Subversion/1.14.2\n</code></pre>"},{"location":"intro-evolving/04_Modules/#a-note-on-caching","title":"A note on caching","text":"<p>Modules are stored as (small) files in the file system. Having a large module system with much software preinstalled for everybody means a lot of small files which will make our Lustre file system very unhappy. Fortunately Lmod does use caches by default. On LUMI we currently have no  system cache and only a user cache. That cache can be found in <code>$HOME/.cache/lmod</code> (and in some versions of LMOD in <code>$HOME/.lmod.d/.cache</code>). </p> <p>That cache is also refreshed automatically every 24 hours. You'll notice when this happens as, e.g., the <code>module spider</code> and <code>module available</code> commands will be slow during the rebuild. you may need to clean the cache after installing new software as on LUMI Lmod does not always detect changes to the installed software,</p> <p>Sometimes you may have to clear the cache also if you get very strange answers from  <code>module spider</code>. It looks like the non-standard way in which the HPE Cray Programming Environment does certain things in Lmod can cause inconsistencies in the cache. This is also one of the reasons whey we do not yet have a central cache for that  software that is installed in the central stacks as we are not sure when that cache is in good shape.</p>"},{"location":"intro-evolving/04_Modules/#a-note-on-other-commands","title":"A note on other commands","text":"<p>As this tutorial assumes some experience with using modules on other clusters, we haven't paid much attention to some of the basic commands that are mostly the same across all three module environments implementations.  The <code>module load</code>, <code>module unload</code> and <code>module list</code> commands work largely as you would expect, though the output style of <code>module list</code> may be a little different from what you expect. The latter may show some inactive modules. These are modules that were loaded at some point, got unloaded when a module closer to the root of the hierarchy of the module system got unloaded, and they will be reloaded automatically when that module or an equivalent (family or name) module is loaded that makes this one or an equivalent module available again.</p> <p>Example</p> <p>To demonstrate this, try in a fresh login shell (with the lines starting with a <code>$</code> the commands that you should enter at the command prompt):</p> <pre><code>$ module unload craype-network-ofi\n\nInactive Modules:\n  1) cray-mpich\n\n$ module load craype-network-ofi\n\nActivating Modules:\n  1) cray-mpich/8.1.27\n</code></pre> <p>The <code>cray-mpich</code> module needs both a valid network architecture target module to be loaded (not <code>craype-network-none</code>) and a compiler module. Here we remove the network target module which inactivates the <code>cray-mpich</code> module, but the module gets reactivated again as soon as the network target module is reloaded.</p> <p>The <code>module swap</code> command is basically equivalent to a <code>module unload</code> followed by a <code>module load</code>.  With one argument it will look for a module with the same name that is loaded and unload that one  before loading the given module. With two modules, it will unload the first one and then load the second one. The <code>module swap</code> command is not really needed on LUMI as loading a conflicting module (name or family) will automatically unload the previously loaded one. However, in case of replacing  a module of the same family with a different name, <code>module swap</code> can be a little faster than just a <code>module load</code> as that command will need additional operations as in the first step it will  discover the family conflict and then try to resolve that in the following steps (but explaining that in detail would take us too far in the internals of Lmod).</p>"},{"location":"intro-evolving/04_Modules/#links","title":"Links","text":"<p>These links were OK at the time of the course. This tutorial will age over time though and is not maintained but may be replaced with evolved versions when the course is organised again, so links may break over time.</p> <ul> <li>Lmod documentation and more specifically     the User Guide for Lmod which is the part specifically for regular users who do not     want to design their own modules.</li> <li>Information on the module environment in the LUMI documentation</li> </ul>"},{"location":"intro-evolving/04_Modules/#local-materials","title":"Local materials","text":"<ul> <li> <p>VSC</p> <ul> <li> <p>VSC@UAntwerpen: Modules are covered in the HPC@UAntwerp introduction</p> </li> <li> <p>VSC@VUB: Modules are covered in the HPC Introduction course,</p> <p>with local documentation in the \"Module System\" section of the documentation</p> </li> <li> <p>VSC@UGent: Modules are briefly coverd in the \"Introduction to HPC-UGent\" course. The Lmod setup on the     clusters at UGent is more restrictive than on LUMI with several usefull features disabled.</p> <ul> <li>Slides and YouTube recording     of the June 9, 2023 session.</li> </ul> </li> <li> <p>VSC@KULeuven: Modules are discussed in the Linux for HPC course</p> </li> </ul> </li> <li> <p>C\u00c9CI: </p> <ul> <li> <p>Choosing and activating software with system modules on C\u00c9CI clusters     from the fall 2022 introductory courses.</p> </li> <li> <p>YouTube video: Modules: How to find/use software on cluster     from the fall 2020 introductory courses.</p> </li> </ul> </li> </ul>"},{"location":"intro-evolving/05_Software_stacks/","title":"LUMI Software Stacks","text":"<p>In this section we discuss</p> <ul> <li>Several of the ways in which we offer software on LUMI</li> <li>Managing software in our primary software stack which is based on EasyBuild</li> </ul>"},{"location":"intro-evolving/05_Software_stacks/#the-software-stacks-on-lumi","title":"The software stacks on LUMI","text":""},{"location":"intro-evolving/05_Software_stacks/#design-considerations","title":"Design considerations","text":"<ul> <li> <p>LUMI is a very leading edge and also an inhomogeneous machine. Leading edge often implies     teething problems and inhomogeneous doesn't make life easier either.</p> <ol> <li>It uses a novel interconnect which is an extension of Ethernet rather than being based on InfiniBand,      and that interconnect has a different software stack of your typical Mellanox InfiniBand cluster. </li> <li>It also uses a relatively new GPU architecture, AMD CDNA2, with an immature software ecosystem.      The GPU nodes are really GPU-first, with the interconnect cards connected directly to the GPU packages      and only one CPU socket, and another feature which is relatively new: the option to use a partly coherent fully unified memory     space between the CPU and GPUs, though of course very NUMA. This is a feature that has previously     only been seen in some clusters with NVIDIA P100 and V100 GPUs and IBM Power 8 and 9 CPUs used     for some USA pre-exascale systems, and of course in Apple Silicon M-series but then without the NUMA character     (except maybe for the Ultra version that consists of two dies).</li> <li>LUMI is also inhomogeneous because some nodes have zen2 processors while the two main compute partitions     have zen3-based CPUs, and the compute GPU nodes have AMD GPUs while the visualisation nodes have     NVIDIA GPUs. </li> </ol> <p>Given the novel interconnect and GPU we do expect that both system and application software will be immature at first and evolve quickly, hence we needed a setup that enables us to remain very agile, which leads to different compromises compared to a software stack for a more conventional and mature system as an x86 cluster with NVIDIA GPUs and Mellanox InfiniBand.</p> </li> <li> <p>Users also come to LUMI from 12 different channels, not counting subchannels as some countries have     multiple organisations managing allocations, and those channels all have different expectations about     what LUMI should be and what kind of users should be served. For the major LUMI stakeholder, the EuroHPC JU,     LUMI is a pre-exascale system meant to prepare users and applications to make use of future even large     systems, while some of the LUMI consortium countries see LUMI more as an extension of their tier-1 or     even tier-2 machines.</p> </li> <li> <p>The central support team of LUMI is also relatively small compared to the nature of LUMI with its     many different partitions and storage services and the expected number of projects and users.      Support from users coming in via the national channels will rely a lot on efforts from local organisations     also. So we must set up a system so that they can support their users without breaking things on     LUMI, and to work with restricted rights. And in fact, LUMI User Support team members also have very limited additional     rights on the machine compared to regular users or support people from the local organisations.     LUST is currently 10 FTE. Compare this to 41 people in the J\u00fclich Supercomputer Centre for software     installation and support only... (I give this number because it was mentioned in a a talk in the     EasyBuild user meeting in 2022.)</p> </li> <li> <p>The Cray Programming Environment is also a key part of LUMI and the environment for which we get     support from HPE Cray. It is however different from more traditional environments such as a typical     Intel oneAPI installation of a typical installation build around the GNU Compiler Collection and Open MPI     or MPICH. The programming environment is installed with the operating system rather than through the     user application software stack hence not managed through the tools used for the application software     stack, and it also works differently with its universal compiler wrappers that are typically configured     through modules. </p> </li> <li> <p>There is an increasing need for customised setups. Everybody wants a central stack as long as their     software is in there but not much more as otherwise it is hard to find, and as long as software is      configured in the way they are used to. And everybody would like LUMI to look as much as possible      as their home system. But this is of course impossible. Moreover, there are more and more conflicts     between software packages and modules are only a partial solution to this problem. The success of     containers, conda and Python virtual environments is certainly to some extent explained by the      need for more customised setups and the need for multiple setups as it has become nearly impossible     to combine everything in a single setup due to conflicts between packages and the dependencies they need.</p> </li> </ul>"},{"location":"intro-evolving/05_Software_stacks/#the-lumi-solution","title":"The LUMI solution","text":"<p>The LUMI User Support Team (LUST) tried to take all these considerations into account  and came up with a solution that may look a little unconventional to many users.</p> <p>In principle there should be a high degree of compatibility between releases of the HPE Cray Programming Environment but LUST decided not to take the risk and build our software for a specific release of the  programming environment, which is also a better fit with the typical tools used to manage a scientific  software stack such as EasyBuild and Spack as they also prefer precise versions for all dependencies and compilers etc. The stack is also made very easy to extend. So LUMI has many base libraries and some packages already pre-installed but also provides an easy and very transparent way to install additional packages in your project space in exactly the same way as is done for the central stack, with the same performance but the benefit that the installation can be customised more easily to the needs of your project. Not everybody needs the same configuration of GROMACS or LAMMPS or other big packages, and in fact a one-configuration-that-works-for-everybody may even be completely impossible due to conflicting options that cannot be used together.</p> <p>For the module system a choice had to be made between two systems supported by HPE Cray. They support  Environment Modules with module files based on the TCL scripting language, but only the old version that is no longer really developed and not the newer versions 4 and 5 developed in France, and Lmod, a module system based on the LUA scripting language that also support many TCL module files through a translation layer. LUMI chose to go with Lmod as LUA is an easier and more modern language to work with and as Lmod is much more powerful than Environment Modules 3, certainly for searching modules.</p> <p>To manage the software installations there was a choice between EasyBuild, which is mostly developed in Europe and hence a good match with a EuroHPC project as EuroHPC wants to develop a European HPC technology stack from hardware to application software, and Spack, a package developed in the USA national labs. Both have their own strengths and weaknesses. LUMI chose to go with EasyBuild as the primary tool for which the LUST also do some development.  However, as we shall see, the EasyBuild installation is not your typical EasyBuild installation that you may be accustomed with from clusters at your home institution. It uses toolchains specifically for the HPE Cray programming environment so recipes need to be adapted. LUMI does offer a growing library of Cray-specific installation recipes though. The whole setup of EasyBuild is done such that you can build on top of the central software stack and such that your modules appear in your module view without having to add directories by hand to environment variables etc. You only need to point to the place where you want to install software for your project as LUMI cannot automatically determine a suitable place. </p> <p>The LUST does offer some help to set up Spack also but it is mostly offered \"as is\" and LUST will not do bug-fixing or development in Spack package files. Spack is very attractive for users who want to set up a personal environment with fully customised versions of the software rather than the rather fixed versions provided by EasyBuild for every version of the software stack. It is possible to specify versions for the main packages that you need and then let Spack figure out a minimal compatible set of dependencies to install  those packages.</p>"},{"location":"intro-evolving/05_Software_stacks/#software-policies","title":"Software policies","text":"<p>As any site, LUMI also has a number of policies about software installation, and these policies are further developed as the LUMI team gains experience in what they can do with the amount of people  in LUST and what they cannot do.</p> <p>LUMI uses a bring-your-on-license model except for a selection of tools that are useful to a larger community. </p> <ul> <li>This is partly caused by the distributed user management as the LUST does not even have the necessary     information to determine if a particular user can use a particular license, so that      responsibility must be shifted to people who have that information, which is often the PI of your project.</li> <li>You also have to take into account that up to 20% of LUMI is reserved for industry use which makes      negotiations with software vendors rather difficult as they will want to push LUMI onto the industrial     rather than academic pricing as they have no guarantee that LUMI operations will obey      to the academic license restrictions. </li> <li>And lastly, the LUMI project doesn't have an infinite budget.      There was a questionnaire sent out to      some groups even before the support team was assembled and that contained a number of packages that     by themselves would likely consume the whole LUMI software budget for a single package if I look at the      size of the company that produces the package and the potential size of their industrial market.      So LUMI has to make choices and with any choice for a very specialised package you favour a few      groups. And there is also a political problem as without doubt the EuroHPC JU would prefer that LUMI     invests in packages that are developed by European companies or at least have large development     teams in Europe.</li> </ul> <p>The LUMI User Support Team tries to help with installations of recent software but porting or bug correction in software is not their task. In Flanders some help is possible by the VSC Tier-0 support team but do not expect that they will port your whole application. As a user, you have to realise that not all Linux or even supercomputer software will work on LUMI. This holds even more for software that comes only as a binary. The biggest problems are the GPU and anything that uses distributed memory and requires high performance from the interconnect. For example,</p> <ul> <li>software that use NVIDIA proprietary programming models and     libraries needs to be ported. </li> <li>Binaries that do only contain NVIDIA code paths, even if the programming     model is supported on AMD GPUs, will not run on LUMI. </li> <li>The LUMI interconnect requires libfabric, the Open Fabrics Interface (OFI) library,     using a specific provider for the NIC used on LUMI, the so-called Cassini provider (CXI),      so any software compiled with an MPI library that     requires UCX, or any other distributed memory model built on top of UCX, will not work on LUMI, or at     least not work efficiently as there might be a fallback path to TCP communications. </li> <li>Even intra-node interprocess communication can already cause problems     as there are three different kernel extensions     that provide more efficient interprocess messaging than the standard Linux mechanism. Many clusters     use knem for that but on LUMI xpmem is used. So software that is not build to support xpmem will     also fall back to the default mechanism or fail. </li> <li>Also, the MPI implementation needs to collaborate     with certain modules in our Slurm installation to start correctly and experience has shown that this     can also be a source of trouble as the fallback mechanisms that are often used do not work on LUMI. </li> <li>Containers solve none of these problems. There can be more subtle compatibility problems also.      As has been discussed earlier in the course, LUMI runs SUSE Linux and not Ubuntu which is popular on      workstations or a Red Hat-derived Linux popular on many clusters. Subtle differences between Linux      versions can cause compatibility problems that in some cases can be solved with containers. But containers     won't help you if they are build for different kernel extensions and hardware interfaces.</li> <li>The compute nodes also lack some Linux daemons that may be present on smaller clusters. HPE Cray use an     optimised Linux version called COS or Cray Operating System on the compute nodes. It is optimised to     reduce OS jitter and hence to enhance scalability of applications as that is after all the primary     goal of a pre-exascale machine. But that implies that certain Linux daemons that your software may      expect to find are not present on the compute nodes. D-Bus comes to mind.</li> </ul> <p>Also, the LUMI user support team is too small to do all software installations which is why LUMI currently states in its policy that a LUMI user should be capable of installing their software themselves or have another support channel. The LUST cannot install every single piece of often badly documented research-quality code that was never meant to be used by people who don't understand the code. Again some help is possible  at the Belgian level but our resources are also limited.</p> <p>Another soft compatibility problem that I did not yet mention is that software that accesses tens of thousands of small files and abuses the file system as a database rather than using structured data formats designed to organise data on supercomputers is not welcome on LUMI. For that reason LUMI also requires to containerize conda and Python installations. The LUST does offer a container-based wrapper that offers a way to install conda packages or to install Python packages with pip on top of  the Python provided by the <code>cray-python</code> module. On LUMI the tool is called lumi-container-wrapper but it may by some from CSC also be known as Tykky. As an alternative LUMI also offers cotainr, a tool developed by the Danish LUMI-partner DeIC that helps with building some types of containers that can be built in user space and can be used to containerise a conda-installation.</p>"},{"location":"intro-evolving/05_Software_stacks/#organisation-of-the-software-in-software-stacks","title":"Organisation of the software in software stacks","text":"<p>LUMI offers several software stacks:</p> <p>CrayEnv is the minimal software stack for users who only need the  Cray Programming Environment but want a more recent set of build tools etc  than the OS provides. We also take care of a few issues that we will discuss on the next slide that are present right after login on LUMI.</p> <p>Next we have the stacks called \"LUMI\". Each one corresponds to a particular release of the HPE Cray Programming Environment. It is the stack in which the LUST installs software using that programming environment and mostly EasyBuild. The Cray Programming Environment modules are still used, but they are accessed through a replacement for the PrgEnv modules that is managed by EasyBuild. There are tuned versions for the 3 types of hardware in the regular LUMI system: zen2 CPUs in the login nodes and large memory nodes, zen3 for the  LUMI-C compute nodes and zen3 + MI250X for the LUMI-G partition. IF the need would arrise, a fourth partition could be created for the visualisation nodes with zen2 CPUs and NVIDIA GPUs.</p> <p>LUMI also offers an extensible software stack based on Spack which has been pre-configured to use the compilers from the Cray PE. This stack is offered as-is for users who know how to use Spack, but support is limited and no bug-fixing in Spack is done.</p> <p>In the far future the LUST will also look at a stack based on the common EasyBuild toolchains as-is,  but problems are expected with MPI that will make this difficult to implement, and the common toolchains also do not yet support the AMD GPU ecosystem, so no promises whatsoever are made about a time frame for this development.</p>"},{"location":"intro-evolving/05_Software_stacks/#3-ways-to-access-the-cray-programming-environment-on-lumi","title":"3 ways to access the Cray Programming environment on LUMI.","text":""},{"location":"intro-evolving/05_Software_stacks/#bare-environment-and-crayenv","title":"Bare environment and CrayEnv","text":"<p>Right after login you have a very bare environment available with the Cray Programming Environment with the PrgEnv-cray module loaded. It gives you basically what you can expect on a typical Cray system. There aren't many tools available, basically mostly only the tools in the base OS image and some tools that will not impact software installed in one of the software stacks. The set of target modules loaded is the one for the login nodes and not tuned to any particular node type. As a user you're fully responsible for managing the target modules, reloading them when needed or loading the appropriate set for the hardware you're using or want to cross-compile for.</p> <p>The second way to access the Cray Programming Environment is through the CrayEnv software stack. This stack offers an \"enriched\" version of the Cray environment. It takes care of the target modules: Loading or reloading CrayEnv will reload an optimal set of target modules for the node you're on. It also provides some additional  tools like newer build tools than provided with the OS. They are offered here and not in the bare environment to be sure that those tools don't create conflicts with software in other stacks. But otherwise the Cray Programming  Environment works exactly as you'd expect from this course or the 4-day comprehensive courses that LUST organises.</p>"},{"location":"intro-evolving/05_Software_stacks/#lumi-stack","title":"LUMI stack","text":"<p>The third way to access the Cray Programming Environment is through the LUMI software stacks, where each stack is based on a particular release of the HPE Cray Programming Environment. We advise against mixing with modules that came with other versions of the Cray PE, but they remain accessible although they are hidden from the default view for regular users. It is also better to not use the PrgEnv modules, but the equivalent LUMI EasyBuild  toolchains instead as indicated by the following table:</p> HPE Cray PE LUMI toolchain What? <code>PrgEnv-cray</code> <code>cpeCray</code> Cray Compiling Environment <code>PrgEnv-gnu</code> <code>cpeGNU</code> GNU C/C++ and Fortran <code>PrgEnv-aocc</code> <code>cpeAOCC</code> AMD CPU compilers (login nodes and LUMI-C only) <code>PrgEnv-amd</code> <code>cpeAMD</code> AMD ROCm GPU compilers (LUMI-G only) <p>The cpeCray etc modules also load the MPI libraries and Cray LibSci just as the PrgEnv modules do. And they are sometimes used to work around problems in Cray-provided modules that cannot changed easily due to the way system administration on a Cray system is done. </p> <p>This is also the environment in which the LUST installs most software,  and from the name of the modules you can see which compilers we used.</p>"},{"location":"intro-evolving/05_Software_stacks/#lumi-stack-module-organisation","title":"LUMI stack module organisation","text":"<p>To manage the heterogeneity in the hardware, the LUMI software stack uses two levels of modules</p> <p>First there are the LUMI/22.08, LUMI/22.12, LUMI/23.03 and LUMI/23.09 modules.  Each of the LUMI modules loads a particular version of the LUMI stack.</p> <p>The second level consists of partition modules.  There is partition/L for the login and large memory nodes, partition/C for the regular compute nodes and  partition/G for the AMD GPU nodes. There may be a separate partition for the visualisation nodes in the future  but that is not clear yet.</p> <p>There is also a hidden partition/common module in which software is installed that is available everywhere,  but we advise you to be careful to install software in there in your own installs as it is risky to rely on software in one of the regular partitions, and impossible in our EasyBuild setup.</p> <p>The LUMI module will automatically load the best partition module for the current hardware whenever it is loaded or reloaded. So if you want to cross-compile, you can do so by loading a different partition  module after loading the LUMI module, but you'll have to reload every time you reload the LUMI module.</p> <p>Hence you should also be very careful in your job scripts. On LUMI the environment from the login nodes is used when your job starts, so unless you switched to the suitable partition for the compute nodes, your job will start with the software stack for the login nodes. If in your job script you reload the  LUMI module it will instead switch to the software stack that corresponds to the type of compute node you're using and more optimised binaries can be available. If for some reason you'd like to use the same software on LUMI-C and on the login or large memory nodes and don't want two copies of locally installed software, you'll have to make sure that after reloading the LUMI module in your job script you explicitly load the partition/L module.</p>"},{"location":"intro-evolving/05_Software_stacks/#easybuild-to-extend-the-lumi-software-stack","title":"EasyBuild to extend the LUMI software stack","text":""},{"location":"intro-evolving/05_Software_stacks/#installing-software-on-hpc-systems","title":"Installing software on HPC systems","text":"<p>Software on HPC systems is rarely installed from RPMs (a popular format to package Linux software distributed as binaries) or any other similar format for various reasons. Generic RPMs are rarely optimised for the specific CPU of the system as they have to work on a range of systems and including optimised code paths in a single executable for multiple architectures is hard to even impossible. Secondly generic RPMs might not even work with the specific LUMI environment. They may not fully support the SlingShot interconnect and hence run at reduced speed, or they may need particular kernel modules or daemons that are not present on the system or they may not work well with the resource manager on the system.  This is expected to happen especially with packages that require specific MPI versions or implementations. Moreover, LUMI is a multi-user system so there is usually no \"one version fits all\". And LUMI needs a small system image as nodes are diskless which means that RPMs need to be relocatable so that they can be installed elsewhere.</p> <p>Spack and EasyBuild are the two most popular HPC-specific software build and installation frameworks.  These two systems usually install packages from sources so that the software can be adapted to the underlying hardware and operating system. They do offer a mean to communicate and execute installation instructions easily so that in practice once a package is well supported by these tools a regular user can install them also. Both packages make software available via modules so that you can customise your environment and select appropriate versions for your work.  And they do take care of dependency handling in a way that is compatible with modules.</p>"},{"location":"intro-evolving/05_Software_stacks/#extending-the-lumi-stack-with-easybuild","title":"Extending the LUMI stack with EasyBuild","text":"<p>On LUMI EasyBuild is the primary software installation tool.  EasyBuild was selected as there is already a lot of experience with EasyBuild in several LUMI consortium countries and as it is also a tool developed in Europe which makes it a nice fit with EuroHPC's goal of creating a fully European HPC ecosystem.</p> <p>EasyBuild is fully integrated in the LUMI software stack. Loading the LUMI module will not only make centrally installed packages available, but also packages installed in your personal or project stack. Installing packages in that space is done by loading the EasyBuild-user module that will load a suitable version of EasyBuild and configure it for installation in a way that is compatible with the LUMI stack. EasyBuild will then use existing modules for dependencies if those are already on the system or in your personal or project stack.</p> <p>Note however that the build-in easyconfig files that come with EasyBuild do not work on LUMI at the moment.</p> <ul> <li>For the GNU toolchain there would be problems with MPI. EasyBuild uses Open MPI and that     needs to be configured differently to work well on LUMI, and there are also still issues with     getting it to collaborate with the resource manager as it is installed on LUMI.</li> <li>The Intel-based toolchains have their problems also. At the moment, the Intel compilers with the     AMD CPUs are a problematic cocktail. There have recently been performance and correctness problems      with the MKL math library and also failures with some versions of Intel MPI,      and you need to be careful selecting compiler options and not use <code>-xHost</code>     or the classic Intel compilers will simply optimize for a two decades old CPU.     The situation is better with the new LLVM-based compilers though, and it looks like     very recent versions of MKL are less AMD-hostile. Problems have also been reported     with Intel MPI running on LUMI.</li> </ul> <p>Instead LUMI has its own EasyBuild build recipes that are also made available in the  LUMI-EasyBuild-contrib GitHub repository. The EasyBuild configuration done by the EasyBuild-user module will find a copy of that repository on the system or in your own install directory. The latter is useful if you always want the very latest, before it is even deployed on the system. </p> <p>LUMI also offers the LUMI Software Library which documents all software for which there are LUMI-specific EasyBuild recipes available.  This includes both the pre-installed software and the software for which recipes are provided in the LUMI-EasyBuild-contrib GitHub repository, and even instructions for some software that is not suitable for installation through EasyBuild or Spack, e.g., because it likes to write in its own directories while running.</p>"},{"location":"intro-evolving/05_Software_stacks/#easybuild-recipes-easyconfigs","title":"EasyBuild recipes - easyconfigs","text":"<p>EasyBuild uses a build recipe for each individual package, or better said, each individual module as it is possible to install more than one software package in the same module. That installation description relies on either a generic or a specific installation process provided by an easyblock. The build recipes are called easyconfig files or simply easyconfigs and are Python files with  the extension <code>.eb</code>. </p> <p>The typical steps in an installation process are:</p> <ol> <li>Downloading sources and patches. For licensed software you may have to provide the sources as     often they cannot be downloaded automatically.</li> <li>A typical configure - build - test - install process, where the test process is optional and     depends on the package providing useable pre-installation tests.</li> <li>An extension mechanism can be used to install perl/python/R extension packages</li> <li>Then EasyBuild will do some simple checks (some default ones or checks defined in the recipe)</li> <li>And finally it will generate the module file using lots of information specified in the      EasyBuild recipe.</li> </ol> <p>Most or all of these steps can be influenced by parameters in the easyconfig.</p>"},{"location":"intro-evolving/05_Software_stacks/#the-toolchain-concept","title":"The toolchain concept","text":"<p>EasyBuild uses the toolchain concept. A toolchain consists of compilers, an MPI implementation and some basic mathematics libraries. The latter two are optional in a toolchain. All these  components have a level of exchangeability as there are language standards, as MPI is standardised, and the math libraries that are typically included are those that provide a standard API for which several implementations exist. All these components also have in common that it is risky to combine  pieces of code compiled with different sets of such libraries and compilers because there can be conflicts in names in the libraries.</p> <p>LUMI doesn't use the standard EasyBuild toolchains but its own toolchains specifically for Cray and these are precisely the <code>cpeCray</code>, <code>cpeGNU</code>, <code>cpeAOCC</code> and <code>cpeAMD</code> modules already mentioned  before.</p> HPE Cray PE LUMI toolchain What? <code>PrgEnv-cray</code> <code>cpeCray</code> Cray Compiling Environment <code>PrgEnv-gnu</code> <code>cpeGNU</code> GNU C/C++ and Fortran <code>PrgEnv-aocc</code> <code>cpeAOCC</code> AMD CPU compilers (login nodes and LUMI-C only) <code>PrgEnv-amd</code> <code>cpeAMD</code> AMD ROCm GPU compilers (LUMI-G only) <p></p> <p>There is also a special toolchain called the SYSTEM toolchain that uses the compiler provided by the operating system. This toolchain does not fully function in the same way as the other toolchains when it comes to handling dependencies of a package and is therefore a bit harder to use. The EasyBuild designers had in mind that this compiler would only be used to bootstrap an EasyBuild-managed software stack, but on LUMI it is used for a bit more as it offers a relatively easy way to compile some packages also for the CrayEnv stack and do this in a way that they interact as little as possible with other software.</p> <p>It is not possible to load packages from different cpe toolchains at the same time. This is an EasyBuild restriction, because mixing libraries compiled with different compilers does not always work. This could happen, e.g., if a package compiled with the Cray Compiling Environment and one compiled with the GNU compiler collection would both use a particular  library, as these would have the same name and hence the last loaded one would be used by both executables (LUMI doesn't use rpath or runpath linking in EasyBuild for those familiar with that technique).</p> <p>However, as LUMI does not use hierarchy in the Lmod implementation of the software stack at the toolchain level, the module system will not protect you from these mistakes.  When the LUST set up the software stack, most people in the support team considered it too misleading and difficult to ask users to first select the toolchain they want to use and then see the  software for that toolchain.</p> <p>It is however possible to combine packages compiled with one CPE-based toolchain with packages compiled with the system toolchain, but you should avoid mixing those when linking as that may cause problems. The reason that it works when running software is because static linking is used as much as possible in the SYSTEM toolchain so that these packages are as independent as possible.</p> <p>And with some tricks it might also be possible to combine packages from the LUMI software stack with packages compiled with Spack, but one should make sure that no Spack packages are available when building as mixing libraries could cause problems. Spack uses rpath linking which is why this may work.</p>"},{"location":"intro-evolving/05_Software_stacks/#easyconfig-names-and-module-names","title":"EasyConfig names and module names","text":"<p>There is a convention for the naming of an EasyConfig as shown on the slide. This is not mandatory, but EasyBuild will fail to automatically locate easyconfigs for dependencies  of a package that are not yet installed if the easyconfigs don't follow the naming convention. Each part of the name also corresponds to a parameter in the easyconfig  file.</p> <p>Consider, e.g., the easyconfig file <code>GROMACS-2022.5-cpeGNU-23.09-PLUMED-2.9.0-noPython-CPU.eb</code>.</p> <ol> <li>The first part of the name, <code>GROMACS</code>, is the name of the package, specified by the     <code>name</code> parameter in the easyconfig, and is after installation also the name of the     module.</li> <li>The second part, <code>2022.5</code>, is the version of GROMACS and specified by the     <code>version</code> parameter in the easyconfig.</li> <li> <p>The next part, <code>cpeGNU-23.09</code> is the name and version of the toolchain,     specified by the <code>toolchain</code> parameter in the easyconfig. The version of the     toolchain must always correspond to the version of the LUMI stack. So this is     an easyconfig for installation in <code>LUMI/23.09</code>.</p> <p>This part is not present for the SYSTEM toolchain</p> </li> <li> <p>The final part, <code>-PLUMED-2.9.0-noPython-CPU</code>, is the version suffix and used to provide     additional information and distinguish different builds with different options     of the same package. It is specified in the <code>versionsuffix</code> parameter of the     easyconfig.</p> <p>This part is optional.</p> </li> </ol> <p>The version, toolchain + toolchain version and versionsuffix together also combine to the version of the module that will be generated during the installation process. Hence this easyconfig file will generate the module  <code>GROMACS/2022.5-cpeGNU-23.09-PLUMED-2.9.0-noPython-CPU</code>.</p>"},{"location":"intro-evolving/05_Software_stacks/#installing","title":"Installing","text":""},{"location":"intro-evolving/05_Software_stacks/#step-1-where-to-install","title":"Step 1: Where to install","text":"<p>Let's now discuss how you can extend the central LUMI software stack with packages that you need for your project.</p> <p>The default location for the EasyBuild user modules and software is in <code>$HOME/EasyBuild</code>. This is not the ideal place though as then the software is not available for other users in your project, and as the size of your home directory is also limited and cannot be expanded. The home file system on LUMI  is simply not meant to install software. However, as LUMI users can have multiple projects there is no easy way to figure out automatically where else to install software.</p> <p>The best place to install software is in your project directory so that it also becomes available for the whole project. After all, a project is meant to be a collaboration between all participants on a scientific problem. You'll need to point LUMI to the right location though and that has to be done by setting the environment variable <code>EBU_USER_PREFIX</code> to point to the location where you want to have your custom installation. Also don't forget to export that variable as otherwise the module system and EasyBuild will not find it when they need it. So a good choice would be  something like  <code>export EBU_USER_PREFIX=/project/project_465000000/EasyBuild</code>.  You have to do this before loading the <code>LUMI</code> module as it is then already used to ensure that user modules are included in the module search path. You can do this in your <code>.profile</code> or <code>.bashrc</code>.  This variable is not only used by EasyBuild-user to know where to install software, but also  by the <code>LUMI</code> - or actually the <code>partition</code> - module to find software so all users in your project who want to use the software should set that variable.</p>"},{"location":"intro-evolving/05_Software_stacks/#step-2-configure-the-environment","title":"Step 2: Configure the environment","text":"<p>The next step is to configure your environment. First load the proper version of the LUMI stack for which you want to install software, and you may want to change to the proper partition also if you are cross-compiling.</p> <p>Once you have selected the software stack and partition, all you need to do to  activate EasyBuild to install additional software is to load the <code>LUMI</code> module, load a partition module if you want a different one from the default, and  then load the <code>EasyBuild-user</code> module. In fact, if you switch to a different <code>partition</code>  or <code>LUMI</code> module after loading <code>EasyBuild-user</code> EasyBuild will still be correctly reconfigured  for the new stack and new partition. </p> <p>Cross-compilation which is installing software for a different partition than the one you're working on does not always work since there is so much software around with installation scripts that don't follow good practices, but when it works it is easy to do on LUMI by simply loading a different partition module than the one that is auto-loaded by the <code>LUMI</code> module. It works  correctly for a lot of CPU-only software, but fails more frequently for GPU software as the installation scripts will try to run scripts that detect which GPU is present, or try to run tests on the GPU, even if you tell which GPU type to use, which does not work on the login nodes.</p> <p>Note that the <code>EasyBuild-user</code> module is only needed for the installation process. For using the software that is installed that way it is sufficient to ensure that <code>EBU_USER_PREFIX</code> has the proper value before loading the <code>LUMI</code> module.</p>"},{"location":"intro-evolving/05_Software_stacks/#step-3-install-the-software","title":"Step 3: Install the software.","text":"<p>Let's look at GROMACS as an example. I will not try to do this completely live though as the  installation takes 15 or 20 minutes.</p> <p>First we need to figure out for which versions of GROMACS there is already support on LUMI. An easy way to do that is to simply check the LUMI Software Library. This web site lists all software that we manage via EasyBuild and make available either pre-installed on the system or as an EasyBuild recipe for user installation. Alternatively one can use <code>eb -S</code> or <code>eb --search</code> for that. So in our example this is <pre><code>eb --search GROMACS\n</code></pre></p> <p>Output of the search commands:</p> <p><code>eb --search GROMACS</code> produces:</p> <p> </p> <p>while <code>eb -S GROMACS</code> produces:</p> <p> </p> <p>The information provided by both variants of the search command is the same, but <code>-S</code> presents the information in a more compact form.</p> <p>Now let's take the variant <code>GROMACS-2022.5-cpeGNU-23.09-PLUMED-2.9.0-noPython-CPU.eb</code>.  This is GROMACS 2022.5 with the PLUMED 2.9.0 plugin, build with the GNU compilers from <code>LUMI/23.09</code>, and a build meant for CPU-only systems. The <code>-CPU</code> extension is not always added for CPU-only system, but in case of GROMACS there already is a GPU version for AMD GPUs in active development so even before LUMI-G was active we chose to ensure that we could distinguish between GPU and CPU-only versions. To install it, we first run  <pre><code>eb GROMACS-2022.5-cpeGNU-23.09-PLUMED-2.9.0-noPython-CPU.eb \u2013D\n</code></pre> The <code>-D</code> flag tells EasyBuild to just perform a check for the dependencies that are needed when installing this package, while the <code>-r</code> argument is needed to tell EasyBuild to also  look for dependencies in a preset search path. The installation of dependencies is not automatic since there are scenarios where this is not desired and it cannot be turned off as easily as it can be turned on.</p> <p>The output of this command looks like:</p> <p></p> <p></p> <p>Looking at the output we see that EasyBuild will also need to install <code>PLUMED</code> for us. But it will do so automatically when we run <pre><code>eb GROMACS-2022.5-cpeGNU-23.09-PLUMED-2.9.0-noPython-CPU.eb -r\n</code></pre></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>This takes too long to wait for, but once it finished the software should be available and you should be able to see the module in the output of <pre><code>module avail\n</code></pre></p>"},{"location":"intro-evolving/05_Software_stacks/#step-3-install-the-software-note","title":"Step 3: Install the software - Note","text":"<p>Installing software this way is 100% equivalent to an installation in the central software tree. The application is compiled in exactly the same way as we would do and served from the same file systems. But it helps keep the output of <code>module avail</code> reasonably short and focused on your projects, and it puts you in control of installing updates. For instance, we may find out that something in a module does not work for some users and that it needs to be re-installed.  Do this in the central stack and either you have to chose a different name or risk breaking running jobs as the software would become unavailable during the re-installation and also jobs may get confused if they all of a sudden find different binaries. However, have this in your own stack extension and you can update whenever it suits your project best or even not update at all if  you figure out that the problem we discovered has no influence on your work.</p> <p>Lmod does keep a user cache of modules. EasyBuild will try to erase that cache after a software installation to ensure that the newly installed module(s) show up immediately. We have seen some very rare cases where clearing the cache did not help likely because some internal data structures in Lmod where corrupt. The easiest way to solve this is to simply log out and log in again and rebuild your environment.</p> <p>In case you see strange behaviour using modules you can also try to manually remove the Lmod user cache which is in <code>$HOME/.cache/lmod</code>. You can do this with  <pre><code>rm -rf $HOME/.cache/lmod\n</code></pre> (With older versions of Lmod the cache directory is <code>$HOME/.lmod.d/cache</code>.)</p>"},{"location":"intro-evolving/05_Software_stacks/#more-advanced-work","title":"More advanced work","text":"<p>You can also install some EasyBuild recipes that you got from support. For this it is best to create a subdirectory where you put those files, then go into that directory and run  something like <pre><code>eb my_recipe.eb -r . \n</code></pre> The dot after the <code>-r</code> is very important here as it does tell EasyBuild to also look for  dependencies in the current directory, the directory where you have put the recipes you got from support, but also in its subdirectories so for speed reasons you should not do this just in your home directory but in a subdirectory that only contains those files.</p> <p>In some cases you will have to download sources by hand as packages don't allow to download  software unless you sign in to their web site first. This is the case for a lot of licensed software, for instance, for VASP. LUMI would likely be in violation of the license if it would  offer the download somewhere where EasyBuild can find it, and it is also a way to ensure that you have a license for VASP. For instance,  <pre><code>eb --search VASP\n</code></pre> will tell you for which versions of VASP LUMI provides EasyBuild recipes, but you will still have to download the source file that the EasyBuild recipe expects.  Put it somewhere in a directory, and then from that directory run EasyBuild, for instance for VASP 6.3.0 with the GNU compilers: <pre><code>eb VASP-6.4.1-cpeGNU-22.12-build01.eb \u2013r . \n</code></pre></p>"},{"location":"intro-evolving/05_Software_stacks/#more-advanced-work-2-repositories","title":"More advanced work (2): Repositories","text":"<p>It is also possible to have your own clone of the <code>LUMI-EasyBuild-contrib</code> GitHub repository in your <code>$EBU_USER_PREFIX</code> subdirectory if you want the latest and greatest before it is in the centrally maintained clone of the repository. All you need to do is <pre><code>cd $EBU_USER_PREFIX\ngit clone https://github.com/Lumi-supercomputer/LUMI-EasyBuild-contrib.git\n</code></pre> and then of course keep the repository up to date.</p> <p>And it is even possible to maintain your own GitHub repository. The only restrictions are that it should also be in <code>$EBU_USER_PREFIX</code> and that the subdirectory should be called <code>UserRepo</code>, but that doesn't stop you from using a different name for the repository on GitHub. After cloning your GitHub version you can always change the name of the directory. The structure should also be compatible with the structure that EasyBuild uses, so easyconfig files go in <code>$EBU_USER_PREFIX/UserRepo/easybuild/easyconfigs</code>.</p>"},{"location":"intro-evolving/05_Software_stacks/#more-advanced-work-3-reproducibility","title":"More advanced work (3): Reproducibility","text":"<p>EasyBuild also takes care of a high level of reproducibility of installations.</p> <p>It will keep a copy of all the downloaded sources in the <code>$EBU_USER_PREFIX/sources</code> subdirectory (unless the sources are already available elswhere where EasyBuild can find them, e.g., in the system EasyBuild sources directory),  and use that source file again rather than downloading it again. Of course in some cases those \"sources\" could be downloaded tar files with binaries instead as EasyBuild can install downloaded binaries or relocatable RPMs. And if you know the structure of those directories, this is also a place where you could manually put the downloaded installation files for licensed software.</p> <p>Moreover, EasyBuild also keeps copies of all installed easyconfig files in two locations.</p> <ol> <li>There is a copy in <code>$EBU_USER_PREFIX/ebrepo_files</code>. And in fact, EasyBuild will use this version     first if you try to re-install and did not delete this version first. This is a policy     we set on LUMI which has both its advantages and disadvantages. The advantage is that it ensures     that the information that EasyBuild has about the installed application is compatible with what is     in the module files. But the disadvantage of course is that if you install an EasyConfig file     without being in the subdirectory that contains that file, it is easily overlooked that it     is installing based on the EasyConfig in the <code>ebrepo_files</code> subdirectory and not based on the     version of the recipe that you likely changed and is in your user repository or one of the      other repositories that EasyBuild uses.</li> <li>The second copy is with the installed software in <code>$EBU_USER_PREFIX/SW</code> in a subdirectory     called <code>easybuild</code>. This subdirectory is meant to have all information about how EasyBuild     installed the application, also some other files that play a role in the installation process, and hence     to help in reproducing an installation or checking what's in an existing installation. It is     also the directory where you will find the extensive log file with all commands executed during     the installation and their output.</li> </ol>"},{"location":"intro-evolving/05_Software_stacks/#easybuild-tips-and-tricks","title":"EasyBuild tips and tricks","text":"<p>Updating the version of a package often requires only trivial changes in the easyconfig file. However, we do tend to use checksums for the sources so that we can detect if the available sources have changed. This may point to files being tampered with, or other changes that might need us to be a bit more careful when installing software and check a bit more again.  Should the checksum sit in the way, you can always disable it by using  <code>--ignore-checksums</code> with the <code>eb</code> command.</p> <p>Updating an existing recipe to a new toolchain might be a bit more involving as you also have to make build recipes for all dependencies. When a toolchain is updated on the system,  the versions of all installed libraries are often also bumped to one of the latest versions to have most bug fixes and security patches in the software stack, so you need to check for those versions also to avoid installing yet another unneeded version of a library.</p> <p>LUMI provides documentation on the available software that is either pre-installed or can be user-installed with EasyBuild in the  LUMI Software Library. For most packages this documentation does also contain information about the license. The user documentation for some packages gives more information about how to use the package on LUMI, or sometimes also about things that do not work. The documentation also shows all EasyBuild recipes, and for many packages there is  also some technical documentation that is more geared towards users who want to build or modify recipes. It sometimes also tells why things are done in a particular way.</p>"},{"location":"intro-evolving/05_Software_stacks/#easybuild-training-for-advanced-users-and-developers","title":"EasyBuild training for advanced users and developers","text":"<p>Pointers to all information about EasyBuild can be found on the EasyBuild web site  easybuild.io. This page also includes links to training materials, both written and as recordings on YouTube, and the EasyBuild documentation.</p> <p>Generic EasyBuild training materials are available on  tutorial.easybuild.io. The site also contains a LUST-specific tutorial oriented towards Cray systems.</p> <p>There is also a later course developed by LUST for developers of EasyConfigs for LUMI that can be found on  lumi-supercomputer.github.io/easybuild-tutorial.</p>"},{"location":"intro-evolving/06_Exercises_1/","title":"Exercises 1: Modules, the HPE Cray PE and EasyBuild","text":"<p>See the instructions to set up for the exercises.</p>"},{"location":"intro-evolving/06_Exercises_1/#exercises-on-the-use-of-modules","title":"Exercises on the use of modules","text":"<ol> <li> <p>The <code>Bison</code> program installed in the OS image is pretty old (version 3.0.4) and     we want to use a newer one. Is there one available on LUMI?</p> Click to see the solution. <pre><code>module spider Bison\n</code></pre> <p>tells us that there are indeed newer versions available on the system. </p> <p>The versions that have a compiler name (usually <code>gcc</code>) in their name followed by some seemingly random characters are installed with Spack and not in the CrayEnv or LUMI environments.</p> <p>To get more information about <code>Bison/3.8.2</code>: </p> <pre><code>module spider Bison/3.8.2\n</code></pre> <p>tells us that Bison 3.8.2 is provided by a couple of <code>buildtools</code> modules and available in all partitions in several versions of the <code>LUMI</code> software stack and in <code>CrayEnv</code>.</p> <p>Alternatively, in this case</p> <pre><code>module keyword Bison\n</code></pre> <p>would also have shown that Bison is part of several versions of the <code>buildtools</code> module.</p> <p>The <code>module spider</code> command is often the better command if you use names that with a high  likelihood could be the name of a package, while <code>module keyword</code> is often the better choice for words that are more a keyword. But if one does not return the solution it is a good idea  to try the other one also.</p> </li> <li> <p>The <code>htop</code> command is a nice alternative for the <code>top</code> command with a more powerful user interface.     However, typing <code>htop</code> on the command line produces an error message. Can you find and run <code>htop</code>?</p> Click to see the solution. <p>We can use either <code>module spider htop</code> or <code>module keyword htop</code> to find out that <code>htop</code> is indeed available on the system. With <code>module keyword htop</code> we'll find out immediately that it is in the  <code>systools</code> modules and some of those seem to be numbered after editions of the LUMI stack suggesting that they may be linked to a stack, with <code>module spider</code> you'll first see that it is an extension of a module and see the versions. You may again see some versions installed with Spack.</p> <p>Let's check further for <code>htop/3.2.1</code> that should exist according to <code>module spider htop</code>:</p> <pre><code>module spider htop/3.2.1\n</code></pre> <p>tells us that this version of <code>htop</code> is available in all partitions of <code>LUMI/22.08</code> and <code>LUMI/22.06</code>, and in <code>CrayEnv</code>. Let us just run it in the <code>CrayEnv</code> environment:</p> <pre><code>module load CrayEnv\nmodule load systools/22.08\nhtop\n</code></pre> <p>(You can quit <code>htop</code> by pressing <code>q</code> on the keyboard.)</p> </li> <li> <p>In the future LUMI will offer Open OnDemand as a browser-based interface to LUMI that will also enable     running some graphical programs. At the moment the way to do this is through a so-called VNC server.     Do we have such a tool on LUMI, and if so, how can we use it?</p> Click to see the solution. <p><code>module spider VNC</code> and <code>module keyword VNC</code> can again both be used to check if there is software available to use VNC. Both will show that there is a module <code>lumi-vnc</code> in several versions. If you  try loading the older ones of these (the version number points at the date of some scripts) you will notice that some produce a warning as they are deprecated. However, when installing a new version we  cannot remove older ones in one sweep, and users may have hardcoded full module names in scripts they use to set their environment, so we chose to not immediate delete these older versions.</p> <p>One thing you can always try to get more information about how to run a program, is to ask for the help information of the module. For this to work the module must first be available, or you have to use  <code>module spider</code> with the full name of the module. We see that version <code>20230110</code> is the newest version of the module, so let's try that one:</p> <pre><code>module spider lumi-vnc/20230110\n</code></pre> <p>The output may look a little strange as it mentions <code>init-lumi</code> as one of the modules that you can load. That is because this tool is available even outside <code>CrayEnv</code> or the LUMI stacks. But this command also shows a long help test telling you how to use this module (though it does assume some familiarity with how X11 graphics work on Linux).</p> <p>Note that if there is only a single version on the system, as is the case for the course in May 2023, the <code>module spider VNC</code> command without specific version or correct module name will already display the help information.</p> </li> <li> <p>Search for the <code>bzip2</code> tool (and not just the <code>bunzip2</code> command as we also need the <code>bzip2</code> command) and make      sure that you can use software compiled with the Cray compilers in the LUMI stacks in the same session.</p> Click to see the solution. <pre><code>module spider bzip2\n</code></pre> <p>shows that there are versions of <code>bzip2</code> for several of the <code>cpe*</code> toolchains and in several versions of the LUMI software stack.</p> <p>Of course we prefer to use a recent software stack, the <code>22.08</code> or <code>22.12</code> (but as of early May 2023,  there is a lot more software ready-to-install for <code>22.08</code>).  And since we want to use other software compiled with the Cray compilers also, we really want a <code>cpeCray</code> version to avoid conflicts between  different toolchains. So the module we want to load is <code>bzip2/1.0.8-cpeCray-22.08</code>.</p> <p>To figure out how to load it, use</p> <pre><code>module spider bzip2/1.0.8-cpeCray-22.08\n</code></pre> <p>and see that (as expected from the name) we need to load <code>LUMI/22.08</code> and can then use it in any of the partitions.</p> </li> </ol>"},{"location":"intro-evolving/06_Exercises_1/#exercises-on-compiling-software-by-hand","title":"Exercises on compiling software by hand","text":"<p>These exercises are optional during the session, but useful if you expect  to be compiling software yourself. The source files mentioned can be found in the subdirectory CPE of the download.</p>"},{"location":"intro-evolving/06_Exercises_1/#compilation-of-a-program-1-a-simple-hello-world-program","title":"Compilation of a program 1: A simple \"Hello, world\" program","text":"<p>Four different implementations of a simple \"Hello, World!\" program are provided in the <code>CPE</code> subdirectory:</p> <ul> <li><code>hello_world.c</code> is an implementation in C,</li> <li><code>hello_world.cc</code> is an implementation in C++,</li> <li><code>hello_world.f</code> is an implementation in Fortran using the fixed format source form,</li> <li><code>hello_world.f90</code> is an implementation in Fortran using the more modern free format source form.</li> </ul> <p>Try to compile these programs using the programming environment of your choice.</p> Click to see the solution. <p>We'll use the default version of the programming environment (22.12 at the moment of the course in May 2023), but in case you want to use a particular version, e.g., the 22.08 version, and want to be very sure that all modules are loaded correctly from the start you could consider using</p> <pre><code>module load cpe/22.08\nmodule load cpe/22.08\n</code></pre> <p>So note that we do twice the same command as the first iteration does not always succeed to reload all modules in the correct version. Do not combine both lines into a single <code>module load</code> statement as that would again trigger the bug that prevents all modules to be reloaded in the first iteration.</p> <p>The sample programs that we asked you to compile do not use the GPU. So there are three programming environments that we can use: <code>PrgEnv-gnu</code>, <code>PrgEnv-cray</code> and <code>PrgEnv-aocc</code>. All three will work, and they work almost the same.</p> <p>Let's start with an easy case, compiling the C version of the program with the GNU C compiler. For this all we need to do is</p> <pre><code>module load PrgEnv-gnu\ncc hello_world.c\n</code></pre> <p>which will generate an executable named <code>a.out</code>.  If you are not comfortable using the default version of <code>gcc</code> (which produces the warning message when loading the <code>PrgEnv-gnu</code> module) you can always load the <code>gcc/11.2.0</code> module instead after loading <code>PrgEnv-gnu</code>.</p> <p>Of course it is better to give the executable a proper name which can be done with the <code>-o</code> compiler option:</p> <pre><code>module load PrgEnv-gnu\ncc hello_world.c -o hello_world.x\n</code></pre> <p>Try running this program:</p> <pre><code>./hello_world.x\n</code></pre> <p>to see that it indeed works. We did forget another important compiler option, but we'll discover that in the next exercise.</p> <p>The other programs are equally easy to compile using the compiler wrappers:</p> <pre><code>CC hello_world.cc -o hello_world.x\nftn hello_world.f -o hello_world.x\nftn hello_world.f90 -o hello_world.x\n</code></pre>"},{"location":"intro-evolving/06_Exercises_1/#compilation-of-a-program-2-a-program-with-blas","title":"Compilation of a program 2: A program with BLAS","text":"<p>In the <code>CPE</code> subdirectory you'll find the C program <code>matrix_mult_C.c</code> and the Fortran program <code>matrix_mult_F.f90</code>. Both do the same thing: a matrix-matrix multiplication using the 6 different orders of the three nested loops involved in doing a matrix-matrix multiplication, and a call to the BLAS routine DGEMM that does the same for comparison.</p> <p>Compile either of these programs using the Cray LibSci library for the BLAS routine. Do not use OpenMP shared memory parallelisation. The code does not use MPI.</p> <p>The resulting executable takes one command line argument, the size of the square matrix. Run the script using <code>1000</code> for the matrix size and see what happens.</p> <p>Note that the time results may be very unreliable as we are currently doing this on the login nodes. In the session of Slurm you'll learn how to request compute nodes and it might be interesting to redo this on a compute node with a larger matrix size as the with a matrix size of 1000 all data may stay in the third level cache and you will not notice the differences that you should note. Also, because these nodes are shared with a lot of people any benchmarking is completely unreliable.</p> <p>If this program takes more than half a minute or so before the first result line in the table, starting with <code>ijk-variant</code>, is printed, you've very likely done something wrong (unless the load on the system is extreme). In fact, if you've done things well the time reported for the <code>ijk</code>-variant should be well under 3 seconds for both the C and Fortran versions...</p> Click to see the solution. <p>Just as in the previous exercise, this is a pure CPU program so we can chose between the same three programming environments.</p> <p>The one additional \"difficulty\" is that we need to link with the BLAS library. This is very easy however in  the HPE Cray PE if you use the compiler wrappers rather than calling the compilers yourself: you only need to make sure that the <code>cray-libsci</code> module is loaded and the wrappers will take care of the rest. And on most systems (including LUMI) this module will be loaded automatically when you load the <code>PrgEnv-*</code> module.</p> <p>To compile with the GNU C compiler, all you need to do is</p> <pre><code>module load PrgEnv-gnu\ncc -O3 matrix_mult_C.c -o matrix_mult_C_gnu.x\n</code></pre> <p>will generate the executable <code>matrix_mult_C_gnu.x</code>.</p> <p>Note that we add the <code>-O3</code> option and it is very important to add either <code>-O2</code> or <code>-O3</code> as by default the GNU compiler will generate code without any optimization for debugging purposes, and that code is in this case easily five times or more slower. So if you got much longer run times than indicated this is likely the mistake that you made.</p> <p>To use the Cray C compiler instead only one small change is needed: Loading a different programming  environment module:</p> <pre><code>module load PrgEnv-cray\ncc -O3 matrix_mult_C.c -o matrix_mult_C_cray.x\n</code></pre> <p>will generate the executable <code>matrix_mult_C_cray.x</code>.</p> <p>Likewise for the AMD AOCC compiler we can try with loading yet another <code>PrgEnv-*</code> module:</p> <pre><code>module load PrgEnv-aocc\ncc -O3 matrix_mult_C.c -o matrix_mult_C_aocc.x\n</code></pre> <p>but it turns out that this fails with linker error messages about not being able to find the <code>sin</code> and <code>cos</code> functions. When using the AOCC compiler the <code>libm</code> library with basic math functions is not linked automatically, but this is easily done by adding the <code>-lm</code> flag:</p> <pre><code>module load PrgEnv-aocc\ncc -O3 matrix_mult_C.c -lm -o matrix_mult_C_aocc.x\n</code></pre> <p>For the Fortran version of the program we have to use the <code>ftn</code> compiler wrapper instead, and the issue with the math libraries in the AOCC compiler does not occur. So we get</p> <pre><code>module load PrgEnv-gnu\nftn -O3 matrix_mult_F.f90 -o matrix_mult_F_gnu.x\n</code></pre> <p>for the GNU Fortran compiler,</p> <pre><code>module load PrgEnv-cray\nftn -O3 matrix_mult_F.f90 -o matrix_mult_F_cray.x\n</code></pre> <p>for the Cray Fortran compiler and</p> <pre><code>module load PrgEnv-aocc\nftn -O3 matrix_mult_F.f90 -o matrix_mult_F_aocc.x\n</code></pre> <p>for the AMD Fortran compiler.</p> <p>When running the program you will see that even though the 6 different loop orderings  produce the same result, the time needed to compile the matrix-matrix product is very different and those differences would be even more pronounced with bigger matrices (which you can do after the session on using Slurm).</p> <p>The exercise also shows that not all codes are equal even if they produce a result of the same quality. The six different loop orderings run at very different speed, and none of our simple implementations can beat a good library, in this case the BLAS library included in LibSci.</p> <p>The results with the Cray Fortran compiler are particularly interesting. The result for the BLAS library is slower which we do not yet understand, but it also turns out that  for four of the six loop orderings we get the same result as with the BLAS library DGEMM routine. It looks like the compiler simply recognized that this was code for a matrix-matrix multiplication and replaced it with a call to the BLAS library. The Fortran 90 matrix multiplication is also replaced by a call of the DGEMM routine. To confirm all this, unload the <code>cray-libsci</code> module and try to compile again and you will see five error messages about not being able to find DGEMM.</p>"},{"location":"intro-evolving/06_Exercises_1/#compilation-of-a-program-3-a-hybrid-mpiopenmp-program","title":"Compilation of a program 3: A hybrid MPI/OpenMP program","text":"<p>The file <code>mpi_omp_hello.c</code> is a hybrid MPI and OpenMP C program that sends a message from each thread in each MPI rank. It is basically a simplified version of the programs found in the <code>lumi-CPEtools</code> modules that can be used to quickly check  the core assignment in a hybrid MPI and OpenMP job (see later in this tutorial). It is again just a CPU-based program.</p> <p>Compile the program with your favourite C compiler on LUMI.</p> <p>We have not yet seen how to start an MPI program. However, you can run the executable on the login nodes and it will then contain just a single MPI rank. </p> Click to see the solution. <p>In the HPE Cray PE environment, you don't use <code>mpicc</code> to compile a C MPI program, but you just use the <code>cc</code> wrapper as for any other C program. To enable MPI you  have to make sure that the <code>cray-mpich</code> module is loaded. This module will usually be loaded by loading one of the <code>PrgEnv-*</code> modules, but only if the right network target module, which is <code>craype-network-ofi</code>, is also already loaded. </p> <p>Compiling the program is very simple:</p> <pre><code>module load PrgEnv-gnu\ncc -O3 -fopenmp mpi_omp_hello.c -o mpi_omp_hello_gnu.x\n</code></pre> <p>to compile with the GNU C compiler, </p> <pre><code>module load PrgEnv-cray\ncc -O3 -fopenmp mpi_omp_hello.c -o mpi_omp_hello_cray.x\n</code></pre> <p>to compile with the Cray C compiler, and</p> <pre><code>module load PrgEnv-aocc\ncc -O3 -fopenmp mpi_omp_hello.c -o mpi_omp_hello_aocc.x\n</code></pre> <p>to compile with the AMD AOCC compiler.</p> <p>To run the executables it is not even needed to have the respective <code>PrgEnv-*</code> module loaded since the binaries will use a copy of the libraries stored in a default directory, though there have been bugs in the past preventing this to work with <code>PrgEnv-aocc</code>.</p>"},{"location":"intro-evolving/06_Exercises_1/#information-in-the-lumi-software-library","title":"Information in the LUMI Software Library","text":"<p>Explore the LUMI Software Library.</p> <ul> <li>Search for information for the package ParaView and quickly read through the page</li> </ul> Click to see the solution. <p>Link to the ParaView documentation</p> <p>It is an example of a package for which we have both user-level and some technical information. The page will first show some license information, then the actual user information which in case of this package is very detailed and long. But it is also a somewhat complicated package to use. It will become easier when LUMI evolves a bit further, but there will always be some pain. Next comes the more technical part: Links to the EasyBuild recipe and some information about how we build the package.</p> <p>We currently only provide ParaView in the cpeGNU toolchain. This is because it has a lot of dependencies that are not trivial to compile and to port to the other compilers on the system, and EasyBuild is  strict about mixing compilers basically because it can cause a lot of problems, e.g., due to conflicts between OpenMP runtimes.</p>"},{"location":"intro-evolving/06_Exercises_1/#installing-software-with-easybuild","title":"Installing software with EasyBuild","text":"<p>These exercises are based on material from the EasyBuild tutorials (and we have a special version for LUMI also).</p> <p>Note: If you want to be able to uninstall all software installed through the exercises easily, we suggest you make a separate EasyBuild installation for the course, e.g., in <code>/scratch/project_465000523/$USER/eb-course</code> if you make the exercises during the course:</p> <ul> <li>Start from a clean login shell with only the standard modules loaded.</li> <li> <p>Set <code>EBU_USER_PREFIX</code>: </p> <pre><code>export EBU_USER_PREFIX=/scratch/project_465000523/$USER/eb-course\n</code></pre> <p>You'll need to do that in every shell session where you want to install or use that software.</p> </li> <li> <p>From now on you can again safely load the necessary <code>LUMI</code> and <code>partition</code> modules for the exercise.</p> </li> <li> <p>At the end, when you don't need the software installation anymore, you can simply remove the directory     that you just created.</p> <pre><code>rm -rf /scratch/project_465000523/$USER/eb-course\n</code></pre> </li> </ul>"},{"location":"intro-evolving/06_Exercises_1/#installing-a-simple-program-without-dependencies-with-easybuild","title":"Installing a simple program without dependencies with EasyBuild","text":"<p>The LUMI Software Library contains the package <code>eb-tutorial</code>. Install the version of the package for the <code>cpeCray</code> toolchain in the 22.08 version of the software stack.</p> <p>At the time of this course, in early May 2023, we're still working on EasyBuild build recipes for the 22.12 version of the software stack.</p> Click to see the solution. <ul> <li> <p>We can check the      eb-tutorial page     in the      LUMI Software Library     if we want to see more information about the package.</p> <p>You'll notice that there are versions of the EasyConfigs for <code>cpeGNU</code> and <code>cpeCray</code>. As we want to install software with the <code>cpeCray</code> toolchain for <code>LUMI/22.08</code>, we'll need the <code>cpeCray-22.08</code> version which is the EasyConfig <code>eb-tutorial-1.0.1-cpeCray-22.08.eb</code>.</p> </li> <li> <p>Obviously we need to load the <code>LUMI/22.08</code> module. If we would like to install software     for the CPU compute nodes, you need to also load <code>partition/C</code>.     To be able to use EasyBuild, we also need the <code>EasyBuild-user</code> module.</p> <pre><code>module load LUMI/22.08 partition/C\nmodule load EasyBuild-user\n</code></pre> </li> <li> <p>Now all we need to do is run the <code>eb</code> command from EasyBuild to install the software.</p> <p>Let's however take the slow approach and first check if what dependencies the package needs:</p> <pre><code>eb eb-tutorial-1.0.1-cpeCray-22.08.eb -D\n</code></pre> <p>We can do this from any directory as the EasyConfig file is already in the LUMI Software Library and will be located automatically by EasyBuild. You'll see that all dependencies are already on  the system so we can proceed with the installation:</p> <pre><code>eb eb-tutorial-1.0.1-cpeCray-22.08.eb \n</code></pre> </li> <li> <p>After this you should have a module <code>eb-tutorial/1.0.1-cpeCray-22.08</code> but it may not show up      yet due to the caching of Lmod. Try</p> <pre><code>module av eb-tutorial/1.0.1-cpeCray-22.08\n</code></pre> <p>If this produces an error message complaining that the module cannot be found, it is time to clear the Lmod cache:</p> <pre><code>rm -rf $HOME/.lmod.d/.cache\n</code></pre> </li> <li> <p>Now that we have the module, we can check what it actually does:</p> <pre><code>module help eb-tutorial/1.0.1-cpeCray-22.08\n</code></pre> <p>and we see that it provides the <code>eb-tutorial</code> command.</p> </li> <li> <p>So let's now try to run this command:</p> <pre><code>module load eb-tutorial/1.0.1-cpeCray-22.08\neb-tutorial\n</code></pre> <p>Note that if you now want to install one of the other versions of this module, EasyBuild will complain that some modules are loaded that it doesn't like to see, including the <code>eb-tutorial</code> module and the <code>cpeCray</code> modules so it is better to unload those first:</p> <pre><code>module unload cpeCray eb-tutorial\n</code></pre> </li> </ul>"},{"location":"intro-evolving/06_Exercises_1/#installing-an-easyconfig-given-to-you-by-lumi-user-support","title":"Installing an EasyConfig given to you by LUMI User Support","text":"<p>Sometimes we have no solution ready in the LUMI Software Library, but we prepare one or more custom EasyBuild recipes for you. Let's mimic this case. In practice we would likely send  those as attachments to a mail from the ticketing system and you would be asked to put them in a separate directory (basically since putting them at the top of your home directory would in some cases let EasyBuild search your whole home directory for dependencies which would be a very slow process).</p> <p>You've been given two EasyConfig files to install a tool called <code>py-eb-tutorial</code> which is in fact a Python package that uses the <code>eb-tutorial</code> package installed in the previous exercise. These EasyConfig files are in the <code>EasyBuild</code> subdirectory of the exercises for this course. In the first exercise you are asked to install the version of <code>py-eb-tutorial</code> for the <code>cpeCray/22.08</code> toolchain.</p> Click to see the solution. <ul> <li> <p>Go to the <code>EasyBuild</code> subdirectory of the exercises and check that it indeed contains the     <code>py-eb-tutorial-1.0.0-cpeCray-22.08-cray-python-3.9.12.1.eb</code> and     <code>py-eb-tutorial-1.0.0-cpeGNU-22.08-cray-python-3.9.12.1.eb</code> files.     It is the first one that we need for this exercise.</p> <p>You can see that we have used a very long name as we are also using a version suffix to make clear which version of Python we'll be using.</p> </li> <li> <p>Let's first check for the dependencies (out of curiosity):</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeCray-22.08-cray-python-3.9.12.1.eb -D\n</code></pre> <p>and you'll see that all dependencies are found (at least if you made the previous exercise  successfully). You may find it strange that it shows no Python module but that is because we are using the <code>cray-python</code> module which is not installed through EasyBuild and only known to EasyBuild as an external module.</p> </li> <li> <p>And now we can install the package:</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeCray-22.08-cray-python-3.9.12.1.eb\n</code></pre> </li> <li> <p>To use the package all we need to do is to load the module and to run the command that it     defines:</p> <pre><code>module load py-eb-tutorial/1.0.0-cpeCray-22.08-cray-python-3.9.12.1\npy-eb-tutorial\n</code></pre> <p>with the same remark as in the previous exercise if Lmod fails to find the module.</p> <p>You may want to do this step in a separate terminal session set up the same way, or you will get an error message in the next exercise with EasyBuild complaining that there are some modules loaded that should not be loaded.</p> </li> </ul>"},{"location":"intro-evolving/06_Exercises_1/#installing-software-with-uninstalled-dependencies","title":"Installing software with uninstalled dependencies","text":"<p>Now you're asked to also install the version of <code>py-eb-tutorial</code> for the <code>cpeGNU</code> toolchain in <code>LUMI/22.08</code> (and the solution given below assumes you haven'ty accidentally installed the wrong EasyBuild recipe in one of the previous two exercises).</p> Click to see the solution. <ul> <li> <p>We again work in the same environment as in the previous two exercises. Nothing has changed here.     Hence if not done yet we need</p> <pre><code>module load LUMI/22.08 partition/C\nmodule load EasyBuild-user\n</code></pre> </li> <li> <p>Now go to the <code>EasyBuild</code> subdirectory of the exercises (if not there yet from the previous     exercise) and check what the <code>py-eb-tutorial-1.0.0-cpeGNU-22.08-cray-python-3.9.12.1.eb</code> needs:</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeGNU-22.08-cray-python-3.9.12.1.eb -D\n</code></pre> <p>We'll now see that there are two missing modules. Not only is the  <code>py-eb-tutorial/1.0.0-cpeGNU-22.08-cray-python-3.9.12.1</code> that we try to install missing, but also the <code>eb-tutorial/1.0.1-cpeGNU-22.08</code>. EasyBuild does however manage to find a recipe from which this module can be built in the pre-installed build recipes.</p> </li> <li> <p>We can install both packages separately, but it is perfectly possible to install both packages in a single     <code>eb</code> command by using the <code>-r</code> option to tell EasyBuild to also install all dependencies.</p> <pre><code>eb py-eb-tutorial-1.0.0-cpeGNU-22.08-cray-python-3.9.12.1.eb -r\n</code></pre> </li> <li> <p>At the end you'll now notice (with <code>module avail</code>) that both the module      <code>eb-tutorial/1.0.1-cpeGNU-22.08</code> and <code>py-eb-tutorial/1.0.0-cpeGNU-22.08-cray-python-3.9.12.1</code>     are now present.</p> <p>To run you can use</p> <pre><code>module load py-eb-tutorial/1.0.0-cpeGNU-22.08-cray-python-3.9.12.1\npy-eb-tutorial\n</code></pre> </li> </ul>"},{"location":"intro-evolving/07_Slurm/","title":"Slurm on LUMI","text":"<p>Who is this for?</p> <p>We assume some familiarity with job scheduling in this section. The notes will cover some of the more basic aspects of Slurm also, though as this version of the notes is intended for Belgian users and since all but one HPC site in Belgium currently teaches Slurm to their users, some elements will be covered only briefly.</p> <p>Even if you have a lot of experience with Slurm, it may still be useful to have a quick look at this section as Slurm is not always configured in the same way.</p> <p>Links to Slurm material</p> <p>Links to Slurm material on this web page are all for the version on LUMI at the time of the course. Links in the PDF of the slides however are to the newest version.</p>"},{"location":"intro-evolving/07_Slurm/#what-is-slurm","title":"What is Slurm","text":"<p>Slurm is both a resource manager and job scheduler for supercomputers in a single package.</p> <p>A resource manager manages all user-exposed resources on a supercomputer: cores, GPUs or other accelerators, nodes, ... It sets up the resources to run a job and cleans up after the job, and may also give additional facilities to start applications in a job. Slurm does all this.</p> <p>But Slurm is also a job scheduler. It will assign jobs to resources, following policies set by sysadmins to ensure a good use of the machine and a fair distribution of resources among projects.</p> <p>Slurm is the most popular resource manager and job scheduler at the moment and is used on more than 50% of all big supercomputers. It is an open source package with commercial support. Slurm is a very flexible and configurable tool with the help of tens or even hundreds of plugins. This also implies that Slurm installations on different machines can also differ a lot and that not all features available on one computer are also available on another. So do not expect that Slurm will behave the same on LUMI as on that other computer you're familiar with, even if that other computer may have hardware that is very similar to LUMI.</p> <p>Slurm is starting to show its age and has trouble dealing in an elegant and proper way with the deep hierarcy of resources in modern supercomputers. So Slurm will not always be as straightforward to use as we would like it, and some tricks will be needed on LUMI. Yet there is no better option at this moment that is sufficiently mature.</p> <p>Other systems in Belgium</p> <p>Previously at the VSC Torque was used as the resource manager and Moab as the scheduler. All VSC sites now use Slurm, though at UGent it is still hidden behind wrappers that mimic Torque/Moab. As we shall see in this and the next session, Slurm, which is a more modern resource manager and scheduler than the Torque-Moab combination, has  already trouble dealing well with the hierarchy of resources in a modern supercomputer. Yet it is still a lot better at it than Torque and Moab. So no, the wrappers used on the HPC systems managed by UGent will not be installed on LUMI.</p> <p>Nice to know...</p> <p>Lawrence Livermore National Laboratory, the USA national laboratory that  originally developed Slurm is now working on the  development of andother resource and job management framework called  flux. It will be used on the third USA exascale supercomputer El Capitan which is currently being assembled. </p>"},{"location":"intro-evolving/07_Slurm/#slurm-concepts-physical-resources","title":"Slurm concepts: Physical resources","text":"<p>The machine model of Slurm is bit more limited than what we would like for LUMI. </p> <p>On the CPU side it knows:</p> <ul> <li> <p>A node: The hardware that runs a single operating system image</p> </li> <li> <p>A socket: On LUMI a Slurm socket corresponds to a physical socket, so there are two sockets on the      CPU nodes and a single socket on a GPU node.</p> <p>Alternatively a cluster could be configured to let a Slurm socket correspond to a NUMA domain or  L3 cache domain, but this is something that sysadmins need to do so even if this would be useful for your job, you cannot do so.</p> </li> <li> <p>A core is a physical core in the system</p> </li> <li> <p>A thread is a hardware thread in the system (virtual core)</p> </li> <li> <p>A CPU is a \"consumable resource\" and the unit at which CPU processing capacity is allocated to a job.     On LUMI a Slurm CPU corresponds to a physical core, but Slurm could also be configured to let it correspond     to a hardware thread.</p> </li> </ul> <p>The first three bullets already show the problem we have with Slurm on LUMI: For three levels in the hierarchy of CPU resources on a node: the socket, the NUMA domain and the L3 cache domain, there is only one concept in Slurm, so we are not able to fully specify the hierarchy in resources that we want when sharing nodes with  other jobs.</p> <p>A GPU in Slurm is an accelerator and on LUMI corresponds to one GCD of an MI250X, so one half of an MI250X.</p>"},{"location":"intro-evolving/07_Slurm/#slurm-concepts-logical-resources","title":"Slurm concepts: Logical resources","text":"<ul> <li> <p>A partition: is a job queue with limits and access control. Limits include maximum     wall time for a job, the maximum number of nodes a single job can use, or the maximum     number of jobs a user can run simultaneously in the partition. The access control      mechanism determines who can run jobs in the partition.</p> <p>It is different from what we call LUMI-C and LUMI-G, or the <code>partition/C</code> and <code>partition/G</code> modules in the LUMI software stacks.</p> <p>Each partition covers a number of nodes, but partitions can overlap. This is not the case for the partitions that are visible to users on LUMI. Each partition covers a disjunct set of nodes. There are hidden partitions though that overlap with other partitions, but they are not accessible to regular users.</p> </li> <li> <p>A job in Slurm is basically only a resource allocation request.</p> </li> <li> <p>A job step is a set of (possibly parallel) tasks within a job</p> <ul> <li> <p>Each batch job always has a special job step called the batch job step which runs     the job script on the first node of a job allocation.</p> </li> <li> <p>An MPI application will typically run in its own job step.</p> </li> <li> <p>Serial or shared memory applications are often run in the batch job step but there     can be good reasons to create a separate job step for those applications.</p> </li> </ul> </li> <li> <p>A task executes in a job step and corresponds to a Linux process (and possibly subprocesses)</p> <ul> <li> <p>A shared memory program is a single task</p> </li> <li> <p>In an MPI application: Each rank (so each MPI process) is a task</p> <ul> <li> <p>Pure MPI: Each task uses a single CPU (which is also a core for us)</p> </li> <li> <p>Hybrid MPI/OpenMP applications: Each task can use multiple CPUs</p> </li> </ul> </li> </ul> <p>Of course a task cannot use more CPUs than available in a single node as a process can only run within a single operating system image.</p> </li> </ul>"},{"location":"intro-evolving/07_Slurm/#slurm-is-first-and-foremost-a-batch-scheduler","title":"Slurm is first and foremost a batch scheduler","text":"<p>And LUMI is in the first place a batch processing supercomputer.</p> <p>A supercomputer like LuMI is a very large and very expensive machine. This implies that it also has to be used as efficiently as possible which in turn implies that we don't want to wast time waiting for input as is the case in an interactive program.</p> <p>On top of that, very few programs can use the whole capacity of the supercomputer, so in practice a supercomputer is a shared resource and each simultaneous user gets a fraction on the machine depending on the requirements that they specify. Yet, as parallel applications work best when performance is predictable, it is also important to isolate users enough from each other.</p> <p>Research supercomputers are also typically very busy with lots of users so one often has to wait a little  before resources are available. This may be different on some commercial supercomputers and is also different on commercial cloud infrastructures, but the \"price per unit of work done on the cluster\" is also very  different from an academic supercomputer and few or no funding agencies are willing to carry that cost.</p> <p>Due to all this the preferred execution model on supercomputer is via batch jobs as they don't have to wait for input from the user, specified via batch scripts with resource specification where the user asks  precisely the amount of resources needed for the job, submitted to a queueing system with a scheduler to select the next job in a fair way based on available resources and scheduling policies set by the  compute centre.</p> <p>LUMI does have some facilities for interactive jobs, and with the introduction of Open On Demand some more may be available. But it is far from ideal, and you will also be billed for the idle time of the resources you request. In fact, if you only need some interactive resources for a quick 10-minute experiment and don't  need too many resources, the wait may be minimal thanks to a scheduler mechanism called backfill where the scheduler looks for small and short jobs to fill up the gaps left while the scheduler is collecting resources for a big job.</p>"},{"location":"intro-evolving/07_Slurm/#a-slurm-batch-script","title":"A Slurm batch script","text":"<p>Slurm batch scripts (also called job scripts) are conceptually not that different from batch scripts for other HPC schedulers. A typical batch script will have 4 parts:</p> <ol> <li> <p>The shebang line with the shell to use. We advise to use the bash shell (<code>/bin/bash</code> or <code>/usr/bin/bash</code>)     If omitted, a very restricted shell will be used and some commands (e.g., related to modules)     may fail. In principle any shell language that uses a hashtag to denote comments can be used, but     we would advise against experimenting and the LUMI User Support Team and VSC support teams will only     support bash.</p> </li> <li> <p>Specification of resources and some other instructions for the scheduler and resource manager. This part     is also optional as one can also pass the instructions via the command line of <code>sbatch</code>, the command to     submit a batch job. But again, we would advise against omitting this block as specifying all options on     the command line can be very tedious.</p> </li> <li> <p>Building a suitable environment for the job. This part is also optional as on LUMI, Slurm will copy the     environment from the node from which the job was submitted. This may not be the ideal envrionment for your job,     and if you later resubmit the job you may do so accidentally from a different environment so it is a good practice     to specify the environment.</p> </li> <li> <p>The commands you want to execute.</p> </li> </ol> <p>Blocks 3 and 4 can of course be mixed as you may want to execute a second command in a different environment.</p> <p>On the following slides we will explore in particular the second block and to some extent how to start programs (the fourth block).</p> <p>lumi-CPEtools module</p> <p>The <code>lumi-CPEtools</code> module will be used a lot in this session of the course and in the next one on binding. It contains among other things a number of programs to quickly visualise how a serial, OpenMP, MPI or hybrid OpenMP/MPI application would run on LUMI and which cores and GPUs would be used. It is a very useful tool to  discover how Slurm options work without using a lot of billing units and we would advise you to  use it whenever you suspect Slurm isn't doing what you meant to do.</p> <p>It has its documentation page in the LUMI Software Library.</p>"},{"location":"intro-evolving/07_Slurm/#partitions","title":"Partitions","text":"<p>Remark</p> <p>Jobs run in partitions so the first thing we should wonder when setting up a job is which partition to use for a job (or sometimes partitions in case of a heterogeneous job which will be discussed later).</p> <p>Slurm partitions are possibly overlapping groups of nodes with similar resources or associated limits.  Each partition typically targets a particular job profile. E.g., LUMI has partitions for large multi-node jobs, for smaller jobs that often cannot fill a node, for some quick debug work and for some special resources that  are very limited (the nodes with 4TB of memory and the nodes with GPUs for visualisation). The number of jobs a user can have running simultaneously in each partition or have in the queue, the maximum wall time for a job, the number of nodes a job can use are all different for different partitions.</p> <p>There are two types of partitions on LUMI:</p> <ul> <li> <p>Exclusive node use by a single job. This ensures that parallel jobs can have a clean environment     with no jitter caused by other users running on the node and with full control of how tasks and threads     are mapped onto the available resources. This may be essential for the performance of a lot of codes.</p> </li> <li> <p>Allocatable by resources (CPU and GPU). In these partitions nodes are shared by multiple users and     multiple jobs, though in principle it is possible to ask for exclusive use which will however increase     your waiting time in the queue. The cores you get are not always continuously numbered, nor do you      always get the minimum number of nodes needed for the number of tasks requested. A proper mapping      of cores onto GPUs is also not ensured at all. The fragmentation of resources is a real problem on     these nodes and this may be an issue for the performance of your code.</p> </li> </ul> <p>It is also important to realise that the default settings for certain Slurm parameters may differ between partitions and hence a node in a partition allocatable by resource but for which exclusive  access was requested may still react differently to a node in the exclusive partitions.</p> <p>In general it is important to use some common sense when requesting resources and to have some understanding of what each Slurm parameter really means. Overspecifying resources (using more parameters than needed for the desired effect) may result in unexpected conflicts between parameters and error messages.</p> <p></p> <p>For the overview of Slurm partitions, see the LUMI dpcumentation, \"Slurm partitions\" page. In the overview on the slide we did not mention partitions that are hidden to regular users.</p> <p>The policies for partitions and the available partitions may change over time to fine tune the operation of LUMI and depending on needs observed by the system administrators and LUMI User Support Team.</p> <p></p> <p>Some useful commands with respect to Slurm partitions:</p> <ul> <li> <p>To request information about the available partitions, use <code>sinfo -s</code>: </p> <pre><code>$ sinfo -s\nPARTITION   AVAIL  TIMELIMIT   NODES(A/I/O/T) NODELIST\nstandard       up 2-00:00:00  595/71/354/1020 nid[001002-001009,001012-002023]\nsmall          up 3-00:00:00   354/16/124/494 nid[002028-002060,002062-002152,002154-002523]\ninteractive    up    8:00:00          0/3/1/4 nid[002524-002527]\ndebug          up      30:00          0/5/3/8 nid[002528-002535]\nlumid          up 1-00:00:00          0/0/8/8 nid[000016-000023]\nlargemem       up 1-00:00:00          0/0/8/8 nid[000101-000108]\neap            up 1-00:00:00        2/26/4/32 nid[005000-005031]\nstandard-g     up 2-00:00:00 1809/319/80/2208 nid[005032-007239]\nsmall-g        up 3-00:00:00    62/196/22/280 nid[007240-007519]\ndev-g          up    6:00:00        1/43/4/48 nid[007520-007531,007534-007545,007548-007559,007562-007573]\nq_nordiq       up      15:00          0/1/0/1 nid001001\nq_fiqci        up      15:00          0/1/0/1 nid002153\n</code></pre> <p>The fourth column shows 4 numbers: The number of nodes that are currently fully or partially allocated to jobs, the number of idle nodes, the number of nodes in one of the other possible states (and not user-accessible) and the total number of nodes in the partition. When these nodes were written a large number of nodes were suffering from hardware problems with the Slingshot network cards and we were waiting for replacement parts which explains the thigh number of CPU nodes in the \"other\" field.</p> <p>Note that this overview may show partitions that are not hidden but also not accessible to everyone. E.g.,  the <code>q_nordic</code> and <code>q_fiqci</code> partitions are used to access experimental quantum computers that are only available to some users of those countries that paid for those machines, which does not include Belgium.</p> <p>The <code>eap</code> partition will likely be phased out over time and is a remainder of a platform used for early  development before LUMI-G was attached to the machine. At the moment it allows users to experiment freely  with the GPU nodes.</p> <p>It is not clear to the LUMI Support Team what the <code>interactive</code> partition, that uses dome GPU nodes, is  meant for as it was introduced without informting the support. The resources in that partition are very limited so it is not meant for widespread use.</p> </li> <li> <p>For technically-oriented people, some more details about a partition can be obtained with     <code>scontrol show partition &lt;partition-name&gt;</code>.</p> </li> </ul> Additional example with <code>sinfo</code> <p>Try</p> <p><pre><code>$ sinfo --format \"%4D %10P %25f %.4c %.8m %25G %N\"\nNODE PARTITION  AVAIL_FEATURES            CPUS   MEMORY GRES                      NODELIST\n256  standard   AMD_EPYC_7763,x1001        256   229376 (null)                    nid[001256-001511]\n256  standard   AMD_EPYC_7763,x1002        256   229376 (null)                    nid[001512-001767]\n256  standard   AMD_EPYC_7763,x1003        256   229376 (null)                    nid[001768-002023]\n252  standard   AMD_EPYC_7763,x1000        256   229376 (null)                    nid[001002-001009,001012-001255]\n244  small      AMD_EPYC_7763,x1005        256  229376+ (null)                    nid[002280-002523]\n250  small      AMD_EPYC_7763,x1004        256   229376 (null)                    nid[002028-002060,002062-002152,002154-002279]\n4    interactiv AMD_EPYC_7763,x1005        256   229376 (null)                    nid[002524-002527]\n8    debug      AMD_EPYC_7763,x1005        256   229376 (null)                    nid[002528-002535]\n8    lumid      AMD_EPYC_7742              256  2048000 gpu:a40:8,nvme:40000      nid[000016-000023]\n8    largemem   AMD_EPYC_7742              256 4096000+ (null)                    nid[000101-000108]\n32   eap        AMD_EPYC_7A53,x1100        128   491520 gpu:mi250:8               nid[005000-005031]\n112  standard-g AMD_EPYC_7A53,x1401        128   491520 gpu:mi250:8               nid[007016-007127]\n112  standard-g AMD_EPYC_7A53,x1402        128   491520 gpu:mi250:8               nid[007128-007239]\n80   standard-g AMD_EPYC_7A53,x1100        128   491520 gpu:mi250:8               nid[005032-005111]\n112  standard-g AMD_EPYC_7A53,x1101        128   491520 gpu:mi250:8               nid[005112-005223]\n112  standard-g AMD_EPYC_7A53,x1102        128   491520 gpu:mi250:8               nid[005224-005335]\n112  standard-g AMD_EPYC_7A53,x1103        128   491520 gpu:mi250:8               nid[005336-005447]\n112  standard-g AMD_EPYC_7A53,x1104        128   491520 gpu:mi250:8               nid[005448-005559]\n112  standard-g AMD_EPYC_7A53,x1105        128   491520 gpu:mi250:8               nid[005560-005671]\n112  standard-g AMD_EPYC_7A53,x1200        128   491520 gpu:mi250:8               nid[005672-005783]\n112  standard-g AMD_EPYC_7A53,x1201        128   491520 gpu:mi250:8               nid[005784-005895]\n112  standard-g AMD_EPYC_7A53,x1202        128   491520 gpu:mi250:8               nid[005896-006007]\n112  standard-g AMD_EPYC_7A53,x1203        128   491520 gpu:mi250:8               nid[006008-006119]\n112  standard-g AMD_EPYC_7A53,x1204        128   491520 gpu:mi250:8               nid[006120-006231]\n112  standard-g AMD_EPYC_7A53,x1205        128   491520 gpu:mi250:8               nid[006232-006343]\n112  standard-g AMD_EPYC_7A53,x1300        128   491520 gpu:mi250:8               nid[006344-006455]\n112  standard-g AMD_EPYC_7A53,x1302        128   491520 gpu:mi250:8               nid[006568-006679]\n112  standard-g AMD_EPYC_7A53,x1303        128   491520 gpu:mi250:8               nid[006680-006791]\n112  standard-g AMD_EPYC_7A53,x1304        128   491520 gpu:mi250:8               nid[006792-006903]\n112  standard-g AMD_EPYC_7A53,x1305        128   491520 gpu:mi250:8               nid[006904-007015]\n112  standard-g AMD_EPYC_7A53,x1301        128   491520 gpu:mi250:8               nid[006456-006567]\n56   small-g    AMD_EPYC_7A53,x1405        128   491520 gpu:mi250:8               nid[007464-007519]\n112  small-g    AMD_EPYC_7A53,x1404        128   491520 gpu:mi250:8               nid[007352-007463]\n112  small-g    AMD_EPYC_7A53,x1403        128   491520 gpu:mi250:8               nid[007240-007351]\n48   dev-g      AMD_EPYC_7A53,x1405        128   491520 gpu:mi250:8               nid[007520-007531,007534-007545,007548-007559,007562-007573]\n1    q_nordiq   AMD_EPYC_7763,x1000        256   229376 (null)                    nid001001\n1    q_fiqci    AMD_EPYC_7763,x1004        256   229376 (null)                    nid002153\n</code></pre> (Output may vary over time)</p> <p>This shows more information about the system. The <code>xNNNN</code> feature corresponds to groups in  the Slingshot interconnect and may be useful if you want to try to get a job running in a single group (which is too advanced for this course).</p> <p>The memory size is given in megabyte (MiB, multiples of 1024). The \"+\" in the second group of the small partition is because that partition also contains the 512 GB and 1 TB regular  compute nodes. The memory reported is always 32 GB less than you would expect from the  node specifications. This is because 32 GB on each node is reserved for the OS and the  RAM disk it needs.</p>"},{"location":"intro-evolving/07_Slurm/#queueing-and-fairness","title":"Queueing and fairness","text":"<p>Remark</p> <p>Jobs are queued until they can run so we should wonder how that system works.</p> <p>LUMI is a pre-exascale machine meant to foster research into exascale applications.  As a result the scheduler setup of LUMI favours large jobs (though some users with large jobs will claim that it doesn't do so enough yet). Most nodes are reserved for larger  jobs (in the <code>standard</code> and <code>standard-g</code> partitions), and the priority computation also favours larger jobs (in terms of number of nodes).</p> <p>When you submit a job, it will be queued until suitable resources are available for the requested time window. Keep in mind that you may see a lot of free nodes on LUMI yet your  small job may not yet start immediately as the scheduler may be gathering nodes for a big job.</p> <p>The command to check the status of the queue is <code>squeue</code>. Two command line flags are useful:</p> <ul> <li> <p><code>--me</code> will restrict the output to your jobs only</p> </li> <li> <p><code>--start</code> will give an estimated start time for each job. Note that this really doesn't say      much as the scheduler cannot predict the future. On one hand, other jobs that are running     already or scheduled to run before your job, may have overestimated the time they need and     end early. But on the other hand, the scheduler does not use a \"first come, first serve\" policy     so another user may submit a job that gets a higher priority than yours, pushing back the start     time of your job. So it is basically a random number generator.</p> </li> </ul> <p>The <code>sprio</code> command will list the different elements that determine the priority of your job but is basically a command for system administrators as users cannot influence those numbers nor do  they say a lot unless you understand all intricacies of the job policies chosen by the site, and those policies may be fine-tuned over time to optimise the operation of the cluster. The fairshare parameter influences the priority of jobs depending on how much users or projects (this is not clear to us at the moment) have been running jobs in the past few days and is a very dangerous parameter on a supercomputer where the largest project is over 1000 times the size of the smallest projects, as treating all projects equally for the fair share would make it impossible for big projects to consume all their CPU time.</p> <p>Another concept of the scheduler on LUMI is backfill. On a system supporting very large jobs as LUMI, the scheduler will often be collecting nodes to run those large jobs, and this may take a while, particularly since the maximal wall time for a job in the standard partitions is rather large for such a system. If you need one quarter of the nodes for a big job on a partition on which most  users would launch jobs that use the full two days of walltime, one can expect that it takes half a day to gather those nodes. However, the LUMI scheduler will schedule short jobs even though they have a lower priority on the nodes already collected if it expects that those jobs will be finisehd before it expects to have all nodes for the big job. This mechanism is called backfill and is the reason why short experiments of half an hour or so often start quickly on LUMI even though the queue is very long.</p>"},{"location":"intro-evolving/07_Slurm/#accounting-of-jobs","title":"Accounting of jobs","text":"<p>Billing of jobs</p> <p>Jobs are billed against your allocation, so how does this work?</p> <p>The use of resources by a job is billed to projects, not users. All management is also done at the project level, not at the \"user-in-a-project\" level. As users can have multiple projects, the system cannot know to which project a job should be billed, so it is mandatory to specify a project account (of the form <code>project_46YXXXXXX</code>) with every command that creates an allocation.</p> <p>Billing on LUMI is not based on which resources you effectively use, but on the amount of resources that others cannot use well because of your allocation.  This assumes that you make proportional use of CPU cores, CPU memory and GPUs (actually GCDs). If you job makes a disproportionally high use of one of those resources, you will be billed based on that use. For the CPU nodes, the billing is based on both the number of cores you request in your allocation and the amount of memory compared to the amount of memory per core in a regular node, and the highest of the two numbers is used. For the GPU nodes, the formula looks at the number of cores compared to he number of cores per GPU, the amount of CPU memory compared  to the amount of memory per GCD (so 64 GB), and the amount of GPUs and the highest amount determines for how many GCDs you will be billed (with a cost of 0.5 GPU-hour per hour per GCD). For jobs in job-exclusive partitions you are automatically billed for the full node as no other job can use that node, so 128 core-hours per hour for the standard partition or 4 GPU-hours per hour for the standard-g partition.</p> <p>E.g., if you would ask for only one core but 128 GB of memory, half of what a regular LUMI-C node has, you'd be billed for the use of 64 cores. Or assume you want to use only one GCD but want to use 16 cores and 256 GB of system RAM with it, then you would be billed for 4 GPUs/GCDs: 256 GB of memory makes it impossible for other users to use 4 GPUs/GCDs in the system, and 16 cores make it impossible to use 2 GPUs/GCDs, so the highest number of those is 4, which means that you will pay 2 GPU-hours per hour that you use the allocation (as GPU-hours are based on a full MI250x and not on a GCD which is the GPU for Slurm).</p> <p>This billing policy is unreasonable!</p> <p>Users who have no experience with performance optimisation may think this way of billing is unfair. After all, there may be users who need far less than 2 GB of memory per core so they could still use the other cores on a node where I am using only one core but 128 GB of memory, right? Well, no, and this has everything to do with the very hierarchical nature of a modern compute node, with on LUMI-C 2 sockets, 4 NUMA domains per socket, and 2 L3 cache domains per NUMA domain. Assuming your job would get the first core on the first socket (called core 0 and socket 0 as computers tend to number from 0). Linux will then allocate the memory of the job as close as possible to that core, so it will fill up the 4 NUMA domains of that socket. It can migrate unused memory to the other socket, but let's assume your  code does not only need 128 GB but also accesses bits and pieces from it everywhere all the time. Another application running on socket 0 may then get part or all of its memory on socket 1, and the latency to access that memory is more than  3 times higher, so performance of that application will suffer. In other words, the other cores in socket 0 cannot be used with full efficiency.</p> <p>This is not a hypothetical scenario. The author of this text has seem benchmarks run on one of the largest systems in Flanders that didn't scale at all and for some core configuration ran at only 10% of the speed they should have been running at...</p> <p>Still, even with this billing policy Slurm oon LUMI is a far from perfect scheduler and core, GPU and memory allocation on the non-exclusive partitions are far from optimal. Which is why we spend a section of the course on binding applications to resources.</p> <p>The billing is done in a postprocessing step in the system based on data from the Slurm  job database, but the Slurm accounting features do not produce the correct numbers.  E.g., Slurm counts the core hours based on the virtual cores so the numbers are double of what they should be. There are two ways to check the state of an allocation, though both work with some delay.</p> <ul> <li> <p>The <code>lumi-workspaces</code> and <code>lumi-allocations</code> commands show the total amount of      billing units consumed. In regular operation of the system these numbers are updated     approximately once an hour.</p> <p><code>lumi-workspaces</code> is the all-in command that intends to show all information that is  useful to a regular user, while <code>lumi-allocations</code> is a specialised tool that only shows billing units, but he numbers shown by both tools come from the same database and are identical.</p> </li> <li> <p>For projects managed via Puhuri, Puhuri can show billing unit use per month, but the     delay is larger than with the <code>lumi-workspaces</code> command.</p> </li> </ul> <p>Billing unit use per user in a project</p> <p>The current project management system in LUMI cannot show the use of billing units per person within a project.</p> <p>For storage quota this would be very expensive to organise as quota are managed by Lustre on a group basis. </p> <p>For CPU and GPU billing units it would in principle be possible as the Slurm database contains the necessary information, but there are no plans to implement such a feature. It is assumed that every PI makes sure that members of their  projects use LUMI in a responsible way and ensures that they have sufficient  experience to realise what they are doing.</p>"},{"location":"intro-evolving/07_Slurm/#managing-slurm-jobs","title":"Managing Slurm jobs","text":"<p>Before experimenting with jobs on LUMI, it is good to discuss how to manage those jobs. We will not discuss the commands in detail and instead refer to the pretty decent manual pages that in fact can also be found on the web.</p> <p>Remember that each job is identified by a unique job ID, a number that will be shown when you submit a job and is also shown in the output of <code>squeue</code>.  This job ID will be used to manage jobs.</p> <ul> <li> <p>To delete a job, use <code>scancel &lt;jobID&gt;</code></p> </li> <li> <p>An important command to manage jobs while they are running is      <code>sstat -j &lt;jobID&gt;</code>.     This command display real-time information directly gathered from the resource manager     component of Slurm and can also be used to show information about individual job steps using     the job step identifier (which is in most case <code>&lt;jobID&gt;.0</code> for the first rebular job step and so on).     The <code>sstat</code> command can display a lot more information than is shown by default. The output can     be adapted via the <code>--format</code> or <code>-o</code> command line option     with a list of options in the \"Job status fields\" section of the manual page.</p> </li> <li> <p>The <code>sacct -j &lt;jobID&gt;</code> command can be used both while the     job is running and when the job has finished. It is the main command to get information about a job     after the job has finished. All information comes from a database, also while the job is running, so      the information is available with some delay compared to the information obtained with <code>sstat</code> for     a running job. It will also produce information about individual job steps. Just as with <code>sstat</code>, the     fields to display can be selected via the     <code>--format</code> or <code>-o</code> command line option with an even     longer list in the \"Job accounting fields\" section of the manual page.</p> </li> </ul> <p>The <code>sacct</code> command will also be used in various examples in this section of the tutorial to investigate the behaviour of Slurm.</p>"},{"location":"intro-evolving/07_Slurm/#creating-a-slurm-job","title":"Creating a Slurm job","text":"<p>Slurm has three main commands to create jobs and job steps.  Remember that a job is just a request for an allocation. Your applications always have to run inside a job step.</p> <p>The <code>salloc</code> command only creates an allocation but does not create a job step. The behaviour of <code>salloc</code> differs between clusters!  On LUMI, <code>salloc</code> will put you in a new shell on the node from which you issued the <code>salloc</code> command, typically the login node. Your allocation will exist until you exit that shell with the <code>exit</code> command or with the CONTROL-D key combination. Creating an allocation with <code>salloc</code> is good for interactive work.</p> <p>Differences in <code>salloc</code> behaviour.</p> <p>On some systems <code>salloc</code> does not only create a job allocation but will also create a job step, the so-called \"interactive job step\" on a node of the allocation, similar to the way that the <code>sbatch</code> command discussed later will create a so-called \"batch job step\".</p> <p>The main purpose of the <code>srun</code> command is to create a job step in an allocation. When run outside of a job (outside an allocation) it will also create a job allocation. However, be careful when using this command to also create the job in which the job step will run as some options work differently as for the commands meant to create an allocation. When creating a job with <code>salloc</code> you will have to use <code>srun</code> to start anything on the node(s) in the allocation as it is not possible to, e.g., reach the nodes with <code>ssh</code>.</p> <p>The <code>sbatch</code> command both creates a job and then start a job step, teh so-called batch job step, to run the job script on the first node of the job allocation. In principle it is possible to start both sequential and shared memory processes directly in the batch job step without creating a new job step with <code>srun</code>,  but keep in mind that the resources may be different from what you expect to see in some cases as some of the options given with the <code>sbatch</code> command will only be enforced when starting another job step from the batch job step. To run any multi-process job (e.g., MPI) you will have to use <code>srun</code> or a process starter that internally calls <code>srun</code> to start the job. When using Cray MPICH as the MPI implementation (and it is the only one that is fully supported on LUMI) you will have to use <code>srun</code> as the process starter.</p>"},{"location":"intro-evolving/07_Slurm/#passing-options-to-srun-salloc-and-sbatch","title":"Passing options to srun, salloc and sbatch","text":"<p>There are several ways to pass options and flags to the <code>srun</code>, <code>salloc</code> and <code>sbatch</code> command.</p> <p>The lowest priority way and only for the <code>sbatch</code> command is specifying the options (mostly resource-related) in the batch script itself on <code>#SBATCH</code> lines. These lines should not be interrupted by commands, and it is not possible to use environment variables to specify values of options. </p> <p>Higher in priority is specifying options and flags through environment variables.  For the <code>sbatch</code> command this are the <code>SBATCH_*</code> environment variables, for <code>salloc</code> the <code>SALLOC_*</code> environment variables and for <code>srun</code> the <code>SLURM_*</code> and some <code>SRUN_*</code> environment variables. For the <code>sbatch</code> command this will overwrite values on the <code>#SBATCH</code> lines. You can find lists in the manual pages of the  <code>sbatch</code>,  <code>salloc</code> and <code>srun</code> command. Specifying command line options via environment variables that are hidden in your <code>.profile</code> or <code>.bashrc</code> file or any script that you run before starting your work, is not free of risks. Users often forget that they set those environment variables and are then surprised that the Slurm commands act differently then expected. E.g., it is very tempting to set the project account to use in environment variables but if you  then get a second project you may be running inadvertently in the wrong project.</p> <p>The highest priority is for flags and options given on the command line. The position of  those options is important though. With the <code>sbatch</code> command they have to be specified before the batch script as otherwise they will be passed to the batch script as command line options for  that script. Likewise, with <code>srun</code> they have to be specified before the command you want to execute as otherwise they would be passed to that command as flags and options.</p> <p>Several options specified to <code>sbatch</code> or <code>salloc</code> are also forwarded to <code>srun</code> via <code>SLURM_*</code> environment variables set in the job by these commands.</p>"},{"location":"intro-evolving/07_Slurm/#specifying-options","title":"Specifying options","text":"<p>Slurm commands have way more options and flags than we can discuss in this course or even the 4-day comprehensive course organised by the LUMI User Support Team. Moreover, if and how they work may depend on the specific configuration of Slurm. Slurm has so many options that no two clusters are the same. </p> <p>Slurm command can exist in two variants:</p> <ol> <li> <p>The long variant, with a double dash, is of the form <code>--long-option=&lt;value&gt;</code> or      <code>--long-option &lt;value&gt;</code></p> </li> <li> <p>But many popular commands also have a single letter variant, with a single dash:     <code>-S &lt;value&gt;</code> or <code>-S&lt;value&gt;</code></p> </li> </ol> <p>This is no different from many popular Linux commands.</p> <p>Slurm commands for creating allocations and job steps have many different flags for specifying the allocation and the organisation of tasks in that allocation. Not all combinations are valid, and it is not possible to sum up all possible configurations for all possible scenarios. Use  common sense and if something does not work, check the manual page and try something different. Overspecifying options is not a good idea as you may very well create conflicts, and we will see some examples in this section and the next section on binding. However, underspecifying is not a good idea either as some defaults may be used you didn't think of. Some combinations also just  don't make sense, and we will warn for some on the following slides and try to bring some  structure in the wealth of options.</p>"},{"location":"intro-evolving/07_Slurm/#some-common-options-to-all-partitions","title":"Some common options to all partitions","text":"<p>For CPU and GPU requests, a different strategy should be used for \"allocatable by node\" and \"allocatable by resource\" partitions, and this will be discussed later. A number of options however are common to both strategies and will be discussed first. All are typically used on <code>#SBATCH</code> lines in job scripts, but can also be used on the command line and the first three are certainly needed with <code>salloc</code> also.</p> <ul> <li> <p>Specify the account to which the job should be billed with <code>--account=project_46YXXXXXX</code> or <code>-A project_46YXXXXXX</code>.     This is mandatory; without this your job will not run.</p> </li> <li> <p>Specify the partition: <code>--partition=&lt;partition&gt;  or</code>-p `. This option is also necessary     on LUMI as there is currently no default partition. <li> <p>Specify the wall time for the job: <code>--time=&lt;timespec&gt;</code> or <code>-t &lt;timespec&gt;</code>. There are multiple formats for     the time specifications, but the most common ones are minutes (one number), minutes:seconds (two numbers separated     by a colon) and hours:minutes:seconds (three numbers separated by a column). If not specified, the partition-dependent     default time is used.</p> <p>It does make sense to make a reasonable estimate for the wall time needed. It does protect you a bit in case your application hangs for some reason, and short jobs that also don't need too many nodes have a high chance of running quicker as they can be used as backfill while the scheduler is gathering nodes for a big job.</p> </li> <li> <p>Completely optional: Specify a name for the job with <code>--job-name=&lt;name&gt;</code> or <code>-J &lt;name&gt;</code>. Short but clear     job names help to make the output of <code>squeue</code> easier to interpret, and the name can be used to generate      a name for the output file that captures output to stdout and stderr also.</p> </li> <li> <p>For courses or other special opportunities such as the \"hero runs\" (a system for projects that want to test     extreme scalability beyond the limits of the regular partitions), reservations are used. You can specify the     reservation (or even multiple reservations as a comma-separated list) with <code>--reservation=&lt;name&gt;</code>.</p> <p>In principle no reservations are given to regular users for regular work as this is unfair to other users. It would not be possible to do all work in reservations and bypass the scheduler as the scheduling would be extremely complicated and the administration enormous. And empty reservations do not lead to efficient machine use. Schedulers have been developed for a reason.</p> </li> <li> <p>Slurm also has options to send mail to a given address when a job starts or ends or some other job-related     events occur, but this is currently not configured on LUMI.</p> </li>"},{"location":"intro-evolving/07_Slurm/#redirecting-output","title":"Redirecting output","text":"<p>Slurm has two options to redirect stdout and stderr respectively: <code>--output=&lt;template&gt;</code> or <code>-o &lt;template&gt;</code> for stdout and <code>--error=&lt;template&gt;</code> or <code>-e &lt;template&gt;</code> for stderr. They work together in the following way:</p> <ul> <li> <p>If neither <code>--output</code> not <code>--error</code> is specified, then stdout and stderr are merged and redirected to the file <code>slurm-&lt;jobid&gt;.out</code>.</p> </li> <li> <p>If <code>--output</code> is specified but <code>--error</code> is not, then stdout and stderr are merged and redirected to the file given with <code>--output</code>.</p> </li> <li> <p>If <code>--output</code> is not specified but <code>--error</code>, then stdout will still be redirected to <code>slurm-&lt;jobid&gt;.out</code>, but     stderr will be redirected to the file indicated by the <code>--error</code> option.</p> </li> <li> <p>If both <code>--output</code> and <code>--error</code> are specified, then stdout is redirected to the file given by <code>--output</code> and     stderr is redirected to the file given by <code>--error</code>.</p> </li> </ul> <p>It is possible to insert codes in the filename that will be replaced at runtime with the corresponding Slurm  information. Examples are <code>%x</code> which will be replaced with the name of the job (that you can then best set with <code>--job-name</code>) and `%j`` which will be replaced with the job ID (job number). It is recommended to always include  the latter in the template for the filename as this ensures unique names if the same job script would be run a  few times with different input files. Discussing all patterns that can be used for the filename is outside the scope of this tutorial, but you can find them all in the sbatch manual page in the \"filename pattern\" section.</p>"},{"location":"intro-evolving/07_Slurm/#requesting-resources-cpus-and-gpus","title":"Requesting resources: CPUs and GPUs","text":"<p>Slurm is very flexible in the way resources can be requested. Covering every scenario and every possible way to request CPUs and GPUs is impossible, so we will present a scheme that works for most users and jobs.</p> <p>First, you have to distinguish between two strategies for requesting resources, each with their own pros and cons. We'll call them \"per-node allocations\" and \"per-core allocations\":</p> <ol> <li> <p>\"Per-node allocations\": Request suitable nodes (number of nodes and partition) with <code>sbatch</code> or <code>salloc</code>     but postpone specifying the full structure of the job step (i.e., tasks, cpus per task, gpus per task, ...)     until you actually start the job step with <code>srun</code>.</p> <p>This strategy relies on job-exclusive nodes, so works on the <code>standard</code> and <code>standard-g</code> partitions that  are \"allocatable-by-node\" partitions, but can be used on the \"allocatable-by-resource\" partitions also it the <code>--exclusive</code> flag is used  with <code>sbatch</code> or <code>salloc</code> (on the command line or with and <code>#SBATCH --exclusive</code> line for <code>sbatch</code>).</p> <p>This strategy gives you the ultimate flexibility in the job as you can run multiple job steps with a different  structure in the same job rather than having to submit multiple jobs with job dependencies to ensure that they are started in the proper order. E.g., you could first have an initialisation step that generates input files in a multi-threaded shared memory program and then run a pure MPI job with a single-threaded process per rank. </p> <p>This strategy also gives you full control over how the application is mapped onto the available hardware: mapping of MPI ranks across nodes and within nodes, binding of threads to cores, and binding of GPUs to MPI ranks. This will be the topic of the next section of the course and is for some applications very important to get optimal performance on modern supercomputer nodes that have a strongly hierarchical architecture (which in fact is not only the case for AMD processors, but will likely be an issue on some Intel Sapphire Rapids processors also).</p> <p>The downside is that allocations and hence billing is always per full node, so if you need only half a node  you waste a lot of billing units. It shows that to exploit the full power of a supercomputer you really need to have problems and applications that can at least exploit a full node.</p> </li> <li> <p>\"Per-core allocations\": Specify the full job step structure when creating the job allocation and optionally     limit the choice fo Slurm for the resource allocation by specifying a number of nodes     that should be used. </p> <p>The problem is that Slurm cannot create a correct allocation on an \"allocatable by resource\" partition if it would only know the total number of CPUs and total number of GPUs that you need. Slurm does not automatically allocate the resources on the minimal number of nodes (and even then there could be problems) and cannot know how you intend to use the resources to ensure that the resources are actually useful for you job. E.g., if you ask for 16 cores and Slurm would spread them over two or more nodes, then they would not be useful to run a shared memory program as such a program cannot  span nodes. Or if you really want to run an MPI application that needs 4 ranks and 4 cores per rank, then those cores must be assigned in groups of 4 within nodes as an MPI rank cannot span nodes. The same holds for GPUs. If you would  ask for 16 cores and 4 GPUs you may still be using them in different ways. Most users will probably intend to start an MPI program with 4 ranks that each use 4 cores and one GPU, and in that case the allocation should be done in groups  that each contain 4 cores and 1 GPU but can be spread over up to 4 nodes, but you may as well intend to run  a 16-thread shared memory application that also needs 4 GPUs. </p> <p>The upside of this is that with this strategy you will only get what you really need when used in an \"allocatable-by-resources\" partition, so  if you don't need a full node, you won't be billed for a full node (assuming of course that you don't request that much memory that you basically need a full node's memory). </p> <p>One downside is that you are now somewhat bound to the job structure. You can run job steps with a different structure, but they may produce a warning or may not run at all if the job step cannot be mapped on the resources allocated to  the job.</p> <p>More importantly, most options to do binding (See the next session) cannot be used or don't make sense anyway as there is no guarantee your cores will be allocated in a dense configuration.</p> <p>However, if you can live with those restrictions and if your job size falls within the limits of the \"allocatable per  resource\" partitions, and cannot fill up the minimal number of nodes that would be used, then this strategy ensures you're only billed for the minimal amount of resources that are made unavailable by your job.</p> </li> </ol> <p>This choice is something you need to think about in advance and there are no easy guidelines. Simply say \"use the first  strategy if your job fills whole nodes anyway and the second one otherwise unless you'd need more than 4 nodes\" doesn't make sense as your job may be so sensitive to its mapping to resources that it could perform very badly in the second case. The real problem is that there is no good way in Slurm to ask for a number of L3 cache domains (CPU chiplets), a number of NUMA domains or a number of sockets and also no easy way to always do the proper binding if you would get resources that way (but that is something that can only be understood after the next session). If a single job needs only a half  node and if all jobs take about the same time anyway, it might be better to bundle them by hand in jobs and do a proper mapping of each subjob on the available resources (e.g., in case of two jobs on a CPU node, map each on a socket).</p>"},{"location":"intro-evolving/07_Slurm/#resources-for-per-node-allocations","title":"Resources for per-node allocations","text":"<p>In a per-node allocation, all you need to specify is the partition and the number of nodes needed, and in some cases, the amount of memory. In this scenario, one should use those Slurm options that specify resources per node also.</p> <p>The partition is specified using <code>--partition=&lt;partition</code> or <code>-p &lt;partition&gt;</code>.</p> <p>The number of nodes is specified with <code>--nodes=&lt;number_of_nodes&gt;</code> or its short form  <code>-N &lt;number_of_nodes&gt;</code>.</p> <p>IF you want to use a per-node allocation on a partition which is allocatable-by-resources such as small and small-g, you also need to specify the <code>--exclusive</code> flag. On LUMI this flag does not have the same effect as running on a partition that is allocatable-by-node. The <code>--exclusive</code> flag does allocate all cores and GPUs on the node to your job, but the memory use is still limited by other parameters in the Slurm configuration. In fact, this can also be the case for allocatable-by-node partitions, but there  the limit is set to allow the use of all available memory. Currently the interplay between various parameters in the Slurm configuration results in a limit of 112 GB of memory on the <code>small</code> partition and 64 GB on the <code>standard</code> partition when running in <code>--exclusive</code> mode. It is possible to change this with the <code>--mem</code> option.</p> <p>You can request all memory on a node by using <code>--mem=0</code>. This is currently the default behaviour on nodes in the <code>standard</code> and <code>standard-g</code> partition so not really needed there. It is needed on all of the partitions that are allocatable-by-resource.</p> <p>We've experienced that it may be a better option to actually specify the maximum amount of useable memory on a node which is the memory capacity of the node you want minus 32 GB, so you can use <code>--mem=224G</code> for a regular CPU node or <code>--mem=480G</code> for a GPU node. In the past we have had memory leaks on compute nodes that were not detected by the node health checks, resulting in users getting nodes with less available memory than expected, but specifying these amounts protected them against getting such nodes. (And similarly you could use <code>--mem=480G</code> and <code>--mem=992G</code> for the 512 GB and 1 TB compute nodes in the small  partition, but note that running on these nodes is expensive!)</p> Example jobscript (click to expand) <p>The following job script runs a shared memory program in the batch job step, which shows that it has access to all hardware threads and all GPUs in a node at that moment:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-perNode-minimal-small-g\n#SBATCH --partition=small-g\n#SBATCH --exclusive\n#SBATCH --nodes=1\n#SBATCH --mem=480G\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n\nmodule load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\ngpu_check\n\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p>As we are using small-g here instead of standard-g, we added the <code>#SBATCH --exclusive</code> and <code>#SBATCH --mem=480G</code> lines.</p> <p>A similar job script for a CPU-node in LIUMI-C and now in the standard partition would look like:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-perNode-minimal-standard\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeCray-22.12\n\nomp_check\n\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p><code>gpu_check</code> and <code>omp_check</code> are two programs provided by the <code>lmi-CPEtools</code> modules to check the allocations. Try <code>man lumi-CPEtools</code> after loading the module. The programs will be used extensively in the next section on binding also, and are written to check how your program would behave in the allocation without burning through tons of billing units.</p> <p></p> <p>By default you will get all the CPUs in each node that is allocated in a per-node allocation. The Slurm options to request CPUs on a per-node basis are not really useful on LUMI, but might be on clusters with multiple node types in a single partition as they enable you to specify the minimum number of sockets, cores and hardware threads a node should have.</p> <p>We advise against using the options to request CPUs on LUMI  because it is more likely to cause problems due to user error than to solve problems. Some of these options also conflict with options that will be used later in the course.</p> <p>There is no direct way to specify the number of cores per node. Instead one has to specify the number sockets and then the number of cores per socket and one can specify even the number of hardware threads per core, though we will favour another mechanism later in these course notes.</p> <p>The two options are:</p> <ol> <li> <p>Specify <code>--sockets-per-node=&lt;sockets</code> and <code>--cores-per-socket=&lt;cores&gt;</code> and maybe even <code>--threads-per-core=&lt;threads&gt;</code>.     For LUMI-C the maximal specification is </p> <pre><code>--sockets-per-node=2 --cores-per-socket-64\n</code></pre> <p>and for LUMI-G</p> <pre><code>--sockets-per-node=1 --cores-per-socket=56\n</code></pre> <p>Note that on LUMI-G, nodes have 64 cores but one core is reserved for the operating system and drivers to  reduce OS jitter that limits the scalability of large jobs. Requesting 64 cores will lead to error messages or jobs getting stuck.</p> </li> <li> <p>There is a shorthand for those parameters: <code>--extra-node-info=&lt;sockets&gt;[:cores]</code> or     <code>-B --extra-node-info=&lt;sockets&gt;[:cores]</code> where the second and third number are optional.     The full maximal specification for LUMI-C would be <code>--extra-node-info=2:64</code> and for LUMI-G     <code>--extra-node-info=1:56</code>.</p> </li> </ol> What about <code>--threads-per-core</code>? <p>Slurm also has a <code>--threads-per-core</code> (or a third number with <code>--extra-node-info</code>) which is a somewhat misleading name. On LUMI, as hardware threads  are turned on, you would expect that you can use <code>--threads-per-core=2</code> but if you try, you will see that your job is not accepted. This because on LUMI, the smallest allocatable processor resource  (called the CPU in Slurm) is a core and not a hardware thread (or virtual core as they are also  called). There is another mechanism to enable or disable hyperthreading in regular job steps that we will discuss later.</p> <p></p> <p>By default you will get all the GPUs in each node that is allocated in a per-node allocation. The Slurm options to request GPUs on a per-node basis are not really useful on LUMI, but might be on clusters with multiple types of GPUs in a single partition as they enable you to specify which type of node you want. If you insist, slurm has several options to specify the number of GPUs for this scenario:</p> <ol> <li> <p>The most logical one to use for a per-node allocation is <code>--gpus-per-node=8</code> to request 8 GPUs per node.     You can use a lower value, but this doesn't make much sense as you will be billed for the full node anyway.</p> <p>It also has an option to also specify the type of the GPU but that doesn't really make sense on LUMI.  On LUMI, you could in principle use <code>--gpus-per-node=mi250:8</code>.</p> </li> <li> <p><code>--gpus=&lt;number&gt;</code> or <code>-G &lt;number&gt;</code> specifies the total number of GPUs needed for the job. In our opinion     this is a dangerous option to use as when you change the number of nodes, you likely also want to change     the number of GPUs for the job and you may overlook this. Here again it is possible to specify the type of     the GPU also.</p> </li> <li> <p>A GPU belongs to the family of \"generic consumable resources\" (or GRES) in Slurm and there is an option to request     any type of GRES that can also be used. Now you also need to specify the type of the GRES. The number you      have to specify if on a per-node basis, so on LUMI you can use  <code>--gres=gpu:8</code> or <code>--gres=gpu:mi250:8</code>.</p> </li> </ol> <p>As these options are also forwarded to <code>srun</code>, it will save you from specifying them there.</p>"},{"location":"intro-evolving/07_Slurm/#per-node-allocations-starting-a-job-step","title":"Per-node allocations: Starting a job step","text":"<p>Serial or shared-memory multithreaded programs in a batch script can in principle be run in  the batch job step. As we shall see though the effect may be different from what you expect.  However, if you are working interactively via <code>salloc</code> you are in a shell on the node on which you called <code>salloc</code>, typically a login node, and to run anything on the compute nodes you  will have to start a job step.</p> <p>The command to start a new job step is <code>srun</code>. But it needs a number of arguments in most cases. After all, a job step consists of a number of equal-sized tasks (considering only homogeneous job steps at the moment, the typical case for most users) that each need a number of cores or hardware threads and, in case of GPU compute, access to a number of GPUs.</p> <p></p> <p>There are several ways telling Slurm how many tasks should be created and what the  resources are for each individual task, but this scheme is an easy scheme:</p> <ol> <li> <p>Specifying the number of tasks: You can specify per node or the total number:</p> <ol> <li> <p>Specifying the total number of tasks:      <code>--ntasks=&lt;ntasks</code> or <code>-n ntasks</code>.     There is a risk associated to this approach which is the same as when specifying the     total number of GPUs for a job: IF you change the number of nodes, then you should     change the total number of tasks also. However, it is also very useful in certain cases.     Sometimes the number of tasks cannot be easily adapted and does not fit perfectly into     your allocation (cannot be divided by the number of nodes). In that case, specifying the     total number of nodes makes perfect sense.</p> </li> <li> <p>Specifying on a per node basis:      <code>--ntasks-per-node=&lt;ntasks&gt;</code>      is possible in combination with <code>--nodes</code> according to the Slurm manual.      In fact, this would be a logical thing to do in a per node allocation.      However, we see it fail on LUMI when it is used as an option for <code>srun</code> and not with <code>sbatch</code>,      even though it should work     according to the documentation.</p> <p>The reason for the failure is that Slurm when starting a batch job defines a large number of <code>SLURM_*</code> and <code>SRUN_*</code> variables. Some only give information about the allocation, but others are picked up by <code>srun</code> as options and some of those options have a higher priority than <code>--ntasks-per-node</code>. So the trick is to  unset both <code>SLURM_NTASKS</code> and <code>SLURM_NPROCS</code>. The <code>--ntasks</code> option triggered by <code>SLURM_NTASKS</code> has a higher priority than <code>--ntasks-per-node</code>.  <code>SLURM_NPROCS</code> was used in older versions of Slurm as with the same function as the current environment variable <code>SLURM_NTASKS</code> and therefore also implicitly specifies  <code>--ntasks</code> if <code>SLURM_NTASKS</code> is removed from the environment.</p> <p>The option is safe to use with <code>sbatch</code> though.</p> </li> </ol> <p>Lesson: If you want to play it safe and not bother with modifying the environment that Slurm creates, use the total number of tasks <code>--ntasks</code> if you want to specify the number of tasks with <code>srun</code>.</p> </li> <li> <p>Specifying the number of CPUs (cores on LUMI) for each task. The easiest way to do this is by     using <code>--cpus-per-task=&lt;number_CPUs&gt;</code> or <code>-c &lt;number_CPUs&gt;</code>.</p> </li> <li> <p>Specifying the number of GPUs per task. Following the Slurm manuals, the following     seems the easiest way:</p> <ol> <li> <p>Use <code>--gpus-per-task=&gt;number_GPUs&gt;</code> to bind one or more GPUs to each task.     This is probably the most used option in this scheme.</p> </li> <li> <p>If however you want multiple tasks to share a GPU, then you should use      <code>--ntasks-per-gpu=&lt;number_of_tasks&gt;</code>. There are use cases where this makes sense.</p> </li> </ol> <p>This however does not always work...</p> </li> </ol> <p>The job steps created in this simple scheme do not always run the programs at optimal efficiency. Slurm has various strategies to assign tasks to nodes, and there is an option which we will discuss in the next session of the course (binding) to change that. Moreover, not all clusters use the same default setting for this strategy. Cores and CPUs are assigned in order and this is not always the best order.</p> <p>It is particularly difficult to get a good distribution on the GPU nodes because of the single core reserved for low noise mode, which leaves the system in a very asymmetric state: There is one L3 cache domain with 7 available cores (the first one) and 7 with 8 available cores. Assume you want to use one task per GPU then there is no easy way to get each task bound to its own L3 cache domain. Any strategy trying this with 8 cores per task will fail as there are no 64 cores available, while using 7 cores per task will set the first task on the first cache domain, the second task on the second cache  domain, but the third task will already start on the last core of the second cache domain. There are solutions to this problem which we will discuss in the session on binding.</p> <p>It is also possible to specify these options already on <code>#SBATCH</code> lines. Slurm will transform those options into <code>SLURM_*</code> environment variables that will then be picked up by <code>srun</code>. However, this  behaviour has changed in more recent versions of Slurm. E.g., <code>--cpus-per-task</code> is no longer  automatically picked up by <code>srun</code> as there were side effects with some MPI implementations on some clusters. CSC has modified the configuration to again forward that option (now via an <code>SRUN_*</code>  environment variable) but certain use cases beyond the basic one described above are not covered. And take into account that not all cluster operators will do that as there are also good reasons not to do so. Otherwise the developers of Slurm wouldn't have changed that behaviour in the first place.</p> Demonstrator for the problems with <code>--tasks-per-node</code> (click to expand) <p>Try the batch script:</p> <p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-perNode-jobstart-standard-demo1\n#SBATCH --partition=standard\n#SBATCH --nodes=2\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeCray-22.12\n\necho \"Submitted from $SLURM_SUBMIT_HOST\"\necho \"Running on $SLURM_JOB_NODELIST\"\necho\necho -e \"Job script:\\n$(cat $0)\\n\"\necho \"SLURM_* and SRUN_* environment variables:\"\nenv | egrep ^SLURM\nenv | egrep ^SRUN\n\nset -x\n# This works\nsrun --ntasks=32 --cpus-per-task=8 hybrid_check -r\n\n# This does not work\nsrun --ntasks-per-node=16 --cpus-per-task=8 hybrid_check -r\n\n# But this works again\nunset SLURM_NTASKS\nunset SLURM_NPROCS\nsrun --ntasks-per-node=16 --cpus-per-task=8 hybrid_check -r\nset +x\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre></p>"},{"location":"intro-evolving/07_Slurm/#a-warning-for-gpu-applications","title":"A warning for GPU applications","text":"<p>Allocating GPUs with <code>--gpus-per-task</code> or <code>--tasks-per-gpu</code> may seem the most logical thing to do when reading the Slurm manual pages. It does come with a problem though resulting of how Slurm currently manages the AMD GPUs, and now the discussion becomes more technical.</p> <p>Slurm currently uses a separate control group per task for the GPUs. Now control groups are a mechanism in Linux for restricting resources available to a process and its childdren. Putting the GPUs in a separate control group per task limits the ways in intra-node communication can be done between GPUs, and this in turn is incompatible with some software.</p> <p>The solution is to ensure that all tasks within a node see all GPUs in the node and then to manually perform the binding of each task to the GPUs it needs using a different mechanism more like affinity masks for CPUs. It can be tricky to do though as many options for <code>srun</code> do a mapping under the hood.</p> <p>As we need a mechanisms that are not yet discussed yet in this chapter, we refer to the chapter \"Process and thread distribution and binding\" for a more ellaborate discussion and a solution.</p> <p>Unfortunately using AMD GPUs in Slurm is more complicated then it should be (and we will see even more problems).</p>"},{"location":"intro-evolving/07_Slurm/#turning-simultaneous-multithreading-on-or-off","title":"Turning simultaneous multithreading on or off","text":"<p>Hardware threads are enabled by default at the operating system level. In Slurm however, regular job steps start by default with hardware threads disabled. This is not true though for the  batch job step as the example below will show.</p> <p>Hardware threading for a regular job step can be turned on explicitly with <code>--hint=multhithread</code> and turned off explicitly with <code>--hint=nomultithread</code>,  with the latter the default on LUMI. The hint should be given as an option to <code>sbatch</code>(e.g., as a line <code>#SBATCH --hint=multithread</code>) and not as an option of <code>srun</code>. </p> <p>The way it works is a bit confusing though. We've always told, and that is also what the Slurm manual tells, that a CPU is the  smallest allocatable unit and that on LUMI, Slurm is set to use the core as the smallest allocatable unit. So you would expect that <code>srun --cpus-per-task=4</code> combined with <code>#SBATCH --hint=multithread</code> would give you 4 cores with in total 8 threads, but instead you will get 2 cores with 4 hardware threads. In other words, it looks like (at least with the settings on LUMI) <code>#SBATCH --hint=multithread</code> changes the meaning of CPU in the context of an <code>srun</code> command to a hardware thread instead of a  core. This is illustrated with the example below.</p> Use of <code>--hint=(no)multithread</code> (click to expand) <p>We consider the job script </p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-HWT-standard-multithread\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --hint=multithread\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeGNU-22.12\n\necho -e \"Job script:\\n$(cat $0)\\n\"\n\nset -x\nsrun -n 1 -c 4 omp_check -r\nset +x\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p>We consider three variants of this script:</p> <ol> <li> <p>Without the <code>#SBATCH --hint=multithread</code> line to see the default behaviour of Slurm on LUMI.     The relevant lines of the output are:</p> <pre><code>+ srun -n 1 -c 4 omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001847 mask 0-3\n\n+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4238727      slurm-HWT+   standard project_4+        256    RUNNING      0:0 \n4238727.bat+      batch            project_4+        256    RUNNING      0:0 \n4238727.0     omp_check            project_4+          8    RUNNING      0:0 \n</code></pre> <p>The <code>omp_check</code> program detects that it should run 4 threads (we didn't even need to help by setting <code>OMP_NUM_THREADS</code>) and uses cores 0 till 3 which are the first 4 physical cores on the processor.</p> <p>The output of the <code>sacct</code> command claims that the job (which is the first line of the table) got allocated 256 CPUs. This is a confusing feature of <code>sacct</code>: it shows  the number of hardware threads even though the Slurm CPU on LUMI is defined as a core. The next line shows the batch job step which actually does see all hardware threads of all cores (and in general, all hardware threads of all allocated cores of the first node of the job). The final line, with the '.0' job step, shows that that core was using 8 hardware threads, even though <code>omp_check</code> only saw 4. This is because the default  behaviour (as the next test will confirm) is <code>--hint=nomultithread</code>.</p> <p>Note that <code>sacct</code> shows the last job step as running even though it has finished. This is because <code>sacct</code> gets the information not from the compute node but from a database, and it  looks like the full information has not yet derived in the database. A short sleep before the <code>sacct</code> call would cure this problem.</p> </li> <li> <p>Now replace the <code>#SBATCH --hint=multithread</code>  with <code>#SBATCH --hint=nomultithread</code>.     The relevant lines of the output are now</p> <pre><code>+ srun -n 1 -c 4 omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001847 mask 0-3\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001847 mask 0-3\n\n+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4238730      slurm-HWT+   standard project_4+        256    RUNNING      0:0 \n4238730.bat+      batch            project_4+        256    RUNNING      0:0 \n4238730.0     omp_check            project_4+          8    RUNNING      0:0 \n</code></pre> <p>The output is no different from the previous case which confirms that this is the default behaviour.</p> </li> <li> <p>Lastly, we run the above script unmodified, i.e., with <code>#SBATCH --hint=multithread</code>      Now the relevant lines of the output are:</p> <pre><code>+ srun -n 1 -c 4 omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001847 mask 0-1, 128-129\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001847 mask 0-1, 128-129\n++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001847 mask 0-1, 128-129\n++ omp_check: OpenMP thread   3/4   on cpu 129/256 of nid001847 mask 0-1, 128-129\n\n+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4238728      slurm-HWT+   standard project_4+        256    RUNNING      0:0 \n4238728.bat+      batch            project_4+        256    RUNNING      0:0 \n4238728.0     omp_check            project_4+          4  COMPLETED      0:0 \n</code></pre> <p>The <code>omp_check</code> program again detects only 4 threads but now runs them on the first two physical cores and the corresponding second hardware thread for these cores.  The output of <code>sacct</code> now shows 4 in the \"AllocCPUS\" command for the <code>.0</code> job step, which confirms that indeed only 2 cores with both hardware threads were allocated instead of 4 cores.</p> </li> </ol> Buggy behaviour when used with <code>srun</code> <p>Consider the following job script:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-HWT-standard-bug2\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n#SBATCH --hint=multithread\n#SBATCH --account=project_46YXXXXXX\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeGNU-22.12\n\nset -x\nsrun -n 1 -c 4 --hint=nomultithread omp_check -r\n\nsrun -n 1 -c 4 --hint=multithread omp_check -r\n\nOMP_NUM_THREADS=8 srun -n 1 -c 4 --hint=multithread omp_check -r\n\nsrun -n 1 -c 4 omp_check -r\nset +x\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n\nset -x\nsrun -n 1 -c 256 --hint=multithread omp_check -r\n</code></pre> <p>The relevant lines of the output are:</p> <pre><code>+ srun -n 1 -c 4 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001246 mask 0-3\n\n+ srun -n 1 -c 4 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   1/4   on cpu 129/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   3/4   on cpu   1/256 of nid001246 mask 0-1, 128-129\n\n+ OMP_NUM_THREADS=8\n+ srun -n 1 -c 4 --hint=multithread omp_check -r\n\nRunning 8 threads in a single process\n\n++ omp_check: OpenMP thread   0/8   on cpu   0/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   1/8   on cpu 128/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   2/8   on cpu   0/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   3/8   on cpu   1/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   4/8   on cpu 129/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   5/8   on cpu 128/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   6/8   on cpu 129/256 of nid001246 mask 0-1, 128-129\n++ omp_check: OpenMP thread   7/8   on cpu   1/256 of nid001246 mask 0-1, 128-129\n\n+ srun -n 1 -c 4 omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001246 mask 0-3\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001246 mask 0-3\n\n+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4238801      slurm-HWT+   standard project_4+        256    RUNNING      0:0 \n4238801.bat+      batch            project_4+        256    RUNNING      0:0 \n4238801.0     omp_check            project_4+          8  COMPLETED      0:0 \n4238801.1     omp_check            project_4+          8  COMPLETED      0:0 \n4238801.2     omp_check            project_4+          8  COMPLETED      0:0 \n4238801.3     omp_check            project_4+          8  COMPLETED      0:0 \n\n+ srun -n 1 -c 256 --hint=multithread omp_check -r\nsrun: error: Unable to create step for job 4238919: More processors requested than permitted\n</code></pre> <p>The first <code>omp_check</code> runs as expected. The seocnd one uses only 2 cores but all 4 hyperthreads on those cores. This is also not unexpected. In the third case we force the use of 8 threads, and they all land on the 4 hardware threads of 2 cores. Again, this is not unexpected. And neither is the output of the last  run of <code>omp_cehck</code> which is again with multithreading disabled as requested in the <code>#SBATCH</code> lines. What is surprising though is the output of <code>sacct</code>:  It claims there were 8 hardware threads, so 4 cores, allocated to the second  (the <code>.1</code>) and third (the <code>.2</code>) job step while whatever we tried, <code>omp_check</code> could only see 2 cores and 4 hardware threads. Indeed, if we would try to run with <code>-c 256</code> then <code>srun</code> will fail.</p> <p>But now try the reverse: we turn multithreading on in the <code>#SBATCH</code> lines and try to turn it off again with <code>srun</code>:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-HWT-standard-bug2\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --time=2:00\n#SBATCH --output=%x-%j.txt\n#SBATCH --hint=multithread\n#SBATCH --account=project_46YXXXXXX\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeGNU-22.12\n\nset -x\nsrun -n 1 -c 4 --hint=nomultithread omp_check -r\n\nsrun -n 1 -c 4 --hint=multithread omp_check -r\n\nsrun -n 1 -c 4 omp_check -r\nset +x\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p>The relevant part of the output is now</p> <pre><code>+ srun -n 1 -c 4 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   1/256 of nid001460 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   2/256 of nid001460 mask 0-3\n++ omp_check: OpenMP thread   2/4   on cpu   3/256 of nid001460 mask 0-3\n++ omp_check: OpenMP thread   3/4   on cpu   0/256 of nid001460 mask 0-3\n\n+ srun -n 1 -c 4 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   1/4   on cpu 129/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   3/4   on cpu   1/256 of nid001460 mask 0-1, 128-129\n\n++ srun -n 1 -c 4 omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   1/4   on cpu 129/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001460 mask 0-1, 128-129\n++ omp_check: OpenMP thread   3/4   on cpu   1/256 of nid001460 mask 0-1, 128-129\n\n+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4238802      slurm-HWT+   standard project_4+        256    RUNNING      0:0 \n4238802.bat+      batch            project_4+        256    RUNNING      0:0 \n4238802.0     omp_check            project_4+          8  COMPLETED      0:0 \n4238802.1     omp_check            project_4+          4  COMPLETED      0:0 \n4238802.2     omp_check            project_4+          4  COMPLETED      0:0 \n</code></pre> <p>And this is fully as expected. The first <code>srun</code> does not use hardware threads as requested by <code>srun</code>, the second run does use hardware threads and only 2 cores which is also what we requested with the <code>srun</code> command, and the last one also uses hardware threads. The output of <code>sacct</code> (and in particular the <code>AllocCPUS</code> comumn) not fully confirms that indeed there were only 2 cores allocated to the second and third run.</p> <p>So turning hardware threads on in the <code>#SBATCH</code> lines and then off again with <code>srun</code> works as expected, but the opposite, explicitly turning it off in the <code>#SBATCH</code> lines (or relying on the default which is off) and then trying to turn it on again, does not work.</p>"},{"location":"intro-evolving/07_Slurm/#per-core-allocations","title":"Per-core allocations","text":""},{"location":"intro-evolving/07_Slurm/#when-to-use","title":"When to use?","text":"<p>Not all jobs can use entire nodes efficiently, and therefore the LUMI setup does provide some partitions that enable users to define jobs that use only a part of a node. This scheme enables the user to only request the resources that are really needed for the job (and only get billed for those at least if they are proportional to the resources that a node provides), but also comes with the disadvantage that it is not possible to control how cores and GPUs are allocated within a node. Codes that depend on proper mapping of threads and processes on L3 cache domains, NUMA domains or sockets, or on shortest paths between cores in a task and the associated GPU(s)  may see an unpredictable performance lossas (a) the mapping will rarely be optimal unless you are very lucky (and always be suboptimal for GPUs in the current LUMI setup) and (b) will also depend on other jobs already running on the set of nodes assigned to your job.</p> <p>Unfortunately, </p> <ol> <li> <p>Slurm does not seem to fully understand the GPU topology on LUMI and cannot take that properly into     account when assigning resources to a job or task in a job, and</p> </li> <li> <p>Slurm does not support the hierarchy in the compute nodes of LUMI. There is no way to specifically      request all cores in a socket, NUMA domain or L3 cache domain. It is only possible on a per-node level      which is the case that we already discussed.</p> </li> </ol> <p>Instead, you have to specify the task structure in the <code>#SBATCH</code> lines of a job script or as the command line arguments of <code>sbatch</code> and <code>salloc</code> that you will need to run the job.</p>"},{"location":"intro-evolving/07_Slurm/#resource-request","title":"Resource request","text":"<p>To request an allocation, you have to specify the task structure of the job step you want to run using mostly the same options that we have discussed on the slides \"Per-node allocations: Starting a job step\": </p> <ol> <li> <p>Now you should specify just the total amount of tasks needed using     <code>--ntasks=&lt;number&gt;</code> or <code>-n &lt;number&gt;</code>. As the number of nodes is not fixed     in this allocation type, <code>--ntasks-per-node=&lt;ntasks&gt;</code> does not make much sense.</p> <p>It is possible to request a number of nodes using <code>--nodes</code>, and it can even take two arguments: <code>--nodes=&lt;min&gt;-&lt;max&gt;</code> to specify the minimum and maximum number of nodes that Slurm should use rather than the exact number (and there are even more options),  but really the only case where it makes sense to use <code>--nodes</code> with <code>--ntasks-per-node</code> in this case, is if all tasks would fit on a single node and you also want to force them on a single node so that all MPI communications are done through shared memory rather than via the Slingshot interconnect.</p> <p>Restricting the choice of resources for the scheduler may increase your waiting time in the queue though.</p> </li> <li> <p>Specifying the number of CPUs (cores on LUMI) for each task. The easiest way to do this is by     using <code>--cpus-per-task=&lt;number&gt;</code> or <code>-c &lt;number</code>.</p> <p>Note that as has been discussed before, the standard behaviour of recent versions of Slurm is to no longer forward <code>--cpus-per-task</code> from the <code>sbatch</code> or <code>salloc</code> level to the <code>srun</code> level though CSC has made a configuration change in Slurm that will still try to do this though with some limitations.</p> </li> <li> <p>Specifying the number of GPUs per task. The easiest way here is:</p> <ol> <li> <p>Use <code>--gpus-per-task=&gt;number_GPUs&gt;</code> to bind one or more GPUs to each task.     This is probably the most used option in this scheme.</p> </li> <li> <p>If however you want multiple tasks to share a GPU, then you should use      <code>--ntasks-per-gpu=&lt;number_of_tasks&gt;</code>. There are use cases where this makes sense.     However, at the time of writing this does not work properly.</p> </li> </ol> <p>While this does ensure a proper distribution of GPUs across nodes compatible with the  distrubtions of cores to run the requested tasks, we will again run into binding issues when these options are propagated to <code>srun</code> to create the actual job steps, and hre this is even more tricky to solve.</p> <p>We will again discuss a solution in the  Chapter \"Process and thread distribution and binding\"</p> </li> <li> <p>CPU memory. By default you get less than the memory per core on the node type. To change:</p> <ol> <li> <p>Against the logic there is no <code>--mem-per-task=&lt;number&gt;</code>, instead memory needs to be specified in     function of the other allocated resources.</p> </li> <li> <p>Use <code>--mem-per-cpu=&lt;number&gt;</code> to request memory per CPU (use k, m, g to specify kilobytes, megabytes or gigabytes)</p> </li> <li> <p>Alternatively on a GPU allocation <code>--mem-per-gpu=&lt;number&gt;</code>.     This is still CPU memory and not GPU memory!</p> </li> <li> <p>Specifying memory per node with <code>--mem</code> doesn't make much sense unless the number of nodes is fixed.</p> </li> </ol> </li> </ol> <p><code>--ntasks-per-gpu=&lt;number&gt;</code> does not work</p> <p>At the time of writing there were several problems when using <code>--ntasks-per-gpu=&lt;number&gt;</code> in combination with <code>--ntasks=&lt;number&gt;</code>. While according to the Slurm documentation this is a valid request and Slurm should automatically determine the right number of GPUs to allocate, it turns out that instead you need to specify the number of GPUs with <code>--gpus=&lt;number&gt;</code> together with <code>--ntasks-per-gpu=&lt;number&gt;</code> and let Slurm compute the number of tasks.</p> <p>Moreover, we've seen cases where the final allocation was completely wrong, with tasks ending up with the wrong number of GPUs or on the wrong node (like too many tasks on one and too little on another compared to the number of GPUs set aside in each of these nodes).</p> <p></p> <p><code>--sockets-per-node</code> and <code>--ntasks-per-socket</code></p> <p>If you don't read the manual pages of Slurm carefully enough you may have the impression that you can use parameters like <code>--sockets-per-node</code> and <code>--ntasks-per-socket</code> to force all tasks on a single socket (and get a single socket), but these options will not work as you expect.</p> <p>The <code>--sockets-per-node</code> option is not used to request an exact resource, but to specify a  type of node by specifying the minimal number of sockets a node should have.It is an irrelevant option on LUMI as each partition does have only a single node type.</p> <p>If you read the manual carefully, you will also see that there is a subtle difference between <code>--ntasks-per-node</code> and <code>--ntasks-per-socket</code>: With <code>--ntasks-per-node</code> you specify the exact number of tasks for each node while with <code>--tasks-per-socket</code> you specify the  maximum number of tasks for each socket. So all hope that something like</p> <pre><code>--ntasks=8 --ntasks-per-socket=8 --cpus-per-task=8\n</code></pre> <p>would always ensure that you get a socket for yourself with each task nicely assigned to a single L3 cache domain, is futile.</p>"},{"location":"intro-evolving/07_Slurm/#different-job-steps-in-a-single-job","title":"Different job steps in a single job","text":"<p>It is possible to have an <code>srun</code> command with a different task structure in your job script. This will work if no task requires more CPUs or GPUs than in the original request, and if there are either not more tasks either or if an entire number of tasks in the new structure fits in a task in the structure from the allocation and the total number of tasks does not exceed the original number multiplied with that entire number. Other cases may work randomly, depending on how Slurm did the actual allocation. In fact, this may even be abused to ensure that all tasks are allocated to a single node, though this is done more elegantly by just specifying <code>--nodes=1</code>.</p> <p>With GPUs though it can become very complicated to avoid binding problems if the Slurm way of implementing GPU binding does not work for you.</p> Some examples that work and don't work (click to expand) <p>Consider the job script:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-small-multiple-srun\n#SBATCH --partition=small\n#SBATCH --ntasks=4\n#SBATCH --cpus-per-task=4\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n#SBATCH --output %x-%j.txt\n#SBATCH --acount=project_46YXXXXXX\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeCray-22.12\n\necho \"Running on $SLURM_JOB_NODELIST\"\n\nset -x\n\nomp_check\n\nsrun --ntasks=1 --cpus-per-task=3 omp_check\n\nsrun --ntasks=2 --cpus-per-task=4 hybrid_check\n\nsrun --ntasks=4 --cpus-per-task=1 mpi_check\n\nsrun --ntasks=16 --cpus-per-task=1 mpi_check\n\nsrun --ntasks=1 --cpus-per-task=16 omp_check\n\nset +x\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p>In the first output example (with lots of output deleted) we got the full allocation of 16 cores on a single node, and in fact, even 16 consecutive cores though spread across 3 L3 cache domains. We'll go over the output in steps:</p> <pre><code>Running on nid002154\n\n+ omp_check\n\nRunning 32 threads in a single process\n\n++ omp_check: OpenMP thread   0/32  on cpu  20/256 of nid002154\n++ omp_check: OpenMP thread   1/32  on cpu 148/256 of nid002154\n...\n</code></pre> <p>The first <code>omp_check</code> command was started without using <code>srun</code> and hence ran on all hardware cores allocated to the job. This is why hardware threading is enabled and why the executable sees 32 cores.</p> <pre><code>+ srun --ntasks=1 --cpus-per-task=3 omp_check\n\nRunning 3 threads in a single process\n\n++ omp_check: OpenMP thread   0/3   on cpu  20/256 of nid002154\n++ omp_check: OpenMP thread   1/3   on cpu  21/256 of nid002154\n++ omp_check: OpenMP thread   2/3   on cpu  22/256 of nid002154\n</code></pre> <p>Next <code>omp_check</code> was started via <code>srun --ntasks=1 --cpus-per-task=3</code>. One task instead of 4, and the task is also smaller in terms of number of nodes as the tasks requested in <code>SBATCH</code> lines, and Slurm starts the executable without problems. It runs on three cores, correctly detects that number, and also correctly does not use hardware threading.</p> <pre><code>+ srun --ntasks=2 --cpus-per-task=4 hybrid_check\n\nRunning 2 MPI ranks with 4 threads each (total number of threads: 8).\n\n++ hybrid_check: MPI rank   0/2   OpenMP thread   0/4   on cpu  23/256 of nid002154\n++ hybrid_check: MPI rank   0/2   OpenMP thread   1/4   on cpu  24/256 of nid002154\n++ hybrid_check: MPI rank   0/2   OpenMP thread   2/4   on cpu  25/256 of nid002154\n++ hybrid_check: MPI rank   0/2   OpenMP thread   3/4   on cpu  26/256 of nid002154\n++ hybrid_check: MPI rank   1/2   OpenMP thread   0/4   on cpu  27/256 of nid002154\n++ hybrid_check: MPI rank   1/2   OpenMP thread   1/4   on cpu  28/256 of nid002154\n++ hybrid_check: MPI rank   1/2   OpenMP thread   2/4   on cpu  29/256 of nid002154\n++ hybrid_check: MPI rank   1/2   OpenMP thread   3/4   on cpu  30/256 of nid002154\n</code></pre> <p>Next we tried to start 2 instead of 4 MPI processes with 4 cores each which also works without problems. The allocation now starts on core 23 but that is because Slurm was still finishing the job step on cores 20 till 22 from the previous <code>srun</code> command. This may or may not happen and is also related to a remark we made before about using <code>sacct</code> at the end of the job where the last job step may still be shown as running instead of completed.</p> <pre><code>+ srun --ntasks=4 --cpus-per-task=1 mpi_check\n\nRunning 4 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/4   on cpu  20/256 of nid002154\n++ mpi_check: MPI rank   1/4   on cpu  21/256 of nid002154\n++ mpi_check: MPI rank   2/4   on cpu  22/256 of nid002154\n++ mpi_check: MPI rank   3/4   on cpu  23/256 of nid002154\n</code></pre> <p>Now we tried to start 4 tasks with 1 core each. This time we were lucky and the system  considered the previous <code>srun</code> completely finished and gave us the first 4 cores of the  allocation.</p> <pre><code>+ srun --ntasks=16 --cpus-per-task=1 mpi_check\nsrun: Job 4268529 step creation temporarily disabled, retrying (Requested nodes are busy)\nsrun: Step created for job 4268529\n\nRunning 16 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/16  on cpu  20/256 of nid002154\n++ mpi_check: MPI rank   1/16  on cpu  21/256 of nid002154\n++ mpi_check: MPI rank   2/16  on cpu  22/256 of nid002154\n++ mpi_check: MPI rank   3/16  on cpu  23/256 of nid002154\n++ mpi_check: MPI rank   4/16  on cpu  24/256 of nid002154\n++ mpi_check: MPI rank   5/16  on cpu  25/256 of nid002154\n...\n</code></pre> <p>With the above <code>srun</code> command we try to start 16 single-threaded MPI processes. This fits  perfectly in the allocation as it simply needs to put 4 of these tasks in the space reserved  for one task in the <code>#SBATCH</code> request. The warning at the start may or may not happen. Basically Slurm was still freeing up the cores from the previous run and therefore the new <code>srun</code> dind't  have enough resources the first time it tried to, but it automatically tried a second time.</p> <pre><code>+ srun --ntasks=1 --cpus-per-task=16 omp_check\nsrun: Job step's --cpus-per-task value exceeds that of job (16 &gt; 4). Job step may never run.\nsrun: Job 4268529 step creation temporarily disabled, retrying (Requested nodes are busy)\nsrun: Step created for job 4268529\n\nRunning 16 threads in a single process\n\n++ omp_check: OpenMP thread   0/16  on cpu  20/256 of nid002154\n++ omp_check: OpenMP thread   1/16  on cpu  21/256 of nid002154\n++ omp_check: OpenMP thread   2/16  on cpu  22/256 of nid002154\n...\n</code></pre> <p>In the final <code>srun</code> command we try to run a single 16-core OpenMP run. This time Slurm produces a warning as it would be impossible to fit a 16-cpre shared memory run in the space of 4 4-core  tasks if the resources for those tasks would have been spread across multiple nodes. The next warning is again for the same reason as in the previous case, but ultimately the command does run on all 16 cores allocated and without using hardware threading.</p> <pre><code>+ set +x\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4268529      slurm-sma+      small project_4+         32    RUNNING      0:0 \n4268529.bat+      batch            project_4+         32    RUNNING      0:0 \n4268529.0     omp_check            project_4+          6  COMPLETED      0:0 \n4268529.1    hybrid_ch+            project_4+         16  COMPLETED      0:0 \n4268529.2     mpi_check            project_4+          8  COMPLETED      0:0 \n4268529.3     mpi_check            project_4+         32  COMPLETED      0:0 \n4268529.4     omp_check            project_4+         32    RUNNING      0:0 \n</code></pre> <p>The output of <code>sacct</code> confirms what we have been seeing. The first <code>omp_check</code> was run without srun and ran in the original batch step which had all hardware threads of all 16 allocated cores available. The next <code>omp_check</code> ran on 3 cores but 6 is shwon in this scheme which is normal as the \"other\" hardware thread on each core is implicitly also reserved. And the same holds for all other numbers in that column.</p> <p>At another time I was less lucky and got the tasks spread out across 4 nodes, each  running a single 4-core task. Let's go through the output again:</p> <pre><code>Running on nid[002154,002195,002206,002476]\n\n+ omp_check\n\nRunning 8 threads in a single process\n\n++ omp_check: OpenMP thread   0/8   on cpu  36/256 of nid002154\n++ omp_check: OpenMP thread   1/8   on cpu 164/256 of nid002154\n++ omp_check: OpenMP thread   2/8   on cpu  37/256 of nid002154\n++ omp_check: OpenMP thread   3/8   on cpu 165/256 of nid002154\n++ omp_check: OpenMP thread   4/8   on cpu  38/256 of nid002154\n++ omp_check: OpenMP thread   5/8   on cpu 166/256 of nid002154\n++ omp_check: OpenMP thread   6/8   on cpu  39/256 of nid002154\n++ omp_check: OpenMP thread   7/8   on cpu 167/256 of nid002154\n</code></pre> <p>The first <code>omp_check</code> now uses all hardware threads of the 4 cores allocated in the first node of the job (while using 16 cores/32 threads in the configuration where all cores were allocated on a single node).</p> <pre><code>+ srun --ntasks=1 --cpus-per-task=3 omp_check\n\nRunning 3 threads in a single process\n\n++ omp_check: OpenMP thread   0/3   on cpu  36/256 of nid002154\n++ omp_check: OpenMP thread   1/3   on cpu  37/256 of nid002154\n++ omp_check: OpenMP thread   2/3   on cpu  38/256 of nid002154\n</code></pre> <p>Running a three core OpenMP job goes without problems as it nicely fits within the space of a single task of the <code>#SBATCH</code> allocation.</p> <pre><code>+ srun --ntasks=2 --cpus-per-task=4 hybrid_check\n\nRunning 2 MPI ranks with 4 threads each (total number of threads: 8).\n\n++ hybrid_check: MPI rank   0/2   OpenMP thread   0/4   on cpu  36/256 of nid002195\n++ hybrid_check: MPI rank   0/2   OpenMP thread   1/4   on cpu  37/256 of nid002195\n++ hybrid_check: MPI rank   0/2   OpenMP thread   2/4   on cpu  38/256 of nid002195\n++ hybrid_check: MPI rank   0/2   OpenMP thread   3/4   on cpu  39/256 of nid002195\n++ hybrid_check: MPI rank   1/2   OpenMP thread   0/4   on cpu  46/256 of nid002206\n++ hybrid_check: MPI rank   1/2   OpenMP thread   1/4   on cpu  47/256 of nid002206\n++ hybrid_check: MPI rank   1/2   OpenMP thread   2/4   on cpu  48/256 of nid002206\n++ hybrid_check: MPI rank   1/2   OpenMP thread   3/4   on cpu  49/256 of nid002206\n</code></pre> <p>Running 2 4-thread MPI processes also goes without problems. In this case we got the second and third task from the original allocation, likely because Slurm was still freeing up the first node after the previous <code>srun</code> command.</p> <pre><code>+ srun --ntasks=4 --cpus-per-task=1 mpi_check\nsrun: Job 4268614 step creation temporarily disabled, retrying (Requested nodes are busy)\nsrun: Step created for job 4268614\n\nRunning 4 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/4   on cpu  36/256 of nid002154\n++ mpi_check: MPI rank   1/4   on cpu  36/256 of nid002195\n++ mpi_check: MPI rank   2/4   on cpu  46/256 of nid002206\n++ mpi_check: MPI rank   3/4   on cpu   0/256 of nid002476\n</code></pre> <p>Running 4 single threaded processes also goes without problems (but the fact that they are scheduled on 4 different nodes here is likely an artifact of the way we had to force to get more than one node as the small partition on LUMI was not very busy at that time).</p> <pre><code>+ srun --ntasks=16 --cpus-per-task=1 mpi_check\n\nRunning 16 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/16  on cpu  36/256 of nid002154\n++ mpi_check: MPI rank   1/16  on cpu  37/256 of nid002154\n++ mpi_check: MPI rank   2/16  on cpu  38/256 of nid002154\n++ mpi_check: MPI rank   3/16  on cpu  39/256 of nid002154\n++ mpi_check: MPI rank   4/16  on cpu  36/256 of nid002195\n++ mpi_check: MPI rank   5/16  on cpu  37/256 of nid002195\n++ mpi_check: MPI rank   6/16  on cpu  38/256 of nid002195\n++ mpi_check: MPI rank   7/16  on cpu  39/256 of nid002195\n++ mpi_check: MPI rank   8/16  on cpu  46/256 of nid002206\n++ mpi_check: MPI rank   9/16  on cpu  47/256 of nid002206\n++ mpi_check: MPI rank  10/16  on cpu  48/256 of nid002206\n++ mpi_check: MPI rank  11/16  on cpu  49/256 of nid002206\n++ mpi_check: MPI rank  12/16  on cpu   0/256 of nid002476\n++ mpi_check: MPI rank  13/16  on cpu   1/256 of nid002476\n++ mpi_check: MPI rank  14/16  on cpu   2/256 of nid002476\n++ mpi_check: MPI rank  15/16  on cpu   3/256 of nid002476\n</code></pre> <p>16 single threaded MPI processes also works without problems.</p> <pre><code>+ srun --ntasks=1 --cpus-per-task=16 omp_check\nsrun: Job step's --cpus-per-task value exceeds that of job (16 &gt; 4). Job step may never run.\nsrun: Warning: can't run 1 processes on 4 nodes, setting nnodes to 1\nsrun: error: Unable to create step for job 4268614: More processors requested than permitted\n...\n</code></pre> <p>However, trying to run a single 16-thread process now fails. Slurm first warns us that it might fail, then tries and lets it fail.</p>"},{"location":"intro-evolving/07_Slurm/#the-job-environment","title":"The job environment","text":"<p>On LUMI, <code>sbatch</code>, <code>salloc</code> and <code>srun</code> will all by default copy the environment in which they run to the job step they start (the batch job step for <code>sbatch</code>, an interactive job step for <code>salloc</code> and a regular job step for <code>srun</code>). For <code>salloc</code> this is normal behaviour as it also starts an interactive shell on the login nodes (and it cannot be changed with a command line parameter). For <code>srun</code>, any other behaviour would be a pain as each job step would need to set up an environment. But for <code>sbatch</code> this may be surprising to some as the environment on the login nodes may not be the best environment for the compute nodes. Indeed, we do recommend to reload, e.g., the LUMI modules to use software optimised specifically for the compute nodes or to have full support of ROCm.</p> <p>It is possible to change this behaviour or to define extra environment variables with <code>sbatch</code> and <code>srun</code> using the command line option <code>--export</code>: </p> <ul> <li> <p><code>--export=NONE</code> will start the job (step) in a clean environment. The environment will not be inherited,     but Slurm will attempt to re-create the user environment even if no login shell is called or used in     the batch script. (<code>--export=NIL</code> would give you a truly empty environment.)</p> </li> <li> <p>To define extra environment variables, use <code>--export=ALL,VAR1=VALUE1</code> which would pass all existing      environment variables and define a new one, <code>VAR1</code>, with the value <code>VALUE1</code>. It is of course also possible     to define more environment variables using a comma-separated list (without spaces).      With <code>sbatch</code>, specifying <code>--export</code> on the command line that way is a way to parameterise a batch script.     With <code>srun</code> it can be very useful with heterogeneous jobs if different parts of the job need a different      setting for an environment variable (e.g., <code>OMP_NUM_THREADS</code>).</p> <p>Note however that <code>ALL</code> in the above <code>--export</code> option is essential as otherwise only the environment  variable <code>VAR1</code> would be defined.</p> <p>It is in fact possible to pass only select environment variables by listing them without assigning a new  value and omitting the <code>ALL</code> but we see no practical use of that on LUMI as the list of environment variables that is needed to have a job script in which you can work more or less normally is rather long.</p> </li> </ul> <p></p> <p>Passing argumetns to a batch script</p> <p>With the Slurm <code>sbatch</code> command, any argument passed after the name of the job script is passed to the job script as an argument, so you can use regular bash shell argument processing to pass arguments to the bash script and do not necessarily need to use <code>--export</code>. Consider the following job script to demonstrate both options:</p> <pre><code>! /usr/bin/bash\n#SBATCH --job-name=slurm-small-parameters\n#SBATCH --partition=small\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n#SBATCH --output %x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n\necho \"Batch script parameter 0: $0\"\necho \"Batch script parameter 1: $1\"\necho \"Environment variable PAR1: $PAR1\"\n</code></pre> <p>Now start this with (assuming the job script is saved as <code>slurm-small-parameters.slurm</code>)</p> <pre><code>$ sbatch --export=ALL,PAR1=\"Hello\" slurm-small-parameters.slurm 'Waw, this works!'\n</code></pre> <p>and check the output file when the job is completed:</p> <pre><code>Batch script parameter 0: /var/spool/slurmd/job4278998/slurm_script\nBatch script parameter 1: Waw, this works!\nEnvironment variable PAR1: Hello\n</code></pre> <p>You see that you do not get the path to the job script as it was submitted (which you may expect  to be the value of <code>$0</code>). Instead the job script is buffered when you execute <code>sbatch</code> and started from a different directory. <code>$1</code> works as expected, and <code>PAR1</code> is also defined.</p> <p>In fact, passing arguments through command line arguments of the bash script is a more robust mechanism than using <code>--export</code> as can be seen from the bug discussed below...</p> <p>Fragile behaviour of <code>--export</code></p> <p>One of the problems with <code>--export</code> is that you cannot really assign any variable to a new environment variable the way you would do it on the bash command line. It is not clear what internal processing is going on, but the value is not always what you would expect.  In particular, problems can be expected when the value of the variable contains a semicolon.</p> <p>E.g., try the command from the previous example with <code>--export=ALL,PAR1='Hello, world'</code>  and it turns out that only <code>Hello</code> is passed as the value of the variable.</p> <p>Differences with some VSC systems</p> <p>The job environment in Slurm is different from that of some other resource managers, and in paritcular  Torque which was in use on VSC clusters and whose behaviour is still emulated on some.  LUMI uses the default settings of Slurm when it comes to environment management which is to start a job or job step in the environment from which the Slurm command was called.</p>"},{"location":"intro-evolving/07_Slurm/#automatic-requeueing","title":"Automatic requeueing","text":"<p>LUMI has the Slurm automatic requeueing of jobs upon node failure enabled. So jobs will be automatically resubmitted when one of the allocated nodes fails. For this an identical job ID is used and by default the prefious output will be truncated when the requeueed job starts.</p> <p>There are some options to influence this behaviour:</p> <ul> <li> <p>Automatic requeueing can be disabled at job submission with the <code>--no-requeue</code> option     of the <code>sbatch</code> command.</p> </li> <li> <p>Truncating of the output files can be avoided by specifying <code>--open-mode=append</code>.</p> </li> <li> <p>It is also possible to detect in a job script if a job has been restarted or not. For this     Slurm sets the environment variable <code>SLURM_RESTART_COUNT</code> which is 0 the first time a job      script runs and augmented by one at every restart.</p> </li> </ul>"},{"location":"intro-evolving/07_Slurm/#job-dependencies","title":"Job dependencies","text":"<p>The maximum wall time that a job can run on LUMI is fairly long for a Tier-0 system. Many other big systems in  Europe will only allow a maximum wall time of 24 hours. Despite this, this is not yet enough for some users. One way to deal with this is ensure that programs end in time and write the necessary restart information in a file, then start a new job that continues from that file. </p> <p>You don't have to wait to submit that second job. Instead, it is possible to tell Slurm that the second job should not start before the first one has ended (and ended successfully). This is done through job dependencies. It would take us too far to discuss all possible cases in this tutorial.</p> <p>One example is</p> <pre><code>$ sbatch --dependency=afterok:&lt;jobID&gt; jobdepend.slurm \n</code></pre> <p>With this statement, the job defined by the job script <code>jobdpend.slurm</code> will not start until the job with the given jobID has ended successfully (and you may have to clean up the queue if it never ends successfully). But  there are other possibilities also, e.g., start another job after a list of jobs has ended, or after a job has failed. We refer to the  sbatch manual page where you should  look for <code>--dependency</code> on the page.</p> <p>It is also possible to automate the process of submitting a chain of dependent jobs. For this the <code>sbatch</code> flag <code>--parsable</code> can be used which on LUMI will only print the job number of the job being submitted. So to  let the job defined by <code>jobdepend.slurm</code> run after the job defined by <code>jobfirst.slurm</code> while  submitting both at the same time, you can use something like</p> <pre><code>first=$(sbatch --parsable jobfirst.slurm)\nsbatch --dependency=afterok:$first jobdepend.slurm\n</code></pre>"},{"location":"intro-evolving/07_Slurm/#interactive-jobs","title":"Interactive jobs","text":"<p>Interactive jobs can have several goals, e.g.,</p> <ol> <li> <p>Simply testing a code or steps to take to get a code to run while developing a job script.     In this case you will likely want an allocation in which you can also easily run parallel MPI     jobs.</p> </li> <li> <p>Compiling a code usually works better interactively, but here you only need an allocation for     a single task supporting multiple cores if your code supports a parallel build process.     Building on the compute nodes is needed if architecture-specific optimisations are desired     while the code building process does not support cross-compiling (e.g., because the build process     adds <code>-march=native</code> or a similar compiler switch even if it is told not to do so) or ie you want     to compile software for the GPUs that during the configure or build process needs a GPU to be      present in the node to detect its features.</p> </li> <li> <p>Attaching to a running job to inspect how it is doing.</p> </li> </ol>"},{"location":"intro-evolving/07_Slurm/#interactive-jobs-with-salloc","title":"Interactive jobs with salloc","text":"<p>This is a very good way of working for the first scenario described above. </p> <p>Using <code>salloc</code> will create a pool of resources reserved for interactive execution, and will start a new shell on the node where you called <code>salloc</code>(usually a login node). As such it does not take resources away from other job steps that you will create so the shell is a good environment to test most stuff that you would execute in the  batch job step of a job script.</p> <p>To execute any code on one of the allocated compute nodes, be it a large sequential program, a shared memory program, distributed memory program or hybrid code, you can use <code>srun</code> in the same way as we have discussed for job scripts.</p> <p>It is possible to obtain an interactive shell on the first allocated compute node with</p> <pre><code>srun --pty $SHELL\n</code></pre> <p>(which is nothing more is specified would give you a single core for the shell), but keep in mind that this takes away resources from other job steps so if you try to start further job steps from that interactive shell you will note that you have fewer  resources available, and will have to force overlap (with <code>--overlap</code>), so it is not very practical to work that way.</p> <p>To terminate the allocation, simply exit the shell that was created by <code>salloc</code> with <code>exzit</code> or  the CTRL-D key combination (and the same holds for the interactive shell in the previous paragraph).</p> Example with <code>salloc</code> and a GPU code (click to expand) <pre><code>$ salloc --account=project_46YXXXXXX --partition=standard-g --nodes=2 --time=15\nsalloc: Pending job allocation 4292946\nsalloc: job 4292946 queued and waiting for resources\nsalloc: job 4292946 has been allocated resources\nsalloc: Granted job allocation 4292946\n$ module load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\n...\n\n$ srun -n 16 -c 2 --gpus-per-task 1 gpu_check\nMPI 000 - OMP 000 - HWT 001 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 001 - HWT 002 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 001 - OMP 000 - HWT 003 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6\nMPI 001 - OMP 001 - HWT 004 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6\nMPI 002 - OMP 000 - HWT 005 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 002 - OMP 001 - HWT 006 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 007 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce\nMPI 003 - OMP 001 - HWT 008 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 009 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1\nMPI 004 - OMP 001 - HWT 010 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1\nMPI 005 - OMP 000 - HWT 011 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6\nMPI 005 - OMP 001 - HWT 012 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6\nMPI 006 - OMP 000 - HWT 013 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9\nMPI 006 - OMP 001 - HWT 014 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9\nMPI 007 - OMP 000 - HWT 015 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc\nMPI 007 - OMP 001 - HWT 016 - Node nid005191 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc\nMPI 008 - OMP 000 - HWT 001 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 008 - OMP 001 - HWT 002 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 009 - OMP 000 - HWT 003 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6\nMPI 009 - OMP 001 - HWT 004 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6\nMPI 010 - OMP 000 - HWT 005 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 010 - OMP 001 - HWT 006 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 007 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce\nMPI 011 - OMP 001 - HWT 008 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 009 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1\nMPI 012 - OMP 001 - HWT 010 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1\nMPI 013 - OMP 000 - HWT 011 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6\nMPI 013 - OMP 001 - HWT 012 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6\nMPI 014 - OMP 000 - HWT 013 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9\nMPI 014 - OMP 001 - HWT 014 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9\nMPI 015 - OMP 000 - HWT 015 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc\nMPI 015 - OMP 001 - HWT 016 - Node nid005192 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc\n</code></pre>"},{"location":"intro-evolving/07_Slurm/#interactive-jobs-with-srun","title":"Interactive jobs with srun","text":"<p>Starting an interactive job with <code>srun</code> is good to get an interactive shell in which you want to do some work without starting further job steps, e.g., for compilation on the compute nodes or to run an interactive shared memory program such as R. It is not ideal if you want to spawn further job steps with <code>srun</code> within the same allocation as the interactive shell already fills a task slot, so you'd have to overlap if you want to use all resources of the job in the  next job step. </p> <p>For this kind of work you';ll rarely need a whole node so small and small-g will likely be your partitions of choice.</p> <p>To start such a job, you'd use </p> <pre><code>srun --account=project_46YXXXXXX --partition=&lt;partition&gt; --ntasks=1 --cpus-per-task=&lt;number&gt; --time=&lt;time&gt; --pty=$SHELL\n</code></pre> <p>or with the short options</p> <pre><code>srun -A project_46YXXXXXX -p &lt;partition&gt; -n 1 -c &lt;number&gt; -t &lt;time&gt; --pty $SHELL\n</code></pre> <p>For the GPU nodes you'd also add a <code>--gpus-per-task=&lt;number&gt;</code> to request a number of GPUs.</p> <p>To end the interactive job, all you need to do is to leave the shell with <code>exit</code> or the CTRL-D key combination.</p>"},{"location":"intro-evolving/07_Slurm/#inspecting-a-running-job","title":"Inspecting a running job","text":"<p>On LUMI it is not possible to use <code>ssh</code> to log on to a compute node in use by one of your jobs. Instead you need to use Slurm to attach a shell to an already running job. This can be done with <code>srun</code>, but there are two differences with the previous scenario. First, you do not need a new allocation but need to tell <code>srun</code> to use an existing allocation. As there is already an allocation, <code>srun</code> does not need your project account in this case. Second, usually the job will be using all its resources so there is no room in the allocation to create another job step with the interactive shell. This is solved by teslling <code>srun</code> that the resources should overlap with those already in use.</p> <p>To start an interactive shell on the first allocated node of a specific job/allocation, use</p> <pre><code>srun --jobid=&lt;jobID&gt; --overlap --pty $SHELL\n</code></pre> <p>and to start an interactive shell on another node of the jobm simply add a <code>-w</code> or <code>--nodelist</code> argument:</p> <pre><code>srun --jobid=&lt;jobID&gt; --nodelist=nid00XXXX --overlap --pty $SHELL\nsrun --jobid=&lt;jobID&gt; -w nid00XXXX --overlap --pty $SHELL\n</code></pre> <p>Instead of starting a shell, you could also just run a command, e.g., <code>top</code>, to inspect what the nodes are doing.</p> <p>Note that you can find out the nodes allocated to your job using <code>squeue</code> (probably the easiest as the nodes are shown by default), <code>sstat</code> or <code>salloc</code>.</p>"},{"location":"intro-evolving/07_Slurm/#job-arrays","title":"Job arrays","text":"<p>Job arrays is a mechanism to submit a large number of related jobs with the same batch script in a single <code>sbatch</code> operation.</p> <p>As an example, consider the job script <code>job_array.slurm</code></p> <pre><code>#!/bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --partition=small\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=1G\n#SBATCH --time=15:00\n\nINPUT_FILE=\"input_${SLURM_ARRAY_TASK_ID}.dat\"\nOUTPUT_FILE=\"output_${SLURM_ARRAY_TASK_ID}.dat\"\n\n./test_set -input ${INPUT_FILE} -output ${OUTPUT_FILE}\n</code></pre> <p>Note that Slurm defines the environment variable <code>SLURM_ARRAY_TASK_ID</code> which will have a unique value for each job of the job array, varying in the range given at job submission. This enables to distinguish between the different runs and can be used to generate names of input and output files.</p> <p>Submitting this job script and running it for values of <code>SLURM_ARRAY_TASK_ID</code> going from 1 to 100 could be done with </p> <pre><code>$ sbatch --array 1-100 job_array.slurm\n</code></pre> <p>Note that this will count for 100 Slurm jobs so the size of your array jobs on LUMI is limited by the rather strict limit on job size. LUMI is made as a system for big jobs, and is a system with a lot of users, and there are only that many simultaneous jobs that a scheduler can deal with. Users doing  throughput computing should do some kind of hierarchical scheduling, running a subscheduler in the  job that then further start subjobs.</p>"},{"location":"intro-evolving/07_Slurm/#heterogeneous-jobs","title":"Heterogeneous jobs","text":"<p>A heterogeneous job is one in which multiple executables run in a single <code>MPI_COMM_WORLD</code>, or a single executable runs in different combinations (e.g., some multithreaded and some single-threaded MPI ranks where the latter take a different code path from the former and do a different task). Onme example is large  simulation codes that use separate I/O servers to take care of the parallel IO ot the file system.</p> <p>There are two ways to start such a job:</p> <ol> <li> <p>Create groups in the <code>SBATCH</code> lines, separated by <code>#SBATCH hetjob</code> lines, and then recall these groups with     <code>srun</code>. This is the most powerful mechanism as in principle one could use nodes in different partitions     for different parts of the heterogeneous job.</p> </li> <li> <p>Request the total number of nodes needed with the <code>#SBATCH</code> lines and then do the rest entirely with     <code>srun</code>, when starting the heterogeneous job step. The different blocks in <code>srun</code> are separated by a colon.     In this case we can only use a single partition.</p> </li> </ol> <p>The Slurm support for heterogeneous jobs is not very good and problems to often occur, or new bugs are being introduced.</p> <ul> <li> <p>The different parts of heterogeneous jobs in the first way of specifying them, are treated as different     jobs which may give problems with the scheduling.</p> </li> <li> <p>When using the <code>srun</code> method, these are still separate job steps and it looks like a second job is created     internally to run these, and on a separate set of nodes.</p> </li> </ul> <p></p> <p></p> Let's show with an example (worked out more in the text than in the slides) <p>Consider the following case of a 2-component job:</p> <ul> <li> <p>Part 1: Application A on 1 node with 32 tasks with 4 OpenMP threads each</p> </li> <li> <p>Part 2: Application B on 2 nodes with 4 tasks per node with 32 OpenMP threads each</p> </li> </ul> <p>We will simulate this case with the <code>hybrid_check</code> program from the <code>lumi-CPEtools</code> module  that we have used in earlier examples also.</p> <p>The job script for the first method would look like:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-herterogeneous-sbatch\n#SBATCH --time=5:00\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --cpus-per-task=4\n#SBATCH hetjob\n#SBATCH --partition=standard\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=32\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeCray-22.12\n\nsrun --het-group=0 --cpus-per-task=$SLURM_CPUS_PER_TASK_HET_GROUP_0 --export=ALL,OMP_NUM_THREADS=4  hybrid_check -l app_A : \\\n     --het-group=1 --cpus-per-task=$SLURM_CPUS_PER_TASK_HET_GROUP_1 --export=ALL,OMP_NUM_THREADS=32 hybrid_check -l app_B\n\nsrun --het-group=0 --cpus-per-task=$SLURM_CPUS_PER_TASK_HET_GROUP_0 hybrid_check -l hybrid_check -l app_A : \\\n     --het-group=1 --cpus-per-task=$SLURM_CPUS_PER_TASK_HET_GROUP_1 hybrid_check -l hybrid_check -l app_B\n\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p>There is a single <code>srun</code> command. <code>--het-group=0</code> tells <code>srun</code> to pick up the settings for the first heterogeneous group (before the <code>#SBATCH hetjob</code> line), and use that to start the <code>hybrid_check</code> program with the command line arguments <code>-l app_A</code>.  Next we have the column to tell <code>srun</code> that we start with the  second group, which is done in the same way. Note that since recent versions of Slurm do no longer  propagate the value for <code>--cpus-per-task</code>, we need to specify the value here explicitly which we can do via an environment variable. This is one of the cases where the patch to work around this new behaviour on LUMI does not work.</p> <p>This job script shows also demonstrates how a different value of a variable can be passed to each component using <code>--export</code>, even though this was not needed as the second case would show.</p> <p>The output of this job script would look lik (with a lot omitted):</p> <pre><code>srun: Job step's --cpus-per-task value exceeds that of job (32 &gt; 4). Job step may never run.\n\nRunning 40 MPI ranks with between 4 and 32 threads each (total number of threads: 384).\n\n++ app_A: MPI rank   0/40  OpenMP thread   0/4   on cpu   0/256 of nid001083\n++ app_A: MPI rank   0/40  OpenMP thread   1/4   on cpu   1/256 of nid001083\n...\n++ app_A: MPI rank  31/40  OpenMP thread   2/4   on cpu 126/256 of nid001083\n++ app_A: MPI rank  31/40  OpenMP thread   3/4   on cpu 127/256 of nid001083\n++ app_B: MPI rank  32/40  OpenMP thread   0/32  on cpu   0/256 of nid001544\n++ app_B: MPI rank  32/40  OpenMP thread   1/32  on cpu   1/256 of nid001544\n...\n++ app_B: MPI rank  35/40  OpenMP thread  30/32  on cpu 126/256 of nid001544\n++ app_B: MPI rank  35/40  OpenMP thread  31/32  on cpu 127/256 of nid001544\n++ app_B: MPI rank  36/40  OpenMP thread   0/32  on cpu   0/256 of nid001545\n++ app_B: MPI rank  36/40  OpenMP thread   1/32  on cpu   1/256 of nid001545\n...\n++ app_B: MPI rank  39/40  OpenMP thread  30/32  on cpu 126/256 of nid001545\n++ app_B: MPI rank  39/40  OpenMP thread  31/32  on cpu 127/256 of nid001545\n... (second run produces identical output)\n\nsacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4285795+0    slurm-her+   standard project_4+        256    RUNNING      0:0 \n4285795+0.b+      batch            project_4+        256    RUNNING      0:0 \n4285795+0.0  hybrid_ch+            project_4+        256  COMPLETED      0:0 \n4285795+0.1  hybrid_ch+            project_4+        256  COMPLETED      0:0 \n4285795+1    slurm-her+   standard project_4+        512    RUNNING      0:0 \n4285795+1.0  hybrid_ch+            project_4+        512  COMPLETED      0:0 \n4285795+1.1  hybrid_ch+            project_4+        512  COMPLETED      0:0 \n</code></pre> <p>The warning at the start can be safely ignored. It jsut shows how heterogeneous job were an afterthought in Slurm and likely implemented in a very dirty way. We see that we get what we expected: 32 MPI ranks on the first node of the allocation, then 4 on  each of the other two nodes.</p> <p>The output of <code>sacct</code> is somewhat surprising. Slurm has essnetially started two jobs, with jobIDs that end with <code>+0</code> and <code>+1</code>, and it first shows all job steps for the first job, which is the batch job step and the first group of both <code>srun</code> commands, and then shows the second job and its job steps, again indicating that heterogeneous jobs are not really treated as a single job.</p> <p>The same example can also be done by just allocating 3 nodes and then using more arguments with <code>srun</code> to start the application:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-herterogeneous-srun\n#SBATCH --time=5:00\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard\n#SBATCH --nodes=3\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeCray-22.12\n\nsrun --ntasks=32 --cpus-per-task=4  --export=ALL,OMP_NUM_THREADS=4  hybrid_check -l app_A : \\\n     --ntasks=8  --cpus-per-task=32 --export=ALL,OMP_NUM_THREADS=32 hybrid_check -l app_B\n\nsrun --ntasks=32 --cpus-per-task=4  hybrid_check -l app_A : \\\n     --ntasks=8  --cpus-per-task=32 hybrid_check -l app_B\n\necho -e \"\\nsacct for the job:\\n$(sacct -j $SLURM_JOB_ID)\\n\"\n</code></pre> <p>The output of the two <code>srun</code> commands is essentially the same as before, but the output  of <code>sacct</code> is different:</p> <pre><code>sacct for the job:\nJobID           JobName  Partition    Account  AllocCPUS      State ExitCode \n------------ ---------- ---------- ---------- ---------- ---------- -------- \n4284021      slurm-her+   standard project_4+        768    RUNNING      0:0 \n4284021.bat+      batch            project_4+        256    RUNNING      0:0 \n4284021.0+0  hybrid_ch+            project_4+        256  COMPLETED      0:0 \n4284021.0+1  hybrid_ch+            project_4+        512  COMPLETED      0:0 \n4284021.1+0  hybrid_ch+            project_4+        256  COMPLETED      0:0 \n4284021.1+1  hybrid_ch+            project_4+        512  COMPLETED      0:0 \n</code></pre> <p>We now get a single job ID but the job step for each of the <code>srun</code> commands is split  in two separate job steps, a <code>+0</code> and a <code>+1</code>. </p> <p>Erratic behaviour of <code>--nnodes=&lt;X&gt; --ntasks-per-node=&lt;Y&gt;</code> </p> <p>One can wonder if in the second case we could still specify resources on a per-node basis in the <code>srun</code> command:</p> <pre><code>#! /usr/bin/bash\n#SBATCH --job-name=slurm-herterogeneous-srun\n#SBATCH --time=5:00\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard\n#SBATCH --nodes=3\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeCray-22.12\n\nsrun --nodes=1 --ntasks-per-node=32 --cpus-per-task=4  hybrid_check -l hybrid_check -l app_A : \\\n     --nodes=2 --ntasks-per-node=4  --cpus-per-task=32 hybrid_check -l hybrid_check -l app_B\n</code></pre> <p>It turns out that this does not work at all. Both components get the wrong number of tasks. For some reason only 3 copies were started of the first application on the first node of the allocation, the 2 32-thread processes on the second node and one 32-thread process on the third node, also with an unexpected thread distribution.</p> <p>This shows that before starting a big application it may make sense to check with the tools from the <code>lumi-CPEtools</code> module if the allocation would be what you expect as Slurm is definitely not free of problems when it comes to hetereogeneous jobs.</p>"},{"location":"intro-evolving/07_Slurm/#local-trainings-and-materials","title":"Local trainings and materials","text":"<ul> <li> <p>Docs VSC: Slurm material for the general docs is still under development and will be published     later in 2023.</p> <ul> <li> <p>Slurm in the UAntwerp-specific documentation</p> </li> <li> <p>Slurm in the VUB-specific documentation</p> </li> </ul> </li> <li> <p>Docs C\u00c9CI: Extensive documentation on the use of Slurm</p> </li> <li> <p>VSC training materials</p> <ul> <li> <p>Slurm Lunchbox training KU Leuven</p> </li> <li> <p>VSC@KULeuven HPC-intro training     covers Slurm in the \"Starting to Compute\" section.</p> </li> <li> <p>VSC@UAntwerpen covers Slurm in the \"HPC@UAntwerp introduction\" training</p> </li> </ul> </li> <li> <p>C\u00c9CI training materials: Slurm is covered in the \"Learning how to use HPC infrastructure\" training.</p> <ul> <li>2022 session: Lecture \"Preparing, submitting and managing jobs with Slurm\"</li> </ul> </li> </ul>"},{"location":"intro-evolving/08_Binding/","title":"Process and thread distribution and binding","text":""},{"location":"intro-evolving/08_Binding/#what-are-we-talking-about-in-this-session","title":"What are we talking about in this session?","text":"<p>Distribution is the process of distributing processes and threads across the available resources of the job (nodes, sockets, NUMA domains, cores, ...), and binding is the process of ensuring they stay there as naturally processes and threads are only bound to a node  (OS image) but will migrate between cores. Binding can also ensure that processes cannot use resources they shouldn't use.</p> <p>When running a distributed memory program, the process starter - <code>mpirun</code> or <code>mpiexec</code> on many clusters, or <code>srun</code> on LUMI - will distribute  the processes over the available nodes. Within a node, it is possible to pin or attach processes or even individual threads in processes to one or more cores (actually hardware threads) and other resources,  which is called process binding.</p> <p>Linux has several mechanisms for that. Slurm uses cgroups or control groups to limit the  resources that a job can use within a node and thus to isolate jobs from one another on a node so that one job cannot deplete the resources of another job, and even uses a hierarchy up to the task level to restrict some resources for a task (hardware threads and GPU access). The second mechanism is processor affinity which works at the process and thread level and can be used by the OpenMP runtime to limit thread migration. It works through affinity masks which indicate the hardware threads that a thread or process can use. There is also a third mechanism provided by the ROCm run time to control which GPUs can be used.</p> <p>Some of the tools in the <code>lumi-CPEtools</code> module can show the affinity mask for each thread (or effectively the process for single-threaded processes) so you can use these tools to study the affinity masks and check the distribution and binding of processes and threads. The <code>serial_check</code>, <code>omp_check</code>, <code>mpi_check</code> and <code>hybrid_check</code> programs can be used to study thread binding. In fact, <code>hybrid_check</code> can be used in all cases, but the other three show more compact output for serial, shared memory OpenMP and single-threaded MPI processes  resoectively. The <code>gpu_check</code> command can be used to study the steps in GPU binding.</p> Credits for these programs <p>The <code>hybrid_check</code> program and its derivatives <code>serial_check</code>, 'omp_check<code>and</code>mpi_check<code>are similar to the [</code>xthi<code>program](https://support.hpe.com/hpesc/public/docDisplay?docId=a00114008en_us&amp;docLocale=en_US&amp;page=Run_an_OpenMP_Application.html) used in the 4-day comprehensive LUMI course organised by the LUST in collaboration with  HPE Cray and AMD. Its main source of inspiration is a very similar program,</code>acheck`, written by HArvey Richardson of HPE Cray and used in an earlier course, but it is a complete rewrite of that application.</p> <p>One of the advantages of <code>hybrid_check</code> and its derivatives is that the output is  sorted internally already and hence is more readable. The tool also has various extensions, e.g., putting some load on the CPU cores so that you can in some cases demonstrate thread migration as the Linux scheduler tries to distribute the load in a good way.</p> <p>The <code>gpu_check</code> program builds upon the  <code>hello_jobstep</code> program from ORNL with several extensions implemented by the LUST.</p> <p>(ORNL is the national lab that operates Frontier, an exascale supercomputer based on the same node type as LUMI-G.)</p> <p></p> <p>In this section we will consider process and thread distribution and binding at several levels:</p> <ul> <li> <p>When creating an allocation, Slurm will already reserve resources at the node level, but this     has been discussed already in the Slurm session of the course.</p> <p>It will also already employ control groups to restrict the access to those reaources on a per-node per-job basis.</p> </li> <li> <p>When creating a job step, Slurm will distribute the tasks over the available resources,     bind them to CPUs and depending on how the job step was started, bind them to a subset of the     GPUs available to the task on the node it is running on.</p> </li> <li> <p>With Cray MPICH, you can change the binding between MPI ranks and Slurm tasks. Normally MPI rank i     would be assigned to task i in the job step, but sometimes there are reasons to change this.     The mapping options offered by Cray MPICH are more powerful than what can be obtained with the      options to change the task distribution in Slurm.</p> </li> <li> <p>The OpenMP runtime also uses library calls and environment variables to redistribute and pin threads     within the subset of hardware threads available to the process. Note that different compilers     use different OpenMP runtimes so the default behaviour will not be the same for all compilers,     and on LUMI is different for the Cray compiler compared to the GNU and AMD compilers.</p> </li> <li> <p>Finally, the ROCm runtime also can limit the use of GPUs by a process to a subset of the ones that     are available to the process through the use of the <code>ROCR_VISIBLE_DEVICES</code> environment variable.</p> </li> </ul> <p>Binding only makes sense on job-exclusive nodes as only then you have full control over all available  resources. On \"allocatable by resource\" partitions you usually do not know which resources are available. The advanced Slurm binding options that we will discuss do not work in those cases, and the options offered by the MPICH, OpenMP and ROCm runtimes may work very unpredictable. </p> <p>Warning</p> <p>Note also that some <code>srun</code> options that we have seen (sometimes already given at the <code>sbatch</code> or <code>salloc</code> level but picket up by <code>srun</code>) already do a simple binding, so those options cannot be combined with the options that we will discuss in this session. This is the case for <code>--cpus-per-task</code>, <code>--gpus-per-task</code> and <code>--ntasks-per-gpu</code>.  In fact, the latter two options will also change the numbering of the GPUs visible to the ROCm runtime, so  using <code>ROCR_VISIBLE_DEVICES</code> may also lead to surprises!</p>"},{"location":"intro-evolving/08_Binding/#why-do-i-need-this","title":"Why do I need this?","text":"<p>As we have seen in the \"LUMI Architecture\" session of this course and is discussed into even more detail in some other courses lectures in Belgium (in particular the \"Supercomputers for Starters\" course given twice a year at VSC@UAntwerpen), modern supercomputer nodes have increasingly a very hierarchical architecture.  This hierarchical architecture is extremely pronounced on the AMD EPYC architecture used in LUMI but is also increasingly showing up with Intel processors and the ARM server processors, and is also relevant but often ignored in GPU clusters.</p> <p>A proper binding of resources to the application is becoming more and more essential for good performance and  scalability on supercomputers. </p> <ul> <li> <p>Memory locality is very important, and even if an application would be written to take the NUMA character     properly into account at the thread level, a bad mapping of these threads to the cores may result into threads     having to access memory that is far away (with the worst case on a different socket) extensively.</p> <p>Memory locality at the process level is easy as usually processes share little or no memory. So if you would have an MPI application where each rank needs 14 GB of memory and so only 16 ranks can run on a regular node, then it is essential to ensure that these ranks are spread out nicely over the whole node, with one rank per CCD. The default of  Slurm when allocating 8 single-thread tasks on a node would be to put them all on the first CCD, which would give very poor performance as a lot of memory accesses would have to go across sockets.</p> </li> <li> <p>If threads in a process don't have sufficient memory locality it may be very important to run all threads      in as few L3 cache domains as possible, ideally just one, as otherwise you risk having a lot of conflicts     between the different L3 caches that require resolution and can slow down the process a lot.</p> <p>This already shows that there is no single works-for-all solution, because if those threads would use all memory on a  node and each have good memory locality then it would be better to spread them out as much possible. You really need to understand your application to do proper resource mapping, and the fact that it can be so application-dependent is  also why Slurm and the various runtimes cannot take care of it automatically.</p> </li> <li> <p>In some cases it is important on the GPU nodes to ensure that tasks are nicely spread out over CCDs with each task     using the GPU (GCD) that is closest to the CCD the task is running on. This is certainly the case if the application     would rely on cache-coherent access to GPU memory from the CPU.</p> </li> <li> <p>With careful mapping of MPI ranks on nodes you can often reduce the amount of inter-node data transfer in favour of the     faster intra-node transfers. This requires some understanding of the communication pattern of your MPI application.</p> </li> <li> <p>For GPU-aware MPI: Check if the intra-node communication pattern can map onto the links between the GCDs.</p> </li> </ul>"},{"location":"intro-evolving/08_Binding/#core-numbering","title":"Core numbering","text":"<p>Linux core numbering is not hierarchical and may look a bit strange. This is because Linux core numbering was fixed before hardware threads were added, and later on hardware threads were simply added to the numbering scheme.</p> <p>As is usual with computers, numbering starts from 0. Core 0 is the first hardware thread (or we could say the actual core) of the first CCD (CCD 0) of the first NUMA domain (NUMA domain 0) of the first socket (socket 0). Core 1 is then the second core of the same CCD, and so on, going over all cores in a CCD, then NUMA domain and then socket. So on LUMI-C, core 0 till 63 are on the first socket and core 64 till 127 on the second one. The numbering of the second hardware thread of each core - we could say the virtual core - then starts where the numbering of the actual cores ends, so 64 for LUMI-G (which has only one socket per node) or 128 for LUMI-C. This has the advantage that if hardware threading is turned off at the BIOS/UEFI level, the numbering of the actual  cores does not change. </p> <p>On LUMI G, core 0 and its second hardware thread 64 are reserved by the low noise mode and cannot be used by Slurm or applications. This is done to help reduce OS jitter which can kill scalability of large parallel applications. However, it also creates an assymetry that is hard to deal with. (For this reason they chose to disable the first core of every CCD on Frontier, so core 0, 8, 16, ... and  corresponding hardware threads 64, 72, ..., but on LUMI this is not yet the case).</p> <p>Note that even with <code>--hiint=nomulthread</code> the hardware threads will still be turned on at the hardware level and be visible in the  OS (e.c., in <code>/proc/cpuinfo</code>). In fact, the batch job step will use them, but they will not be used by applications in job steps started with subsequent <code>srun</code> commands.</p> Slurm under-the-hoods example <p>We will use the Linux <code>lstopo</code> and <code>taskset</code> commands to study how a job step sees the system and how task affinity is used to manage the CPUs for a task. Consider the job script:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=cpu-numbering-demo1\n#SBATCH --output %x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --partition=small\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=16\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeGNU-22.12\n\ncat &lt;&lt; EOF &gt; task_lstopo_$SLURM_JOB_ID\n#!/bin/bash\necho \"Task \\$SLURM_LOCALID\"                            &gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"Output of lstopo:\"                              &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nlstopo -p                                             &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"Taskset of current shell: \\$(taskset -p \\$\\$)\"  &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nEOF\n\nchmod +x ./task_lstopo_$SLURM_JOB_ID\n\necho -e \"\\nFull lstopo output in the job:\\n$(lstopo -p)\\n\\n\"\necho -e \"Taskset of the current shell: $(taskset -p $$)\\n\"\n\necho \"Running two tasks on 4 cores each, extracting parts from lstopo output in each:\"\nsrun -n 2 -c 4 ./task_lstopo_$SLURM_JOB_ID\necho\ncat output-$SLURM_JOB_ID-0\necho\ncat output-$SLURM_JOB_ID-1\n\necho -e \"\\nRunning hybrid_check in the same configuration::\"\nsrun -n 2 -c 4 hybrid_check -r\n\n/bin/rm task_lstopo_$SLURM_JOB_ID output-$SLURM_JOB_ID-0 output-$SLURM_JOB_ID-1\n</code></pre> <p>It creates a small test program that we will use to run lstopo and gather its output on two tasks with 4 cores each. All this is done in a job allocation with 16 cores on the  <code>small</code> partition.</p> <p>Let's first look at the output of the <code>lstopo</code> and <code>taskset</code> commands run in the batch job step:</p> <pre><code>Full lstopo output in the job:\nMachine (251GB total)\n  Package P#0\n    Group0\n      NUMANode P#0 (31GB)\n    Group0\n      NUMANode P#1 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 41:00.0 (Ethernet)\n            Net \"nmn0\"\n    Group0\n      NUMANode P#2 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 21:00.0 (Ethernet)\n            Net \"hsn0\"\n    Group0\n      NUMANode P#3 (31GB)\n  Package P#1\n    Group0\n      NUMANode P#4 (31GB)\n    Group0\n      NUMANode P#5 (31GB)\n    Group0\n      NUMANode P#6 (31GB)\n      L3 P#12 (32MB)\n        L2 P#100 (512KB) + L1d P#100 (32KB) + L1i P#100 (32KB) + Core P#36\n          PU P#100\n          PU P#228\n        L2 P#101 (512KB) + L1d P#101 (32KB) + L1i P#101 (32KB) + Core P#37\n          PU P#101\n          PU P#229\n        L2 P#102 (512KB) + L1d P#102 (32KB) + L1i P#102 (32KB) + Core P#38\n          PU P#102\n          PU P#230\n        L2 P#103 (512KB) + L1d P#103 (32KB) + L1i P#103 (32KB) + Core P#39\n          PU P#103\n          PU P#231\n      L3 P#13 (32MB)\n        L2 P#104 (512KB) + L1d P#104 (32KB) + L1i P#104 (32KB) + Core P#40\n          PU P#104\n          PU P#232\n        L2 P#105 (512KB) + L1d P#105 (32KB) + L1i P#105 (32KB) + Core P#41\n          PU P#105\n          PU P#233\n        L2 P#106 (512KB) + L1d P#106 (32KB) + L1i P#106 (32KB) + Core P#42\n          PU P#106\n          PU P#234\n        L2 P#107 (512KB) + L1d P#107 (32KB) + L1i P#107 (32KB) + Core P#43\n          PU P#107\n          PU P#235\n        L2 P#108 (512KB) + L1d P#108 (32KB) + L1i P#108 (32KB) + Core P#44\n          PU P#108\n          PU P#236\n        L2 P#109 (512KB) + L1d P#109 (32KB) + L1i P#109 (32KB) + Core P#45\n          PU P#109\n          PU P#237\n        L2 P#110 (512KB) + L1d P#110 (32KB) + L1i P#110 (32KB) + Core P#46\n          PU P#110\n          PU P#238\n        L2 P#111 (512KB) + L1d P#111 (32KB) + L1i P#111 (32KB) + Core P#47\n          PU P#111\n          PU P#239\n    Group0\n      NUMANode P#7 (31GB)\n      L3 P#14 (32MB)\n        L2 P#112 (512KB) + L1d P#112 (32KB) + L1i P#112 (32KB) + Core P#48\n          PU P#112\n          PU P#240\n        L2 P#113 (512KB) + L1d P#113 (32KB) + L1i P#113 (32KB) + Core P#49\n          PU P#113\n          PU P#241\n        L2 P#114 (512KB) + L1d P#114 (32KB) + L1i P#114 (32KB) + Core P#50\n          PU P#114\n          PU P#242\n        L2 P#115 (512KB) + L1d P#115 (32KB) + L1i P#115 (32KB) + Core P#51\n          PU P#115\n          PU P#243\n\nTaskset of the current shell: pid 81788's current affinity mask: ffff0000000000000000000000000000ffff0000000000000000000000000\n</code></pre> <p>Note the way the cores are represented.  There are 16 lines the lines <code>L2 ... + L1d ... + L1i ... + Core ...</code> that represent the 16 cores requested. We have used the <code>-p</code> option of <code>lstopo</code> to ensure that <code>lstopo</code> would show us the physical number as seen by the bare OS. The numbers indicated after each core are within the socket but the number indicated right after <code>L2</code> is the global core numbering within the node as seen by the bare OS. The two <code>PU</code> lines (Processing Unit) after each core are correspond to the  hardware threads and are also the numbers as seen by the bare OS.</p> <p>We see that in this allocation the cores are not spread over the minimal number of L3 cache domains that would be possible, but across three domains. In this particular allocation the cores are still consecutive cores, but even that is not guaranteed in an \"Allocatable by resources\" partition. Despite <code>--hint=nomultithread</code> being the default behaviour, at this level we still see both hardware threads for each pysical core in the taskset. </p> <p>Next look at the output printed by lines 29 and 31:</p> <pre><code>Task 0\nOutput of lstopo:\nMachine (251GB total)\n  Package P#0\n    Group0\n      NUMANode P#0 (31GB)\n    Group0\n      NUMANode P#1 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 41:00.0 (Ethernet)\n            Net \"nmn0\"\n    Group0\n      NUMANode P#2 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 21:00.0 (Ethernet)\n            Net \"hsn0\"\n    Group0\n      NUMANode P#3 (31GB)\n  Package P#1\n    Group0\n      NUMANode P#4 (31GB)\n    Group0\n      NUMANode P#5 (31GB)\n    Group0\n      NUMANode P#6 (31GB)\n      L3 P#12 (32MB)\n        L2 P#100 (512KB) + L1d P#100 (32KB) + L1i P#100 (32KB) + Core P#36\n          PU P#100\n          PU P#228\n        L2 P#101 (512KB) + L1d P#101 (32KB) + L1i P#101 (32KB) + Core P#37\n          PU P#101\n          PU P#229\n        L2 P#102 (512KB) + L1d P#102 (32KB) + L1i P#102 (32KB) + Core P#38\n          PU P#102\n          PU P#230\n        L2 P#103 (512KB) + L1d P#103 (32KB) + L1i P#103 (32KB) + Core P#39\n          PU P#103\n          PU P#231\n      L3 P#13 (32MB)\n        L2 P#104 (512KB) + L1d P#104 (32KB) + L1i P#104 (32KB) + Core P#40\n          PU P#104\n          PU P#232\n        L2 P#105 (512KB) + L1d P#105 (32KB) + L1i P#105 (32KB) + Core P#41\n          PU P#105\n          PU P#233\n        L2 P#106 (512KB) + L1d P#106 (32KB) + L1i P#106 (32KB) + Core P#42\n          PU P#106\n          PU P#234\n        L2 P#107 (512KB) + L1d P#107 (32KB) + L1i P#107 (32KB) + Core P#43\n          PU P#107\n          PU P#235\n    Group0\n      NUMANode P#7 (31GB)\nTaskset of current shell: pid 82340's current affinity mask: f0000000000000000000000000\n\nTask 1\nOutput of lstopo:\nMachine (251GB total)\n  Package P#0\n    Group0\n      NUMANode P#0 (31GB)\n    Group0\n      NUMANode P#1 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 41:00.0 (Ethernet)\n            Net \"nmn0\"\n    Group0\n      NUMANode P#2 (31GB)\n      HostBridge\n        PCIBridge\n          PCI 21:00.0 (Ethernet)\n            Net \"hsn0\"\n    Group0\n      NUMANode P#3 (31GB)\n  Package P#1\n    Group0\n      NUMANode P#4 (31GB)\n    Group0\n      NUMANode P#5 (31GB)\n    Group0\n      NUMANode P#6 (31GB)\n      L3 P#12 (32MB)\n        L2 P#100 (512KB) + L1d P#100 (32KB) + L1i P#100 (32KB) + Core P#36\n          PU P#100\n          PU P#228\n        L2 P#101 (512KB) + L1d P#101 (32KB) + L1i P#101 (32KB) + Core P#37\n          PU P#101\n          PU P#229\n        L2 P#102 (512KB) + L1d P#102 (32KB) + L1i P#102 (32KB) + Core P#38\n          PU P#102\n          PU P#230\n        L2 P#103 (512KB) + L1d P#103 (32KB) + L1i P#103 (32KB) + Core P#39\n          PU P#103\n          PU P#231\n      L3 P#13 (32MB)\n        L2 P#104 (512KB) + L1d P#104 (32KB) + L1i P#104 (32KB) + Core P#40\n          PU P#104\n          PU P#232\n        L2 P#105 (512KB) + L1d P#105 (32KB) + L1i P#105 (32KB) + Core P#41\n          PU P#105\n          PU P#233\n        L2 P#106 (512KB) + L1d P#106 (32KB) + L1i P#106 (32KB) + Core P#42\n          PU P#106\n          PU P#234\n        L2 P#107 (512KB) + L1d P#107 (32KB) + L1i P#107 (32KB) + Core P#43\n          PU P#107\n          PU P#235\n    Group0\n      NUMANode P#7 (31GB)\nTaskset of current shell: pid 82341's current affinity mask: f00000000000000000000000000\n</code></pre> <p>The output of <code>lstopo -p</code> is the same for both: we get the same 8 cores. This is because all cores for all tasks on a node are gathered in a single control group. Instead,  affinity masks are used to ensure that both tasks of 4 threads are scheduled on different cores. If we have a look at booth taskset lines:</p> <pre><code>Taskset of current shell: pid 82340's current affinity mask: 0f0000000000000000000000000\nTaskset of current shell: pid 82341's current affinity mask: f00000000000000000000000000\n</code></pre> <p>we see that they are indeed different (a zero was added to the front of the first to make the difference clearer). The first task got cores 100 till 103 and the second task got cores 104 till 107. This also shows an important property: Tasksets are defined based on the bare OS numbering of the cores, not based on a numbering relative to the control group, with cores numbered from 0 to 15 in this example. It also implies that it is not possible to set a taskset manually without knowing which physical cores can be used!</p> <p>The output of the <code>srun</code> command on line 34 confirms this:</p> <pre><code>Running 2 MPI ranks with 4 threads each (total number of threads: 8).\n\n++ hybrid_check: MPI rank   0/2   OpenMP thread   0/4   on cpu 101/256 of nid002040 mask 100-103\n++ hybrid_check: MPI rank   0/2   OpenMP thread   1/4   on cpu 102/256 of nid002040 mask 100-103\n++ hybrid_check: MPI rank   0/2   OpenMP thread   2/4   on cpu 103/256 of nid002040 mask 100-103\n++ hybrid_check: MPI rank   0/2   OpenMP thread   3/4   on cpu 100/256 of nid002040 mask 100-103\n++ hybrid_check: MPI rank   1/2   OpenMP thread   0/4   on cpu 106/256 of nid002040 mask 104-107\n++ hybrid_check: MPI rank   1/2   OpenMP thread   1/4   on cpu 107/256 of nid002040 mask 104-107\n++ hybrid_check: MPI rank   1/2   OpenMP thread   2/4   on cpu 104/256 of nid002040 mask 104-107\n++ hybrid_check: MPI rank   1/2   OpenMP thread   3/4   on cpu 105/256 of nid002040 mask 104-107\n</code></pre> <p>Note however that this output will depend on the compiler used to compile <code>hybrid_check</code>. The Cray compiler will produce different output as it has a different default strategy for OpenMP threads  and will by default pin each thread to a different hardware thread if possible.</p>"},{"location":"intro-evolving/08_Binding/#gpu-numbering","title":"GPU numbering","text":"<p>The numbering of the GPUs is a very tricky thing on LUMI.</p> <p>The only way to reliably identify the physical GPU is through the PCIe bus ID. This does not change over time  or in an allocation where access to some resources is limited through cgroups. It is the same on all nodes.</p> <p>Based on these PICe bus IDs, the OS will assign numbers to the GPU. It are those numbers that are shown in the figure in the  Architecture chapter - \"Building LUMI: What a LUMI-G node really looks like\". We will call this the bare OS numbering or * global numbering* in these notes.</p> <p>Slurm manages GPUs for jobs through the control group mechanism. Now if a job requesting 4 GPUs would get the GPUs that are numbered 4 to 7 in bare OS numbering,  it would still see them as GPUs 0 to 3, and this is the numbering that one would have to use for the <code>ROCR_VISIBLE_DEVICES</code> environment variable that is used to further limit the GPUs that the ROCm runtime will use in an application. We will call this the job-local numbering.</p> <p>Inside task of a regular job step, Slurm can further restrict the GPUs that are visibile through control groups at the task level, leading to yet another numbering that starts from 0 which we will call the  task-local numbering. </p> <p>Note also that Slurm does take care of setting the <code>ROCR_VISIBLE_DEVICES</code> environment variable. It will be set at the start of a batch job step giving access to all GPUs that are available in the allocation, and will also be set by <code>srun</code> for each task. But you don't need to know in your application which numbers these are as, e.g., the HIP runtime will number the GPUs that are available from 0 on.</p> <p></p> A more technical example demonstrating what Slurm does (click to expand) <p>We will use the Linux <code>lstopo</code>command and the <code>ROCR_VISIBLE_DEVICES</code> environment variable to study how a job step sees the system and how task affinity is used to manage the CPUs for a task.  Consider the job script:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=gpu-numbering-demo1\n#SBATCH --output %x-%j.txt\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --partition=standard-g\n#SBATCH --nodes=1\n#SBATCH --hint=nomultithread\n#SBATCH --time=15:00\n\nmodule load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\ncat &lt;&lt; EOF &gt; task_lstopo_$SLURM_JOB_ID\n#!/bin/bash\necho \"Task \\$SLURM_LOCALID\"                                                   &gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"Relevant lines of lstopo:\"                                             &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nlstopo -p | awk '/ PCI.*Display/ || /GPU/ || / Core / || /PU L/ {print \\$0}' &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"ROCR_VISIBLE_DEVICES: \\$ROCR_VISIBLE_DEVICES\"                          &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nEOF\nchmod +x ./task_lstopo_$SLURM_JOB_ID\n\necho -e \"\\nFull lstopo output in the job:\\n$(lstopo -p)\\n\\n\"\necho -e \"Extract GPU info:\\n$(lstopo -p | awk '/ PCI.*Display/ || /GPU/ {print $0}')\\n\" \necho \"ROCR_VISIBLE_DEVICES at the start of the job script: $ROCR_VISIBLE_DEVICES\"\n\necho \"Running two tasks with 4 GPUs each, extracting parts from lstopo output in each:\"\nsrun -n 2 -c 1 --gpus-per-task=4 ./task_lstopo_$SLURM_JOB_ID\necho\ncat output-$SLURM_JOB_ID-0\necho\ncat output-$SLURM_JOB_ID-1\n\necho -e \"\\nRunning gpu_check in the same configuration::\"\nsrun -n 2 -c 1 --gpus-per-task=4 gpu_check -l\n\n/bin/rm task_lstopo_$SLURM_JOB_ID output-$SLURM_JOB_ID-0 output-$SLURM_JOB_ID-1\n</code></pre> <p>It creates a small test program that is run on two tasks and records some information on the system. The output is not sent to the screen directly as it could end up mixed between the tasks which is far  from ideal. </p> <p>Let's first have a look at the first lines of the <code>lstopo -p</code> output:</p> <pre><code>Full lstopo output in the job:\nMachine (503GB total) + Package P#0\n  Group0\n    NUMANode P#0 (125GB)\n    L3 P#0 (32MB)\n      L2 P#1 (512KB) + L1d P#1 (32KB) + L1i P#1 (32KB) + Core P#1\n        PU P#1\n        PU P#65\n      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2\n        PU P#2\n        PU P#66\n      L2 P#3 (512KB) + L1d P#3 (32KB) + L1i P#3 (32KB) + Core P#3\n        PU P#3\n        PU P#67\n      L2 P#4 (512KB) + L1d P#4 (32KB) + L1i P#4 (32KB) + Core P#4\n        PU P#4\n        PU P#68\n      L2 P#5 (512KB) + L1d P#5 (32KB) + L1i P#5 (32KB) + Core P#5\n        PU P#5\n        PU P#69\n      L2 P#6 (512KB) + L1d P#6 (32KB) + L1i P#6 (32KB) + Core P#6\n        PU P#6\n        PU P#70\n      L2 P#7 (512KB) + L1d P#7 (32KB) + L1i P#7 (32KB) + Core P#7\n        PU P#7\n        PU P#71\n      HostBridge\n        PCIBridge\n          PCI d1:00.0 (Display)\n            GPU(RSMI) \"rsmi4\"\n    L3 P#1 (32MB)\n      L2 P#8 (512KB) + L1d P#8 (32KB) + L1i P#8 (32KB) + Core P#8\n        PU P#8\n        PU P#72\n      L2 P#9 (512KB) + L1d P#9 (32KB) + L1i P#9 (32KB) + Core P#9\n        PU P#9\n        PU P#73\n      L2 P#10 (512KB) + L1d P#10 (32KB) + L1i P#10 (32KB) + Core P#10\n        PU P#10\n        PU P#74\n      L2 P#11 (512KB) + L1d P#11 (32KB) + L1i P#11 (32KB) + Core P#11\n        PU P#11\n        PU P#75\n      L2 P#12 (512KB) + L1d P#12 (32KB) + L1i P#12 (32KB) + Core P#12\n        PU P#12\n        PU P#76\n      L2 P#13 (512KB) + L1d P#13 (32KB) + L1i P#13 (32KB) + Core P#13\n        PU P#13\n        PU P#77\n      L2 P#14 (512KB) + L1d P#14 (32KB) + L1i P#14 (32KB) + Core P#14\n        PU P#14\n        PU P#78\n      L2 P#15 (512KB) + L1d P#15 (32KB) + L1i P#15 (32KB) + Core P#15\n        PU P#15\n        PU P#79\n      HostBridge\n        PCIBridge\n          PCI d5:00.0 (Ethernet)\n            Net \"hsn2\"\n        PCIBridge\n          PCI d6:00.0 (Display)\n            GPU(RSMI) \"rsmi5\"\n    HostBridge\n      PCIBridge\n        PCI 91:00.0 (Ethernet)\n          Net \"nmn0\"\n...\n</code></pre> <p>We see only 7 cores in the first block (the lines <code>L2 ... + L1d ... + L1i ... + Core ...</code>) because the first physical core is reserved for the OS. </p> <p>The <code>lstopo -p</code> output also clearly suggests that each GCD has a special link to a particular CCD</p> <p>Next check the output generated by lines 22 and 23 where we select the lines that show information about the GPUs and print some more information:</p> <pre><code>Extract GPU info:\n          PCI d1:00.0 (Display)\n            GPU(RSMI) \"rsmi4\"\n          PCI d6:00.0 (Display)\n            GPU(RSMI) \"rsmi5\"\n          PCI c9:00.0 (Display)\n            GPU(RSMI) \"rsmi2\"\n          PCI ce:00.0 (Display)\n            GPU(RSMI) \"rsmi3\"\n          PCI d9:00.0 (Display)\n            GPU(RSMI) \"rsmi6\"\n          PCI de:00.0 (Display)\n            GPU(RSMI) \"rsmi7\"\n          PCI c1:00.0 (Display)\n            GPU(RSMI) \"rsmi0\"\n          PCI c6:00.0 (Display)\n            GPU(RSMI) \"rsmi1\"\n\nROCR_VISIBLE_DEVICES at the start of the job script: 0,1,2,3,4,5,6,7\n</code></pre> <p>All 8 GPUs are visible and note the numbering on each line below the line with the PCIe bus ID.  We also notice that <code>ROCR_VISIBLE_DEVICSES</code> was set by Slurm and includes all 8 GPUs.</p> <p>Next we run two tasks requesting 4 GPUs and a single core without hardware threading each.  The output of those two tasks is gathered in files that are then sent to the standard  output in lines 28 and 30:</p> <pre><code>Task 0\nRelevant lines of lstopo:\n      L2 P#1 (512KB) + L1d P#1 (32KB) + L1i P#1 (32KB) + Core P#1\n      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2\n          PCI d1:00.0 (Display)\n          PCI d6:00.0 (Display)\n          PCI c9:00.0 (Display)\n            GPU(RSMI) \"rsmi2\"\n          PCI ce:00.0 (Display)\n            GPU(RSMI) \"rsmi3\"\n          PCI d9:00.0 (Display)\n          PCI de:00.0 (Display)\n          PCI c1:00.0 (Display)\n            GPU(RSMI) \"rsmi0\"\n          PCI c6:00.0 (Display)\n            GPU(RSMI) \"rsmi1\"\nROCR_VISIBLE_DEVICES: 0,1,2,3\n\nTask 1\nRelevant lines of lstopo:\n      L2 P#1 (512KB) + L1d P#1 (32KB) + L1i P#1 (32KB) + Core P#1\n      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2\n          PCI d1:00.0 (Display)\n            GPU(RSMI) \"rsmi0\"\n          PCI d6:00.0 (Display)\n            GPU(RSMI) \"rsmi1\"\n          PCI c9:00.0 (Display)\n          PCI ce:00.0 (Display)\n          PCI d9:00.0 (Display)\n            GPU(RSMI) \"rsmi2\"\n          PCI de:00.0 (Display)\n            GPU(RSMI) \"rsmi3\"\n          PCI c1:00.0 (Display)\n          PCI c6:00.0 (Display)\nROCR_VISIBLE_DEVICES: 0,1,2,3\n</code></pre> <p>Each task sees GPUs named 'rsmi0' till 'rsmi3', but look better and you see that these are not the same. If you compare with the first output of <code>lstopo</code> which we ran in the batch job step, we notice that task 0 gets the first 4 GPUs in the node while task 1 gets the next 4, that were named <code>rsmi4</code> till <code>rsmi7</code> before.  The other 4 GPUs are invisible in each of the tasks. Note also that in both tasks  <code>ROCR_VISIBLE_DEVICES</code> has the same value <code>0,1,2,3</code> as the numbers detected by <code>lstopo</code> in that task are used. </p> <p>Finally we have the output of the <code>gpu_check</code> command run in the same configuration. The <code>-l</code> option that was used prints some extra information that makes it easier to check the mapping: For the hardware threads it shows the CCD and for each GPU it shows the GCD number based on the physical order of the GPUs and the corresponding CCD that should be used for best performance:</p> <pre><code>MPI 000 - OMP 000 - HWT 001 (CCD0) - Node nid005163 - RT_GPU_ID 0,1,2,3 - GPU_ID 0,1,2,3 - Bus_ID c1(GCD0/CCD6),c6(GCD1/CCD7),c9(GCD2/CCD2),cc(GCD3/CCD3)\nMPI 001 - OMP 000 - HWT 002 (CCD0) - Node nid005163 - RT_GPU_ID 0,1,2,3 - GPU_ID 0,1,2,3 - Bus_ID d1(GCD4/CCD0),d6(GCD5/CCD1),d9(GCD6/CCD4),dc(GCD7/CCD5)\n</code></pre> <p><code>RT_GPU_ID</code> is the numbering of devices used in the program itself, <code>GPU_ID</code> is essentially the value of <code>ROCR_VISIBLE_DEVICES</code>, the logical numbers of the GPUs in the control group and <code>Bus_ID</code> shows the relevant part of the PCIe bus ID.</p> <p>The above example is very technical and not suited for every reader. One important conclusion though that is of use when running on LUMI is that Slurm works differently with CPUs and GPUs on LUMI.  Cores and GPUs are treated differently. Cores access is controlled by control groups at the job step level on each node and at the task level by affinity masks.  The equivalent for GPUs would be to also use control groups at the job step level and then <code>ROCR_VISIBLE_DEVICES</code> to further set access to GPUs for each task, but this is not what  is currently happening in Slurm on LUMI. Instead it is using control groups at the  task level. </p> Playing with control group and <code>ROCR_VISIBLE_DEVICES</code> (click to expand) <p>Consider the following (tricky and maybe not very realistic) job script.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=gpu-numbering-demo2\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard-g\n#SBATCH --nodes=1\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\ncat &lt;&lt; EOF &gt; select_1gpu_$SLURM_JOB_ID\n#!/bin/bash\nexport ROCR_VISIBLE_DEVICES=\\$SLURM_LOCALID\nexec \\$*\nEOF\nchmod +x ./select_1gpu_$SLURM_JOB_ID\n\ncat &lt;&lt; EOF &gt; task_lstopo_$SLURM_JOB_ID\n#!/bin/bash\nsleep \\$((SLURM_LOCALID * 5))\necho \"Task \\$SLURM_LOCALID\"                                                   &gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"Relevant lines of lstopo:\"                                             &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nlstopo -p | awk '/ PCI.*Display/ || /GPU/ || / Core / || /PU L/ {print \\$0}' &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\necho \"ROCR_VISIBLE_DEVICES: \\$ROCR_VISIBLE_DEVICES\"                          &gt;&gt; output-\\$SLURM_JOB_ID-\\$SLURM_LOCALID\nEOF\nchmod +x ./task_lstopo_$SLURM_JOB_ID\n\n# Start a background task to pick GPUs with global numbers 0 and 1\nsrun -n 1 -c 1 --gpus=2 sleep 60 &amp;\nsleep 5\n\nset -x\nsrun -n 4 -c 1 --gpus=4 ./task_lstopo_$SLURM_JOB_ID\nset +x\n\ncat output-$SLURM_JOB_ID-0\n\nset -x\nsrun -n 4 -c 1 --gpus=4 ./select_1gpu_$SLURM_JOB_ID gpu_check -l\nset +x\n\nwait\n\n/bin/rm select_1gpu_$SLURM_JOB_ID task_lstopo_$SLURM_JOB_ID output-$SLURM_JOB_ID-*\n</code></pre> <p>We create two small programs that we will use in here. The first one is used to set <code>ROCR_VISIBLE_DEVICES</code> to the value of <code>SLURM_LOCALID</code> which is the local task number within a node of a Slurm task (so always numbered starting from 0 per node). We will use this to tell the <code>gpu_check</code> program that we will run which GPU should be used by which task. The second program is one we have seen before already and just shows some relevant output of <code>lstopo</code> to see which GPUs are in principle available to the task and then also prints the value of <code>ROCR_VISIBLE_DEVICES</code>. We did have to put in some task-dependent delay  as it turns out that running multiple <code>lstopo</code> commands on a node together can cause problems.</p> <p>The tricky bit is line 29. Here we start an <code>srun</code> command on the background that steals two GPUs. In this way, we ensure that the next <code>srun</code> command will not be able to get the GCDs 0 and 1 from the regular full-node numbering. The delay is again to ensure that the next <code>srun</code> works without conflicts as internally Slurm is still finishing steps from the first <code>srun</code>.</p> <p>On line 33 we run our command that extracts info from <code>lstopo</code>. As we already know from the more technical example above the output will be the same for each task so in line 36 we only look at the output of the first task:</p> <pre><code>Relevant lines of lstopo:\n      L2 P#2 (512KB) + L1d P#2 (32KB) + L1i P#2 (32KB) + Core P#2\n      L2 P#3 (512KB) + L1d P#3 (32KB) + L1i P#3 (32KB) + Core P#3\n      L2 P#4 (512KB) + L1d P#4 (32KB) + L1i P#4 (32KB) + Core P#4\n      L2 P#5 (512KB) + L1d P#5 (32KB) + L1i P#5 (32KB) + Core P#5\n          PCI d1:00.0 (Display)\n            GPU(RSMI) \"rsmi2\"\n          PCI d6:00.0 (Display)\n            GPU(RSMI) \"rsmi3\"\n          PCI c9:00.0 (Display)\n            GPU(RSMI) \"rsmi0\"\n          PCI ce:00.0 (Display)\n            GPU(RSMI) \"rsmi1\"\n          PCI d9:00.0 (Display)\n          PCI de:00.0 (Display)\n          PCI c1:00.0 (Display)\n          PCI c6:00.0 (Display)\nROCR_VISIBLE_DEVICES: 0,1,2,3\n</code></pre> <p>If you'd compare with output from a full-node <code>lstopo -p</code> shown in the previous example, you'd see that we actually got the GPUs with regular full node numbering 2 till 5, but they have been renumbered from  0 to 3. And notice that <code>ROCR_VISIBLE_DEVICES</code> now also refers to this numbering and not the  regular full node numbering when setting which GPUs can be used. </p> <p>The <code>srun</code> command on line 40 will now run <code>gpu_check</code> through the <code>seledct_1gpu_$SLURM_JOB_ID</code> wrapper that gives task 0 access to GPU 0 in the \"local\" numbering, which should be GPU2/CCD2 in the regular full node numbering, etc. Its output is</p> <pre><code>MPI 000 - OMP 000 - HWT 002 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)\nMPI 001 - OMP 000 - HWT 003 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID cc(GCD3/CCD3)\nMPI 002 - OMP 000 - HWT 004 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID d1(GCD4/CCD0)\nMPI 003 - OMP 000 - HWT 005 (CCD0) - Node nid005350 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID d6(GCD5/CCD1)\n</code></pre> <p>which confirms that out strategy worked. So in this example we have 4 tasks running in a control group that in principle gives each task access to all 4 GPUs, but with actual access further restricted to a different GPU per task via <code>ROCR_VISIBLE_DEVICES</code>.</p> <p>This again rather technical example demonstrates another difference between the way one works with  CPUs and with GPUs. Affinity masks for CPUs refer to the \"bare OS\" numbering of hardware threads, while the numbering used for <code>ROCR_VISIBLE_DEVICES</code> which determines which GPUs the ROCm runtime can use, uses the numbering within the current control group.</p> <p>Running GPUs in a different control group per task has consequences for the way inter-GPU communication within a node can be organised so the above examples are important. It is essential to run MPI applications with optimal efficiency.</p>"},{"location":"intro-evolving/08_Binding/#task-distribution-with-slurm","title":"Task distribution with Slurm","text":"<p>The Slurm <code>srun</code> command offers the <code>--distribution</code> option to influence the distribution of  tasks across nodes (level 1), sockets or NUMA domains (level 2 and sockets or NUMA) or  even across cores in the socket or NUMA domain (third level). The first level is the most useful level, the second level is sometimes used but the third level is very tricky and both the second and third level are often better replaced with other mechanisms that will also be discussed in this chapter on distribution and binding.</p> <p>The general form of the <code>--distribution</code> option is </p> <pre><code>--distribution={*|block|cyclic|arbitrary|plane=&lt;size&gt;}[:{*|block|cyclic|fcyclic}[:{*|block|cyclic|fcyclic}]][,{Pack|NoPack}]\n</code></pre> <ul> <li> <p>Level 1: Distribution across nodes. There are three useful options for LUMI:</p> <ul> <li> <p><code>block</code> which is the default: A number of consecutive tasks is allocated on the first     node, then another number of consecutive tasks on the second node, and so on till the last     node of the allocation. Not all nodes may have the same number of tasks and this is determined     by the optional  <code>pack</code> or <code>nopack</code> parameter at the end.</p> <ul> <li> <p>With <code>pack</code> the first node in the allocation is first filled up as much as possible, then the     second node, etc.</p> </li> <li> <p>With <code>nopack</code> a more balanced approach is taken filling up all nodes as equally as possible.     In fact, the number of tasks on each node will correspond to that of the <code>cyclic</code> distribution,     but the task numbers will be different.</p> </li> </ul> </li> <li> <p><code>cyclic</code> assigns the tasks in a round-robin fashion to the nodes of the allocation. The first task     is allocated to the first node, then the second one to the second node, and so on, and when all nodes     of the allocation have received one task, the next one will be allocated again on the first node. </p> </li> <li> <p><code>plane=&lt;size&gt;</code> is a combination of both of the former methods: Blocks of <code>&lt;size&gt;</code> consecutive tasks     are allocated in a cyclic way. </p> </li> </ul> </li> <li> <p>Level 2: Here we are distributing and pinning the tasks assigned to a node at level 1 across the sockets     and cores of that node.</p> <p>As this option already does a form of binding, it may conflict with other options that we will discuss later that also perform binding. In practice, this second level is less useful as often other mechanisms will be  preferred for doing a proper binding, or the default behaviour is OK for simple distribution problems.</p> <ul> <li> <p><code>block</code> will assign whole tasks to consecutive sets of cores on the node. On LIUMI-C, it will first fill up     the first socket before moving on to the second socket.</p> </li> <li> <p><code>cyclic</code> assigns the first task of a node to a set of consecutive cores on the first socket, then the second task to a set      of cores on the second socket, etc., in a round-robin way. It will do its best to not allocate tasks across sockets.</p> </li> <li> <p><code>fcyclic</code> is a very strange distribution, where tasks requesting more than 1 CPU per task will see those      spread out across sockets. </p> <p>We cannot see how this is useful on an AMD CPU except for cases where we have only one task per node which accesses a lot of memory (more than offered by a single socket) but does so in a very NUMA-aware way.</p> </li> </ul> </li> <li> <p>Level 3 is beyond the scope of an introductory course and rarely used.</p> </li> </ul> <p>The default behaviour of Slurm depends on LUMI seems to be <code>block:block,nopack</code> if <code>--distribution</code> is not specified, though it is best to always verify as it can change over time and as the manual indicates that the default differs according to the number of tasks compared to the number of nodes. The defaults are also very tricky if a binding option at level 2 (or 3) is replaced with a <code>*</code> to mark the default behaviour, e.g., <code>--distribution=\"block:*\"</code> gives the result of <code>--distribution=block:cyclic</code> while <code>--distribution=block</code> has the same effect as <code>--distribution=block:block</code>.</p> <p>This option only makes sense on job-exclusive nodes.</p>"},{"location":"intro-evolving/08_Binding/#task-to-cpu-binding-with-slurm","title":"Task-to-CPU binding with Slurm","text":"<p>The level 2 and 3 options from the previous section already do some binding. But we will now  discuss a different option that enables very precise binding of tasks to hardware threads in Slurm.</p> <p>The mechanism does conflict with some Slurm options that implicitly already do some binding, e.g.,  it will not always work together with <code>--cpus-per-task</code> and <code>--hint=[no]multithread</code> may also not  act as expected depending on how the options are used.  Level 2/3 control via <code>--distribution</code> sometimes also make no sense when this option is used (and will be ignored).</p> <p>Task-to-CPU binding is controlled through the Slurm option </p> <pre><code>--cpu-bind=[{quiet|verbose},]&lt;type&gt;\n</code></pre> <p>We'll describe a few of the possibilities for the <code>&lt;type&gt;</code> parameter but for a more concrete overview we refer to the Slurm <code>srun</code> manual page</p> <ul> <li> <p><code>--cpu-bind=threads</code> is the default behaviour on LUMI.</p> </li> <li> <p><code>--cpu-bind=map_cpu:&lt;cpu_id_for_task_0&gt;,&lt;cpu_id_for_task_1&gt;, ...</code> is used when tasks are bound to single     cores. The first number is the number of the hardware thread for the task with local task ID 0, etc.      In other words, this option at the same time also defines the slots that can be used by the      <code>--distribrution</code> option above and replaces level 2 and level 3 of that option. </p> <p>E.g.,</p> <pre><code>module load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeGNU-22.12\nsrun --ntasks=8 --cpu-bind=map_cpu:49,57,17,25,1,9,33,41 mpi_check -r\n</code></pre> <p>will run the first task on hardware threads 49, the second task on 57, third on 17, fourth on  25, fifth on 1, sixth on 9, seventh on 33 and eight on 41.</p> <p>This may look like a very strange numbering, but we will see an application for it further in this chapter.</p> </li> <li> <p><code>--cpu-bind=mask_cpu:&lt;mask_for_task_0&gt;,&lt;mask_for_task_1&gt;,...</code> is similar to <code>map_cpu</code>, but now multiple     hardware threads can be specified per task through a mask. The mask is a hexadecimal number and leading      zeros can be omitted. The least significant bit in the mask corresponds to HWT 0, etc. </p> <p>Masks can become very long, but we shall see that this option is very useful on the nodes of the  <code>standard-g</code> partition. Just as with <code>map_cpu</code>, this option replaces level 2 and 3 of the <code>--distribution</code> option. </p> <p>E.g.,</p> <pre><code>module load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeGNU-22.12\nsrun --ntasks=8 --cpu-bind=mask_cpu:7e000000000000,7e00000000000000,7e0000,7e000000,7e,7e00,7e00000000,7e0000000000 hybrid_check -r\n</code></pre> <p>will run the first task on hardware threads 49-54, the second task on 57-62, third on 17-22, fourth on  25-30, fifth on 1-6, sixth on 9-14, seventh on 33-38 and eight on 41-46.</p> </li> </ul> <p>The <code>--cpu-bind=map_cpu</code> and <code>--cpu-bind=mask_gpu</code> options also do not go together with <code>-c</code> / <code>--cpus-per-task</code>. Both commands define a binding (the latter in combination with the default <code>--gpu-bind=threads</code>)  and these will usually conflict.</p> <p>There are more options, but these are currently most relevant ones on LUMI. That may change in the future as LUMI User support is investigating whether it isn't better to change the concept of \"socket\" in Slurm given how important it sometimes is to carefully map onto L3 cache domains for performance.</p>"},{"location":"intro-evolving/08_Binding/#task-to-gpu-binding-with-slurm","title":"Task-to-GPU binding with Slurm","text":"<p>Doing the task-to-GPU binding fully via Slurm is currently not recommended on LUMI.  The problem is that Slurm uses control groups at the task level rather than just <code>ROCR_VISIBLE_DEVICES</code> with the latter being more or less the equivalent of affinity masks. As sue to the control groups the other GPUs in a job step on a node become completely invisible to a task, communication between tasks through shared memory becomes impossible.</p> <p>We present the options for completeness, and as it may still help users if the control group setup is not a problem for the application.</p> <p>Task-to-GPU binding is done with</p> <pre><code>--gpu-bind=[verbose,]&lt;type&gt;\n</code></pre> <p>(see the Slurm manual) which is somewhat similar to <code>--cpu-binding</code> (to the extent that that makes sense).</p> <p>Some options for the <code>&lt;type&gt;</code> parameter that are worth considering:</p> <ul> <li> <p><code>--gpu-bind=closest</code>: This currently does not work well on LUMI. The problem is being investigated     so the situation may have changed by the time you read this.</p> </li> <li> <p><code>--gpu-bind=none</code>: Turns off the GPU binding of Slurm. This can actually be useful on shared node     jobs where doing a proper allocation of GPUS is difficult. You can then first use Slurm options such      as <code>--gpus-per-task</code> to get a working allocation of GPUs and CPUs, then un-bind and rebind using a      different mechanism that we will discuss later.</p> </li> <li> <p><code>--gpu-bind=map_gpu:&lt;list&gt;</code> is the equivalent of <code>--cpu-bind=map_cpu:&lt;list&gt;</code>.     This option only makes sense on a job-exclusive node and is for jobs that need a single      GPU per task. It defines the list of GPUs that should be used, with the task with local ID 0     using the first one in the list, etc.     The numbering and topology was already discussed in the \"LUMI ARchitecture\" chapter, section     \"Building LUMI: What a LUMI-G really looks like.</p> </li> <li> <p><code>--gpu-bind=mask_gpu:&gt;list&gt;</code> is the equivalent of <code>--cpu-bind=mask_cpu:&lt;list&gt;</code>.      Now the bits in the mask correspond to individual GPUs, with GPU 0 the least significant bit.      This option again only makes sense on a job-exclusive node.</p> </li> </ul> <p>Though <code>map_gpu</code> and <code>mask_gpu</code> could be very useful to get a proper mapping taking the topology of the  node into account, due to the current limitation of creating a control group per task it can not often be used as it breaks some efficient communication mechanisms between tasks, including the GPU Peer2Peer  IPC used by Cray MPICH for intro-node MPI transfers if GPU aware MPI support is enabled.</p> What do the HPE Cray manuals say about this? (Click to expand) <p>From the HPE Cray CoE:  \"Slurm may choose to use cgroups to implement the required affinity settings. Typically, the use of cgroups has the downside of preventing the use of  GPU Peer2Peer IPC mechanisms. By default Cray MPI uses IPC for implementing intra-node, inter-process MPI data movement operations that involve GPU-attached user buffers.  When Slurm\u2019s cgroups settings are in effect, users are advised to set <code>MPICH_SMP_SINGLE_COPY_MODE=NONE</code> or <code>MPICH_GPU_IPC_ENABLED=0</code> to disable the use of IPC-based implementations.  Disabling IPC also has a noticeable impact on intra-node MPI performance when  GPU-attached memory regions are involved.\"</p> <p>This is exactly what Slurm does on LUMI.</p>"},{"location":"intro-evolving/08_Binding/#mpi-rank-redistribution-with-cray-mpich","title":"MPI rank redistribution with Cray MPICH","text":"<p>By default MPI rank i will use Slurm task i in a parallel job step.  With Cray MPICH this can be changed via the environment variable  <code>MPICH_RANK_REORDER_METHOD</code>. It provides an even more powerful way of reordering MPI ranks than the Slurm <code>--distribution</code> option as one can define fully custom orderings.</p> <p>Rank reordering is an advanced topic that is discussed in more detail in the 4-day LUMI comprehensive courses organised by the LUMI User Support Team. The material of the latest one can be found via the course archive web page and is discussed in the  \"MPI Topics on the HPE Cray EX Supercomputer\" which is often given on day 3.</p> <p>Rank reordering can be used to reduce the number of inter-node messages or to spread those ranks that do parallel I/O over more nodes to increase the I/O bandwidth that can be obtained in the application.</p> <p>Rank reordering makes most sense if the block distribution method is used in Slurm as  otherwise it become very difficult to understand on which node which task will land.</p> <p>Possible values for <code>MPICH_RANK_REORDER_METHOD</code> are:</p> <ul> <li> <p><code>export MPICH_RANK_REORDER_METHOD=0</code>: Round-robin placement of the MPI ranks.     This is the equivalent of the cyclic ordering in Slurm.</p> </li> <li> <p><code>export MPICH_RANK_REORDER_METHOD=1</code>: This is the default and it preserves the     ordering of Slurm.</p> </li> <li> <p><code>export MPICH_RANK_REORDER_METHOD=2</code>: Folded rank placement. This is somewhat similar      to round-robin, but when the last node is reached, the node list is transferred in the      opposite direction.</p> </li> <li> <p><code>export MPICH_RANK_REORDER_METHOD=3</code>: Use a custom ordering, given by the      <code>MPICH_RANK_ORDER</code> environment variable which gives a comma-separated list of the MPI ranks     in the order they should be assigned to slots on the nodes.</p> </li> </ul> <p>Rank reordering does not always work well if Slurm is not using the block ordering.  As the <code>lumi-CPEtools</code> <code>mpi_check</code>, <code>hybrid_check</code> and <code>gpu_check</code> commands use Cray MPICH they can be used to test the Cray MPICH rank reordering also. The MPI ranks that are  displayed are the MPI ranks as seen through MPI calls and not the value of <code>SLURM_PROCID</code> which is the Slurm task number.</p> <p>The HPE Cray Programming Environment actually has profiling tools that help you determine the optimal rank ordering for a particular run, which is useful if you do a lot of runs with the same problem size (and hence same number of nodes and tasks).</p> Try the following job script (click to expand) <pre><code>#!/bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --job-name=renumber-demo\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard\n#SBATCH --nodes=2\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeGNU-22.12\n\nset -x\necho -e \"\\nSMP distribution on top of block.\"\nexport MPICH_RANK_REORDER_METHOD=1\nsrun -n 8 -c 32 -m block mpi_check -r\necho -e \"\\nSMP distribution on top of cyclic.\"\nexport MPICH_RANK_REORDER_METHOD=1\nsrun -n 8 -c 32 -m cyclic mpi_check -r\necho -e \"\\nRound-robin distribution on top of block.\"\nexport MPICH_RANK_REORDER_METHOD=0\nsrun -n 8 -c 32 -m block mpi_check -r\necho -e \"\\nFolded distribution on top of block.\"\nexport MPICH_RANK_REORDER_METHOD=2\nsrun -n 8 -c 32 -m block mpi_check -r\necho -e \"\\nCustom distribution on top of block.\"\nexport MPICH_RANK_REORDER_METHOD=3\ncat &gt;MPICH_RANK_ORDER &lt;&lt;EOF\n0,1,4,5,2,3,6,7\nEOF\nsrun -n 8 -c 32 -m block mpi_check -r\n/bin/rm MPICH_RANK_ORDER\nset +x\n</code></pre> <p>Ths script starts 8 tasks that each take a quarter node. </p> <ol> <li> <p>The first <code>srun</code> command is just the block distribution. The first 4 MPI ranks are     on the first node, the next 4 on the second node.</p> <pre><code>+ export MPICH_RANK_REORDER_METHOD=1\n+ MPICH_RANK_REORDER_METHOD=1\n+ srun -n 8 -c 32 -m block mpi_check -r\n\nRunning 8 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/8   on cpu  17/256 of nid001804 mask 0-31\n++ mpi_check: MPI rank   1/8   on cpu  32/256 of nid001804 mask 32-63\n++ mpi_check: MPI rank   2/8   on cpu  65/256 of nid001804 mask 64-95\n++ mpi_check: MPI rank   3/8   on cpu 111/256 of nid001804 mask 96-127\n++ mpi_check: MPI rank   4/8   on cpu   0/256 of nid001805 mask 0-31\n++ mpi_check: MPI rank   5/8   on cpu  32/256 of nid001805 mask 32-63\n++ mpi_check: MPI rank   6/8   on cpu  64/256 of nid001805 mask 64-95\n++ mpi_check: MPI rank   7/8   on cpu 120/256 of nid001805 mask 96-127\n</code></pre> </li> <li> <p>The second <code>srun</code> command uses Cray MPICH rank reordering to get a round-robin ordering     rather than using the Slurm <code>--distribution=cyclic</code> option. MPI rank 0 now lands on the first     32 cores of node 0 of the allocation, MPI rank 1 on the first 32 cores of node 1 of the allocation,     then task 2 on the second 32 cores of node 0, and so on:</p> <pre><code>+ export MPICH_RANK_REORDER_METHOD=1\n+ MPICH_RANK_REORDER_METHOD=1\n+ srun -n 8 -c 32 -m cyclic mpi_check -r\n\nRunning 8 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31\n++ mpi_check: MPI rank   1/8   on cpu   1/256 of nid001805 mask 0-31\n++ mpi_check: MPI rank   2/8   on cpu  32/256 of nid001804 mask 32-63\n++ mpi_check: MPI rank   3/8   on cpu  33/256 of nid001805 mask 32-63\n++ mpi_check: MPI rank   4/8   on cpu  79/256 of nid001804 mask 64-95\n++ mpi_check: MPI rank   5/8   on cpu  64/256 of nid001805 mask 64-95\n++ mpi_check: MPI rank   6/8   on cpu 112/256 of nid001804 mask 96-127\n++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001805 mask 96-127\n</code></pre> </li> <li> <p>Now we use the Slurm cyclic ordering and the default of 1 for the Cray MPICH rank     reordering (which is no reordering) and the result is the same as the preovious case: </p> <pre><code>+ export MPICH_RANK_REORDER_METHOD=0\n+ MPICH_RANK_REORDER_METHOD=0\n+ srun -n 8 -c 32 -m block mpi_check -r\n\nRunning 8 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31\n++ mpi_check: MPI rank   1/8   on cpu   1/256 of nid001805 mask 0-31\n++ mpi_check: MPI rank   2/8   on cpu  32/256 of nid001804 mask 32-63\n++ mpi_check: MPI rank   3/8   on cpu  47/256 of nid001805 mask 32-63\n++ mpi_check: MPI rank   4/8   on cpu  64/256 of nid001804 mask 64-95\n++ mpi_check: MPI rank   5/8   on cpu  64/256 of nid001805 mask 64-95\n++ mpi_check: MPI rank   6/8   on cpu 112/256 of nid001804 mask 96-127\n++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001805 mask 96-127\n</code></pre> </li> <li> <p>The fourth <code>srun</code> command demonstrates the folded ordering: Rank 0 runs on the first 32      cores of node 0 of the allocation, rank 1 on the first 32 of node 1, then rank 2 runs on      the second set of 32 cores again on node 1, with rank 3 then running on the second 32 cores     of node 0, rank 4 on the third group of 32 cores of node 0, rank 5 on the third group of     32 cores on rank 1, and so on. So the nodes are filled in the order 0, 1, 1, 0, 0, 1, 1, 0.</p> <pre><code>+ export MPICH_RANK_REORDER_METHOD=2\n+ MPICH_RANK_REORDER_METHOD=2\n+ srun -n 8 -c 32 -m block mpi_check -r\n\nRunning 8 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31\n++ mpi_check: MPI rank   1/8   on cpu  17/256 of nid001805 mask 0-31\n++ mpi_check: MPI rank   2/8   on cpu  32/256 of nid001805 mask 32-63\n++ mpi_check: MPI rank   3/8   on cpu  32/256 of nid001804 mask 32-63\n++ mpi_check: MPI rank   4/8   on cpu  64/256 of nid001804 mask 64-95\n++ mpi_check: MPI rank   5/8   on cpu  64/256 of nid001805 mask 64-95\n++ mpi_check: MPI rank   6/8   on cpu 112/256 of nid001805 mask 96-127\n++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001804 mask 96-127\n</code></pre> </li> <li> <p>The fifth example demonstrate a custon reordering. Here we fance a 4x2-grid which we want     to split in 2 2x2 groups. In this example rank 0, 1, 4 and 5 will run on node 0 with rank 2, 3, 6 and 7     running on node 1.</p> <pre><code>+ export MPICH_RANK_REORDER_METHOD=3\n+ MPICH_RANK_REORDER_METHOD=3\n+ cat\n+ srun -n 8 -c 32 -m block mpi_check -r\n\nRunning 8 single-threaded MPI ranks.\n\n++ mpi_check: MPI rank   0/8   on cpu   0/256 of nid001804 mask 0-31\n++ mpi_check: MPI rank   1/8   on cpu  32/256 of nid001804 mask 32-63\n++ mpi_check: MPI rank   2/8   on cpu   1/256 of nid001805 mask 0-31\n++ mpi_check: MPI rank   3/8   on cpu  32/256 of nid001805 mask 32-63\n++ mpi_check: MPI rank   4/8   on cpu  64/256 of nid001804 mask 64-95\n++ mpi_check: MPI rank   5/8   on cpu 112/256 of nid001804 mask 96-127\n++ mpi_check: MPI rank   6/8   on cpu  64/256 of nid001805 mask 64-95\n++ mpi_check: MPI rank   7/8   on cpu 112/256 of nid001805 mask 96-127\n</code></pre> </li> </ol>"},{"location":"intro-evolving/08_Binding/#refining-core-binding-in-openmp-applications","title":"Refining core binding in OpenMP applications","text":"<p>In a Slurm batch job step, threads of a shared memory process will be contained to all  hardware threads of all available cores on the first node of your allocation. To contain a shared memory program to the hardware threads asked for in the allocation (i.e., to ensure that <code>--hint=[no]multithread</code> has effect) you'd have to start the shared memory program with <code>srun</code> in a regular job step.</p> <p>Any multithreaded executable run as a shared memory job or ranks in a hybrid MPI/multithread job, will - when started properly via <code>srun</code> - get access to a group of cores via an affinity mask. In some cases you will want to manually refine the way individual threads of each process are mapped onto the available hardware threads.</p> <p>In OpenMP, this is usually done through environment variables (it can also be done partially in the program through library calls). A number of environment variables is standardised in the  OpenMP standard, but some implementations offer some addtional non-standard ones. Below we discuss the more important of the standard ones:</p> <ul> <li> <p><code>OMP_NUM_THREADS</code> is used to set the number of CPU threads OpenMP will use. In its most basic     form this is a single number (but you can give multiple comma-separated numbers for nested     parallelism). </p> <p>OpenMP programs on LUMI will usually correctly detect how many hardware threads are available to the task and use one OpenMP thread per hardware thread. There are cases where you may want to ask for a certain number of hardware threads when allocating resources, e.g., to easily get a good mapping of tasks on cores, but do not want to use them all, e.g., because your application is too memory bandwidth or cache constrained and using fewer threads actually gives better overall performance on a per-node basis.</p> </li> <li> <p><code>OMP_PLACES</code> is used to restrict each OpenMP thread to a group of hardware threads. Possible values     include: </p> <ul> <li><code>OMP_PLACES=threads</code> to restrict OpenMP threads to a single hardware thread</li> <li><code>OMP_PLACES=cores</code> to restrict each OpenMP threads to a single core (but all hardware threads associated with that core)</li> <li><code>OMP_PLACES=sockets</code> to restrict each OpenMP thread to the hardware threads of a single socket</li> <li> <p>And it is possible to give a list with explicit values, e.g.,</p> <pre><code>export OMP_PLACES=\"{0:4}:4:4\"\n</code></pre> <p>which is also equivalent to</p> <pre><code>export OMP_PLACES=\"{0,1,2,3}.{4,5,6,7},{8.9.10.122}. {12.13.14.15}\"\n</code></pre> <p>so each OpenMP thread is restricted to a different group of 4 hardware threads. The numbers in the list are not the physical Linux hardware thread numbers, but are relative to the hardware threads available in the  affinity mask of the task. </p> <p>Note that this is different from the core numbers that would be used in <code>--cpu-bind=map_cpu</code> or <code>-=-gpi-bind=mask_cpu</code> which sets the CPUs or groups of CPUs available to each thread and which always use the physical numbering and not a numbering that is local to the job allocation.</p> </li> </ul> </li> <li> <p><code>OMP_PROC_BIND</code>: Sets how threads are distributed over the places. Possible values are:</p> <ul> <li> <p><code>OMP_PROC_BIND=false</code>: Turn off OpenMP thread binding. Each thread will get access to all hardware threads     available in to the task (and defined by a Linux affinity mask in Slurm).</p> </li> <li> <p><code>OMP_PROC_BIND=close</code>: If more places are available than there are OpenMP threads, then try     to put the OpenMP threads as close as possible to the master thread. In general, bind as close as possible     to the master thread while still distributing for load balancing.</p> </li> <li> <p><code>OMP_PROC_BIND=spread</code>: Spread threads out as evenly as possible over the places available     to the task.</p> </li> <li> <p><code>OMP_PROC_BIND=master</code>: Bind threads to the same place as the master thread. The place is determined by the     <code>OMP_PLACES</code> environment variable and it is clear this makes no sense if that place is just a single hardware     thread or single core as all threads would then be competing for the resources of a single core.</p> </li> </ul> <p>Multiple values of <code>close</code>, <code>spread</code> and <code>master</code> in a comma-separated list are possible but this is outside of the scope of this tutorial. </p> <p>The Cray Compilation Environment also has an additional non-standard option <code>auto</code> which is actually the default and tries to do a reasonable job for most cases. On the other compilers on LUMI, the default behaviour is <code>false</code> unless the next environment variable, <code>OMP_PLACES</code>, is specified.</p> </li> <li> <p><code>OMP_DISPLAY_AFFINITY</code>: When set tot <code>TRUE</code> information about the affinity binding of each thread will be       shown which is good for debugging purposes.</p> </li> </ul> <p>For single-level OpenMP parallelism, the <code>omp_check</code> and <code>hybrid_check</code> programs from the <code>lumi-CPEtools</code> modules can also be used to check the OpenMP thread binding.</p> Some examples (click to expand) <p>Consider the following job script:</p> <p> <pre><code>#!/bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --job-name=omp-demo\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --hint=multithread\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/C lumi-CPEtools/1.1-cpeCray-22.12\n\nset -x\nexport OMP_NUM_THREADS=4\nexport OMP_PROC_BIND=false\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nsrun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nexport OMP_NUM_THREADS=4\nunset OMP_PROC_BIND\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nsrun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nexport OMP_NUM_THREADS=4\nexport OMP_PROC_BIND=close\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nsrun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nexport OMP_NUM_THREADS=4\nexport OMP_PROC_BIND=spread\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nsrun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nexport OMP_NUM_THREADS=4\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=threads\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nexport OMP_PLACES=cores\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\n\nexport OMP_NUM_THREADS=4\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=\"{0:8}:4:8\"\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\n\nexport OMP_PLACES=\"{0:4,16:4}:4:4\"\nsrun -n 1 -c 32 --hint=multithread   omp_check -r\nset +x\n</code></pre></p> <p>Let's check the output step by step:</p> <p>In the first block we run 2 <code>srun</code> commands that actually both use 16 cores, but first with hardware threading enabled in Slurm and then with multithread mode off in Slurm:</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ export OMP_PROC_BIND=false\n+ OMP_PROC_BIND=false\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-15, 128-143\n++ omp_check: OpenMP thread   1/4   on cpu 137/256 of nid001077 mask 0-15, 128-143\n++ omp_check: OpenMP thread   2/4   on cpu 129/256 of nid001077 mask 0-15, 128-143\n++ omp_check: OpenMP thread   3/4   on cpu 143/256 of nid001077 mask 0-15, 128-143\n\n+ srun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-15\n++ omp_check: OpenMP thread   1/4   on cpu  15/256 of nid001077 mask 0-15\n++ omp_check: OpenMP thread   2/4   on cpu   1/256 of nid001077 mask 0-15\n++ omp_check: OpenMP thread   3/4   on cpu  14/256 of nid001077 mask 0-15\n</code></pre> <p><code>OMP_PROC_BIND</code> was explicitly set to false to disable the Cray Compilation Environment default behaviour. The masks reported by <code>omp_Check</code> cover all hardware threads available to the task in Slurm: Both hardware threads for the 16 first cores in the multithread case and just the primary hardware thread on the first 16 cores in the second case. So each OpenMP thread can in principle migrate over all available hardware threads.</p> <p>In the second block we unset the <code>PROC_BIND</code> environment variable to demonstrate the behaviour of the Cray Compilation Environment. The output would be different had we used the cpeGNU or cpeAOCC version.</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ unset OMP_PROC_BIND\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   1/256 of nid001077 mask 0-3, 128-131\n++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4-7, 132-135\n++ omp_check: OpenMP thread   2/4   on cpu   8/256 of nid001077 mask 8-11, 136-139\n++ omp_check: OpenMP thread   3/4   on cpu 142/256 of nid001077 mask 12-15, 140-143\n\n+ srun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-3\n++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4-7\n++ omp_check: OpenMP thread   2/4   on cpu   9/256 of nid001077 mask 8-11\n++ omp_check: OpenMP thread   3/4   on cpu  15/256 of nid001077 mask 12-15\n</code></pre> <p>The default behaviour of the CCE is very nice: Threads are nicely spread out over the available cores and then all get access to their own group of hardware threads that in this case with 4 threads for 16 cores spans 4 cores for each thread. In fact, also in other cases the default behaviour of CCE will be a binding  that works well for many cases. </p> <p>In the next experiment we demonstrate the <code>close</code> binding:</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ export OMP_PROC_BIND=close\n+ OMP_PROC_BIND=close\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0\n++ omp_check: OpenMP thread   1/4   on cpu 128/256 of nid001077 mask 128\n++ omp_check: OpenMP thread   2/4   on cpu   1/256 of nid001077 mask 1\n++ omp_check: OpenMP thread   3/4   on cpu 129/256 of nid001077 mask 129\n\n+ srun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001077 mask 1\n++ omp_check: OpenMP thread   2/4   on cpu   2/256 of nid001077 mask 2\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001077 mask 3\n</code></pre> <p>In the first case, with Slurm multithreading mode on, we see that the 4 threads are now concentrated on only 2 cores but each gets pinned to its own hardware thread. In general  this behaviour is not what one wants if more cores are available as on each core two threads will now be competing for available resources. In the second case, with Slurm multithreading  disabled, the threads are bound to the first 4 cores, with one core for each thread.</p> <p>Next we demonstrate the <code>spread</code> binding:</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ export OMP_PROC_BIND=spread\n+ OMP_PROC_BIND=spread\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0\n++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4\n++ omp_check: OpenMP thread   2/4   on cpu   8/256 of nid001077 mask 8\n++ omp_check: OpenMP thread   3/4   on cpu  12/256 of nid001077 mask 12\n\n+ srun -n 1 -c 16 --hint=nomultithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0\n++ omp_check: OpenMP thread   1/4   on cpu   4/256 of nid001077 mask 4\n++ omp_check: OpenMP thread   2/4   on cpu   8/256 of nid001077 mask 8\n++ omp_check: OpenMP thread   3/4   on cpu  12/256 of nid001077 mask 12\n</code></pre> <p>The result is now the same in both cases as we have fewer threads than physical cores. Each OpenMP thread is bound to a single core, but these cores are spread out over the first 16 cores of the node. </p> <p>Next we return to the <code>close</code> binding but try both <code>threads</code> and <code>cores</code> as places with Slurm multithreading turned on for both cases:</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ export OMP_PROC_BIND=close\n+ OMP_PROC_BIND=close\n+ export OMP_PLACES=threads\n+ OMP_PLACES=threads\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0\n++ omp_check: OpenMP thread   1/4   on cpu 128/256 of nid001077 mask 128\n++ omp_check: OpenMP thread   2/4   on cpu   1/256 of nid001077 mask 1\n++ omp_check: OpenMP thread   3/4   on cpu 129/256 of nid001077 mask 129\n\n+ export OMP_PLACES=cores\n+ OMP_PLACES=cores\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0, 128\n++ omp_check: OpenMP thread   1/4   on cpu   1/256 of nid001077 mask 1, 129\n++ omp_check: OpenMP thread   2/4   on cpu 130/256 of nid001077 mask 2, 130\n++ omp_check: OpenMP thread   3/4   on cpu   3/256 of nid001077 mask 3, 131\n</code></pre> <p>With <code>threads</code> as places we get again the distribution with two OpenMP threads on each physical core, each with their own hardware thread. With cores as places, we get only one thread per physical core, but each thread has access to both hardware threads of that physical core.</p> <p>And lastly we play a bit with custom placements:</p> <pre><code>+ export OMP_NUM_THREADS=4\n+ OMP_NUM_THREADS=4\n+ export OMP_PROC_BIND=close\n+ OMP_PROC_BIND=close\n+ export 'OMP_PLACES={0:8}:4:8'\n+ OMP_PLACES='{0:8}:4:8'\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-7\n++ omp_check: OpenMP thread   1/4   on cpu   8/256 of nid001077 mask 8-15\n++ omp_check: OpenMP thread   2/4   on cpu 128/256 of nid001077 mask 128-135\n++ omp_check: OpenMP thread   3/4   on cpu 136/256 of nid001077 mask 136-143\n</code></pre> <p><code>OMP_PLACES='{0:8}:4:8</code> means: Take starting from core with logical number 0 8 cores and  repeat this 4 times, shifting by 8 each time, so effectively</p> <pre><code>OMP_PLACES=\"{ 0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15},{16,17,18,19,20,21,22,23},{24,25,26,27,27,28,30,31}\"\n</code></pre> <p><code>omp_check</code> however shows the OS numbering for the hardware threads so we can see what this places variable means: the first thread can get scheduled on the first hardware thread of the first 8 cores, the second thread on the first hardware thread of the next 8 cores, the third OpenMP thread on the second thread of the first 8 cores, and the  fourth OpenMP thread on the second hardware thread of the next 8 cores. In other words, the logical numbering of the  threads follows the same ordering as at the OS level: First the first hardware thread of each core, then the second  hardware thread.</p> <p>When trying another variant with</p> <pre><code>OMP_PACES={0:4,16:4}:4:4\n</code></pre> <p>which is equivalent to</p> <pre><code>OMP_PLACES={0,1,2,3,16,17,18,19},{4,5,6,7,20,21,22,23},{8,9,10,11,24,25,26,27},{12,13,14,15,28,29,30,31}\"\n</code></pre> <p>we get a much nicer distribution:</p> <pre><code>+ export 'OMP_PLACES={0:4,16:4}:4:4'\n+ OMP_PLACES='{0:4,16:4}:4:4'\n+ srun -n 1 -c 32 --hint=multithread omp_check -r\n\nRunning 4 threads in a single process\n\n++ omp_check: OpenMP thread   0/4   on cpu   0/256 of nid001077 mask 0-3, 128-131\n++ omp_check: OpenMP thread   1/4   on cpu 132/256 of nid001077 mask 4-7, 132-135\n++ omp_check: OpenMP thread   2/4   on cpu 136/256 of nid001077 mask 8-11, 136-139\n++ omp_check: OpenMP thread   3/4   on cpu 140/256 of nid001077 mask 12-15, 140-143\n</code></pre> <p>We only discussed a subset of the environment variables defined in the OpenMP standard. Several implementations also offer additional environment variables, e.g.,  a number of <code>GOMP_*</code> environment variables in the GNU Compiler Collection implementation or <code>KMP_*</code> variables in the Intel compiler (not available on LUMI).</p> <p>Some further documentation:</p> <ul> <li> <p>The <code>OMP_*</code> environment variables and a number of environment variables specific for the runtime libraries     of the Cray Compiling Environment are discussed in the      <code>intro_openmp</code> manual page, section \"Environment variables\".</p> </li> <li> <p>A list of OMP_ environment variables in the OpenMP 5.1 standard      (as the current list in the HTML version of the 5.2 standard has some prorblems).</p> </li> </ul>"},{"location":"intro-evolving/08_Binding/#gpu-binding-with-rocr_visible_devices","title":"GPU binding with ROCR_VISIBLE_DEVICES","text":"<p>The <code>ROCR_VISIBLE_DEVICES</code> environment variable restricts access to GPUs at the ROCm platform runtime  level. Contrary to control groups however this mechanism is compatible with the Peer2PEer IPC used by GPU-aware Cray MPI for intra-node communication.</p> <p>The value of the <code>ROCR_VISIBLE_DEVICES</code> environment variable is a list of device indices that will be exposed to the applications. The device indices do depend on the control group. Visible devices in a control group are always numbered from 0.</p> Alternative values for <code>ROCR_VISIBLE_DEVICES</code> <p>Instead of device indices, <code>ROCR_VISIBLE_DEVICES</code> also accepts GPU UUIDs that are unique to each GPU. This is less practical then it seems as the UUIDs of GPUs are different on each node so one would need to discover them first before they can be used.</p>"},{"location":"intro-evolving/08_Binding/#combining-slurm-task-binding-with-rocr_visible_devices","title":"Combining Slurm task binding with ROCR_VISIBLE_DEVICES","text":"<p>In the chapter on the architecture of LUMI we discussed  what a LUMI-G really looks like.</p> <p>The full topology of a LUMI-G compute node is shown in the figure:</p> <p>Note that the numbering of GCDs does not correspond to the numbering of CCDs/cores. However, for optimal memory transfers (and certainly if cache-coherent memory access from CPU to GPU would be used) it is  better to ensure that each GCD collaborates with the matched CCD in an MPI rank. So we have the mapping:</p> CCD HWTs Available HWTs GCD 0 0-7, 64-71 1-7, 65-71 4 1 8-15, 72-79 9-15, 73-79 5 2 16-23, 80-87 17-23, 81-87 2 3 24-32, 88-95 25-32, 89-95 3 4 32-39, 96-103 33-39, 97-103 6 5 40-47, 104-111 41-47, 105-111 7 6 48-55, 112-119 49-55, 113-119 0 7 56-63, 120-127 57-63, 121-127 1 <p>or the reverse mapping</p> GCD CCD HWTs Available HWTs 0 6 48-55, 112-119 49-55, 113-119 1 7 56-63, 120-127 57-63, 121-127 2 2 16-23, 80-87 17-23, 81-87 3 3 24-32, 88-95 25-32, 89-95 4 0 0-7, 64-71 1-7, 65-71 5 1 8-15, 72-79 9-15, 73-79 6 4 32-39, 96-103 33-39, 97-103 7 5 40-47, 104-111 41-47, 105-111 <p></p> <p>Moreover, if you look more carefully at the topology, you can see that the connections between the  GCDs contain a number of rings:</p> <ol> <li> <p>Green ring: 0 - 1 - 3 - 2 - 4 - 5 - 7 - 6 - 0</p> </li> <li> <p>Red ring:   0 - 1 - 5 - 4 - 6 - 7 - 3 - 2 - 0</p> </li> <li> <p>Sharing some connections with the previous ones: 0 - 1 - 5 - 4 - 2 - 3 - 7 - 6 - 0</p> </li> </ol> <p>So if your application would use a ring mapping for communication and use communication from GPU buffers  for that, than it may be advantageous to map the MPI ranks on one of those rings which would mean that neither the order of the CCDs nor the order of the GCDs is trivial.</p> <p>Some other topologies can also be mapped on these connections (but unfortunately not a 3D cube).</p> <p></p> <p>To implement a proper CCD-to-GCD mapping we will use two mechanisms:</p> <ul> <li> <p>On the CPU side we'll use Slurm <code>--cpu-bind</code>. Should LUMI be reconfigured with symmetric CCDs then     this would not be needed in all cases anymore.</p> </li> <li> <p>On the GPU side we will manually assign GPUs via a different value of <code>ROCR_VISIBLE_DEVICES</code> for each     thread. To accomplish this we will have to write a wrapper script which we will generate in the job script.</p> </li> </ul> <p>Let us start with the simplest case:</p>"},{"location":"intro-evolving/08_Binding/#linear-assignment-of-gcd-then-match-the-cores","title":"Linear assignment of GCD, then match the cores","text":"<p>One possible job script to accomplish this is:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=map-linear-GCD\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard-g\n#SBATCH --gpus-per-node=8\n#SBATCH --nodes=1\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nexport ROCR_VISIBLE_DEVICES=\\$SLURM_LOCALID\nexec \\$*\nEOF\nchmod +x select_gpu_$SLURM_JOB_ID\n\nCPU_BIND1=\"map_cpu:49,57,17,25,1,9,33,41\"\n\nCPU_BIND2=\"mask_cpu:0xfe000000000000,0xfe00000000000000\"\nCPU_BIND2=\"$CPU_BIND2,0xfe0000,0xfe000000\"\nCPU_BIND2=\"$CPU_BIND2,0xfe,0xfe00\"\nCPU_BIND2=\"$CPU_BIND2,0xfe00000000,0xfe0000000000\"\n\nexport MPICH_GPU_SUPPORT_ENABLED=0\n\necho -e \"\\nPure MPI:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID mpi_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\necho -e \"\\nHybrid:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID hybrid_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID\n</code></pre> <p>To select the GPUs we either use a map with numbers of cores (ideal for pure MPI programs) or masks (the only option for hybrid programs). The mask that we give in the example uses 7 cores per CCD and always skips the first core, so it is symmetric and works on the current configuration of LUMI or a configuration like Frontier where the first core of each chiplet is reserved and not available to Slurm jobs. To select the right GPU for <code>ROCR_VISIBLE_DEVICES</code>  we can use the Slurm local task ID which is  also what the MPI rank will be.  We use a so-called \"bash here document\" to generate the script. Note that in the bash here document we needed to protect the <code>$</code> with a backslash (so use <code>\\$</code>) as otherwise the variables would already be expanded when generating the script file.</p> <p>Instead of the somewhat complicated <code>--ntasks</code> with <code>srun</code> we could have specified <code>--ntasks-per-node=8</code> on a <code>#SBATCH</code> line which would have fixed the structure for all <code>srun</code> commands. Even though we want to use all GPUs in the node, <code>--gpus-per-node</code> or an equivalent option has to be specified either as an <code>#SBATCH</code> line or with each <code>srun</code> command or no GPUs will be made available to the tasks  started by the <code>srun</code> command.</p> <p>Note the output of the second <code>srun</code> command:</p> <pre><code>MPI 000 - OMP 000 - HWT 049 (CCD6) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 001 - OMP 000 - HWT 057 (CCD7) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 002 - OMP 000 - HWT 017 (CCD2) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 003 - OMP 000 - HWT 025 (CCD3) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\nMPI 004 - OMP 000 - HWT 001 (CCD0) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)\nMPI 005 - OMP 000 - HWT 009 (CCD1) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)\nMPI 006 - OMP 000 - HWT 033 (CCD4) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)\nMPI 007 - OMP 000 - HWT 041 (CCD5) - Node nid006872 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)\n</code></pre> <p>With the <code>-l</code> option we also print some information about the CCD that a core belongs to and the  GCD and corresponding optimal CCD for each PCIe bus ID, which makes it very easy to check if the mapping is as intended. Note that the GCDs are indeed in the linear order starting with GCD0.</p>"},{"location":"intro-evolving/08_Binding/#linear-assignment-of-the-ccds-then-match-the-gcd","title":"Linear assignment of the CCDs, then match the GCD","text":"<p>To modify the order of the GPUs, we now use an array with the desired order in the <code>gpu_Select</code> script. Due to the asymmetric structure structure of the chiplets on the LUMI-G nodes, we still need to define task slots for each task. Should the settings of the schedulers be modified to have one reserved core per task, then we could simply request 7 cores per task and in case of a hybrid program needing less then 7 cores, restrict the number with <code>OMP_NUM_THREADS</code>. The job script now becomes:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=map-linear-CCD\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard-g\n#SBATCH --gpus-per-node=8\n#SBATCH --nodes=1\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nGPU_ORDER=(4 5 2 3 6 7 0 1)\nexport ROCR_VISIBLE_DEVICES=\\${GPU_ORDER[\\$SLURM_LOCALID]}\nexec \\$*\nEOF\nchmod +x select_gpu_$SLURM_JOB_ID\n\nCPU_BIND1=\"map_cpu:1,9,17,25,33,41,49,57\"\n\nCPU_BIND2=\"mask_cpu\"\nCPU_BIND2=\"$CPU_BIND2:0x00000000000000fe,0x000000000000fe00\"\nCPU_BIND2=\"$CPU_BIND2,0x0000000000fe0000,0x00000000fe000000\"\nCPU_BIND2=\"$CPU_BIND2,0x000000fe00000000,0x0000fe0000000000\"\nCPU_BIND2=\"$CPU_BIND2,0x00fe000000000000,0xfe00000000000000\"\n\nexport MPICH_GPU_SUPPORT_ENABLED=0\n\necho -e \"\\nPure MPI:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID mpi_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\necho -e \"\\nHybrid:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID hybrid_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID\n</code></pre> <p>The leading zeros in the masks in the <code>CPU_BIND2</code> environment variable are not needed but we added them as it makes it easier to see which chiplet is used in what position.</p>"},{"location":"intro-evolving/08_Binding/#the-green-ring","title":"The green ring","text":"<p>As a final example for whole node allocations, lets bind tasks such that the MPI ranks are mapped upon the green ring which is GCD 0 - 1 - 3 - 2 - 4 - 5 - 7 - 6 - 0. In other words, we want to create the mapping</p> Task GCD CCD cores 0 0 6 48-55, 112-119 1 1 7 56-63, 120-127 2 3 3 24-32, 88-95 3 2 2 16-23, 80-87 4 4 0 0-7, 64-71 5 5 1 8-15, 72-79 6 7 5 40-47, 104-111 7 6 4 32-39, 96-103 <p>This mapping would be useful when using GPU-to-GPU communication in a scenario where task i only communicates with tasks i-1 and i+1 (module 8), so the communication pattern is a ring.</p> <p>Now we need to reorder both the cores and the GCDs, so we basically combine the approach taken in the two scripts above:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=map-ring-green\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard-g\n#SBATCH --gpus-per-node=8\n#SBATCH --nodes=1\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\n# Mapping:\n# | Task | GCD | CCD | cores          |\n# |-----:|----:|----:|:---------------|\n# |    0 |   0 |   6 | 48-55, 112-119 |\n# |    1 |   1 |   7 | 56-63, 120-127 |\n# |    2 |   3 |   3 | 24-32, 88-95   |\n# |    3 |   2 |   2 | 16-23, 80-87   |\n# |    4 |   4 |   0 | 0-7, 64-71     |\n# |    5 |   5 |   1 | 8-15, 72-79    |\n# |    6 |   7 |   5 | 40-47, 104-111 |\n# |    7 |   6 |   4 | 32-39, 96-103  | \n\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nGPU_ORDER=(0 1 3 2 4 5 7 6)\nexport ROCR_VISIBLE_DEVICES=\\${GPU_ORDER[\\$SLURM_LOCALID]}\nexec \\$*\nEOF\nchmod +x select_gpu_$SLURM_JOB_ID\n\nCPU_BIND1=\"map_cpu:49,57,25,17,1,9,41,33\"\n\nCCD_MASK=( 0x00000000000000fe \\\n           0x000000000000fe00 \\\n           0x0000000000fe0000 \\\n           0x00000000fe000000 \\\n           0x000000fe00000000 \\\n           0x0000fe0000000000 \\\n           0x00fe000000000000 \\\n           0xfe00000000000000 )\nCPU_BIND2=\"mask_cpu\"\nCPU_BIND2=\"$CPU_BIND2:${CCD_MASK[6]},${CCD_MASK[7]}\"\nCPU_BIND2=\"$CPU_BIND2,${CCD_MASK[3]},${CCD_MASK[2]}\"\nCPU_BIND2=\"$CPU_BIND2,${CCD_MASK[0]},${CCD_MASK[1]}\"\nCPU_BIND2=\"$CPU_BIND2,${CCD_MASK[5]},${CCD_MASK[4]}\"\n\nexport MPICH_GPU_SUPPORT_ENABLED=0\n\necho -e \"\\nPure MPI:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID mpi_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND1 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\necho -e \"\\nHybrid:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID hybrid_check -r\nsrun --ntasks=$((SLURM_NNODES*8)) --cpu-bind=$CPU_BIND2 ./select_gpu_$SLURM_JOB_ID gpu_check -l\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID\n</code></pre> <p>The values for <code>GPU_ORDER</code> are easily read from the second column of the table with the mapping that we prepared. The cores to use for the pure MPI run are also easily read from the table: sinply take the first core of each line and add 1 to skip the reserved core on CCD0 and possible extra reserved cores on the other CCDs in a future update of LUMI. Finally, to build the mask, we used some bash trickery. We first define the bash array <code>CCD_MASK</code> with the mask for each chiplet. As this has a regular structure, this is easy to build. Then we compose the mask list for the CPUs by indexing in that array, where the indices are easily read from the third column in the mapping.</p> <p>The alternative code to build <code>CPU_BIND2</code> is</p> <pre><code>CPU_BIND2=\"mask_cpu\"\nCPU_BIND2=\"$CPU_BIND2:0x00fe000000000000,0xfe00000000000000\"\nCPU_BIND2=\"$CPU_BIND2,0x00000000fe000000,0x0000000000fe0000\"\nCPU_BIND2=\"$CPU_BIND2,0x00000000000000fe,0x000000000000fe00\"\nCPU_BIND2=\"$CPU_BIND2,0x0000fe0000000000,0x000000fe00000000\"\n</code></pre> <p>which may be shorter, but requires some puzzling to build and hence is more prone to error.</p> <p>The output of the second <code>srun</code> command is now</p> <pre><code>MPI 000 - OMP 000 - HWT 049 (CCD6) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 001 - OMP 000 - HWT 057 (CCD7) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 002 - OMP 000 - HWT 025 (CCD3) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\nMPI 003 - OMP 000 - HWT 017 (CCD2) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 004 - OMP 000 - HWT 001 (CCD0) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)\nMPI 005 - OMP 000 - HWT 009 (CCD1) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)\nMPI 006 - OMP 000 - HWT 041 (CCD5) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)\nMPI 007 - OMP 000 - HWT 033 (CCD4) - Node nid005083 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)\n</code></pre> <p>Checking the last column, we see that the GCDs are indeed in the desired order for the red ring, and  is is also easy to check that each task is also mapped on the optimal CCD for the GCD.</p> Jobscript with some more advanced bash <p> <pre><code>#!/bin/bash\n#SBATCH --job-name=map-advanced-multiple\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=standard-g\n#SBATCH --gpus-per-node=8\n#SBATCH --nodes=1\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\n#\n# Define the order of the GPUs and the core mask for CCD0\n# It is important that the order of the GPUs is a string with the numbers separated by spaces.\n#\nGCD_ORDER=\"0 1 5 4 6 7 3 2\"\ncoremask='2#00000010'  # Can use the binary representation, hexadecimal with 0x, or decimal\n\n#\n# run_gpu script, takes the string with GCDs as the first argument.\n#\ncat &lt;&lt; EOF &gt; run_gpu_$SLURM_JOB_ID\n#!/bin/bash\nGCD_ORDER=( \\$1 )\nshift\nexport ROCR_VISIBLE_DEVICES=\\${GCD_ORDER[\\$SLURM_LOCALID]}\nexec \"\\$@\"\nEOF\nchmod +x run_gpu_$SLURM_JOB_ID\n\n#\n# Build the CPU binding\n# Argument one is mask, all other arguments are treated as an array of GCD numbers.\n#\n\nfunction generate_mask {\n\n    # First argument is the mask for CCD0\n    mask=$1\n\n    # Other arguments are either a string already with the GCDs, or just one GCD per argument.\n    shift\n    GCDs=( \"$@\" )\n    # Fully expand (doesn't matter as the loop can deal with it, but good if we want to check the number)\n    GCDs=( ${GCDs[@]} )\n\n    # For each GCD, the corresponding CCD number in the optimal mapping.\n    MAP_to_CCD=( 6 7 2 3 0 1 4 5 )\n\n    CPU_BIND=\"\"\n\n    # Loop over the GCDs in the order of the list to compute the corresponding\n    # CPU mask.\n    for GCD in ${GCDs[@]}\n    do\n        # Get the matching CCD for this GCD\n        CCD=${MAP_to_CCD[$GCD]}\n\n        # Shift the mask for CCD0 to the position for CCD $CCD\n        printf -v tmpvar \"0x%016x\" $((mask &lt;&lt; $((CCD*8))))\n\n        # Add to CPU_BIND. We'll remove the leading , this creates later.\n        CPU_BIND=\"$CPU_BIND,$tmpvar\"\n    done\n\n    # Strip the leading ,\n    CPU_BIND=\"${CPU_BIND#,}\"\n\n    # Return the result by printing to stdout\n    printf \"$CPU_BIND\"\n\n}\n\n#\n# Running the check programs\n#\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\n# Some mappings:\nlinear_CCD=\"4 5 2 3 6 7 0 1\"\nlinear_GCD=\"0 1 2 3 4 5 6 7\" \nring_green=\"0 1 3 2 4 5 7 6\"\nring_red=\"0 1 5 4 6 7 3 2\"\n\necho -e \"\\nTest runs:\\n\"\n\necho -e \"\\nConsecutive CCDs:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) \\\n     --cpu-bind=mask_cpu:$(generate_mask $coremask $linear_CCD) \\\n     ./run_gpu_$SLURM_JOB_ID \"$linear_CCD\" gpu_check -l\n\necho -e \"\\nConsecutive GCDs:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) \\\n     --cpu-bind=mask_cpu:$(generate_mask $coremask $linear_GCD) \\\n     ./run_gpu_$SLURM_JOB_ID \"$linear_GCD\" gpu_check -l\n\necho -e \"\\nGreen ring:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) \\\n     --cpu-bind=mask_cpu:$(generate_mask $coremask $ring_green) \\\n     ./run_gpu_$SLURM_JOB_ID \"$ring_green\" gpu_check -l\n\necho -e \"\\nRed ring:\\n\"\nsrun --ntasks=$((SLURM_NNODES*8)) \\\n     --cpu-bind=mask_cpu:$(generate_mask $coremask $ring_red) \\\n     ./run_gpu_$SLURM_JOB_ID \"$ring_red\" gpu_check -l\n\necho -e \"\\nFirst two CPU NUMA domains (assuming one node in the allocation):\"\nhalf=\"4 5 2 3\"\nsrun --ntasks=4 \\\n     --cpu-bind=mask_cpu:$(generate_mask $coremask $half) \\\n     ./run_gpu_$SLURM_JOB_ID \"$half\" gpu_check -l\n\n/bin/rm -f run_gpu_$SLURM_JOB_ID\n</code></pre></p> <p>In this script, we have modified the and renamed the usual <code>select_gpu</code> script (renamed to <code>run_cpu</code>) to take as the first argument a string with a space-separated list of the GCDs to use. This has been combined with the bash function <code>generate_mask</code> (which could have been transformed in a script as well) that computes the CPU mask starting from the mask for CCD0 and shifting that mask as needed. The input is the mask to use and then the GCDs to use, either as a single string or as a series of arguments (e.g., resulting from an array expansion).</p> <p>Both commands are then combined in the <code>srun</code> command. The <code>generate_mask</code> function is used to generate the mask for <code>--gpu-bind</code> while the <code>run_gpu</code> script is used to set <code>ROCR_VISIBLE_DEVICES</code> for each task. The examples also show how easy it is to experiment with different mappings. The one limitation of the  script and function is that there can be only 1 GPU per task and one task per GPU, and the CPU mask is also limited to a single CCD (which makes sense with the GPU restriction). Generating masks that also include the second hardware thread is not supported yet. (We use bash arithmetic internally which is limited to 64-bit integers).</p>"},{"location":"intro-evolving/08_Binding/#what-about-allocate-by-resources-partitions","title":"What about \"allocate by resources\" partitions?","text":"<p>On partitions that are \"allocatable by resource\", e.g., <code>small-g</code>, you are never guaranteed that tasks  will be spread in a reasonable way over the CCDs and that the matching GPUs will be available to your job. Creating an optimal mapping or taking the topology into account is hence impossible. </p> <p>What is possible though is work around the fact that with the usual options for such resource allocations, Slurm will lock up the GPUs for individual tasks in control groups so that the Peer2Peer IPC intra-node communication mechanism has to be turned off. We can do this for job steps that follow the pattern of  resources allocated via the <code>sbatch</code> arguments (usually <code>#SBATCH</code> lines), and rely on three elements for that:</p> <ol> <li> <p>We can turn off the Slurm GPU binding mechanism with <code>--gpu-bind=none</code>.</p> </li> <li> <p>Even then, the GPUs will still be locked up in a control group on each node for the job and hence on each node     be numbered starting from zero.</p> </li> <li> <p>And each task also has a local ID that can be used to map the appropriate number of GPUs to each task.</p> </li> </ol> <p>This can be demonstrated with the following job script:</p> <pre><code>#! /bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --job-name=map-smallg-1gpt\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=small-g\n#SBATCH --ntasks=12\n#SBATCH --cpus-per-task=2\n#SBATCH --gpus-per-task=1\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nexport ROCR_VISIBLE_DEVICES=\\$SLURM_LOCALID\nexec \\$*\nEOF\nchmod +x ./select_gpu_$SLURM_JOB_ID\n\ncat &lt;&lt; EOF &gt; echo_dev_$SLURM_JOB_ID\n#!/bin/bash\nprintf -v task \"%02d\" \\$SLURM_PROCID\necho \"Task \\$task or node.local_id \\$SLURM_NODEID.\\$SLURM_LOCALID sees ROCR_VISIBLE_DEVICES=\\$ROCR_VISIBLE_DEVICES\"\nEOF\nchmod +x ./echo_dev_$SLURM_JOB_ID\n\nset -x\nsrun gpu_check -l\nsrun ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID gpu_check -l\nset +x\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID echo_dev_$SLURM_JOB_ID\n</code></pre> <p>To run this job successfully, we need 12 GPUs so obviously the tasks will be spread over more than  one node. The <code>echo_dev</code> command in this script only shows us the value of <code>ROCR_VISIBLE_DEVICES</code>  for the task at that point, something that <code>gpu_check</code> in fact also reports as <code>GPU_ID</code>, but this is just in case you don't believe...</p> <p>The output of the first <code>srun</code> command is:</p> <pre><code>+ srun gpu_check -l\nMPI 000 - OMP 000 - HWT 001 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 000 - OMP 001 - HWT 002 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 001 - OMP 000 - HWT 003 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)\nMPI 001 - OMP 001 - HWT 004 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)\nMPI 002 - OMP 000 - HWT 005 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)\nMPI 002 - OMP 001 - HWT 006 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)\nMPI 003 - OMP 000 - HWT 007 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)\nMPI 003 - OMP 001 - HWT 008 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)\nMPI 004 - OMP 000 - HWT 009 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1(GCD4/CCD0)\nMPI 004 - OMP 001 - HWT 010 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d1(GCD4/CCD0)\nMPI 005 - OMP 000 - HWT 011 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6(GCD5/CCD1)\nMPI 005 - OMP 001 - HWT 012 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d6(GCD5/CCD1)\nMPI 006 - OMP 000 - HWT 013 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9(GCD6/CCD4)\nMPI 006 - OMP 001 - HWT 014 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID d9(GCD6/CCD4)\nMPI 007 - OMP 000 - HWT 015 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc(GCD7/CCD5)\nMPI 007 - OMP 001 - HWT 016 (CCD2) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID dc(GCD7/CCD5)\nMPI 008 - OMP 000 - HWT 001 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 008 - OMP 001 - HWT 002 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 009 - OMP 000 - HWT 003 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)\nMPI 009 - OMP 001 - HWT 004 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c6(GCD1/CCD7)\nMPI 010 - OMP 000 - HWT 005 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)\nMPI 010 - OMP 001 - HWT 006 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9(GCD2/CCD2)\nMPI 011 - OMP 000 - HWT 007 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)\nMPI 011 - OMP 001 - HWT 008 (CCD1) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID cc(GCD3/CCD3)\n</code></pre> <p>In other words, we see that we did get cores on two nodes that obviously are not well aligned with the GCDs, and 8 GPUS on the first and 4 on the second node.</p> <p>The output of the second <code>srun</code> is:</p> <pre><code>+ srun ./echo_dev_4359428\n+ sort\nTask 00 or node.local_id 0.0 sees ROCR_VISIBLE_DEVICES=0\nTask 01 or node.local_id 0.1 sees ROCR_VISIBLE_DEVICES=0\nTask 02 or node.local_id 0.2 sees ROCR_VISIBLE_DEVICES=0\nTask 03 or node.local_id 0.3 sees ROCR_VISIBLE_DEVICES=0\nTask 04 or node.local_id 0.4 sees ROCR_VISIBLE_DEVICES=0\nTask 05 or node.local_id 0.5 sees ROCR_VISIBLE_DEVICES=0\nTask 06 or node.local_id 0.6 sees ROCR_VISIBLE_DEVICES=0\nTask 07 or node.local_id 0.7 sees ROCR_VISIBLE_DEVICES=0\nTask 08 or node.local_id 1.0 sees ROCR_VISIBLE_DEVICES=0\nTask 09 or node.local_id 1.1 sees ROCR_VISIBLE_DEVICES=0\nTask 10 or node.local_id 1.2 sees ROCR_VISIBLE_DEVICES=0\nTask 11 or node.local_id 1.3 sees ROCR_VISIBLE_DEVICES=0\n</code></pre> <p>It is normal that each task sees <code>ROCR_VISIBLE_DEVICES=0</code> even though we have seen that they all use a different GPU. This is because each task is locked up in a control group with only one GPU, which then  gets number 0.</p> <p>The output of the third <code>srun</code> command is:</p> <pre><code>+ sort\nTask 00 or node.local_id 0.0 sees ROCR_VISIBLE_DEVICES=\nTask 01 or node.local_id 0.1 sees ROCR_VISIBLE_DEVICES=\nTask 02 or node.local_id 0.2 sees ROCR_VISIBLE_DEVICES=\nTask 03 or node.local_id 0.3 sees ROCR_VISIBLE_DEVICES=\nTask 04 or node.local_id 0.4 sees ROCR_VISIBLE_DEVICES=\nTask 05 or node.local_id 0.5 sees ROCR_VISIBLE_DEVICES=\nTask 06 or node.local_id 0.6 sees ROCR_VISIBLE_DEVICES=\nTask 07 or node.local_id 0.7 sees ROCR_VISIBLE_DEVICES=\nTask 08 or node.local_id 1.0 sees ROCR_VISIBLE_DEVICES=\nTask 09 or node.local_id 1.1 sees ROCR_VISIBLE_DEVICES=\nTask 10 or node.local_id 1.2 sees ROCR_VISIBLE_DEVICES=\nTask 11 or node.local_id 1.3 sees ROCR_VISIBLE_DEVICES=\n</code></pre> <p>Slurm in fact did not set <code>ROCR_VISIBLE_DEVICES</code> because we turned binding off.</p> <p>In the next <code>srun</code> command we set <code>ROCR_VISIBLE_DEVICES</code> based on the local task ID and get:</p> <pre><code>+ srun --gpu-bind=none ./select_gpu_4359428 ./echo_dev_4359428\n+ sort\nTask 00 or node.local_id 0.0 sees ROCR_VISIBLE_DEVICES=0\nTask 01 or node.local_id 0.1 sees ROCR_VISIBLE_DEVICES=1\nTask 02 or node.local_id 0.2 sees ROCR_VISIBLE_DEVICES=2\nTask 03 or node.local_id 0.3 sees ROCR_VISIBLE_DEVICES=3\nTask 04 or node.local_id 0.4 sees ROCR_VISIBLE_DEVICES=4\nTask 05 or node.local_id 0.5 sees ROCR_VISIBLE_DEVICES=5\nTask 06 or node.local_id 0.6 sees ROCR_VISIBLE_DEVICES=6\nTask 07 or node.local_id 0.7 sees ROCR_VISIBLE_DEVICES=7\nTask 08 or node.local_id 1.0 sees ROCR_VISIBLE_DEVICES=0\nTask 09 or node.local_id 1.1 sees ROCR_VISIBLE_DEVICES=1\nTask 10 or node.local_id 1.2 sees ROCR_VISIBLE_DEVICES=2\nTask 11 or node.local_id 1.3 sees ROCR_VISIBLE_DEVICES=3\n</code></pre> <p>Finally, we run <code>gpu_check</code> again and see the same assignement of physica GPUs again as when we started, but now with different logical device numbers passed by <code>ROCR_VISIBLE_DEVICES</code>. The device number for the hip runtime is always 0 though which is normal as <code>ROCR_VISIBLE_DEVICES</code> restricts the access of the hip runtime to one GPU.</p> <pre><code>+ srun --gpu-bind=none ./select_gpu_4359428 gpu_check -l\nMPI 000 - OMP 000 - HWT 001 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 000 - OMP 001 - HWT 002 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 001 - OMP 000 - HWT 003 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 001 - OMP 001 - HWT 004 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 002 - OMP 000 - HWT 005 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 002 - OMP 001 - HWT 006 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 003 - OMP 000 - HWT 007 (CCD0) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\nMPI 003 - OMP 001 - HWT 008 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\nMPI 004 - OMP 000 - HWT 009 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)\nMPI 004 - OMP 001 - HWT 010 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1(GCD4/CCD0)\nMPI 005 - OMP 000 - HWT 011 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)\nMPI 005 - OMP 001 - HWT 012 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6(GCD5/CCD1)\nMPI 006 - OMP 000 - HWT 013 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)\nMPI 006 - OMP 001 - HWT 014 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9(GCD6/CCD4)\nMPI 007 - OMP 000 - HWT 015 (CCD1) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)\nMPI 007 - OMP 001 - HWT 016 (CCD2) - Node nid007379 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID dc(GCD7/CCD5)\nMPI 008 - OMP 000 - HWT 001 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 008 - OMP 001 - HWT 002 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1(GCD0/CCD6)\nMPI 009 - OMP 000 - HWT 003 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 009 - OMP 001 - HWT 004 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6(GCD1/CCD7)\nMPI 010 - OMP 000 - HWT 005 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 010 - OMP 001 - HWT 006 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9(GCD2/CCD2)\nMPI 011 - OMP 000 - HWT 007 (CCD0) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\nMPI 011 - OMP 001 - HWT 008 (CCD1) - Node nid007380 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID cc(GCD3/CCD3)\n</code></pre> Example job script when using 2 GPUs per task. <p> <pre><code>#! /bin/bash\n#SBATCH --account=project_46YXXXXXX\n#SBATCH --job-name=map-smallg-2gpt\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=small-g\n#SBATCH --ntasks=6\n#SBATCH --cpus-per-task=2\n#SBATCH --gpus-per-task=2\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nexport ROCR_VISIBLE_DEVICES=\\$((SLURM_LOCALID*2)),\\$((SLURM_LOCALID*2+1))\nexec \\$*\nEOF\nchmod +x ./select_gpu_$SLURM_JOB_ID\n\ncat &lt;&lt; EOF &gt; echo_dev_$SLURM_JOB_ID\n#!/bin/bash\nprintf -v task \"%02d\" \\$SLURM_PROCID\necho \"Task \\$task or node.local_id \\$SLURM_NODEID.\\$SLURM_LOCALID sees ROCR_VISIBLE_DEVICES=\\$ROCR_VISIBLE_DEVICES\"\nEOF\nchmod +x ./echo_dev_$SLURM_JOB_ID\n\nset -x\nsrun gpu_check -l\nsrun ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID ./echo_dev_$SLURM_JOB_ID | sort\nsrun --gpu-bind=none ./select_gpu_$SLURM_JOB_ID gpu_check -l\nset +x\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID echo_dev_$SLURM_JOB_ID\n</code></pre></p> <p>The changes that were required are only minimal. We now assign 2 GPUs to <code>ROCR_VISIBLE_DEVICSES</code> which  is easily done with some bash arithmetic.</p>"},{"location":"intro-evolving/08_Binding/#further-material","title":"Further material","text":"<ul> <li> <p>Distribution and binding is discussed in more detail in our     4-day comprehensive LUMI courses.     Check for the lecture on \"Advanced Placement\" which is usually     given on day 2 of the course.</p> <p>Material of this presentation is available to all LUMI users on the system. Check the course website for the names of the files.</p> </li> <li> <p>Rank reordering in Cray MPICH is discussed is also discussed in more dteail in our     4-day comprehensive LUMI courses,     but in the lecture on \"MPI Topics on the HPE Cray EX Supercomputer\" (often on day 3 of the course)     that discusses more advanced MPI on LUMI, including loads of environment variables that can be used to     improve the performance.</p> </li> </ul>"},{"location":"intro-evolving/09_Exercises_2/","title":"Exercises 2: Running jobs with Slurm","text":""},{"location":"intro-evolving/09_Exercises_2/#exercises-on-the-slurm-allocation-modes","title":"Exercises on the Slurm allocation modes","text":"<ol> <li> <p>In this exercise we check how cores would be assigned to a shared memory program.      Run a single task on the CPU partition with <code>srun</code> using 16 cpu cores.      Inspect the default task allocation with the <code>taskset</code> command      (<code>taskset -cp $$</code> will show you the cpu numbers allocated to the current process). </p> Click to see the solution. <pre><code>srun --partition=small --nodes=1 --tasks=1 --cpus-per-task=16 --time=5 --account=&lt;project_id&gt; bash -c 'taskset -cp $$' \n</code></pre> <p>Note that you need to replace <code>&lt;project_id&gt;</code> with the actual project account ID of the  form  <code>project_</code> plus a 9 digits number.</p> <p>The command runs a single process (<code>bash</code> shell with the native Linux <code>taskset</code> tool showing  process's CPU affinity) on a compute node.  You can use the <code>man taskset</code> command to see how the tool works.</p> </li> <li> <p>Next we'll try a hybrid MPI/OpenMP program.     For this we will use the <code>hybrid_check</code> tool from the <code>lumi-CPEtools</code> module of the LUMI Software Stack.      This module is preinstalled on the system and has versions for all versions of the <code>LUMI</code> software stack     and all toolchains and partitions in those stacks.</p> <p>Use the simple job script below to run a parallel program with multiple tasks (MPI ranks) and threads (OpenMP).  Submit with <code>sbatch</code> on the CPU partition and check task and thread affinity.</p> <pre><code>#!/bin/bash -l\n#SBATCH --partition=small           # Partition (queue) name\n#SBATCH --nodes=1                   # Total number of nodes\n#SBATCH --ntasks-per-node=8         # 8 MPI ranks per node\n#SBATCH --cpus-per-task=16          # 16 threads per task\n#SBATCH --time=5                    # Run time (minutes)\n#SBATCH --account=&lt;project_id&gt;      # Project for billing\n\nmodule load LUMI/23.09\nmodule load lumi-CPEtools/1.1-cpeGNU-23.09\n\nsrun --cpus-per-task=$SLURM_CPS_PER_TASK hybrid_check -n -r\n</code></pre> <p>Be careful with copy/paste of the script body as copy problems with special characters or a double dash may  occur, depending on the editor you use.</p> Click to see the solution. <p>Save the script contents into the file  <code>job.sh</code> (you can use the <code>nano</code> console text editor for instance).  Remember to use valid project account name.</p> <p>Submit the job script using the <code>sbatch</code> command:</p> <pre><code>sbatch job.sh\n</code></pre> <p>The job output is saved in the <code>slurm-&lt;job_id&gt;.out</code> file.  You can view its content with either the <code>less</code> or <code>more</code> shell commands.</p> <p>The actual task/threads affinity may depend on the specific OpenMP runtime  (if you literally use this job script it will be the GNU OpenMP runtime).</p> </li> <li> <p>Improve the thread affinity with OpenMP runtime variables.     Alter the script from the previous exercise and ensure that each thread is bound to    a specific core. </p> Click to see the solution. <p>Add the following OpenMP environment variables definition to your script:</p> <pre><code>export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=cores\n</code></pre> <p>You can also use an MPI runtime variable to have MPI itself report a cpu mask summary for each MPI rank:</p> <pre><code>export MPICH_CPUMASK_DISPLAY=1\n</code></pre> <p>Note <code>hybrid_check</code> and MPICH cpu mask may not be consistent. It is found to be confusing.</p> <p>To avoid having to use the <code>--cpus-per-task</code> flag, you can also set the environment variable <code>SRUN_CPUS_PER_TASK</code> instead: </p> <pre><code>export SRUN_CPUS_PER_TASK=16 \n</code></pre> <p>On LUMI this is not strictly necessary as the Slurm SBATCH processing has been modified to set this environment variable, but that was a clunky patch to reconstruct some old behaviour of Slurm and we have already seen cases where the patch did not work (but that were more complex cases that required different environment variables for a similar function).</p> <p>The list of environment variables that the <code>srun</code> command can use as input, is actually confusing, as some start with <code>SLURM_</code> but a few start with <code>SRUN_</code> while the <code>SLURM_</code> equivalent is ignored.</p> <p>So we end up with the following script:</p> <pre><code>#!/bin/bash -l\n#SBATCH --partition=small           # Partition (queue) name\n#SBATCH --nodes=1                   # Total number of nodes\n#SBATCH --ntasks-per-node=8         # 8 MPI ranks per node\n#SBATCH --cpus-per-task=16          # 16 threads per task\n#SBATCH --time=5                    # Run time (minutes)\n#SBATCH --account=&lt;project_id&gt;      # Project for billing\n\nmodule load LUMI/23.09\nmodule load lumi-CPEtools/1.1-cpeGNU-23.09\n\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=cores\n\nexport MPICH_CPUMASK_DISPLAY=1\n\nsrun hybrid_check -n -r\n</code></pre> <p>Note that MPI returns the CPU mask per process in binary form (a long string of zeros and ones) where the last number is for core 0. Also, you'll see that with the OpenMP environment variables set, it will look like only one core can be used by each MPI task, but that is because it only shows the mask for the main process which becomes OpenMP thread 0. Remove the OpenMP environment variables and you'll see that each task now gets 16 possible cores to run on, and the same is true for each OpenMP thread (at least when using the GNU compilers, the Cray compilers have different default behaviour for OpenMP which actually makes more sense for most scientific computing codes).</p> </li> <li> <p>Build the <code>hello_jobstep</code> program tool using interactive shell on a GPU node.      You can pull the source code for the program from git repository      <code>https://code.ornl.gov/olcf/hello_jobstep.git</code>.      It uses a <code>Makefile</code> for building and requires Clang and HIP.      The <code>hello_jobstep</code> program is actually the main source of inspiration for the      <code>gpu_check</code> program in the <code>lumi-CPEtools</code> modules for <code>partition/G</code>.     Try to run the program interactively. </p> Click to see the solution. <p>Clone the code using <code>git</code> command:</p> <pre><code>git clone https://code.ornl.gov/olcf/hello_jobstep.git\n</code></pre> <p>It will create <code>hello_jobstep</code> directory consisting source code and <code>Makefile</code>.</p> <p>Allocate resources for a single task with a single GPU with <code>salloc</code>:</p> <pre><code>salloc --partition=small-g --nodes=1 --tasks=1 --cpus-per-task=1 --gpus-per-node=1 --time=10 --account=&lt;project_id&gt;\n</code></pre> <p>Note that, after allocation is granted, you receive new shell but are still on the compute node.  You need to use the <code>srun</code> command to run on the allocated node. </p> <p>Start interactive session on a GPU node:</p> <pre><code>srun --pty bash -i\n</code></pre> <p>Note now you are on the compute node. <code>--pty</code> option for <code>srun</code> is required to interact with the remote shell.</p> <p>Enter the <code>hello_jobstep</code> directory and issue <code>make</code> command. </p> <p>As an example we will built with the system default programming environment, <code>PrgEnv-cray</code> in <code>CrayEnv</code>.  Just to be sure we'll load even the programming environment module explicitly.</p> <p>The build will fail if the <code>rocm</code> module is not loaded when using <code>PrgEnv-cray</code>.</p> <pre><code>module load CrayEnv\nmodule load PrgEnv-cray\nmodule load rocm\n</code></pre> <p>To build the code, use</p> <pre><code>make LMOD_SYSTEM_NAME=\"frontier\"\n</code></pre> <p>You need to add <code>LMOD_SYSTEM_NAME=\"frontier\"</code> variable for make as the code originates from the Frontier system and doesn't know LUMI.</p> <p>(As an exercise you can try to fix the <code>Makefile</code> and enable it for LUMI :))</p> <p>Finally you can just execute <code>./hello_jobstep</code> binary program to see how it behaves:</p> <pre><code>./hello_jobstep\n</code></pre> <p>Note that executing the program with <code>srun</code> in the srun interactive session will result in a hang. You need to work with <code>--overlap</code> option for srun to mitigate this.</p> <p>Remember to terminate your interactive session with <code>exit</code> command.</p> <p><pre><code>exit\n</code></pre> and then do the same for the shell created by <code>salloc</code> also.</p> </li> </ol>"},{"location":"intro-evolving/09_Exercises_2/#slurm-custom-binding-on-gpu-nodes","title":"Slurm custom binding on GPU nodes","text":"<ol> <li> <p>Allocate one GPU node with one task per GPU and bind tasks to each CCD (8-core group sharing L3 cache)      leaving the first (#0) and last (#7) cores unused.      Run a program with 6 threads per task and inspect the actual task/threads affinity     using either the <code>hello_jobstep</code> executable generated in the previous exercise, or the     <code>gpu_check</code> command from tne <code>lumi-CPEtools</code> module.</p> Click to see the solution. <p>We can chose between different approaches. In the example below, we follow the  \"GPU binding: Linear GCD, match cores\"  slides and we only need to adapt the CPU mask:</p> <pre><code>#!/bin/bash -l\n#SBATCH --partition=standard-g  # Partition (queue) name\n#SBATCH --nodes=1               # Total number of nodes\n#SBATCH --ntasks-per-node=8     # 8 MPI ranks per node\n#SBATCH --gpus-per-node=8       # Allocate one gpu per MPI rank\n#SBATCH --time=5                # Run time (minutes)\n#SBATCH --account=&lt;project_id&gt;  # Project for billing\n#SBATCH --hint=nomultithread\n\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nexport ROCR_VISIBLE_DEVICES=\\$SLURM_LOCALID\nexec \\$*\nEOF\nchmod +x ./select_gpu_$SLURM_JOB_ID\n\nCPU_BIND=\"mask_cpu:0xfe000000000000,0xfe00000000000000,\"\nCPU_BIND=\"${CPU_BIND}0xfe0000,0xfe000000,\"\nCPU_BIND=\"${CPU_BIND}0xfe,0xfe00,\"\nCPU_BIND=\"${CPU_BIND}0xfe00000000,0xfe0000000000\"\n\nexport OMP_NUM_THREADS=6\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=cores\n\nsrun --cpu-bind=${CPU_BIND} ./select_gpu_$SLURM_JOB_ID ./hello_jobstep\n</code></pre> <p>The base mask we need for this exercise, with each first and last core of a chiplet disabled, is <code>01111110</code> which is <code>0x7e</code> in hexadecimal notation.</p> <p>Save the job script as <code>job_step.sh</code> then simply submit it with sbatch from the directory that contains the <code>hello_jobstep</code> executable. Inspect the job output.</p> <p>Note that in fact as this program was compiled with the Cray compiler in the previous exercise, you don't even need to use the <code>OMP_*</code> environment variables above as the threads are  automatically pinned to a single core and as the correct number of threads is derived from the affinity mask for each task.</p> <p>Or using <code>gpu_check</code> instead (and we'll use the <code>cpeGNU</code> version again):</p> <pre><code>#!/bin/bash -l\n#SBATCH --partition=standard-g  # Partition (queue) name\n#SBATCH --nodes=1               # Total number of nodes\n#SBATCH --ntasks-per-node=8     # 8 MPI ranks per node\n#SBATCH --gpus-per-node=8       # Allocate one gpu per MPI rank\n#SBATCH --time=5                # Run time (minutes)\n#SBATCH --account=&lt;project_id&gt;  # Project for billing\n#SBATCH --hint=nomultithread\n\nmodule load LUMI/23.09\nmodule load lumi-CPEtools/1.1-cpeGNU-23.09\n\ncat &lt;&lt; EOF &gt; select_gpu_$SLURM_JOB_ID\n#!/bin/bash\nexport ROCR_VISIBLE_DEVICES=\\$SLURM_LOCALID\nexec \\$*\nEOF\nchmod +x ./select_gpu_$SLURM_JOB_ID\n\nCPU_BIND=\"mask_cpu:0xfe000000000000,0xfe00000000000000,\"\nCPU_BIND=\"${CPU_BIND}0xfe0000,0xfe000000,\"\nCPU_BIND=\"${CPU_BIND}0xfe,0xfe00,\"\nCPU_BIND=\"${CPU_BIND}0xfe00000000,0xfe0000000000\"\n\nexport OMP_NUM_THREADS=6\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=cores\n\nsrun --cpu-bind=${CPU_BIND} ./select_gpu_$SLURM_JOB_ID gpu_check -l\n</code></pre> </li> </ol>"},{"location":"intro-evolving/10_Lustre/","title":"I/O and file systems on LUMI","text":""},{"location":"intro-evolving/10_Lustre/#file-systems-on-lumi","title":"File systems on LUMI","text":"<p>Supercomputing since the second half of the 1980s has almost always been about  trying to build a very fast system from relatively cheap volume components or technologies (as low as you can go without loosing too much reliability) and very cleverly written software both at the system level (to make the system look like a true single system as much as possible) and at the application level (to deal with the restrictions that inevitably come with such a setup).</p> <p>The Lustre parallel file system that we use on LUMI (and is its main file system serving user files) fits in that way of thinking. A large file system is built by linking many fairly regular file servers through a fast network to the compute resources to build a single system with a lot of storage capacity and a lot of bandwidth. It has its restrictions also though: IOPs (number of I/O operations per second) doesn't scale as well or as easily as bandwidth and capacity so this comes with usage restrictions on large clusters that may a lot more severe than you are used to from small systems. And yes, it is completely normal that some file operations are slower than on the SSD of a good PC.</p> <p>HPE Cray EX systems go even one step further.  Lustre is the only network file system directly served to the compute nodes. Other network file system come via a piece of software called Data Virtualisation Service (abbreviated DVS) that basically forward I/O requests to servers in the management section of the cluster where the actual file system runs.  This is part of the measures that Cray takes in Cray Operating System to minimise OS jitter on the compute nodes to improve scalability of applications.</p>"},{"location":"intro-evolving/10_Lustre/#lustre-building-blocks","title":"Lustre building blocks","text":"<p>A key element of Lustre - but also of other parallel file systems for large parallel computers such as BeeGFS, Spectrum Scale (formerly GPFS) or PanFS - is the separation of metadata and the actual file data, as the way both are accessed and used is very different.</p> <p>A Lustre system consists of the following blocks:</p> <ol> <li> <p>Metadata servers (MDSes) with one or more metadata targets (MDTs) each store     namespace metadata such as filenames, directories and access permissions, and the     file layout.</p> <p>Each MDT is a filesystem, usually with some level of RAID or similar technologies for increased reliability. Usually there is also more than one MDS and they are put in a high availability setup for increased availability (and this is the case in  LUMI).</p> <p>Metadata is accessed in small bits and it does not need much capacity. However,  metadata accesses are hard to parallelise so it makes sense to go for the fastest storage possible even if that storage is very expensive per terabyte. On LUMI all metadata servers use SSDs.</p> </li> <li> <p>Object storage servers (OSSes) with one or more object storage targets (OSTs) each     store the actual data. Data from a single file can be distributed across multiple     OSTs and even multiple OSSes. As we shall see, this is also the key to getting very     high bandwidth access to the data.</p> <p>Each MDT is a filesystem, again usually with some level of RAID or similar technologies to survive disk failures. One OSS typically has between 1 and 8 OSTs, and in a big setup a high availability setup will be used again (as is the case in LUMI).</p> <p>The total capacity of a Lustre file system is the sum of the capacity of all OSTs in the  Lustre file system. Lustre file systems are often many petabytes large.</p> <p>Now you may think different from prices that you see in the PC market for hard drives and SSDs, but SSDs of data centre quality are still up to 10 times as expensive as  hard drives of data centre quality. Building a file system of several tens of petabytes out of SSDs is still extremely expensive and rarely done, certainly in an environment  with a high write pressure on the file system as that requires the highest quality SSDs. Hence it is not uncommon that supercomputers will use mostly hard drives for their large storage systems.</p> <p>On LUMI there is roughly 80 PB spread across 4 large hard disk based Lustre file systems, and 8.5 PB in an SSD-based Lustre file system. However, all MDSes use SSD storage.</p> </li> <li> <p>Lustre clients access and use the data. They make the whole Lustre file system look like     a single file system.</p> <p>Now Lustre is transparent in functionality. You can use a Lustre file system just as any  other regular file system, with all your regular applications. However, it is not at really transparent when it comes to performance: How you use Lustre can have a huge impact on  performance. Lustre is optimised very much for high bandwidth access to large chunks of data at a time from multiple nodes in the application simultaneously, and is not very good  at handling access to a large pool of small files instead. </p> <p>So you have to store your data (but also your applications as the are a kind of data also) in an appropriate way, in fewer but larger files instead of more smaller files.  Some centres with large supercomputers will advise you to containerise software for optimal performance. On LUMI we do advise Python users or users who install software through Conda to do so. </p> </li> <li> <p>All these components are linked together through a high performance interconnect. On HPE Cray EX     systems - but on more an more other systems also - there is no separate network anymore for      storage access and the same high performance interconnect that is also used for internode     communication by applications (through, e.g., MPI) is used for that purpose.</p> </li> <li> <p>There is also a management server which is not mentioned on the slides, but that component     is not essential to understand the behaviour of Lustre for the purpose of this lecture.</p> </li> </ol> <p>Links</p> <p>See also the \"Lustre Components\" in \"Understanding Lustre Internals\" on the Lustre Wiki</p>"},{"location":"intro-evolving/10_Lustre/#striping-large-files-are-spread-across-osts","title":"Striping: Large files are spread across OSTs","text":"<p>On Lustre, large files are typically broken into blocks called stripes that are then cyclically spread across a number of chunk files called objects in LUSTRE, each on a separate OST.  In the figure above, the file is spread across the OSTs 0, 2, 4 and 6.</p> <p>This process is completely transparent to the user with respect to correctness. The Lustre client takes care of the process and presents a traditional file to the application program. It is however not transparent with respect to performance. The performance of reading and writing a file depends a lot on how the file is spread across the OSTs of a file system.</p> <p>Basically, there are two parameters that have to be chosen: The size of the stripes (all stripes have the same size in this example, except for the last one which may be smaller) and the number of OSTs  that should be used for a file. Lustre itself takes care of choosing the OSTs in general.</p> <p>There are variants of Lustre where one has multiple layouts per file which can come in handy if one  doesn't know the size of the file in advance. The first part of the file will then typically be written with fewer OSTs and/or smaller chunks, but this is outside the scope of this course. The feature is known as Progressive File Layout.</p> <p>The stripe size and number of OSTs used can be chosen on a file-by-file basis. The default on LUMI is to use only one OST for a file. This is done because that is the most reasonable choice for the many small files that many unsuspecting users have, and as we shall see, it is sometimes even the best choice for users working with large files. But it is not always the best choice. And unfortunately there is no single set of parameters that is good for all users.</p> <p>Objects</p> <p>The term \"object\" nowadays has different meanings, even in the storage world. The Object Storage Servers in Lustre should not be confused with the object storage used in cloud solutions such as Amazon Web Services (AWS) with the S3 storage service or the LUMI-O object storage that we will discuss later. In fact, the Object Storage Servers use a regular file system such as ZFS or ldiskfs to store the \"objects\".</p>"},{"location":"intro-evolving/10_Lustre/#accessing-a-file","title":"Accessing a file","text":"<p>Let's now study how Lustre will access a file for reading or writing. Let's assume that the second client in the above picture wants to write something to the file.</p> <ul> <li> <p>The first step is opening the file. </p> <p>For that, the Lustre client has to talk to the metadata server (MDS) and query some information about the file.</p> <p>The MDS in turn will return information about the file, including the layout of the file: chunksize and the OSSes/OSTs that keep the chunks of the file.</p> </li> <li> <p>From that point on, the client doesn't need to talk to the MDS anymore and can talk directly to      the OSSes to write data to the OSTs or read data from the OSTs.</p> </li> </ul>"},{"location":"intro-evolving/10_Lustre/#parallelism-is-key","title":"Parallelism is key!","text":"<p>The metadata servers can be the bottleneck in a Lustre setup.  It is not easy to spread metadata across multiple MDSes efficiently. Moreover, the amount of metadata for any given file is small, so any metadata operation will translate into small disk accesses on the MDTs and hence not  fully exploit the speed that some RAID setups can give you.</p> <p>However, when reading and writing data, there are up to four levels of parallelism:</p> <ol> <li> <p>The read and write operations can engage multiple OSSes.</p> </li> <li> <p>Since a single modern OSS can handle more bandwidth than a some OSTs can     deliver, OSSes may have multiple OSTs.</p> <p>How many OSTs are engaged is something that a user has control over.</p> </li> <li> <p>An OST will contain many disks or SSDs, typically with some kind of RAID, but hence     each read or write operation to an OST can engage multiple disks.</p> <p>An OST will only be used optimally when doing large enough file accesses. But the  file system client may help you here with caching.</p> </li> <li> <p>Internally, SSDs are also parallel devices. The high bandwidth of modern high-end SSDs     is the result of engaging multiple channels to the actual storage \"chips\" internally.</p> </li> </ol> <p>So to fully benefit from a Lustre file system, it is best to work with relatively few files (to not overload the MDS) but very large disk accesses. Very small I/O operations wouldn't even benefit from the RAID acceleration, and this is especially true for very small files as they cannot even benefit from caching provided by the file system client (otherwise a file system client may read in more data than requested, as file access is often sequential anyway so it would be prepared for the next access). To make efficient use of the OSTs it is important to have a relatively large chunk size and relatively large I/O operations, even more so for hard disk based file systems as if the OST file system manages to organise the data well on disk, it is a good way to reduce the impact on effective bandwidth of the inherent latency of disk access. And to engage multiple OSTs simultaneously (and thus reach a bandwidth which is much  higher than a single OST can provide), even larger disk accesses will be needed so that multiple chunks are read or written simultaneously. Usually you will have to do the I/O in a distributed memory application from multiple nodes simultaneously as otherwise the bandwidth to the interconnect and processing capacity of the client software  of a single node might become the limiting factor.</p> <p>Not all codes are using Lustre optimally though, even with the best care of their users.</p> <ul> <li> <p>Some codes use files in scientific data formats like HDF5 and netCDF, and when written     properly they can have very scalable performance.</p> <p>A good code will write data to large files, from multiple nodes simultaneously, but  will avoid doing I/O from too many ranks simultaneously to avoid bombarding the OSSes/OSTs with I/O requests. But that is a topic for a more advanced course...</p> <p>One user has reported reading data from the hard disk based parallel file systems at about 25% of the maximal bandwidth, which is very good given that other users where also using that file system at the same time and not always in an optimal way.</p> <p>Surprisingly many of these codes may be rather old. But their authors grew up with noisy floppy drives (do you still know what that is) and slow hard drives so learned how to program efficiently.</p> </li> <li> <p>But some code open one or more files per MPI rank. Those codes may have difficulties     scaling to a large number of ranks, as they will put a heavy burden on the MDS when those     files are created, but also may bombard each OSS/OST with too many I/O requests.</p> <p>Some of these codes are rather old also, but were never designed to scale to thousands of MPI ranks. However, nowadays some users are trying to solve such big problems that  the computations do scale reasonably well. But the I/O of those codes becomes a problem...</p> </li> <li> <p>But some users simply abuse the file system as an unstructured database and simply drop     their data as tens of thousands or even millions of small files with each one data element,     rather than structuring that data in suitable file formats. This is especially common in     science fields that became popular relatively recently - bio-informatics and AI - as those     users typically started their work on modern PCs with fast SSDs. </p> <p>The problem there is that metadata access and small I/O operations don't scale well to large systems. Even copying such a data set to a local SSD would be a problem should a compute node have a local SSD, but local SSDs suitable for supercomputers are also very expensive as they  have to deal with lots of write operations. Your gene data or training data set may be relatively static, but on a supercomputer you cannot keep the same node for weeks so you'd need to rewrite  your data to local disks very often. And there are shared file systems with better small file performance than Lustre, but those that scale to the size of even a fraction of Lustre, are also very expensive. And remember that supercomputing works exactly the opposite way: Try to reduce costs by using relatively cheap hardware but cleverly written software at all levels (system and application) as at a very large scale, this is ultimately cheaper than investing more in hardware and less in software. </p> </li> </ul> <p>Lustre was originally designed to achieve very high bandwidth to/from a small number of files, and that is in fact a good match for well organised scientific data sets and/or checkpoint data, but was not designed to handle large numbers of small files. Nowadays of course optimisations to  deal better with small files are being made, but they may come at a high hardware cost.</p>"},{"location":"intro-evolving/10_Lustre/#how-to-determine-the-striping-values","title":"How to determine the striping values?","text":"<p>If you only access relatively small files (up to a few hundreds of kilobytes) and access them  sequentially, then you are out of luck. There is not much you can do. Engaging multiple OSTs  for a single file is not very useful in this case, and you will also have no parallelism from accessing multiple files that may be stored on different OSTs.</p> <p>As a rule of thumb, if you access a lot of data with a data access pattern that can exploit parallelism, try to use all OSTs of the Lustre file system:</p> <ul> <li> <p>If the number of files that will be accessed simultaneously is larger than the number of     OSTs, it is best to not spread a single file across OSTs and hence use a stripe count of 1.</p> <p>It will also reduce Lustre contention and OST file locking and as such gain performance for everybody.</p> </li> <li> <p>At the opposite end, if you access only one very large file and use large or parallel disk     accesses, set the stripe count to the number of OSTs (or a smaller number if you notice in     benchmarking that the I/O performance plateaus). On a system the size of LUMI with storage     as powerful as on LUMI, this will only work if you have more than on I/O client. </p> </li> <li> <p>When using multiple similar sized files simultaneously but less files than there are OSTs,     you should probably chose the stripe count such that the product of the number of files     and the stripe count is approximately the number of OSTs. E.g., with 32 OSTs and 8 files,     set the stripe count to 4.</p> </li> </ul> <p>It is better not to force the system to use specific OSTs but to let it chose OSTs at random.</p> <p>The typical stripe size  (size of the chunks) to use can be a bit harder to determine. Typically this will be 1MB or more, and it can be up to 4 GB, but that only makes sense for very large files. The ideal stripe size will also depend on the characteristics of the I/O in the file. If the application never writes more than 1 GB of data in a single  sequential or parallel I/O operation before continuing with more computations, obviously with a stripe size of 1 GB you'd be engaging only a single OST for each write operation.</p>"},{"location":"intro-evolving/10_Lustre/#managing-the-striping-parameters","title":"Managing the striping parameters","text":"<p>The basic Lustre command for regular users to do special operations on Lustre is the <code>lfs</code> command, which has various subcommands.</p> <p>The first interesting subcommand is <code>df</code> which has a similar purpose as the regular Linux <code>df</code> command: Return information about the filesystem. In particular,</p> <pre><code>lfs df -h\n</code></pre> <p>will return information about all available Lustre filesystems. The <code>-h</code> flag tells the command to use \"human-readable\" number formats: return sizes in gigabytes and terabytes rather than blocks. On LUMI, the output starts with:</p> <pre><code>$ lfs df -h\nUUID                       bytes        Used   Available Use% Mounted on\nlustref1-MDT0000_UUID       11.8T       16.8G       11.6T   1% /pfs/lustref1[MDT:0]\nlustref1-MDT0001_UUID       11.8T        4.1G       11.6T   1% /pfs/lustref1[MDT:1]\nlustref1-MDT0002_UUID       11.8T        2.8G       11.7T   1% /pfs/lustref1[MDT:2]\nlustref1-MDT0003_UUID       11.8T        2.7G       11.7T   1% /pfs/lustref1[MDT:3]\nlustref1-OST0000_UUID      121.3T       21.5T       98.5T  18% /pfs/lustref1[OST:0]\nlustref1-OST0001_UUID      121.3T       21.6T       98.4T  18% /pfs/lustref1[OST:1]\nlustref1-OST0002_UUID      121.3T       21.4T       98.6T  18% /pfs/lustref1[OST:2]\nlustref1-OST0003_UUID      121.3T       21.4T       98.6T  18% /pfs/lustref1[OST:3]\n</code></pre> <p>so the command can also be used to see the number of MDTs and OSTs available in each filesystem, with the capacity.</p> <p></p> <p>Striping in Lustre can be set at a filesystem level by the sysadmins, but users can adjust the settings at the directory level (which then sets the default for files created in that directory) and file level. Once a file is created, the striping configuration cannot be changed anymore on-the-fly. </p> <p>To inspect the striping configuration, one can use the <code>getstripe</code> subcommand of <code>lfs</code>.</p> <p>Let us first use it at the directory level:</p> <pre><code>$ lfs getstripe -d /appl/lumi/SW\nstripe_count:  1 stripe_size:   1048576 pattern:       0 stripe_offset: -1\n\n$ lfs getstripe -d --raw /appl/lumi/SW\nstripe_count:  0 stripe_size:   0 pattern:       0 stripe_offset: -1\n</code></pre> <p>The <code>-d</code> flag tells that we only want information about the directory itself and not about everything in that directory. The first <code>lfs getstripe</code> command tells us that files  created in this directory will use only a single OST and have a stripe size of 1 MiB.  By adding the <code>--raw</code> we actually see the settings that have been made specifically for this directory. The - for <code>stripe_count</code> and <code>stripe_size</code> means that the default value is being used, and the <code>stripe_offset</code> of <code>-1</code> also indicates the default value.</p> <p>We can also use <code>lfs getstripe</code> for individual files:</p> <pre><code>$ lfs getstripe /appl/lumi/LUMI-SoftwareStack/etc/motd.txt\n/appl/lumi/LUMI-SoftwareStack/etc/motd.txt\nlmm_stripe_count:  1\nlmm_stripe_size:   1048576\nlmm_pattern:       raid0\nlmm_layout_gen:    0\nlmm_stripe_offset: 10\n        obdidx           objid           objid           group\n            10        56614379      0x35fddeb                0\n</code></pre> <p>Now <code>lfs getstripe</code> does not only return the stripe size and number of OSTs used, but it will also show the OSTs that are actually used (in the column <code>obdidx</code> of the output). The <code>lmm_stripe_offset</code> is also the number of the OST with the first object of the file.</p> <p></p> <p></p> <p>The final subcommand that we will discuss is the <code>setstripe</code> subcommand to set the striping policy for a file or directory.</p> <p>Let us first look at setting a striping policy at the directory level:</p> <pre><code>$ module load lumi-training-tools\n$ mkdir testdir\n$ lfs setstripe -S 2m -c 4 testdir\n$ cd testdir\n$ mkfile 2g testfile1\n$ lfs getstripe testfile1\ntestfile1\nlmm_stripe_count:  4\nlmm_stripe_size:   2097152\nlmm_pattern:       raid0\nlmm_layout_gen:    0\nlmm_stripe_offset: 28\n        obdidx           objid           objid           group\n            28        66250987      0x3f2e8eb                0\n            30        66282908      0x3f3659c                0\n             1        71789920      0x4476d60                0\n             5        71781120      0x4474b00                0\n</code></pre> <p>The <code>lumi-training-tools</code> module only serves to provide the <code>mkfile</code> command that we use in this example.</p> <p>We first create a directory and then set the striping parameters to a stripe size of 2 MiB (the <code>-S</code> flag) and a so-called stripe count, the number of OSTs used for  the file, of 4 (the <code>-c</code> flag).</p> <p>Next we go into the subdirectory and use the <code>mkfile</code> command to generate a file of  2 GiB. </p> <p>When we now check the file layout of the file that we just created with <code>lfs getstripe</code>, we see that the file now indeed uses 4 OSTs with a stripe size of 2 MiB, and has object on in this case OSTs 28, 30, 1 and 5. </p> <p>However, we can even control striping at the level of an individual file. The condition is that the layout of the file is set as soon as it is created. We can do this also with <code>lfs setstripe</code>:</p> <pre><code>$ lfs setstripe -S 16m -c 2 testfile2\n$ ls -lh\ntotal 0\n-rw-rw---- 1 XXXXXXXX project_462000000 2.0G Jan 15 16:17 testfile1\n-rw-rw---- 1 XXXXXXXX project_462000000    0 Jan 15 16:23 testfile2\n$ lfs getstripe testfile2\ntestfile2\nlmm_stripe_count:  2\nlmm_stripe_size:   16777216\nlmm_pattern:       raid0\nlmm_layout_gen:    0\nlmm_stripe_offset: 10\n        obdidx           objid           objid           group\n            10        71752411      0x446dadb                0\n            14        71812909      0x447c72d                0\n</code></pre> <p>In this example, the <code>lfs setstripe</code> command will create an empty file but with the required layout. In this case we have set the stripe size to 16 MiB and use only 2 OSTs, and the <code>lfs getstripe</code> command confirms that information. We can now open the file to write data into it with the regular file operations of the Linux glibc library or your favourite programming language (though of course you need to take into account that the file already exists so you should use routines that do not return an error if the file already exists).</p> <p>Lustre API</p> <p>Lustre also offers a C API to directly set file layout properties, etc., from your package. Few scientific packages seem to support it though.</p>"},{"location":"intro-evolving/10_Lustre/#the-metadata-servers","title":"The metadata servers","text":"<p>Parallelising metadata access is very difficult. Even large Lustre filesystems have very few metadata servers. They are a finite and shared resource, and overloading the metadata server slows down the file system for all users.</p> <p>The metadata servers are involved in many operations. The play a role in creating, opening and also closing files. The provide some of the attributes of a file. And they also play a role in file locking.</p> <p>Yet the metadata servers have a very finite capacity. The Lustre documentation claims that in theory a single metadata server should be capable of up to 200,000 operations per second, depending on the type of request. However, 75,000 operations per second may be more realistic.</p> <p>As a user, many operations that you think are harmless from using your PC, are in fact expensive operations on a supercomputer with a large parallel file system and you will find \"Lustre best practices\" pages on web sites of many large supercomputer centres. Some tips for regular users:</p> <ul> <li> <p>Any command that requests attributes if fairly expensive and should not be used      in large directories. This holds even for something as trivial as <code>ls -l</code>.      But it is even more so for commands as <code>du</code> that run recursively through attributes     of lots of files.</p> </li> <li> <p>Opening a file is also rather expensive as it involves a metadata server and one or more     object servers. It is not a good idea to frequently open and close the same file while      processing data. </p> </li> <li> <p>Therefore access to many small files from many processes is not a good idea. One example of this     is using Python, and even more so if you do distributed memory parallel computing with Python.     This is why on LUMI we ask to do big Python installations in containers. Another alternative is     to run such programs from <code>/tmp</code> (and get them on <code>/tmp</code> from an archive file).</p> <p>For data, it is not a good idea to dump a big dataset as lots of small files on the filesystem. Data should be properly organised, preferably using file formats that support parallel access from many processes simultaneously. Technologies popular in supercomputing are  HDF5, netCDF and ADIOS2. Sometimes libraries to read tar-files or other archive file formats without first  fully uncompressing may even be enough for read-only data.  Or if your software runs into a container, you may be able to put your read-only dataset into a  SquashFS file and mount into a container.</p> <p>Likewise, shuffling data in a distributed memory program should not be done via the  filesystem (put data on a shared filesystem and then read it again in a different order) but by direct communication between the processes over the interconnect.</p> </li> <li> <p>It is also obvious that directories with thousands of files should be avoided as even an      <code>ls -l</code> command on that directory generates a high load on the metadata servers. But the same     holds for commands such as <code>du</code> or <code>find</code>.</p> <p>Note that the <code>lfs</code> command also has a subcommand <code>find</code> (see <code>man lfs-find</code>), but it cannot do everything that the regular Linux <code>find</code> command can do. E.g., the <code>--exec</code> functionality is missing. But to simply list files it will put less strain on the filesystem as running the  regular Linux <code>find</code> command.</p> </li> </ul> <p>There are many more tips more specifically for programmers. As good use of the filesystems on a supercomputer is important and wrong use has consequences for all other users, it is an important topic in the 4-day comprehensive LUMI course  that the LUMI User Support Team organises a few times per year, and you'll find many more tips about proper use of Lustre in that lecture (which is only available to actual users on LUMI unfortunately).</p>"},{"location":"intro-evolving/10_Lustre/#lustre-on-lumi","title":"Lustre on LUMI","text":"<p>LUMI has 5 Lustre filesystems:</p> <p>The file storage sometimes denoted as LUMI-P consists of 4 disk based Lustre filesystems, each with a capacity of roughly 18 PB and 240 GB/s aggregated bandwidth in the optimal case (which of course is shared by all users, no single user will ever observe that bandwidth unless they have the machine for themselves). Each of the 4 systems has 2 MDTs, one per MDS (but in a high availability setup), and 32 OSTs spread across 16 OSSes, so 2 OSTs per OSS. All 4 systems are used to serve the home directories, persistent project space and regular scratch space, but also, e.g., most of the software pre-installed on LUMI. Some of that pre-installed software is copied on all 4 systems to  distribute the load.</p> <p>The fifth Lustre filesystem of LUMI is also known as LUMI-F, where the \"F\" stands for flash as it is entirely based on SSDs. It currently has a capacity of approximately 8.5 PB and a total of over 2 TB/s aggregated bandwidth. The system has 4 MDTs spread across 4 MDSes, and 72 OSTs and 72 OSSes, os 1 OST per OSS (as a single OST already offers a lot more bandwidth and hence needs more server capacity than a hard disk based OST).</p>"},{"location":"intro-evolving/10_Lustre/#storage-areas","title":"Storage areas","text":"<p>The table on the slide above summarises the available file areas on the Lustre filesystems. That information is also available in the LUMI docs.</p> <ul> <li> <p>The home directory your personal area and mostly meant for configuration files and caches of     Linux commands, etc., and not meant for project-related work. It is fixed in size and number      of files. No extension of those limits is ever granted.</p> </li> <li> <p>The persistent project directory is meant to be the main work place for your project. A LUMI     project is also meant to be a close collaboration between people, and not something simply provided     to a research group for all their work.</p> <p>By default users get 50 GB of storage and 100,000 inodes (files and directories). The storage volume can be increased to 500 GB on request to the central LUMI help desk, but the number of inodes is fixed (though some small extensions have been granted if the user has a very good reason) as we want to avoid that the filesystems get overloaded by users working with lots of small files directly on the Lustre filesystem.</p> <p>The data is persistent for the project lifetime, but removed 90 days after the end of the project.</p> </li> <li> <p>The project scratch space on LUMI-P is by default 50 TB but can on request be extended up to 500 TB.     The number of files is limited to 2M, again to discourage using many small files.</p> <p>Data is only guaranteed to stay on the file system for 90 days. After 90 days, an automatic cleaning procedure may remove data (but it has not yet been necessary so far as other policies work well to  keep the storage reasonably clean).</p> </li> <li> <p>Finally, each project also gets some fast scratch space on LUMI-F. The default amount of storage is      12 TB, but can be extended to up to 100 TB. The number of inodes is limited to 1M, and data can be     removed automatically after 30 days (but again this has not happened yet).</p> </li> </ul> <p>It is important to note that LUMI is not meant to be used as a safe data archive. There is no backup of any of the filesystems, and there is no archiving of data of expired projects or user accounts.  Users are responsible for moving their data to systems suitable for archival.</p> <p>Storage use on LUMI is limited in two independent ways:</p> <ul> <li> <p>Traditional Linux block and file quota limit the maximum capacity you can use (in volume and number of     inodes, roughly the number of files and directories combined).</p> </li> <li> <p>But actual storage use is also \"billed\" on a use-per-hour basis. The idea behind this is that a user may     run a program that generates a lot of data, but after some postprocessing much of the data can be deleted     so that other users can use that capacity again, and to encourage that behaviour you are billed based not     on peak use, but based on the combination of the volume that you use and the time you use it for. </p> <p>E.g., if you would have 10 GB on the LUMI-P storage for 100 hours, you'd be billed 1 TB hour for that. If subsequently you reduce your usage to 2 GB, then it would take 500 hours before you have consumed another TB hour. Storage use is monitored hourly for this billing process, and if you run out of storage billing units you will not be able to run jobs anymore.</p> <p>The billing rate also depends on the file system used. As the flash storage system was roughly 10 times as  expensive in purchase as the hard disk based file systems, storage on <code>/flash</code> is also billed at 10 times the rate, so with 1 TB hour you can only store 100 GB for 1 hour on that system.</p> </li> </ul> <p>Storage in your home directory is not billed but that should not mean that you should abuse your home directory for other purposes then a home directory is meant to be used, and an extension of the home directory will never be granted. </p>"},{"location":"intro-evolving/10_Lustre/#object-storage-lumi-o","title":"Object storage: LUMI-O","text":"<p>LUMI has yet another storage system: An object storage system based on CEPH with a capacity of 30 PB. It can be used for storing, sharing and staging data.  It is not mounted on the compute nodes in a traditional way (as it is also structured differently) but can be accessed with tools such as <code>rclone</code> and <code>s3cmd</code>. </p> <p>It can also be reached easily from outside LUMI and is a proper intermediate stage to get data to and from LUMI, also because several object storage tools perform much better on high latency long-distance connections than tools as <code>sftp</code>. </p> <p>The downside is that to access LUMI-O, temporary authentication credentials have to be generated, which currently can only be done via a web interface, after which information about those credentials needs to be copied to configuration files or to fields in a GUI for GUI tools.</p> <p>Data on LUMI-O is persistent for the duration of the project. It is also billed, but as object storage is fairly cheap, is is billed at half the rate of LUMI-P.</p> <p>This short course does not offer enough time to fully discuss working with the object storage of LUMI. For this we refer to the LUMI documentation.</p>"},{"location":"intro-evolving/10_Lustre/#links","title":"Links","text":"<ul> <li> <p>The <code>lfs</code> command itself is documented through a manual page that can be accessed at the     LUMI command line with <code>man lfs</code>. The various subcommands each come with their own man page,     e.g., <code>lfs-df</code>, <code>lfs-getstripe</code>, <code>lfs-setstripe</code> and <code>lfs-find</code>.</p> </li> <li> <p>Understanding Lustre Internals     on the Lustre Wiki.</p> </li> <li> <p>Lustre Basics and     Lustre Best Practices     in the knowledge base of the NASA supercomputers.</p> </li> </ul>"},{"location":"intro-evolving/11_Containers/","title":"Containers on LUMI","text":""},{"location":"intro-evolving/11_Containers/#what-are-we-talking-about-in-this-chapter","title":"What are we talking about in this chapter?","text":"<p>Let's now switch to using containers on LUMI.  This section is about using containers on the login nodes and compute nodes.  Some of you may have heard that there were plans to also have an OpenShift Kubernetes container cloud platform for running microservices but at this point it is not clear if and when this will materialize due to a lack of personpower to get this running and then to support this.</p> <p>In this section, we will </p> <ul> <li> <p>discuss what to expect from containers on LUMI: what can they do and what can't they do,</p> </li> <li> <p>discuss how to get a container on LUMI,</p> </li> <li> <p>discuss how to run a container on LUMI,</p> </li> <li> <p>and discuss some enhancements we made to the LUMI environment that are based on containers or help     you use containers.</p> </li> </ul> <p>Remember though that the compute nodes of LUMI are an HPC infrastructure and not a container cloud!</p>"},{"location":"intro-evolving/11_Containers/#what-do-containers-not-provide","title":"What do containers not provide","text":"<p>What is being discussed in this subsection may be a bit surprising. Containers are often marketed as a way to provide reproducible science and as an easy way to transfer software from one machine to another machine. However, containers are neither of those and this becomes  very clear when using containers built on your typical Mellanox/NVIDIA InfiniBand based clusters with Intel processors and NVIDIA GPUs on LUMI.</p> <p>First, computational results are almost never 100% reproducible because of the very nature of how computers work. You can only expect reproducibility of sequential codes between equal hardware. As soon as you change the CPU type, some floating point computations may produce slightly different results, and as soon as you go parallel this may even be the case between two runs on exactly the same hardware and software. Containers may offer more reproducibility than recompiling software for a different platform, but all you're trying to do is reproducing  the same wrong result as in particular floating point operations are only an approximation for real numbers.  When talking about reproducibility, you should think like way experimentalists do: You have a result and an  error margin, and it is important to have an idea of that error margin too.</p> <p>But full portability is a much greater myth. Containers are really only guaranteed to be portable between similar systems. They may be a little bit more portable than just a binary as you may be able to deal with missing or different libraries in the container, but that is where it stops. Containers are usually built for a particular CPU architecture and GPU architecture, two elements where everybody can easily see that if you change this, the container will not run. But  there is in fact more: containers talk to other hardware too, and on an HPC system the first piece of hardware that comes to mind is the interconnect. And they use the kernel of the host and the kernel modules and drivers provided by that kernel. Those can be a problem. A container that is not build to support the SlingShot interconnect, may fail  (or if you're lucky just fall back to TCP sockets in MPI, completely killing scalability, but technically speaking still working so portable).  Containers that expect a certain version range of a particular driver on the system may fail if a different, out-of-range version of that driver is on the system instead (think the ROCm driver).</p> <p>Even if a container is portable to LUMI, it may not yet be performance-portable. E.g., without proper support for the interconnect it may still run but in a much slower mode.  Containers that expect the knem kernel extension for good  intra-node MPI performance may not run as efficiently as LUMI uses xpmem instead. But one should also realise that speed gains in the x86 family over the years come to a large extent from adding new instructions to the CPU set, and that two processors with the same instructions set extensions may still benefit from different optimisations by the compilers.  Not using the proper instruction set extensions can have a lot of influence. At UAntwerpen we've seen GROMACS  doubling its speed by choosing proper options, and the difference can even be bigger.</p> <p>Many HPC sites try to build software as much as possible from sources to exploit the available hardware as much as  possible. You may not care much about 10% or 20% performance difference on your PC, but 20% on a 160 million EURO investment represents 32 million EURO and a lot of science can be done for that money...</p>"},{"location":"intro-evolving/11_Containers/#but-what-can-they-then-do-on-lumi","title":"But what can they then do on LUMI?","text":"<ul> <li> <p>A very important reason to use containers on LUMI is reducing the pressure on the file system by software     that accesses many thousands of small files (Python and R users, you know who we are talking about).     That software kills the metadata servers of almost any parallel file system when used at scale.</p> <p>As a container on LUMI is a single file, the metadata servers of the parallel file system have far less  work to do, and all the file caching mechanisms can also work much better.</p> </li> <li> <p>Software installations that would otherwise be impossible.      E.g., some software may not even be suited for installation in     a multi-user HPC system as it uses fixed paths that are not compatible with installation in      module-controlled software stacks.     HPC systems want a lightweight <code>/usr</code> etc. structure as that part of the system     software is often stored in a RAM disk, and to reduce boot times. Moreover, different users may need     different versions of a software library so it cannot be installed in its default location in the system     software region. However, some software is ill-behaved and cannot be relocated to a different directory,     and in these cases containers help you to build a private installation that does not interfere with other     software on the system.</p> </li> <li> <p>As an example, Conda installations are not appreciated on the main Lustre file system.</p> <p>On one hand, Conda installations tend to generate lots of small files (and then even more due to a linking strategy that does not work on Lustre). So they need to be containerised just for storage manageability.</p> <p>They also re-install lots of libraries that may already be on the system in a different version.  The isolation offered by a container environment may be a good idea to ensure that all software picks up the right versions.</p> </li> <li> <p>Another example where containers have proven to be useful on LUMI is to experiment with newer versions     of ROCm than we can offer on the system. </p> <p>This often comes with limitations though, as (a) that ROCm version is still limited by the drivers on the  system and (b) we've seen incompatibilities between newer ROCm versions and the Cray MPICH libraries.</p> </li> <li> <p>And a combination of both: LUST with the help of AMD have prepared some containers with popular AI applications.     These containers use some software from Conda, a newer ROCm version installed through RPMs, and some      performance-critical code that is compiled specifically for LUMI.</p> </li> </ul> <p>Remember though that whenever you use containers, you are the system administrator and not LUST. We can impossibly support all different software that users want to run in containers, and all possible Linux distributions they may want to run in those containers. We provide some advice on how to build a proper container, but if you chose to neglect it it is up to you to solve the problems that occur.</p>"},{"location":"intro-evolving/11_Containers/#managing-containers","title":"Managing containers","text":"<p>On LUMI, we currently support only one container runtime.</p> <p>Docker is not available, and will never be on the regular compute nodes as it requires elevated privileges to run the container which cannot be given safely to regular users of the system.</p> <p>Singularity is currently the only supported container runtime and is available on the login nodes and the compute nodes. It is a system command that is installed with the OS, so no module has to be loaded to enable it. We can also offer only a single version of singularity or its close cousin AppTainer  as singularity/AppTainer simply don't really like running multiple versions next to one another,  and currently the version that we offer is determined by what is offered by the OS.</p> <p>To work with containers on LUMI you will either need to pull the container from a container registry, e.g., DockerHub, or bring in the container by copying the singularity <code>.sif</code> file.</p> <p>Singularity does offer a command to pull in a Docker container and to convert it to singularity format. E.g., to pull a container for the Julia language from DockerHub, you'd use</p> <pre><code>singularity pull docker://julia\n</code></pre> <p>Singularity uses a single flat sif file for storing containers. The <code>singularity pull</code> command does the  conversion from Docker format to the singularity format.</p> <p>Singularity caches files during pull operations and that may leave a mess of files in the <code>.singularity</code> cache directory. This can lead to exhaustion of your disk quota for your home directory. So you may want to use the environment variable <code>SINGULARITY_CACHEDIR</code> to put the cache in, e.g,, your scratch space (but even then you want to clean up after the pull operation so save on your storage billing units).</p> Demo singularity pull <p>Let's try the <code>singularity pull docker://julia</code> command:</p> <p> </p> <p>We do get a lot of warnings but usually this is perfectly normal and usually they can be safely ignored.</p> <p> </p> <p>The process ends with the creation of the file <code>jula_latest.sif</code>. </p> <p>Note however that the process has left a considerable number of files in <code>~/.singularity</code> also:</p> <p> </p> <p></p> <p>There is currently limited support for building containers on LUMI and I do not expect that to change quickly. Container build strategies that require elevated privileges, and even those that require fakeroot or user namespaces, cannot be supported for security reasons (with user namespaces in particular a huge security concern as the Linux implementation is riddled with security issues).  Enabling features that are known to have had several serious security vulnerabilities in the recent past, or that themselves are unsecure by design and could allow users to do more on the system than a regular user should be able to do, will never be supported.</p> <p>So you should pull containers from a container repository, or build the container on your own workstation and then transfer it to LUMI.</p> <p>There is some support for building on top of an existing singularity container. We are also working on a number of base images to build upon, where the base images are tested with the OS kernel on LUMI.</p>"},{"location":"intro-evolving/11_Containers/#interacting-with-containers","title":"Interacting with containers","text":"<p>There are basically three ways to interact with containers.</p> <p>If you have the sif file already on the system you can enter the container with an interactive shell:</p> <pre><code>singularity shell container.sif\n</code></pre> Demo singularity shell <p> </p> <p>In this screenshot we checked the contents of the <code>/opt</code> directory before and after the <code>singularity shell julia_latest.sif</code> command. This shows that we are clearly in a different environment. Checking the <code>/etc/os-release</code> file only confirms this as LUMI runs SUSE Linux on the login nodes, not a version of Debian.</p> <p>The second way is to execute a command in the container with <code>singularity exec</code>. E.g., assuming the  container has the <code>uname</code> executable installed in it,</p> <pre><code>singularity exec container.sif uname -a\n</code></pre> Demo singularity exec <p> </p> <p>In this screenshot we execute the <code>uname -a</code> command before and with the <code>singularity exec julia_latest.sif</code> command. There are some slight differences in the output though the same kernel version is reported as the container uses the host kernel. Executing</p> <pre><code>singularity exec julia_latest.sif cat /etc/os-release\n</code></pre> <p>confirms though that the commands are executed in the container.</p> <p>The third option is often called running a container, which is done with singularity run:</p> <pre><code>singularity run container.sif\n</code></pre> <p>It does require the container to have a special script that tells singularity what  running a container means. You can check if it is present and what it does with <code>singularity inspect</code>: </p> <pre><code>singularity inspect --runscript container.sif\n</code></pre> Demo singularity run <p> </p> <p>In this screenshot we start the julia interface in the container using <code>singularity run</code>. The second command shows that the container indeed includes a script to tell singularity what <code>singularity run</code> should do.</p> <p>You want your container to be able to interact with the files in your account on the system. Singularity will automatically mount <code>$HOME</code>, <code>/tmp</code>, <code>/proc</code>, <code>/sys</code> and <code>dev</code> in the container, but this is not enough as your home directory on LUMI is small and only meant to be used for storing program settings, etc., and not as your main work directory. (And it is also not billed and therefore no extension is allowed.) Most of the time you want to be able to access files in your project directories in <code>/project</code>, <code>/scratch</code> or <code>/flash</code>, or maybe even in <code>/appl</code>. To do this you need to tell singularity to also mount these directories in the container, either using the  <code>--bind src1:dest1,src2:dest2</code>  flag or via the <code>SINGULARITY_BIND</code> or <code>SINGULARITY_BINDPATH</code> environment variables. E.g.,</p> <pre><code>export SINGULARITY_BIND='/pfs,/scratch,/projappl,/project,/flash'\n</code></pre> <p>will ensure that you have access to the scratch, project and flash directories of your project.</p>"},{"location":"intro-evolving/11_Containers/#running-containers-on-lumi","title":"Running containers on LUMI","text":"<p>Just as for other jobs, you need to use Slurm to run containers on the compute nodes.</p> <p>For MPI containers one should use <code>srun</code> to run the <code>singularity exec</code> command, e.g,,</p> <pre><code>srun singularity exec --bind ${BIND_ARGS} \\\n${CONTAINER_PATH} mp_mpi_binary ${APP_PARAMS}\n</code></pre> <p>(and replace the environment variables above with the proper bind arguments for <code>--bind</code>, container file and parameters for the command that you want to run in the container).</p> <p>On LUMI, the software that you run in the container should be compatible with Cray MPICH, i.e., use the MPICH ABI (currently Cray MPICH is based on MPICH 3.4). It is then possible to tell the container to use Cray MPICH (from outside the container) rather than the MPICH variant installed in the container, so that it can offer optimal performance on the LUMI SlingShot 11 interconnect.</p> <p>Open MPI containers are currently not well supported on LUMI and we do not recommend using them. We only have a partial solution for the CPU nodes that is not tested in all scenarios,  and on the GPU nodes Open MPI is very problematic at the moment. This is due to some design issues in the design of Open MPI, and also to some piece of software that recent versions of Open MPI require but that HPE only started supporting recently on Cray EX systems and that we haven't been able to fully test. Open MPI has a slight preference for the UCX communication library over the OFI libraries, and  currently full GPU support requires UCX. Moreover, binaries using Open MPI often use the so-called rpath linking process so that it becomes a lot harder to inject an Open MPI library that is installed elsewhere. The good news though is that the Open MPI developers of course also want Open MPI to work on biggest systems in the USA, and all three currently operating or planned exascale systems use the SlingShot 11 interconnect, so work is going on for better support for OFI and for full GPU support on systems that rely on OFI and do not support UCX.</p>"},{"location":"intro-evolving/11_Containers/#enhancements-to-the-environment","title":"Enhancements to the environment","text":"<p>To make life easier, LUST with the support of CSC did implement some modules that are either based on containers or help you run software with containers.</p>"},{"location":"intro-evolving/11_Containers/#bindings-for-singularity","title":"Bindings for singularity","text":"<p>The <code>singularity-bindings/system</code> module which can be installed via EasyBuild helps to set <code>SINGULARITY_BIND</code> and <code>SINGULARITY_LD_LIBRARY_PATH</code> to use  Cray MPICH. Figuring out those settings is tricky, and sometimes changes to the module are needed for a specific situation because of dependency conflicts between Cray MPICH and other software in the container, which is why we don't provide it in the standard software stacks but instead make it available as an EasyBuild recipe that you can adapt to your situation and install.</p> <p>As it needs to be installed through EasyBuild, it is really meant to be  used in the context of a LUMI software stack (so not in <code>CrayEnv</code>). To find the EasyConfig files, load the <code>EasyBuild-user</code> module and  run</p> <pre><code>eb --search singularity-bindings\n</code></pre> <p>You can also check the  page for the <code>singularity-bindings</code> in the LUMI Software Library.</p> <p>You may need to change the EasyConfig for your specific purpose though. E.g., the singularity command line option <code>--rocm</code> to import the ROCm installation from the system doesn't fully work (and in fact, as we have alternative ROCm versions on the system cannot work in all cases) but that can also be fixed by extending the <code>singularity-bindings</code> module  (or by just manually setting the proper environment variables).</p>"},{"location":"intro-evolving/11_Containers/#vnc-container","title":"VNC container","text":"<p>The second tool is a container that we provide with some bash functions to start a VNC server as temporary way to be able to use some GUI programs on LUMI until the final setup which will be based on Open OnDemand is ready (expected late 2023). It can be used in <code>CrayEnv</code> or in the LUMI stacks through the <code>lumi-vnc</code> module.  The container also contains a poor men's window manager (and yes, we know that there are sometimes some problems with fonts). It is possible to connect to the VNC server either through a regular VNC client on your PC or a web browser, but in both cases you'll have to create an ssh tunnel to access the server. Try</p> <pre><code>module help lumi-vnc\n</code></pre> <p>for more information on how to use <code>lumi-vnc</code>.</p>"},{"location":"intro-evolving/11_Containers/#cotainr-build-conda-containers-on-lumi","title":"cotainr: Build Conda containers on LUMI","text":"<p>The third tool is <code>cotainr</code>,  a tool developed by DeIC, the Danish partner in the LUMI consortium. It is a tool to pack a Conda installation into a container. It runs entirely in user space and doesn't need any special rights. (For the container specialists: It is based on the container sandbox idea to build containers in user space.)</p>"},{"location":"intro-evolving/11_Containers/#container-wrapper-for-python-packages-and-conda","title":"Container wrapper for Python packages and conda","text":"<p>The fourth tool is a container wrapper tool that users from Finland may also know as Tykky. It is a tool to wrap Python and conda installations in a container and then create wrapper scripts for the commands in the bin subdirectory so that for most practical use cases the commands can be used without directly using singularity commands.  Whereas cotainr fully exposes the container to users and its software is accessed through the regular singularity commands, Tykky tries to hide this complexity with wrapper scripts that take care of all bindings and calling singularity. On LUMI, it is provided by the <code>lumi-container-wrapper</code> module which is available in the <code>CrayEnv</code> environment and in the LUMI software stacks. It is also documented in the LUMI documentation.</p> <p>The basic idea is that you run the tool to either do a conda installation or an installation of Python packages from a file that defines the environment in either standard conda format (a Yaml file) or in the <code>requirements.txt</code> format used by <code>pip</code>. </p> <p>The container wrapper will then perform the installation in a work directory, create some wrapper commands in the <code>bin</code> subdirectory of the directory where you tell the container wrapper tool to do the installation, and it will use SquashFS to create as single file that contains the conda or Python installation.</p> <p>We do strongly recommend to use the container wrapper tool for larger conda and Python installation. We will not raise your file quota if it is to house such installation in your <code>/project</code> directory.</p> Demo lumi-container-wrapper <p>Create a subdirectory to experiment. In that subdirectory, create a file named <code>env.yml</code> with the content:</p> <pre><code>channels:\n  - conda-forge\ndependencies:\n  - python=3.8.8\n  - scipy\n  - nglview\n</code></pre> <p>and create an empty subdirectory <code>conda-cont-1</code>.</p> <p>Now you can follow the commands on the slides below:</p> <p> </p> <p>On the slide above we prepared the environment.</p> <p>Now lets run the command </p> <pre><code>conda-containerize new --prefix ./conda-cont-1 env.yml\n</code></pre> <p>and look at the output that scrolls over the screen. The screenshots don't show the full output as some parts of the screen get overwritten during the process:</p> <p> </p> <p>The tool will first build the conda installation in a tempororary work directory and also uses a base container for that purpose.</p> <p> </p> <p>The conda installation itself though is stored in a SquashFS file that is then used by the container.</p> <p> </p> <p> </p> <p>In the slide above we see the installation contains both a singularity container and a SquashFS file. They work together to get a working conda installation.</p> <p>The <code>bin</code> directory seems to contain the commands, but these are in fact scripts  that run those commands in the container with the SquashFS file system mounted in it.</p> <p> </p> <p>So as you can see above, we can simply use the <code>python3</code> command without realising what goes on behind the screen...</p> <p>The wrapper module also offers a pip-based command to build upon the Cray Python modules already present on the system</p>"},{"location":"intro-evolving/11_Containers/#pre-build-ai-containers","title":"Pre-build AI containers","text":"<p>This is work in progress and not yet available when this text was written.  The information below is preliminary. More information will follow.</p> <p></p> <p>LUST with the help of AMD is also building some containers with popular AI software. These containers contain a ROCm version that is appropriate for the software, use Conda for some components, but have several of the performance critical components built specifically for LUMI for near-optimal performance. Depending on the software they also contain a RCCL library with the appropriate plugin to work well on the Slingshot 11 interconnect, or a horovod compiled to use Cray MPICH. </p> <p>The containers are provided through a module which sets the <code>SINGULARITY_BIND</code> environment variable to ensure proper bindings (as they need, e.g., the libfabric library from the system and the proper \"CXI provider\" for libfabric to connect to the Slingshot interconnect). The module will also provide an environment variable to refer to the container (name with full path) to make it easy to refer to the container in job scripts.</p> <p>These containers can be found through the LUMI Software Library and are marked with a container label.</p>"},{"location":"intro-evolving/11_Containers/#conclusion-container-limitations-on-lumi","title":"Conclusion: Container limitations on LUMI","text":"<p>To conclude the information on using singularity containers on LUMI, we want to repeat the limitations:</p> <ul> <li> <p>Containers use the host's operating system kernel which is likely different and     may have different drivers and kernel extensions than your regular system.     This may cause the container to fail or run with poor performance.     Also, containers do not abstract the hardware unlike some virtual machine solutions.</p> </li> <li> <p>The LUMI hardware is almost certainly different from that of the systems on which     you may have used the container before and that may also cause problems.</p> <p>In particular a generic container may not offer sufficiently good support for the  SlingShot 11 interconnect on LUMI which requires OFI (libfabric) with the right  network provider (the so-called Cassini provider) for optimal performance. The software in the container may fall back to TCP sockets resulting in poor  performance and scalability for communication-heavy programs.</p> <p>For containers with an MPI implementation that follows the MPICH ABI the solution is often to tell it to use the Cray MPICH libraries from the system instead.</p> <p>Likewise, for containers for distributed AI, one may need to inject an appropriate RCCL plugin to fully use the SlingShot 11 interconnect.</p> </li> <li> <p>As containers rely on drivers in the kernel of the host OS, the AMD driver may also     cause problems. AMD only guarantees compatibility of the driver with two minor versions     before and after the ROCm release for which the driver was meant. Hence containers      using a very old version of ROCm or a very new version compared to what is available     on LUMI, may not always work as expected.</p> </li> <li> <p>The support for building containers on LUMI is currently very limited due to security     concerns. Any build process that requires elevated privileges, fakeroot or user namespaces     will not work.</p> </li> </ul>"},{"location":"intro-evolving/12_Support/","title":"How to get support and documentation","text":""},{"location":"intro-evolving/12_Support/#distributed-nature-of-lumi-support","title":"Distributed nature of LUMI support","text":"<p>User support for LUMI comes from several parties. Unfortunately, as every participating consortium countries has some responsibilities also and solves things differently, there is no central point where you can go with all questions.</p> <p>Resource allocators work independently from each other and the central LUMI User Support Team. This also  implies that they are the only ones who can help you with questions regarding your allocation: How to apply for compute time on LUMI, add users to your project, running out of resources (billing units) for your project, failure to even get access to the portal managing the allocations given by your resource allocator (e.g., because you let expire an invite), ...  For projects allocated in the Belgian allocation, help can be requested via the email address lumi-be-support@enccb.be. However, we cannot help you with similar problems for compute time directly  obtained via EuroHPC. For granted EuroHPC projects, support is available via lumi-customer-accounts@csc.fi.</p> <p>The central LUMI User Support Team (LUST) offers L1 and basic L2 support.  Given that the LUST team is very small compared to the number of project granted annually on LUMI  (roughly 10 FTE for on the order of 700 projects per year, and support is not their only task), it is clear that the amount of support they can give is limited.  E.g., don't expect them to install all software you request for them. There is simply too much software and too much software with badly written install code to do that with that number of people. Nor should you expect domain expertise from them. Though several members of the LUST have been scientist before, it does not mean that they can understand all scientific problems thrown at them or all codes used by users. Also, the team cannot fix bugs for you in the codes that you use, and usually not in the system code either. For fixing bugs in HPE or AMD-provided software, they are backed by a team of experts from those companies. However, fixing bugs in compilers or libraries  and implementing those changes on the system takes time. The system software on a big shared machine cannot be upgraded as easily as on a personal workstation. Usually you will have to look for workarounds, or if they show up in a preparatory project, postpone applying for an allocation until all problems are fixed.</p> <p>In Flanders, the VSC has a Tier-0 support project to offer more advanced L2 and some L3 support. The project unfortunately is not yet fully staffed. VSC Tier-0 support can be contacted via the LUMI-BE help desk at lumi-be-support@enccb.be (the same help desk that you need to contact for allocation problems).</p> <p>In the Walloon region, there is some limited advanced support via Orian Louant. However, this is only a part of all his tasks. Here also the lumi-be-support@enccb.be mail address can be used.</p> <p>EuroHPC has also granted the EPICURE project that starts in February 2024 to set up a network for advanced L2 and L3 support across EuroHPC centres. Belgium also participates in that project as a partner in the LUMI consortium. However, this project is also so small that it will have to select the problems they tackle.</p> <p>In principle the EuroHPC Centres of Excellence should also play a role in porting some applications in their field of expertise and offer some support and training, but so far especially the support and training are not yet what one would like to have.</p> <p>Basically given the growing complexity of scientific computing and diversity in the software field, what one needs is the equivalent of the \"lab technician\" that many experimental groups have who can then work with  various support instances, a so-called Research Software Engineer...</p>"},{"location":"intro-evolving/12_Support/#support-level-0-help-yourself","title":"Support level 0: Help yourself!","text":"<p>Support starts with taking responsibility yourself and use the available sources of information before contacting support. Support is not meant to be a search assistant for already available  information.</p> <p>The LUMI User Support Team has prepared trainings and a lot of documentation about LUMI. Good software packages also come with documentation, and usually it is possible to find trainings for  major packages. And a support team is also not there to solve communication problems in the  team in which you collaborate on a project!</p>"},{"location":"intro-evolving/12_Support/#take-a-training","title":"Take a training!","text":"<p>There exist system-specific and application-specific trainings.  Ideally of course a user would want a one-step solution, having a specific training for a specific application on a specific system (and preferably with the workflow tools they will be using, if any), but that is simply not possible. The group that would be interested in such a training is for most packages too small, and it is nearly impossible to find suitable teachers for such course given the amount of expertise that is needed in both the specific application and the specific system. It would also be hard to repeat such a training with a high enough frequency to deal with the continuous inflow of new users.</p> <p>The LUMI User Support Team organises 2 system-specific trainings:</p> <ol> <li> <p>There is a 1-day introductory course entirely given by members of the LUST.     The training does assume familiarity with HPC systems, e.g., obtained from the introductory     courses taught by VSC and     C\u00c9CI.</p> </li> <li> <p>And there is a 4-day comprehensive training with more attention on how to run efficiently, and on the     development and profiling tools. Even if you are not a developer, you may benefit from more knowledge     about these tools as especially a profiler can give you insight in why your application does not run     as expected.</p> </li> </ol> <p>This training builds upon the 1-day LUMI training offered by the LUST, but has been enriched with  links to the situation specifically in Belgium.</p> <p>Application-specific trainings should come from other instances though that have the necessary domain knowledge: Groups that develop the applications, user groups, the EuroHPC Centres of Excellence, ...</p> <p>Currently the training landscape in Europe is not too well organised. EuroHPC is starting some new training initiatives to succeed the excellent PRACE trainings. Moreover, the centre coordinating the National Competence Centres also  tries to maintain an overview of available trainings (and several National Competence Centres organise trainings open to others also).</p>"},{"location":"intro-evolving/12_Support/#readsearch-the-documentation","title":"Read/search the documentation","text":"<p>The LUST has developed extensive documentation for LUMI. That documentation is split in two parts:</p> <ol> <li> <p>The main documentation at docs.lumi-supercomputer.eu     covers the LUMI system itself and includes topics such as how to get on the      system, where to place your files, how to start jobs, how to use the programming environment,     how to install software, etc.</p> </li> <li> <p>The LUMI Software Library contains     an overview of software pre-installed on LUMI or for which we have install recipes to start from.     For some software packages, it also contains additional information on how to use the software     on LUMI.</p> <p>That part of the documentation is generated automatically from information in the various repositories that are used to manage those installation recipes. It is kept deliberately separate, partly to have a more focused search in both documentation systems and partly because it is managed and updated very differently.</p> </li> </ol> <p>Both documentation systems contain a search box which may help you find pages if you cannot find them  easily navigating the documentation structure. E.g., you may use the search box in the LUMI Software Library to search for a specific package as it may be bundled with other packages in a single module with a  different name. E.g., try searching for the <code>htop</code> command.</p>"},{"location":"intro-evolving/12_Support/#talk-to-your-colleagues","title":"Talk to your colleagues","text":"<p>A LUMI project is meant to correspond to a coherent research project in which usually multiple people collaborate. </p> <p>This implies that your colleagues may have run in the same problem and may already have a solution, or they didn't even experience it as a problem and know how to do it. So talk to your colleagues first.</p> <p>Support teams are not meant to deal with your team communication problems. There is nothing worse than having the same question asked multiple times from different people in the same project. As a project does not have a dedicated support engineer, the second time a question is asked it may land at a different person in the support team so that it is not recognized that the question has been asked already and the answer is readily available, resulting in a loss of time for the support team and other, maybe more important questions, remaining unanswered.  Similarly bad is contacting multiple help desks with the same question without telling them, as that will also duplicate efforts to solve a question. We've seen it often that users contact both a local help desk and the LUST help desk without telling.</p> <p>Resources on LUMI are managed on a project basis, not on a user-in-project basis, so if you want to know what other users in the same project are doing with the resources, you have to talk to them and not to the LUST. We do not have systems in place to monitor use on a per-user, per-project basis, only on a per-project basis, and also have no plans to develop such tools as a project is meant to be a close collaboration of all involved users. </p>"},{"location":"intro-evolving/12_Support/#l1-and-basic-l2-support-lust","title":"L1 and basic L2 support: LUST","text":"<p>The LUMI User Support Team is responsible for providing L1 and basic L2 support to users of the system. Their work starts from the moment that you have userid on LUMI (the local RA is responsible for ensuring that you get a userid when a project has been assigned).</p> <p>The LUST is a distributed team roughly 10 FTE strong, with people in all LUMI consortium countries, but they work as a team, coordinated by CSC. The Belgian contribution currently consists of two people both working half time for LUMI and half time for user support in their own organisation (VSC and C\u00c9CI). However, you will not necessarily be helped by one of the Belgian team members when you contact LUST, but by the team member who is most familiar with your problem. </p> <p>There are some problems that we need to pass on to HPE or AMD, particularly if it may be caused by  bugs in system software, but also because they have more experts with in-depth knowledge of very specific tools. </p> <p>The LUMI help desk is staffed from Monday till Friday between 8am and 6pm Brussels time (except on public holidays in Finland). You can expect a same day first response if your support query is well formulated and submitted long enough before closing time, but a full solution of your problem may of course take longer, depending on how busy the help desk is and the complexity of the problem.</p> <p>Data security on LUMI is very important. Some LUMI projects may host confidential data, and especially industrial LUMI users may have big concerns about who can access their data.  Therefore only very, very few people on LUMI have the necessary rights to access user data on the system, and those people even went through a background check. The LUST members do not have that level of access, so we cannot see your data and you will have to pass all relevant information to the LUST through other means!</p> <p>The LUST help desk should be contacted through  web forms in the \"User Support - Contact Us\" section of the main LUMI web site. The page is also linked from the \"Help Desk\" page in the LUMI documentation. These forms help you to provide more information that we need to deal with your support request. Please do not email directly to the support web address (that you will know as soon as we answer at ticket as that is done through e-mail). Also, separate issues should go in separate tickets so that separate people in the LUST can deal with them, and you should not reopen an old ticket for a new question, also because then only the person who dealt with the previous ticket gets notified, and they may be on vacation or even not work for LUMI anymore, so your new request may remain unnoticed for a long time.</p>"},{"location":"intro-evolving/12_Support/#tips-for-writing-good-tickets-that-we-can-answer-promptly","title":"Tips for writing good tickets that we can answer promptly","text":""},{"location":"intro-evolving/12_Support/#how-not-to-write-a-ticket","title":"How not to write a ticket","text":"<ul> <li> <p>Use a meaningful subject line. All we see in the ticket overview is a number and the subject     line, so we need to find back a ticket we're working on based on that information alone.</p> <p>Yes, we have a user on LUMI who managed to send 8 tickets in a short time with the subject line \"Geoscience\" but 8 rather different problems...</p> <p>Hints: </p> <ul> <li>For common problems, including your name in the subject may be a good idea.</li> <li>For software problems, including the name of the package helps a lot. So not     \"Missing software\" but \"Need help installing QuTiP 4.3.1 on CPU\".     Or not \"Program crashes\" but \"UppASD returns an MPI error when using more than 1000 ranks\".</li> </ul> </li> <li> <p>Be accurate when describing the problem. Support staff members are not clairvoyants with     mysterious superpowers who can read your mind across the internet. </p> <p>We'll discuss this a bit more further in this lecture.</p> </li> <li> <p>If you have no time to work with us on the problem yourself, then tell so.</p> <p>Note: The priorities added to the ticket are currently rather confusing. You have three choices in the forms: \"It affects severely my work\", \"It is annoying, but I can work\", and  \"I can continue to work normally\", which map to \"high\", \"medium\" and \"low\".  So tickets are very easily marked as high priority because you cannot work on LUMI, even though you have so much other work to do that it is really not that urgent or that you  don't even have time to answer quickly.</p> </li> </ul> <p>The improved version could be something like this:</p> <p></p> <ul> <li> <p>The subject line makes it stand out as the person doing login tickets can quickly find back     the ticket.</p> </li> <li> <p>There is already a lot of useful information in the ticket:</p> <ul> <li> <p>When did the user first notice the problem, and has it worked before (and when)?</p> </li> <li> <p>The user clearly tried to check if anything has changed on their side, and in this case at least     the configuration files of the ssh client have remained unchanged.</p> </li> <li> <p>We know what client the user is using and luckily it is a standard one so that we know ourselves     how to use it (apart from local permission problems, but since it still works on the local cluster,     permissions on files in the local .ssh directory cannot be the issue here). Hence we can tell the user     how to gather more information. (Unfortunately, some users use exotic ssh clients that we cannot even     have access to without taking licenses, but expect us to be able to support them...)</p> </li> <li> <p>It is also useful to know that it is not a one-time hiccup.</p> </li> </ul> </li> </ul>"},{"location":"intro-evolving/12_Support/#how-to-write-tickets","title":"How to write tickets","text":""},{"location":"intro-evolving/12_Support/#1-ticket-1-issue-1-ticket","title":"1 ticket = 1 issue = 1 ticket","text":"<ul> <li> <p>If you have multiple unrelated issues, submit them as multiple tickets. In a support team, each member     has their own specialisation so different issues may end up with different people. Tickets need to be     assigned to people who will deal with the problem, and it becomes very inefficient if multiple people      have to work on different parts of the ticket simultaneously.</p> <p>Moreover, the communication in a ticket will also become very confusing if multiple issues are discussed simultaneously.</p> </li> <li> <p>Conversely, don't submit multiple tickets for a single issue just because you are too lazy to     look for the previous e-mail if you haven't been able to do your part of the work for some days.     If you've really lost the email, at least tell us that it is related to a previous ticket so that     we can try to find it back.</p> <p>So keep the emails you get from the help desk to reply!</p> </li> <li> <p>Avoid reusing exactly the same subject line. Surely there must be something different for the new     problem?</p> </li> <li> <p>Avoid reopening old tickets that have been closed long ago.</p> <p>If you get a message that a ticket has been closed (basically because there has been no reply for several weeks so we assume the issue is not relevant anymore) and you feel it should not have been closed, reply immediately.</p> <p>When you reply to a closed ticket and the person who did the ticket is not around (e.g., on vacation or left the help desk team), your reply may get unnoticed for weeks. Closed tickets are not passed to a colleague when we go on a holiday or leave.</p> </li> <li> <p>Certainly do not reopen old tickets with new issues. Apart from the fact that the person who did     the ticket before may not be around, they may also have no time to deal with the ticket quickly     or may not even be the right person to deal with it.</p> </li> </ul>"},{"location":"intro-evolving/12_Support/#the-subject-line-is-important","title":"The subject line is important!","text":"<ul> <li> <p>The support team has two identifiers in your ticket: Your mail address and the subject that you      specified in the form (LUST help desk) or email (LUMI-BE help desk). So:</p> <ul> <li> <p>Use consistently the same mail address for tickets. This helps us locate previous requests from     you and hence can give us more background about what you are trying to do. </p> <p>The help desk is a professional service, and you use LUMI for professional work, so use your company or university mail address and not some private one.</p> </li> <li> <p>Make sure your subject line is already descriptive and likely unique in our system.</p> <p>We use the subject line to distinguish between tickets we're dealing with so make sure that it can easily be distinguished from others and is easy to find back.</p> </li> </ul> </li> <li> <p>So include relevant keywords in your subject, e.g.,</p> <ul> <li> <p>The userid you were using and the way of logging in to the system for login problems.</p> </li> <li> <p>Name of software packages for software installations or crashes.</p> </li> </ul> </li> </ul> <p>Some proper examples are</p> <ul> <li> <p>User abcdefgh cannot log in via web interface</p> <p>So we know we may have to pass this to our Open OnDemand experts, and your userid makes the message likely unique. Moreover, after looking into account databases etc., we can immediately find back the ticket as the userid is in the subject.</p> </li> <li> <p>ICON installation needs libxml2</p> </li> <li> <p>VASP produces MPI error message when using more than 1024 ranks</p> </li> </ul>"},{"location":"intro-evolving/12_Support/#think-with-us","title":"Think with us","text":"<ul> <li> <p>Provide enough information for us to understand who you are:</p> <ul> <li> <p>Name: and the name as we would see it on the system, not some nickname.</p> </li> <li> <p>Userid: Important especially for login problems.</p> </li> <li> <p>Project number:</p> <ul> <li> <p>When talking to the LUST: they don't know EuroHPC or LUMI-BE project numbers, only     the 465xxxxxx numbers, and that is what they need.</p> </li> <li> <p>When talking to LUMI-BE: for LUMI-BE projects, the LUMI-BE project number may be useful.     It is the ticket number of you project submission. Not all LUMI-BE help team members have     access to the data associated with your LUMI project number 465xxxxxx, but some have and can      use this to diagnose some problems.</p> </li> </ul> </li> </ul> </li> <li> <p>For login and data transfer problems, your client environment is often also important to diagnose     the problem. </p> <ul> <li> <p>This does include your geographical location. Doing things from work may have different issues     then doing things from your home connection, and being abroad may be part of the problem!</p> <ul> <li>Extreme example: Iran blocks most encrypted internet traffic going abroad.     This is an extreme example though as you are not allowed to use LUMI while being in Iran as     it breaks the US export restrictions and hence the LUMI conditions of use.</li> </ul> </li> </ul> </li> <li> <p>What software are you using, and how was it installed or where did you find it?</p> <p>We know that certain installation procedures (e.g., simply downloading a binary) may cause certain problems on LUMI. Also, there are some software installations on LUMI for which neither LUST nor the LUMI-BE help desk is responsible, so we need to direct to to their support instances when problems occur that are likely related to that software.</p> </li> <li> <p>Describe your environment (though experience learns that some errors are caused by users     not even remembering they've changed things while those changes can cause problems)</p> <ul> <li> <p>Which modules are you using?</p> </li> <li> <p>Do you have special stuff in <code>.bashrc</code> or <code>.profile</code>? </p> </li> <li> <p>For problems with running jobs, the batch script that you are using can be very useful.</p> </li> </ul> </li> <li> <p>Describe what worked so far, and if it ever worked: when? E.g., was this before a system update?</p> <p>The LUST has had tickets were a user told that something worked before but as we questioned further it was long ago before a system update that we know broke some things that affects some programs...</p> </li> <li> <p>What did you change since then? Think carefully about that. When something worked some time ago but     doesn't work know the cause is very often something you changed as a user and not something going on     on the system.</p> </li> <li> <p>What did you already try to solve the problem?</p> </li> <li> <p>How can we reproduce the problem? A simple and quick reproducer speeds up the time to answer your ticket.     Conversely, if it takes a 24-hour run on 256 nodes to see the problem it is very, very likely that the      support team cannot help you.</p> <p>Moreover, if you are using licensed software with a license that does not cover the support team members, usually we cannot do much for you. Neither the LUMI-BE help desk team nor the LUST will knowingly violate software licenses only to solve your problems!</p> </li> <li> <p>The LUST and LUMI-BE help desk members know a lot about LUMI but they are (usually) not researchers in     your field so cannot help you with problems that require domain knowledge in your field. We can impossibly     know all software packages and tell you how to use them (and, e.g., correct errors in your input files).</p> <p>You as a user should be the domain expert, and since you are doing computational science, somewhat  multidisciplinary and know something about both the \"computational\" and the \"science\".</p> <p>We as the support team should be the expert in the \"computational\". Some of us where researchers in the past so have some domain knowledge about a the specific subfield we were working in, but there are simply too many scientific domains and subdomains to have full coverage of that in a central support team for a generic infrastructure.</p> <p>We do see that lots of crashes and performance problems with software are in fact caused by wrong use of the package!</p> <p>However, some users expect that we understand the science they are doing, find the errors in their model and run that on LUMI, preferably by the evening they submitted the ticket. If we could do that, then we could basically make a Ph.D that usually takes 4 years in 4 weeks and wouldn't need users anymore as it would be more fun to produce the science that our funding agencies expect ourselves.</p> </li> <li> <p>The LUST and LUMI-BE help desk members know a lot about LUMI but cannot know or solve everything and     may need to pass your problem to other instances, and in particular HPE or AMD.</p> <p>Debugging system software is not the task of the LUMI-BE help desk nor of the LUST.  Issues with compilers or libraries can only be solved by those instances that produce those compilers or libraries, and this takes time.</p> <p>We have a way of working that enables us to quickly let users test changes to software in the user software stack by making user installations relatively easy and reproducible using EasyBuild, but changing the  software installed in the system images - which includes the Cray programming environment - where changes  have an effect on how the system runs and can affect all users, are non-trivial and many of those changes can only be made during maintenance breaks.</p> </li> </ul>"},{"location":"intro-evolving/12_Support/#beware-of-the-xy-problem","title":"Beware of the XY-problem!","text":"<p>Partly quoting from xyproblem.info:  Users are often tempted to ask questions about the solution they have in mind and where they got stuck, while it may actually be the wrong solution to the actual problem. As a result one can waste a lot of time attempting to get the solution they have in mind to work, while at the end it turns out that that solution does not work. It goes as follows:</p> <ol> <li>The user wants to do X.</li> <li>The user doesn't really know how to do X. However, they think that doing Y first would     be a good step towards solving X.</li> <li>But the user doesn't really know how to do Y either and gets stuck there too.</li> <li>So the user contacts the help desk to help with solving problem Y.</li> <li>The help desk tries to help with solving Y, but is confused because Y seems a very strange and     unusual problem to solve.</li> <li>Once Y is solved with the help of the help desk, the user is still stuck and cannot solve X yet.</li> <li>User contacts the help desk again for further help and it turns out that Y wasn't needed in the      first place as it is not part of a suitable solution for X.</li> </ol> <p>Or as one of the colleagues of the author of these notes says: \"Often the help desk knows the solution, but doesn't know the problem so cannot give the solution.\"</p> <p>To prevent this, you as a user has to be complete in your description:</p> <ol> <li> <p>Give the broader problem and intent (so X), not just the small problem (Y) on which you got stuck.</p> </li> <li> <p>Promptly provide information when the help desk asks you, even if you think that information is     irrelevant. The help desk team member may have a very different look on the problem and come up     with a solution that you couldn't think of, and you may be too focused on the solution that you have     in mind to see a better solution.</p> </li> <li> <p>Being complete also means that if you ruled out some solutions, share with the help desk why you ruled     them out as it can help the help desk team member to understand what you really want.</p> </li> </ol> <p>After all, if your analysis of your problem was fully correct, you wouldn't need to ask for help, don't you?</p>"},{"location":"intro-evolving/12_Support/#what-support-can-we-offer","title":"What support can we offer?","text":""},{"location":"intro-evolving/12_Support/#restrictions","title":"Restrictions","text":"<p>Contrary to what you may be familiar with from your local Tier-2 system and support staff, team members of the LUMI help desks have no elevated privileges. This holds for both the LUST and LUMI-BE help desk.</p> <p>As a result,</p> <ul> <li> <p>We cannot access user files.     A specific person of the LUMI-BE help desk can access your project, scratch and flash folders     if you make them part of the project. This requires a few steps and therefore is only done     for a longer collaboration between a LUMI project and that help desk member.     The LUST members don't do that.</p> </li> <li> <p>help desk team members cannot install or modify system packages or settings. </p> <p>A good sysadmin usually wouldn't do so either. You are working on a multi-user system and you  have to take into account that any change that is beneficial for you, may have adverse effects for other users or for the system as a whole.</p> <p>E.g., installing additional software in the images takes away from the available memory on each node, slows down the system boot slightly, and can conflict with software that is installed through other ways.</p> </li> <li> <p>The help desk cannot extend the walltime of jobs.</p> <p>Requests are never granted, even not if the extended wall time would still be within the limits of the partition. </p> </li> <li> <p>The LUST is in close contact with the sysadmins, but as the sysadmins are very busy people they will     not promptly deal with any problem. Any problem though endangering the stability of the system gets a     high priority.</p> </li> <li> <p>The help desk does not monitor running jobs. Sysadmins monitor the general health of the system, but will      not try to pick out inefficient jobs unless the job does something that has a very negative effect on     the system.</p> </li> </ul>"},{"location":"intro-evolving/12_Support/#what-support-can-and-cannot-do","title":"What support can and cannot do","text":"<ul> <li> <p>The LUST and LUMI-BE help desk do not replace a good introductory HPC course nor are they a      search engine for the documentation. L0 support is the responsibility of every user.</p> </li> <li> <p>Resource allocators are responsible for the first steps in getting a project and userid on     LUMI. For LUMI-BE project this support is offered through the LUMI-BE help desk at      lumi-be-support@enccb.be and for EuroHPC projects the support is offered through CSC,     the operator of LUMI, at lumi-customer-accounts@csc.fi.</p> <p>Once your project is created and accepted (and the resource allocator can confirm that you properly accepted the invitation), support for account problems (in particular login problems)  moves to the LUST.</p> </li> <li> <p>If you run out of block or file quota, the LUST can increase your quota within      the limits specified in the LUMI documentation.</p> <p>If you run out of billing units for compute or storage, only the instance that granted your project can help you, so contact the LUMI-BE help desk for LUMI-BE projects and CSC for EuroHPC projects on the email addresses mentioned above.</p> <p>Projects cannot be extended past one year unless the granting instance is willing to take a  charge on the annual budget for the remaining billing units.</p> <p>As a rule, for LUMI-BE projects we will not increase the CPU or GPU billing units. However, as  for now we have plenty of storage billing units compared to demand and as we understand that it  takes time for the user community to get used to working with storage billing units, we're  rather flexible in increasing those.</p> </li> <li> <p>The LUST cannot do much complete software installations but often can give useful advice and     do some of the work.</p> <p>In LUMI-BE we have some resources, though mostly for VSC users.</p> <p>Note however that the LUST or LUMI-BE team may not even be allowed to help you due to software license restrictions. Similarly, the LUMI-BE team can only produce an install recipe if we have no access to your project, and though the LUST has technically speaking a zone where they can install software on the system, this is only done for software that the LUST can properly support and that is of interest to a wide enough audience. It is also not done for software where many users may want a specifically customised installation. Neither is it done for software that LUST cannot sufficiently test themselves.</p> </li> <li> <p>Both the LUST and LUMI-BE team can help with questions regarding compute and storage use.     LUST provides L1 and basic L2 support. These are basically problems that can solved in hours rather than     days or weeks. LUMI-BE has some resources to offer more advanced L2 and some L3 support, but      most of these resources are for users eligible for VSC-use only currently.</p> </li> <li> <p>The LUST can help with analysing the source of crashes or poor performance (with the emphasis on      help as they rarely have all the application knowledge required to dig deep).     There are some resources within LUMI-BE also, again mostly on the VSC side.</p> </li> <li> <p>However, neither LUST nor the LUMI-BE help desk is a debugging service (though of course we do     take responsibility for code that we developed)</p> </li> <li> <p>The LUST has some resources for work on porting and optimising codes to/for AMD GPUs     via porting calls and hackathons respectively. </p> <p>VSC also has some resources, but not the same level of access to experts from HPE or AMD to assist.</p> </li> <li> <p>Neither help desk can do your science or solve your science problems though.</p> </li> </ul> <p>Remember:</p> <p>\"Supercomputer support is there to  support you in the computational aspects of your work  related to the supercomputer but not to take over your work.\"</p> <p>Any support will always be a collaboration where you may have to do most of the work. Supercomputer support services are not a free replacement of the lab assistant that many experimental groups have.</p>"},{"location":"intro-evolving/12_Support/#links","title":"Links","text":"<ul> <li> <p>LUMI web sites</p> <ul> <li> <p>The main LUMI web site contains very general information about     LUMI and also has a section in which the trainings organised by the LUST and some other trainings     are announced. It is also the starting point to contact the LUST with your support questions via     web forms. The web forms assure that we have the necessary information to start investigating your      issues.</p> <p>Note that when the support form asks for a project number, this is the project number on LUMI  (of the form 465XXXXXX for most projects) and not the project number used in LUMI-BE or EuroHPC, as the LUST does not know these numbers.</p> </li> <li> <p>The LUMI documentation     covers the LUMI system itself and includes topics such as how to get on the      system, where to place your files, how to start jobs, how to use the programming environment,     how to install software, etc.</p> <p>This is your primary source of information when you are investigating if LUMI might be suitable for you, or once you have obtained a project on LUMI.</p> </li> <li> <p>The LUMI Software Library contains     an overview of software pre-installed on LUMI or for which we have install recipes to start from.     For some software packages, it also contains additional information on how to use the software     on LUMI.</p> </li> </ul> </li> <li> <p>Web sites in Belgium:</p> <ul> <li> <p>The EuroCC Belgium web site announced most local and LUST LUMI trainings     in the \"Trainings\" section     and also contains information on      how to apply for compute time on LUMI via the Belgian share.</p> </li> <li> <p>The VSC web site.     Several VSC trainings are also relevant for (future) LUMI users!</p> <p>The VSC Supercomputers for Starters course lectured at UAntwerpen also covers several topics that are even more relevant to LUMI than to the Tier-2 systems of VSC and C\u00c9CI.  Full course notes are available, so you can have a look at the material if you have no time to join the lectures (twice a year).</p> </li> <li> <p>The C\u00c9CI web site.     Several C\u00c9CI trainings are also relevant for (future) LUMI users!</p> </li> </ul> </li> <li> <p>These course notes also contain a     page with links into technical documentation     of the scheduler and the programming environments on LUMI, and links to the user documentation     of some similar systems.</p> </li> </ul>"},{"location":"intro-evolving/A01_Slurm_issues/","title":"Slurm issues on LUMI","text":"<p>Note: Use <code>sbatch --version</code> to check the version of Slurm.</p>"},{"location":"intro-evolving/A01_Slurm_issues/#wrong-allocations-on-small-g-when-requesting-1-cpu-per-task","title":"Wrong allocations on small-g when requesting 1 CPU per task","text":"<p>Observed on Slurm 22.05.8.</p> <p>When requesting a GPU allocation requesting only 1 CPU per task with <code>--cpus-per-task=1</code> and requesting GPUs with <code>--gpus-per-task</code>, we get invalid allocations at least when  the job has to span multiple nodes. The problems disappear as soon as a value larger than 1 is used for <code>--cpus-per-task</code>.</p> Sample job script showing the bug <p> <pre><code>#! /bin/bash\n#SBATCH --job-name=map-smallg-1gpt-error\n#SBATCH --output %x-%j.txt\n#SBATCH --partition=small-g\n#SBATCH --ntasks=12\n#SBATCH --cpus-per-task=1\n#SBATCH --gpus-per-task=1\n#SBATCH --hint=nomultithread\n#SBATCH --time=5:00\n\nmodule load LUMI/22.12 partition/G lumi-CPEtools/1.1-cpeCray-22.12\n\necho \"Requested resources as reported through SLURM_ variables:\"\necho \"- SLURM_NTASKS: $SLURM_NTASKS\"\necho \"- SLURM_CPUS_PER_TASK: $SLURM_CPUS_PER_TASK\"\necho \"- SLURM_GPUS_PER_TASK: $SLURM_GPUS_PER_TASK\"\necho \"Distribution based on SLURM_ variables:\"\necho \"- SLURM_JOB_NUM_NODES: $SLURM_JOB_NUM_NODES\"\necho \"- SLURM_JOB_NODELIST: $SLURM_JOB_NODELIST\"\necho \"- SLURM_TASKS_PER_NODE: $SLURM_TASKS_PER_NODE\"\necho \"- SLURM_JOB_CPUS_PER_NODE: $SLURM_JOB_CPUS_PER_NODE\"\necho\necho \"Control: All SLURM_ and SRUN_ variables:\"\nenv | egrep ^SLURM_\nenv | egrep ^SRUN_\necho\necho -e \"Control: Job script\\n\\n======================================================\"\ncat $0\necho -e \"======================================================\\n\"\n\nset -x\nsrun -n $SLURM_NTASKS -c $SLURM_CPUS_PER_TASK --gpus-per-task=$SLURM_GPUS_PER_TASK gpu_check -l\nset +x\n\n/bin/rm -f select_gpu_$SLURM_JOB_ID echo_dev_$SLURM_JOB_ID\n</code></pre></p> <p>Workaround: Use a value larger than 1 for <code>--cpus-per-task</code>. A pure MPI application will not use the additional CPU cores, but since in an ideal case a CCD should not be used by more than 1 task unless the GPU is also shared by multiple tasks.</p>"},{"location":"intro-evolving/A02_Documentation/","title":"Documentation links","text":"<p>Note that documentation, and especially web based documentation, is very fluid. Links change rapidly and were correct when this page was developed right after the course. However, there is no guarantee that they are still correct when you read this and will only be updated at the next course on the pages of that course.</p> <p>This documentation page is far from complete but bundles a lot of links mentioned during the presentations, and some more.</p>"},{"location":"intro-evolving/A02_Documentation/#web-documentation","title":"Web documentation","text":"<ul> <li> <p>Slurm version 22.05.10, on the system at the time of the course</p> </li> <li> <p>HPE Cray Programming Environment web documentation has only become available in     May 2023 and is a work-in-progress. It does contain a lot of HTML-processed man pages in an easier-to-browse     format than the man pages on the system.</p> <p>The presentations on debugging and profiling tools referred a lot to pages that can be found on this web site. The manual pages mentioned in those presentations are also in the web documentation and are the easiest way to access that documentation.</p> </li> <li> <p>Cray PE Github account with whitepapers and some documentation.</p> </li> <li> <p>Cray DSMML - Distributed Symmetric Memory Management Library</p> </li> <li> <p>Cray Library previously provides as TPSL build instructions</p> </li> <li> <p>Clang latest version documentation (Usually for the latest version)</p> <ul> <li> <p>Clang 13.0.0 version (basis for aocc/3.2.0)</p> </li> <li> <p>Clang 14.0.0 version (basis for rocm/5.2.3 and amd/5.2.3)</p> </li> <li> <p>Clang 15.0.0 version (cce/15.0.0 and cce/15.0.1 in 22.12/23.03)</p> </li> <li> <p>Clang 16.0.0 version (cce/16.0.0 in 23.09)</p> </li> </ul> </li> <li> <p>AMD Developer Information</p> <ul> <li> <p>AOCC 4.0 Compiler Options Quick Reference Guide      (Version 4.0 compilers will come when the 23.05 or later CPE release gets installed on LUMI)</p> </li> <li> <p>AOCC 4.0 User Guide</p> </li> </ul> </li> <li> <p>ROCm<sup>TM</sup> documentation overview</p> <ul> <li> <p>rocminfo application for reporting system info.</p> </li> <li> <p>rocm-smi</p> </li> <li> <p>HIP porting guide</p> </li> <li> <p>ROCm Software Platform GitHub repository</p> </li> <li> <p>Libraries:</p> <ul> <li> <p>BLAS: rocBLAS and hipBLAS</p> </li> <li> <p>FFTs: rocFFT and hipFFT</p> </li> <li> <p>Random number generation: rocRAND</p> </li> <li> <p>Sparse linear algebra: rocSPARSE and hipSPARSE</p> </li> <li> <p>Iterative solvers: rocALUTION</p> </li> <li> <p>Parallel primitives: rocPRIM and hipCUB</p> </li> <li> <p>Machine Learning Libraries: MIOpen (similar to cuDNN),      Tensile (GEMM Autotuner),     RCCL (ROCm analogue of NCCL) and      Horovod (Distributed ML)</p> </li> <li> <p>Machine Learning Frameworks: Tensorflow,     Pytorch and     Caffe</p> </li> <li> <p>Machine Learning Benchmarks:     DeepBench and      MLPerf</p> </li> </ul> </li> <li> <p>Development tools:</p> <ul> <li> <p>rocgdb resources:</p> <ul> <li> <p>AMD documentation</p> </li> <li> <p>2021 presentation by Justin Chang</p> </li> <li> <p>2021 Linux Plumbers Conference presentation     with youTube video with a part of the presentation</p> </li> </ul> </li> <li> <p>rocprof profiler</p> </li> <li> <p>OmniTrace</p> </li> <li> <p>Omniperf</p> </li> </ul> </li> </ul> </li> <li> <p>HDF5 generic documentation</p> </li> </ul>"},{"location":"intro-evolving/A02_Documentation/#man-pages","title":"Man pages","text":"<p>A selection of man pages explicitly mentioned during the course:</p> <ul> <li> <p>Compilers</p> PrgEnv C C++ Fortran PrgEnv-cray <code>man craycc</code> <code>man crayCC</code> <code>man crayftn</code> PrgEnv-gnu <code>man gcc</code> <code>man g++</code> <code>man gfortran</code> PrgEnv-aocc/PrgEnv-amd - - - Compiler wrappers <code>man cc</code> <code>man CC</code> <code>man ftn</code> </li> <li> <p>Web-based versions of the compiler wrapper manual pages (the version on the system is currently hijacked by     the GNU manual pages):</p> <ul> <li> <p><code>man cc</code></p> </li> <li> <p><code>man CC</code></p> </li> <li> <p><code>man ftn</code> </p> </li> </ul> </li> <li> <p>OpenMP in CCE</p> <ul> <li><code>man intro_openmp</code></li> </ul> </li> <li> <p>OpenACC in CCE</p> <ul> <li><code>man intro_openacc</code></li> </ul> </li> <li> <p>MPI:</p> <ul> <li> <p>MPI itself: <code>man intro_mpi</code> or <code>man mpi</code></p> </li> <li> <p>libfabric: <code>man fabric</code></p> </li> <li> <p>CXI: `man fi_cxi'</p> </li> </ul> </li> <li> <p>LibSci</p> <ul> <li> <p><code>man intro_libsci</code> and <code>man intro_libsci_acc</code></p> </li> <li> <p><code>man intro_blas1</code>,     <code>man intro_blas2</code>,     <code>man intro_blas3</code>,     <code>man intro_cblas</code></p> </li> <li> <p><code>man intro_lapack</code></p> </li> <li> <p><code>man intro_scalapack</code> and <code>man intro_blacs</code></p> </li> <li> <p><code>man intro_irt</code></p> </li> <li> <p><code>man intro_fftw3</code></p> </li> </ul> </li> <li> <p>DSMML - Distributed Symmetric Memory Management Library </p> <ul> <li><code>man intro_dsmml</code></li> </ul> </li> <li> <p>Slurm manual pages are also all on the web      and are easily found by Google, but are usually those for the latest version.</p> <ul> <li> <p><code>man sbatch</code></p> </li> <li> <p><code>man srun</code></p> </li> <li> <p><code>man salloc</code></p> </li> <li> <p><code>man squeue</code></p> </li> <li> <p><code>man scancel</code></p> </li> <li> <p><code>man sinfo</code></p> </li> <li> <p><code>man sstat</code></p> </li> <li> <p><code>man sacct</code></p> </li> <li> <p><code>man scontrol</code></p> </li> </ul> </li> </ul>"},{"location":"intro-evolving/A02_Documentation/#via-the-module-system","title":"Via the module system","text":"<p>Most HPE Cray PE modules contain links to further documentation. Try <code>module help cce</code> etc.</p>"},{"location":"intro-evolving/A02_Documentation/#from-the-commands-themselves","title":"From the commands themselves","text":"PrgEnv C C++ Fortran PrgEnv-cray <code>craycc --help</code><code>craycc --craype-help</code> <code>crayCC --help</code><code>crayCC --craype-help</code> <code>crayftn --help</code><code>crayftn --craype-help</code> PrgEnv-gnu <code>gcc --help</code> <code>g++ --help</code> <code>gfortran --help</code> PrgEnv-aocc <code>clang --help</code> <code>clang++ --help</code> <code>flang --help</code> PrgEnv-amd <code>amdclang --help</code> <code>amdclang++ --help</code> <code>amdflang --help</code> Compiler wrappers <code>cc --craype-help</code><code>cc --help</code> <code>CC --craype-help</code><code>CC --help</code> <code>ftn --craype-help</code><code>ftn --help</code> <p>For the PrgEnv-gnu compiler, the <code>--help</code> option only shows a little bit of help information, but mentions further options to get help about specific topics.</p> <p>Further commands that provide extensive help on the command line:</p> <ul> <li><code>rocm-smi --help</code>, even on the login nodes.</li> </ul>"},{"location":"intro-evolving/A02_Documentation/#documentation-of-other-cray-ex-systems","title":"Documentation of other Cray EX systems","text":"<p>Note that these systems may be configured differently, and this especially applies to the scheduler. So not all documentations of those systems applies to LUMI. Yet these web sites do contain a lot of useful information.</p> <ul> <li> <p>Archer2 documentation.      Archer2 is the national supercomputer of the UK, operated by EPCC. It is an AMD CPU-only cluster.     Two important differences with LUMI are that (a) the cluster uses AMD Rome CPUs with groups of 4 instead     of 8 cores sharing L3 cache and (b) the cluster uses Slingshot 10 instead of Slinshot 11 which has its     own bugs and workarounds.</p> <p>It includes a page on cray-python referred to during the course.</p> </li> <li> <p>ORNL Frontier User Guide and      ORNL Crusher Qucik-Start Guide.     Frontier is the first USA exascale cluster and is built up of nodes that are very similar to the     LUMI-G nodes (same CPA and GPUs but a different storage configuration) while Crusher is the     192-node early access system for Frontier. One important difference is the configuration of     the scheduler which has 1 core reserved in each CCD to have a more regular structure than LUMI.</p> </li> <li> <p>KTH Dardel documentation. Dardel is the Swedish \"baby-LUMI\" system.     Its CPU nodes use the AMD Rome CPU instead of AMD Milan, but its GPU nodes are the same as in LUMI.</p> </li> <li> <p>Setonix User Guide.     Setonix is a Cray EX system at Pawsey Supercomputing Centre in Australia. The CPU and GPU compute     nodes are the same as on LUMI.</p> </li> </ul>"},{"location":"intro-evolving/schedule/","title":"Schedule (tentative)","text":"09:00 CEST\u00a0\u00a0                       Welcome and introduction             Presenter: Kurt Lust              09:10 CEST                       LUMI Architecture             Presenter: Kurt Lust              09:40 CEST                       HPE Cray Programming Environment             Presenter: Kurt Lust              10:10 CEST                       Modules on LUMI             Presenter: Kurt Lust              10:45 CEST          Break              11:00 CEST                       LUMI Software Stacks             Presenter: Kurt Lust              11:45 CEST                       Hands-on                       12:15 CEST          Lunch break              13:15 CEST                       Running jobs on LUMI             Presenter: TBA              15:15 CEST             16:15 EEST              Hands-on                       15:30 CEST          Break              15:40 CEST                       Introduction to Lustre and Best Practices             Presenter: TBA              15:50 CEST                       LUMI User Support             Presenter: TBA              16:15 CEST          General Q&amp;A              16:30 CEST          Course end"}]}